# Comparing `tmp/azureml_mlflow-1.8.0-py3-none-any.whl.zip` & `tmp/azureml_mlflow-1.9.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,22 +1,26 @@
-Zip file size: 24040 bytes, number of entries: 20
--rw-rw-rw-  2.0 fat      251 b- defN 20-Jun-22 17:21 azureml/__init__.py
--rw-rw-rw-  2.0 fat    15026 b- defN 20-Jun-22 17:21 azureml/mlflow/__init__.py
--rw-rw-rw-  2.0 fat       19 b- defN 20-Jun-22 17:23 azureml/mlflow/_version.py
--rw-rw-rw-  2.0 fat      183 b- defN 20-Jun-22 17:21 azureml/mlflow/_internal/__init__.py
--rw-rw-rw-  2.0 fat    12673 b- defN 20-Jun-22 17:21 azureml/mlflow/_internal/artifact_repo.py
--rw-rw-rw-  2.0 fat     1408 b- defN 20-Jun-22 17:21 azureml/mlflow/_internal/authentication.py
--rw-rw-rw-  2.0 fat     4232 b- defN 20-Jun-22 17:21 azureml/mlflow/_internal/azureml_reststore.py
--rw-rw-rw-  2.0 fat      892 b- defN 20-Jun-22 17:21 azureml/mlflow/_internal/hybrid_service_context.py
--rw-rw-rw-  2.0 fat     1637 b- defN 20-Jun-22 17:21 azureml/mlflow/_internal/model_registry.py
--rw-rw-rw-  2.0 fat     8289 b- defN 20-Jun-22 17:21 azureml/mlflow/_internal/run_artifacts_extension_client.py
--rw-rw-rw-  2.0 fat    14031 b- defN 20-Jun-22 17:21 azureml/mlflow/_internal/store.py
--rw-rw-rw-  2.0 fat     9247 b- defN 20-Jun-22 17:21 azureml/mlflow/_internal/utils.py
--rw-rw-rw-  2.0 fat       71 b- defN 20-Jun-22 17:23 azureml_mlflow-1.8.0.dist-info/DESCRIPTION.rst
--rw-rw-rw-  2.0 fat     1021 b- defN 20-Jun-22 17:23 azureml_mlflow-1.8.0.dist-info/LICENSE.txt
--rw-rw-rw-  2.0 fat      352 b- defN 20-Jun-22 17:23 azureml_mlflow-1.8.0.dist-info/entry_points.txt
--rw-rw-rw-  2.0 fat     1481 b- defN 20-Jun-22 17:23 azureml_mlflow-1.8.0.dist-info/metadata.json
--rw-rw-rw-  2.0 fat        8 b- defN 20-Jun-22 17:23 azureml_mlflow-1.8.0.dist-info/top_level.txt
--rw-rw-rw-  2.0 fat       97 b- defN 20-Jun-22 17:23 azureml_mlflow-1.8.0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat     1005 b- defN 20-Jun-22 17:23 azureml_mlflow-1.8.0.dist-info/METADATA
--rw-rw-rw-  2.0 fat     1882 b- defN 20-Jun-22 17:23 azureml_mlflow-1.8.0.dist-info/RECORD
-20 files, 73805 bytes uncompressed, 20926 bytes compressed:  71.6%
+Zip file size: 27939 bytes, number of entries: 24
+-rw-rw-rw-  2.0 fat      251 b- defN 20-Jul-06 20:24 azureml/__init__.py
+-rw-rw-rw-  2.0 fat     9392 b- defN 20-Jul-06 20:24 azureml/mlflow/__init__.py
+-rw-rw-rw-  2.0 fat       19 b- defN 20-Jul-06 20:27 azureml/mlflow/_version.py
+-rw-rw-rw-  2.0 fat     2993 b- defN 20-Jul-06 20:24 azureml/mlflow/entry_point_loaders.py
+-rw-rw-rw-  2.0 fat      183 b- defN 20-Jul-06 20:24 azureml/mlflow/_internal/__init__.py
+-rw-rw-rw-  2.0 fat    10817 b- defN 20-Jul-06 20:24 azureml/mlflow/_internal/artifact_repo.py
+-rw-rw-rw-  2.0 fat      883 b- defN 20-Jul-06 20:24 azureml/mlflow/_internal/artifact_repository_client.py
+-rw-rw-rw-  2.0 fat     1408 b- defN 20-Jul-06 20:24 azureml/mlflow/_internal/authentication.py
+-rw-rw-rw-  2.0 fat     4147 b- defN 20-Jul-06 20:24 azureml/mlflow/_internal/azureml_reststore.py
+-rw-rw-rw-  2.0 fat      892 b- defN 20-Jul-06 20:24 azureml/mlflow/_internal/hybrid_service_context.py
+-rw-rw-rw-  2.0 fat     2445 b- defN 20-Jul-06 20:24 azureml/mlflow/_internal/local_artifact_repository_client.py
+-rw-rw-rw-  2.0 fat     2465 b- defN 20-Jul-06 20:24 azureml/mlflow/_internal/model_registry.py
+-rw-rw-rw-  2.0 fat     8549 b- defN 20-Jul-06 20:24 azureml/mlflow/_internal/run_artifact_repository_client.py
+-rw-rw-rw-  2.0 fat    13960 b- defN 20-Jul-06 20:24 azureml/mlflow/_internal/store.py
+-rw-rw-rw-  2.0 fat     4364 b- defN 20-Jul-06 20:24 azureml/mlflow/_internal/store_loader.py
+-rw-rw-rw-  2.0 fat    14018 b- defN 20-Jul-06 20:24 azureml/mlflow/_internal/utils.py
+-rw-rw-rw-  2.0 fat      899 b- defN 20-Jul-06 20:27 azureml_mlflow-1.9.0.dist-info/DESCRIPTION.rst
+-rw-rw-rw-  2.0 fat     1021 b- defN 20-Jul-06 20:27 azureml_mlflow-1.9.0.dist-info/LICENSE.txt
+-rw-rw-rw-  2.0 fat      452 b- defN 20-Jul-06 20:27 azureml_mlflow-1.9.0.dist-info/entry_points.txt
+-rw-rw-rw-  2.0 fat     1581 b- defN 20-Jul-06 20:27 azureml_mlflow-1.9.0.dist-info/metadata.json
+-rw-rw-rw-  2.0 fat        8 b- defN 20-Jul-06 20:27 azureml_mlflow-1.9.0.dist-info/top_level.txt
+-rw-rw-rw-  2.0 fat       97 b- defN 20-Jul-06 20:27 azureml_mlflow-1.9.0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat     1833 b- defN 20-Jul-06 20:27 azureml_mlflow-1.9.0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat     2305 b- defN 20-Jul-06 20:27 azureml_mlflow-1.9.0.dist-info/RECORD
+24 files, 84982 bytes uncompressed, 24139 bytes compressed:  71.6%
```

## zipnote {}

```diff
@@ -3,59 +3,71 @@
 
 Filename: azureml/mlflow/__init__.py
 Comment: 
 
 Filename: azureml/mlflow/_version.py
 Comment: 
 
+Filename: azureml/mlflow/entry_point_loaders.py
+Comment: 
+
 Filename: azureml/mlflow/_internal/__init__.py
 Comment: 
 
 Filename: azureml/mlflow/_internal/artifact_repo.py
 Comment: 
 
+Filename: azureml/mlflow/_internal/artifact_repository_client.py
+Comment: 
+
 Filename: azureml/mlflow/_internal/authentication.py
 Comment: 
 
 Filename: azureml/mlflow/_internal/azureml_reststore.py
 Comment: 
 
 Filename: azureml/mlflow/_internal/hybrid_service_context.py
 Comment: 
 
+Filename: azureml/mlflow/_internal/local_artifact_repository_client.py
+Comment: 
+
 Filename: azureml/mlflow/_internal/model_registry.py
 Comment: 
 
-Filename: azureml/mlflow/_internal/run_artifacts_extension_client.py
+Filename: azureml/mlflow/_internal/run_artifact_repository_client.py
 Comment: 
 
 Filename: azureml/mlflow/_internal/store.py
 Comment: 
 
+Filename: azureml/mlflow/_internal/store_loader.py
+Comment: 
+
 Filename: azureml/mlflow/_internal/utils.py
 Comment: 
 
-Filename: azureml_mlflow-1.8.0.dist-info/DESCRIPTION.rst
+Filename: azureml_mlflow-1.9.0.dist-info/DESCRIPTION.rst
 Comment: 
 
-Filename: azureml_mlflow-1.8.0.dist-info/LICENSE.txt
+Filename: azureml_mlflow-1.9.0.dist-info/LICENSE.txt
 Comment: 
 
-Filename: azureml_mlflow-1.8.0.dist-info/entry_points.txt
+Filename: azureml_mlflow-1.9.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: azureml_mlflow-1.8.0.dist-info/metadata.json
+Filename: azureml_mlflow-1.9.0.dist-info/metadata.json
 Comment: 
 
-Filename: azureml_mlflow-1.8.0.dist-info/top_level.txt
+Filename: azureml_mlflow-1.9.0.dist-info/top_level.txt
 Comment: 
 
-Filename: azureml_mlflow-1.8.0.dist-info/WHEEL
+Filename: azureml_mlflow-1.9.0.dist-info/WHEEL
 Comment: 
 
-Filename: azureml_mlflow-1.8.0.dist-info/METADATA
+Filename: azureml_mlflow-1.9.0.dist-info/METADATA
 Comment: 
 
-Filename: azureml_mlflow-1.8.0.dist-info/RECORD
+Filename: azureml_mlflow-1.9.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## azureml/mlflow/__init__.py

```diff
@@ -22,198 +22,74 @@
 """
 
 import os
 import logging
 import re
 from six.moves.urllib import parse
 
-import mlflow
-from mlflow.exceptions import MlflowException
 
 import azureml
-from azureml.exceptions import RunEnvironmentException
 from azureml.core import Workspace, Experiment, Run
 from azureml.core.authentication import ArmTokenAuthentication
-from ._internal import store
-from ._internal.store import _VERSION_WARNING
+
 try:
     from azureml.mlflow._version import VERSION
 except ImportError:
     VERSION = "0.0.0+dev"
 
-from ._internal.model_registry import AzureMLflowModelRegistry
-from ._internal.utils import (get_service_context_from_tracking_url,
-                              get_aml_experiment_name, _IS_REMOTE,
-                              _TRUE_QUERY_VALUE, _TOKEN_PREFIX,
-                              _AUTH_HEAD, _AUTH_TYPE, _CLOUD_TYPE)
-
-_SUBSCRIPTIONS_PREFIX = "/subscriptions/"
-
-logger = logging.getLogger(__name__)
-
-__version__ = VERSION
-
-__all__ = ["get_portal_url", "register_model", "azureml_store_builder", "azureml_artifacts_builder"]
-
-
-class _AzureMLStoreLoader(object):
-    """
-    _AzureMLStoreLoader loads an AzureMLStore from 3 supported scenarios.
-
-    1, new tracking_uri without is remote set to True. A store is created from the uri
-    2, is remote set to true in a workspace tracking uri. Loads the
-       store information from the current Run context and sets the experiment and ActiveRun.
-    3, a cached result of option 1 or 2, this cache is relative to the netloc + path of the tracking_uri
-    """
-
-    _azure_uri_to_tracking_store = {}
-    _azure_uri_to_model_registry_store = {}
-
-    @classmethod
-    def _load_azureml_tracking_store(cls, store_uri):
-        return cls._load_azureml_store(store_uri, store.AzureMLRestStore, cls._azure_uri_to_tracking_store)
-
-    @classmethod
-    def _load_azureml_model_registry_store(cls, store_uri):
-        return cls._load_azureml_store(store_uri, AzureMLflowModelRegistry, cls._azure_uri_to_model_registry_store)
-
-    @classmethod
-    def _load_azureml_store(cls, store_uri, store_object, cache_dict):
-        parsed_url = parse.urlparse(store_uri)
-        queries = dict(parse.parse_qsl(parsed_url.query))
-
-        # cache the Azure workspace object
-        cache_key = store_uri.split("?")[0]
-        if cache_key in cache_dict:
-            logger.debug("Loading an existing {} from the _AzureMLStoreLoader cache".format(store_object.__name__))
-            return cache_dict[cache_key]
-        elif _IS_REMOTE in queries and queries[_IS_REMOTE] == _TRUE_QUERY_VALUE:
-            try:
-                run = Run.get_context()
-            except RunEnvironmentException:
-                raise MlflowException(
-                    "AzureMlflow tracking URI was set to remote but there was a failure in loading the run.")
-            else:
-                logger.debug("Creating a new {} for a remote run".format(store_object.__name__))
-                amlflow_store = store_object(service_context=run.experiment.workspace.service_context)
-
-                cache_dict[cache_key] = amlflow_store
-                logger.debug("Setting Mlflow experiment with {}".format(run.experiment.name))
-                mlflow.set_experiment(run.experiment.name)
-        else:
-            service_context = get_service_context_from_tracking_url(parsed_url)
-            logger.debug("Creating a new {} for a local run".format(store_object.__name__))
-            cache_dict[cache_key] = store_object(service_context=service_context)
-
-        return cache_dict[cache_key]
-
-
-def azureml_store_builder(store_uri, artifact_uri=None):
-    """Create or return a store to read and record metrics and artifacts in Azure via MLflow.
-
-    :param store_uri: A URI to the store.
-    :type store_uri: str
-    :param artifact_uri: A URI where artifacts are stored.
-    :type artifact_uri: str
-    """
-    if not store_uri:
-        raise MlflowException('Store URI provided to azureml_tracking_store_build cannot be None or empty.')
 
-    return _AzureMLStoreLoader._load_azureml_tracking_store(store_uri)
+from ._internal.store import AzureMLRestStore
 
 
-def azureml_model_registry_builder(store_uri):
-    """Create or return a registry for models in Azure via MLflow.
-
-    :param store_uri: A URI to the registry.
-    :type store_uri: str
-    """
-    if not store_uri:
-        raise MlflowException('Store URI provided to azureml_model_registry_builder cannot be None or empty.')
-
-    return _AzureMLStoreLoader._load_azureml_model_registry_store(store_uri)
-
-
-def adb_azureml_store_builder(store_uri=None, artifact_uri=None):
-    """Create or return a store to read and record metrics and artifacts in Azure via MLflow when using Databricks.
-
-    :param store_uri: A URI to the store.
-    :type store_uri: str
-    :param artifact_uri: A URI where artifacts are stored.
-    :type artifact_uri: str
-    """
-    try:
-        from mlflow import get_tracking_uri
-    except ImportError:
-        logger.warning(_VERSION_WARNING.format("mlflow.get_tracking_uri"))
-        from mlflow.tracking.utils import get_tracking_uri
-    tracking_uri = store_uri if store_uri is not None else get_tracking_uri()
-    return store.AdbAzuremlRestStore(tracking_uri)
-
-
-def azureml_artifacts_builder(artifact_uri=None):
-    """Create an artifact repository for AzureMLflow.
-
-    :param artifact_uri: A URI where artifacts are stored.
-    :type artifact_uri: str
-    """
-    from ._internal.artifact_repo import AzureMLflowArtifactRepository
-    return AzureMLflowArtifactRepository(artifact_uri)
+from ._internal.utils import get_aml_experiment_name, VERSION_WARNING
 
+_SUBSCRIPTIONS_PREFIX = "/subscriptions/"
 
-def adb_azureml_artifacts_builder(artifact_uri=None):
-    """Create an artifact repository for AzureMLflow when using Databricks.
+logger = logging.getLogger(__name__)
 
-    :param artifact_uri: A URI where artifacts are stored.
-    :type artifact_uri: str
-    """
-    from ._internal.artifact_repo import AdbAzuremlArtifactRepository
-    return AdbAzuremlArtifactRepository(artifact_uri)
+__version__ = VERSION
 
 
-def get_mlflow_tracking_uri(workspace, with_auth=True):
+def get_mlflow_tracking_uri(workspace):
     """
     Retrieve the tracking URI from Workspace for use in AzureMLflow.
 
     Return a URI identifying the workspace, with optionally the auth header
     embedded as a query string within the URI. The authentication header does not include the "Bearer " prefix.
 
     :return: Returns the URI pointing to this workspace, with the auth query paramter if with_auth is True.
     :rtype: str
     """
-    queries = []
-    if with_auth:
-        auth = workspace._auth_object
-        logger.debug("Creating a tracking uri with an {} auth token".format(auth.__class__.__name__))
-        header = auth.get_authentication_header()
-        token = header["Authorization"][len(_TOKEN_PREFIX):]
-        queries.append(_AUTH_TYPE + "=" + auth.__class__.__name__)
-        queries.append(_AUTH_HEAD + "=" + token)
-        queries.append(_CLOUD_TYPE + "=" + auth._get_cloud_type().name)
-
+    from ._internal.store_loader import _AzureMLStoreLoader
     service_location = os.environ.get("AZUREML_DEV_URL_RUN_HISTORY",
                                       parse.urlparse(workspace.service_context._get_run_history_url()).netloc)
     workspace_scope = workspace.service_context._get_workspace_scope()
     logger.debug("Creating a tracking uri in {} for workspace {}".format(service_location, workspace_scope))
 
-    return "azureml://{}/mlflow/v1.0{}{}".format(
+    store_uri = "azureml://{}/mlflow/v1.0{}{}".format(
         service_location,
         workspace_scope,
-        "?" + "&".join(queries) if queries else "")
+        "?")
+
+    _AzureMLStoreLoader._add_azureml_tracking_store(store_uri, workspace)
+    _AzureMLStoreLoader._add_azureml_model_registry_store(store_uri, workspace)
+
+    return store_uri
 
 
 def _setup_remote(azureml_run):
+    import mlflow
     logger.debug("Setting up a Remote MLflow run")
     tracking_uri = azureml_run.experiment.workspace.get_mlflow_tracking_uri() + "&is-remote=True"
     mlflow.set_tracking_uri(tracking_uri)
     try:
         from mlflow.tracking._tracking_service.utils import _TRACKING_URI_ENV_VAR
     except ImportError:
-        logger.warning(_VERSION_WARNING.format("_TRACKING_URI_ENV_VAR from " +
-                                               "mlflow.tracking._tracking_service.utils"))
+        logger.warning(VERSION_WARNING.format("_TRACKING_URI_ENV_VAR from " +
+                                              "mlflow.tracking._tracking_service.utils"))
         from mlflow.tracking.utils import _TRACKING_URI_ENV_VAR
     from mlflow.tracking.fluent import _RUN_ID_ENV_VAR
     logger.debug("Setting MLflow tracking uri env var")
     os.environ[_TRACKING_URI_ENV_VAR] = tracking_uri
     logger.debug("Setting MLflow run id env var with {}".format(azureml_run.id))
     os.environ[_RUN_ID_ENV_VAR] = azureml_run.id
     logger.debug("Setting Mlflow experiment with {}".format(azureml_run.experiment.name))
@@ -242,45 +118,45 @@
     :rtype: str
     """
     if isinstance(run, Run):
         return run.get_portal_url()
     else:
         from mlflow.tracking.client import MlflowClient
         experiment_name = MlflowClient().get_experiment(run.info.experiment_id).name
-        run_id = run.info.run_uuid
+        run_id = run.info.run_id
         try:
             def_store = MlflowClient()._tracking_client.store
         except:
-            logger.warning(_VERSION_WARNING.format("MlflowClient()._tracking_client.store"))
+            logger.warning(VERSION_WARNING.format("MlflowClient()._tracking_client.store"))
             def_store = MlflowClient().store
-        aml_store = def_store if isinstance(def_store, store.AzureMLRestStore) else def_store.aml_store
+        aml_store = def_store if isinstance(def_store, AzureMLRestStore) else def_store.aml_store
         host = aml_store.get_host_creds().host
         netloc = "https://mlworkspace.azure.ai/portal"
         uri = "{}{}".format(_SUBSCRIPTIONS_PREFIX, host.split(_SUBSCRIPTIONS_PREFIX, 2)[1])
         experiment_name = get_aml_experiment_name(experiment_name)
         experiment_run_uri = "/experiments/{}/runs/{}".format(experiment_name, run_id)
         return netloc + uri + experiment_run_uri
 
 
 def _azureml_run_from_mlflow_run(mlflow_run):
     from mlflow.tracking.client import MlflowClient
     experiment_name = MlflowClient().get_experiment(mlflow_run.info.experiment_id).name
     try:
         def_store = MlflowClient()._tracking_client.store
     except:
-        logger.warning(_VERSION_WARNING.format("MlflowClient()._tracking_client.store"))
+        logger.warning(VERSION_WARNING.format("MlflowClient()._tracking_client.store"))
         def_store = MlflowClient().store
-    aml_store = def_store if isinstance(def_store, store.AzureMLRestStore) else def_store.aml_store
+    aml_store = def_store if isinstance(def_store, AzureMLRestStore) else def_store.aml_store
     host = aml_store.get_host_creds().host
     auth_token = aml_store.get_host_creds().token
 
     cluster_url = host.split(_SUBSCRIPTIONS_PREFIX, 2)[0].split("/history/")[0]
     scope = "{}{}".format(_SUBSCRIPTIONS_PREFIX, host.split(_SUBSCRIPTIONS_PREFIX, 2)[1])
     auth = ArmTokenAuthentication(auth_token)
-    run_id = mlflow_run.info.run_uuid
+    run_id = mlflow_run.info.run_id
 
     subscription_id = re.search(r'/subscriptions/([^/]+)', scope).group(1)
     resource_group_name = re.search(r'/resourceGroups/([^/]+)', scope).group(1)
     workspace_name = re.search(r'/workspaces/([^/]+)', scope).group(1)
     workspace = Workspace(subscription_id,
                           resource_group_name,
                           workspace_name,
@@ -334,7 +210,9 @@
 
 
 if not hasattr(azureml.core.workspace.Workspace, "get_mlflow_tracking_uri"):
     # To replicate monkeypatched function for older SDKs
     logger.warning("Old Workspace definition detected, adding patched Workspace.get_mlflow_tracking_uri. "
                    "Please upgrade azureml-core.")
     azureml.core.workspace.Workspace.get_mlflow_tracking_uri = get_mlflow_tracking_uri
+
+__all__ = ["get_portal_url", "register_model"]
```

## azureml/mlflow/_version.py

```diff
@@ -1 +1 @@
-VERSION = "1.8.0"
+VERSION = "1.9.0"
```

## azureml/mlflow/_internal/artifact_repo.py

```diff
@@ -9,27 +9,25 @@
 import re
 
 from functools import wraps
 
 import mlflow
 from mlflow.entities import FileInfo
 
-from .utils import execute_func
-from .store import AzureMLRestStore, AdbAzuremlRestStore, _VERSION_WARNING
-from .utils import (get_service_context_from_artifact_url, artifact_uri_decomp,
-                    get_aml_experiment_name, _EXP_NAME, _RUN_ID, _ARTIFACT_PATH)
-from .run_artifacts_extension_client import RunArtifactsExtensionClient
-from six.moves.urllib import parse
+
+from .utils import execute_func, VERSION_WARNING
+from .utils import get_aml_experiment_name, get_artifact_repository_client
+
 
 logger = logging.getLogger(__name__)
 
 try:
     from mlflow.store.artifact.artifact_repo import ArtifactRepository
 except ImportError:
-    logger.warning(_VERSION_WARNING.format("ArtifactRepository from mlflow.store.artifact.artifact_repo"))
+    logger.warning(VERSION_WARNING.format("ArtifactRepository from mlflow.store.artifact.artifact_repo"))
     from mlflow.store.artifact_repo import ArtifactRepository
 
 
 class AzureMLflowArtifactRepository(ArtifactRepository):
     """Define how to upload (log) and download potentially large artifacts from different storage backends."""
 
     def __init__(self, artifact_uri):
@@ -39,48 +37,22 @@
         This object is used with any of the functions called from mlflow or from
         the client which have to do with artifacts.
 
         :param artifact_uri: Azure URI. This URI is never used within the object,
             but is included here, as it is included in ArtifactRepository as well.
         :type artifact_uri: str
         """
-        logger.debug("Initializing the AzureMLflowArtifactRepository")
-        parsed_artifacts_url = parse.urlparse(artifact_uri)
-        parsed_artifacts_path = artifact_uri_decomp(artifact_uri)
-        experiment_name = parsed_artifacts_path[_EXP_NAME]
-        logger.debug("AzureMLflowArtifactRepository for experiment {}".format(experiment_name))
-        self._run_id = parsed_artifacts_path[_RUN_ID]
-        logger.debug("AzureMLflowArtifactRepository for run id {}".format(self._run_id))
-        self._path = parsed_artifacts_path.get(_ARTIFACT_PATH)
-        logger.debug("AzureMLflowArtifactRepository for path {}".format(self._path))
-
-        service_context = self._get_service_context(parsed_artifacts_url)
-        self.run_artifacts = RunArtifactsExtensionClient(service_context, experiment_name)
-
-    def _get_service_context(self, parsed_artifacts_url):
-        from mlflow.tracking.client import MlflowClient
-        try:
-            store = MlflowClient()._tracking_client.store
-        except:
-            logger.warning(_VERSION_WARNING.format("MlflowClient()._tracking_client.store"))
-            store = MlflowClient().store
-        if isinstance(store, AzureMLRestStore):
-            logger.debug("Using the service context from the {} store".format(store.__class__.__name__))
-            return store.service_context
-        elif isinstance(store, AdbAzuremlRestStore):
-            return store.aml_store.service_context
-        else:
-            return get_service_context_from_artifact_url(parsed_artifacts_url)
+        self.artifacts = get_artifact_repository_client(artifact_uri)
 
     def _get_full_artifact_path(self, artifact_path=None):
         path_parts = []
-        if self._path is None and artifact_path is None:
+        if self.artifacts.path is None and artifact_path is None:
             return None
-        if self._path:
-            path_parts.append(self._path)
+        if self.artifacts.path:
+            path_parts.append(self.artifacts.path)
         if artifact_path:
             path_parts.append(artifact_path)
         return "/".join(path_parts)
 
     def log_artifact(self, local_file, artifact_path=None):
         """
         Log a local file as an artifact.
@@ -91,15 +63,15 @@
         :param local_file: Absolute or relative path to the artifact locally.
         :type local_file: str
         :param artifact_path: Path to a file in the AzureML run's outputs, to where the artifact is uploaded.
         :type artifact_path: str
         """
         artifact_path = self._get_full_artifact_path(artifact_path)
         dest_path = self._normalize_slashes(self._build_dest_path(local_file, artifact_path))
-        self.run_artifacts.upload_artifact(local_file, self._run_id, dest_path)
+        self.artifacts.upload_artifact(local_file, dest_path)
 
     def log_artifacts(self, local_dir, artifact_path=None):
         """
         Log the files in the specified local directory as artifacts.
 
         Optionally takes an ``artifact_path``, which specifies the directory of
         the AzureML run under which to place the artifacts in the local directory.
@@ -111,48 +83,47 @@
         """
         artifact_path = self._get_full_artifact_path(artifact_path)
         dest_path = self._normalize_slashes(self._build_dest_path(local_dir, artifact_path))
         local_dir = self._normalize_slash_end(local_dir)
         dest_path = self._normalize_slash_end(dest_path)
 
         if artifact_path is not None and artifact_path not in [".", "./"]:
-            self.run_artifacts.upload_dir(local_dir,
-                                          self._run_id,
-                                          lambda fpath: dest_path + fpath[len(local_dir):])
+            self.artifacts.upload_dir(local_dir,
+                                      lambda fpath: dest_path + fpath[len(local_dir):])
         else:
             files = [os.path.join(local_dir, file_name) for file_name in os.listdir(local_dir)
                      if not os.path.isdir(os.path.join(local_dir, file_name))]
             dirs = [os.path.join(local_dir, dir_name) for dir_name in os.listdir(local_dir)
                     if os.path.isdir(os.path.join(local_dir, dir_name))]
 
-            self.run_artifacts.upload_files(files, self._run_id, [os.path.basename(file_name) for file_name in files])
+            self.artifacts.upload_files(files, [os.path.basename(file_name) for file_name in files])
             for dir_name in dirs:
-                self.run_artifacts.upload_dir(dir_name, self._run_id, skip_first_level=True)
+                self.artifacts.upload_dir(dir_name, skip_first_level=True)
 
     def list_artifacts(self, path):
         """
-        Return all the artifacts for this run_uuid directly under path.
+        Return all the artifacts for this run_id directly under path.
 
         If path is a file, returns an empty list. Will error if path is neither a
         file nor directory. Note that list_artifacts will not return valid
         artifact sizes from Azure.
 
         :param path: Relative source path that contain desired artifacts
         :type path: str
         :return: List of artifacts as FileInfo listed directly under path.
         """
         # get and filter by paths
 
-        if path and self._path and not path.startswith(self._path):
+        if path and self.artifacts.path and not path.startswith(self.artifacts.path):
             path = self._get_full_artifact_path(path)  # Adds prefix if called directly and it is not already set
 
         path_tokens = path.split("/") if path else []
         path_depth = len(path_tokens)
         artifacts = []
-        for file_path in self.run_artifacts.get_file_paths(self._run_id):
+        for file_path in self.artifacts.get_file_paths():
             if path is None or file_path[:len(path)] == path and len(file_path) > len(path):
                 artifacts.append(file_path)
 
         file_infos = []
         for artifact in artifacts:
             artifact_tokens = artifact.split("/")
             if len(artifact_tokens) == path_depth + 1:  # is a file
@@ -182,15 +153,15 @@
         :param remote_file_path: Source path to the remote file, relative to the
         root directory of the artifact repository.
         :type remote_file_path: str
         :param local_path: The path to which to save the downloaded file.
         :type local_path: str
         """
         # kwargs handling was added to protect against a newly introduced kwarg causing a regression
-        self.run_artifacts.download_artifact(self._run_id, remote_file_path, local_path)
+        self.artifacts.download_artifact(remote_file_path, local_path)
 
     @staticmethod
     def _build_dest_path(local_path, artifact_path):
         return artifact_path if artifact_path else os.path.basename(local_path)
 
     @staticmethod
     def _normalize_slashes(path):
@@ -212,16 +183,16 @@
     def get_artifact_repos(self):
         return [self.adb_artifact_repo, self.amlflow_artifact_repo]
 
     def get_dbfs_artifact_repo(self, artifact_uri):
         try:
             from mlflow.store.artifact.dbfs_artifact_repo import DbfsRestArtifactRepository
         except ImportError:
-            logger.warning(_VERSION_WARNING.format("DbfsRestArtifactRepository from " +
-                                                   "mlflow.store.artifact.dbfs_artifact_repo"))
+            logger.warning(VERSION_WARNING.format("DbfsRestArtifactRepository from " +
+                                                  "mlflow.store.artifact.dbfs_artifact_repo"))
             from mlflow.store.dbfs_artifact_repo import DbfsRestArtifactRepository
         # dbfs artifact_uri has to have dbfs as prefix
         # by default it is dbfs:/databricks/mlflow/<exp_id>/<run_id>/artifacts
         artifact_uri = artifact_uri.replace("adbazureml", "dbfs", 1)
         logger.info("DBFS artifact uri is {}".format(artifact_uri))
         return DbfsRestArtifactRepository(artifact_uri)
```

## azureml/mlflow/_internal/azureml_reststore.py

```diff
@@ -11,15 +11,14 @@
 from mlflow.exceptions import RestException
 
 from azureml.core.authentication import AzureMLTokenAuthentication
 from azureml._restclient.clientbase import DEFAULT_BACKOFF, DEFAULT_RETRIES
 from azureml._restclient.run_client import RunClient
 from azureml._restclient.workspace_client import WorkspaceClient
 
-_VERSION_WARNING = "Could not import {}. Please upgrade to Mlflow 1.4.0 or higher."
 
 logger = logging.getLogger(__name__)
 
 
 class AzureMLAbstractRestStore(object):
     """
     Client for a remote rest server accessed via REST API calls.
```

## azureml/mlflow/_internal/model_registry.py

```diff
@@ -1,20 +1,21 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 
 """**AzureMLflowModelRegistry** provides a class to manage MLFlow models in Azure."""
 
 import logging
+import os
+from azureml.core import Model
 from functools import wraps
 
 from mlflow.store.model_registry.rest_store import RestStore
 from .azureml_reststore import AzureMLAbstractRestStore
 
-_VERSION_WARNING = "Could not import {}. Please upgrade to Mlflow 1.4.0 or higher."
 
 logger = logging.getLogger(__name__)
 
 
 class AzureMLflowModelRegistry(AzureMLAbstractRestStore, RestStore):
     """
     Client for a remote model registry accessed via REST API calls.
@@ -34,7 +35,21 @@
         AzureMLAbstractRestStore.__init__(self, service_context, host_creds)
         RestStore.__init__(self, self.get_host_creds)
 
     @wraps(RestStore._call_endpoint)
     def _call_endpoint(self, *args, **kwargs):
         return super(AzureMLflowModelRegistry, self)._call_endpoint_with_retries(
             super(AzureMLflowModelRegistry, self)._call_endpoint, *args, **kwargs)
+
+    @wraps(RestStore.create_model_version)
+    def create_model_version(self, name, source, run_id=None):
+        if source.startswith('file://'):
+            if run_id:
+                raise ValueError('"run_id={}" provided when registering local file.'.format(run_id))
+            source = source.split('file://')[1]
+            if not os.path.isdir(source) or not os.path.exists(os.path.join(source, 'MLmodel')):
+                raise ValueError('Model source must be a directory containing an mlflow MLmodel, as is produced '
+                                 'by an mlflow save_model function.')
+            artifacts_path = Model._create_artifacts(self.service_context, source, name)
+            source = 'azureml://artifacts/{}'.format(artifacts_path)
+
+        return super(AzureMLflowModelRegistry, self).create_model_version(name, source, run_id)
```

## azureml/mlflow/_internal/store.py

```diff
@@ -17,29 +17,28 @@
 from mlflow.entities import ViewType
 
 from azureml.core.authentication import AzureMLTokenAuthentication
 from azureml._restclient.clientbase import DEFAULT_BACKOFF, DEFAULT_RETRIES
 from azureml._restclient.run_client import RunClient
 from azureml._restclient.workspace_client import WorkspaceClient
 from .utils import (execute_func, handle_exception, get_aml_experiment_name,
-                    tracking_uri_decomp, _SUB_ID, _RES_GRP, _WS_NAME)
+                    tracking_uri_decomp, _SUB_ID, _RES_GRP, _WS_NAME, VERSION_WARNING)
 
 from .authentication import DBTokenAuthentication
 from .hybrid_service_context import HybridServiceContext
 
-_VERSION_WARNING = "Could not import {}. Please upgrade to Mlflow 1.4.0 or higher."
 
 logger = logging.getLogger(__name__)
 
 try:
     from mlflow.store.tracking.rest_store import RestStore
     from mlflow.store.tracking import SEARCH_MAX_RESULTS_DEFAULT
     from mlflow.utils.uri import get_db_profile_from_uri
 except ImportError:
-    logger.warning(_VERSION_WARNING.format("from mlflow"))
+    logger.warning(VERSION_WARNING.format("from mlflow"))
     from mlflow.store.rest_store import RestStore
     from mlflow.store import SEARCH_MAX_RESULTS_DEFAULT
     from mlflow.tracking.utils import get_db_profile_from_uri
 
 PARAM_PREFIX = "azureml.param."
 
 _EXPERIMENT_NAME_ENV_VAR = "MLFLOW_EXPERIMENT_NAME"
@@ -167,25 +166,25 @@
         self.get_host_creds = self.get_host_credentials
         super(AdbAzuremlRestStore, self).__init__(self.get_host_creds)
 
     def get_db_store(self):
         try:
             tracking_uri = mlflow.get_tracking_uri()
         except ImportError:
-            logger.warning(_VERSION_WARNING.format("mlflow.get_tracking_uri"))
+            logger.warning(VERSION_WARNING.format("mlflow.get_tracking_uri"))
             tracking_uri = mlflow.tracking.get_tracking_uri()
         profile = get_db_profile_from_uri("databricks")
         logger.info("tracking uri: {} and profile: {}".format(tracking_uri, profile))
         return RestStore(lambda: get_databricks_host_creds(profile))
 
     def get_aml_store(self):
         try:
             tracking_uri = mlflow.get_tracking_uri()
         except ImportError:
-            logger.warning(_VERSION_WARNING.format("mlflow.get_tracking_uri"))
+            logger.warning(VERSION_WARNING.format("mlflow.get_tracking_uri"))
             tracking_uri = mlflow.tracking.get_tracking_uri()
         logger.info("tracking uri: {}".format(tracking_uri))
         parsed_url = parse.urlparse(tracking_uri)
         region = parsed_url.netloc
         parsed_path = tracking_uri_decomp(parsed_url.path)
         subscription_id = parsed_path[_SUB_ID]
         resource_group_name = parsed_path[_RES_GRP]
```

## azureml/mlflow/_internal/utils.py

```diff
@@ -7,27 +7,32 @@
 import logging
 import os
 import re
 
 from azureml.core.authentication import (ArmTokenAuthentication, AzureMLTokenAuthentication,
                                          InteractiveLoginAuthentication)
 from .authentication import DBTokenAuthentication
+from .run_artifact_repository_client import RunArtifactRepositoryClient
+from .local_artifact_repository_client import LocalArtifactRepositoryClient
 from azureml._restclient.service_context import ServiceContext
 
 from mlflow.exceptions import MlflowException
 
 from six.moves.urllib import parse
 
 _IS_REMOTE = "is-remote"
 _REGION = "region"
 _SUB_ID = "sub-id"
 _RES_GRP = "res-grp"
 _WS_NAME = "ws-name"
 _EXP_NAME = "experiment"
 _RUN_ID = "runid"
+_ORIGIN = "origin"
+_CONTAINER = "container"
+
 _AUTH_HEAD = "auth"
 _AUTH_TYPE = "auth-type"
 _CLOUD_TYPE = "cloud-type"
 _TRUE_QUERY_VALUE = "True"
 
 _TOKEN_PREFIX = "Bearer "
 _TOKEN_QUERY_NAME = "token"
@@ -39,14 +44,16 @@
 _ARTIFACT_URI_EXP_RUN_REGEX = r".*/([^/]+)/runs/([^/]+)(/artifacts.*)?"
 
 _WORKSPACE_INFO_REGEX = r".*/subscriptions/(.+)/resourceGroups/(.+)" \
     r"/providers/Microsoft.MachineLearningServices/workspaces/([^/]+)"
 
 _ARTIFACT_URI_REGEX = _WORKSPACE_INFO_REGEX + r"/experiments/([^/]+)/runs/([^/]+)(/artifacts.*)?"
 
+VERSION_WARNING = "Could not import {}. Please upgrade to Mlflow 1.4.0 or higher."
+
 
 def tracking_uri_decomp(tracking_uri):
     """
     Parse the tracking URI into a dictionary.
 
     The tracking URI contains the scope information for the workspace.
 
@@ -62,57 +69,70 @@
     pattern = re.compile(_WORKSPACE_INFO_REGEX)
     mo = pattern.match(parsed_url_path)
 
     ret = {}
     ret[_SUB_ID] = mo.group(1)
     ret[_RES_GRP] = mo.group(2)
     ret[_WS_NAME] = mo.group(3)
-    logger.info("Tracking uri {} has sub id {}, resource group {}, " +
-                "and workspace {}".format(tracking_uri, ret[_SUB_ID], ret[_RES_GRP], ret[_WS_NAME]))
+    logger.info("Tracking uri {} has sub id {}, resource group {}, and workspace {}".format(
+        tracking_uri, ret[_SUB_ID], ret[_RES_GRP], ret[_WS_NAME]))
 
     return ret
 
 
-def artifact_uri_decomp(tracking_uri):
+def get_run_info(parsed_url_path):
+    run_info_dict = {}
+    try:
+        mo = re.compile(_ARTIFACT_URI_REGEX).match(parsed_url_path)
+        run_info_dict[_SUB_ID] = mo.group(1)
+        run_info_dict[_RES_GRP] = mo.group(2)
+        run_info_dict[_WS_NAME] = mo.group(3)
+        run_info_dict[_EXP_NAME] = mo.group(4)
+        run_info_dict[_RUN_ID] = mo.group(5)
+        path_match = mo.group(7)
+    except Exception:
+        try:
+            mo = re.compile(_ARTIFACT_URI_EXP_RUN_REGEX).match(parsed_url_path)
+            run_info_dict[_EXP_NAME] = mo.group(1)
+            run_info_dict[_RUN_ID] = mo.group(2)
+            path_match = mo.group(3)
+        except Exception:
+            return
+
+    if path_match is not None and path_match != "/artifacts":
+        path = path_match[len("/artifacts"):]
+        run_info_dict[_ARTIFACT_PATH] = path if not path.startswith("/") else path[1:]
+    return run_info_dict
+
+
+def artifact_uri_decomp(artifact_uri):
     """
     Parse the artifact URI into a dictionary.
 
     The artifact URI contains the scope information for the workspace, the experiment and the run_id.
 
-    :param tracking_uri: The tracking_uri to parse.
-    :type tracking_uri: str
+    :param artifact_uri: The artifact_uri to parse.
+    :type artifact_uri: str
     :return: Dictionary of the parsed experiment name, and run id and workspace information if available.
     :rtype: dict[str, str]
     """
 
-    logger.info("Parsing artifact uri {}".format(tracking_uri))
-    parsed_url_path = parse.urlparse(tracking_uri).path
-    ret = {}
-    path_match = None
-    try:
-        mo = re.compile(_ARTIFACT_URI_REGEX).match(parsed_url_path)
-        ret[_SUB_ID] = mo.group(1)
-        ret[_RES_GRP] = mo.group(2)
-        ret[_WS_NAME] = mo.group(3)
-        ret[_EXP_NAME] = mo.group(4)
-        ret[_RUN_ID] = mo.group(5)
-        path_match = mo.group(7)
-    except Exception:
-        mo = re.compile(_ARTIFACT_URI_EXP_RUN_REGEX).match(parsed_url_path)
-        ret[_EXP_NAME] = mo.group(1)
-        ret[_RUN_ID] = mo.group(2)
-        path_match = mo.group(3)
-
-    logger.info("Artifact uri {} has experiment name {} and run id {}".format(tracking_uri,
-                                                                              ret[_EXP_NAME],
-                                                                              ret[_RUN_ID]))
-    if path_match is not None and path_match != "/artifacts":
-        path = path_match[len("/artifacts"):]
-        ret[_ARTIFACT_PATH] = path if not path.startswith("/") else path[1:]
-    return ret
+    logger.info("Parsing artifact uri {}".format(artifact_uri))
+    parsed_url_path = parse.urlparse(artifact_uri).path
+    artifact_info_dict = get_run_info(parsed_url_path) or {}
+
+    if not artifact_info_dict:
+        # Remove the starting "/"
+        origin, container, path = parsed_url_path[1:].split("/", 3)
+        artifact_info_dict[_ORIGIN] = origin
+        artifact_info_dict[_CONTAINER] = container
+        artifact_info_dict[_ARTIFACT_PATH] = path if not path.startswith("/") else path[1:]
+
+    logger.info("Artifact uri {} info: {}".format(artifact_uri, artifact_info_dict))
+    return artifact_info_dict
 
 
 def get_service_context_from_artifact_url(parsed_url):
     parsed_artifacts_path = artifact_uri_decomp(parsed_url.path)
     logger.debug("Creating service context from the artifact uri")
     subscription_id = parsed_artifacts_path[_SUB_ID]
     resource_group_name = parsed_artifacts_path[_RES_GRP]
@@ -144,17 +164,16 @@
     resource_group_name = parsed_path[_RES_GRP]
     workspace_name = parsed_path[_WS_NAME]
 
     queries = dict(parse.parse_qsl(parsed_url.query))
     try:
         from mlflow.tracking._tracking_service.utils import _TRACKING_TOKEN_ENV_VAR
     except ImportError:
-        from .store import _VERSION_WARNING
-        logger.warning(_VERSION_WARNING.format("_TRACKING_TOKEN_ENV_VAR from " +
-                                               "mlflow.tracking._tracking_service.utils"))
+        logger.warning(VERSION_WARNING.format("_TRACKING_TOKEN_ENV_VAR from " +
+                                              "mlflow.tracking._tracking_service.utils"))
         from mlflow.tracking.utils import _TRACKING_TOKEN_ENV_VAR
     token = os.environ.get(_TRACKING_TOKEN_ENV_VAR)
     has_auth_info = _AUTH_HEAD in queries or token is not None
     if queries.get(_AUTH_TYPE) == InteractiveLoginAuthentication.__name__ or not has_auth_info:
         logger.debug("Using the default InteractiveLoginAuthentication. "
                      "Auth Type was set to {} and {}".format(queries.get(_AUTH_TYPE),
                                                              "{} token was found in the environment".format(
@@ -186,14 +205,57 @@
                           resource_group_name=resource_group_name,
                           workspace_name=workspace_name,
                           workspace_id=None,
                           workspace_discovery_url=None,
                           authentication=auth)
 
 
+def get_service_context_from_tracking_url_default_auth(parsed_url):
+    """Create a Service Context object out of a parsed URL."""
+    logger.debug("Creating a Service Context object from the tracking uri using default authentication"
+                 " : InteractiveLoginAuthentication")
+    parsed_path = tracking_uri_decomp(parsed_url.path)
+    subscription_id = parsed_path[_SUB_ID]
+    resource_group_name = parsed_path[_RES_GRP]
+    workspace_name = parsed_path[_WS_NAME]
+
+    queries = dict(parse.parse_qsl(parsed_url.query))
+
+    if _AUTH_HEAD in queries or _AUTH_TYPE in queries:
+        logger.warning("Use of {}, {} query parameters in tracking URI is deprecated."
+                       " InteractiveLoginAuthentication will be used by default."
+                       " Please use 'azureml.core.workspace.Workspace.get_mlflow_tracking_uri'"
+                       " to use authentication associated with workspace".format(_AUTH_TYPE, _AUTH_HEAD))
+
+    """
+    Using InteractiveLoginAuthentication poses an issue if customer uses a subscription not belonging to its default
+    tenant. In that case customer needs to provide tenantId as a param to InteractiveLoginAuthentication which is
+    not possible currently in this flow. In this case customer needs to create workspace object with auth param
+
+        from azureml.core.authentication import InteractiveLoginAuthentication
+
+        interactive_auth = InteractiveLoginAuthentication(tenant_id="my-tenant-id")
+
+        ws = Workspace(subscription_id="my-subscription-id",
+                       resource_group="my-ml-rg",
+                       workspace_name="my-ml-workspace",
+                       auth=interactive_auth)
+
+        ws.get_mlflow_tracking_uri()
+    """
+    auth = InteractiveLoginAuthentication()
+
+    return ServiceContext(subscription_id=subscription_id,
+                          resource_group_name=resource_group_name,
+                          workspace_name=workspace_name,
+                          workspace_id=None,
+                          workspace_discovery_url=None,
+                          authentication=auth)
+
+
 def get_aml_experiment_name(exp_name):
     """Extract the actual experiment name from the adb experiment name format."""
     regex = "(.+)\\/(.+)"
     mo = re.compile(regex).match(exp_name)
     if mo is not None:
         logger.info("Parsing experiment name from {} to {}".format(exp_name, mo.group(2)))
         return mo.group(2)
@@ -226,7 +288,47 @@
         handle_exception(operation, store, e)
     for other_store in stores[1:]:
         try:
             func(other_store, *args, **kwargs)
         except Exception as e:
             handle_exception(operation, other_store, e)
     return out
+
+
+def get_service_context(parsed_artifacts_url):
+    from mlflow.tracking.client import MlflowClient
+    from .store import AzureMLRestStore, AdbAzuremlRestStore
+    try:
+        store = MlflowClient()._tracking_client.store
+    except:
+        logger.warning(VERSION_WARNING.format("MlflowClient()._tracking_client.store"))
+        store = MlflowClient().store
+    if isinstance(store, AzureMLRestStore):
+        logger.debug("Using the service context from the {} store".format(store.__class__.__name__))
+        return store.service_context
+    elif isinstance(store, AdbAzuremlRestStore):
+        return store.aml_store.service_context
+    else:
+        return get_service_context_from_artifact_url(parsed_artifacts_url)
+
+
+def get_artifact_repository_client(artifact_uri):
+    logger.debug("Initializing the AzureMLflowArtifactRepository")
+    parsed_artifacts_url = parse.urlparse(artifact_uri)
+    service_context = get_service_context(parsed_artifacts_url)
+
+    parsed_artifacts_path = artifact_uri_decomp(artifact_uri)
+
+    if _EXP_NAME in parsed_artifacts_path and _RUN_ID in parsed_artifacts_path:
+        experiment_name = parsed_artifacts_path[_EXP_NAME]
+        logger.debug("AzureMLflowArtifactRepository for experiment {}".format(experiment_name))
+        run_id = parsed_artifacts_path[_RUN_ID]
+        logger.debug("AzureMLflowArtifactRepository for run id {}".format(run_id))
+        path = parsed_artifacts_path.get(_ARTIFACT_PATH)
+        logger.debug("AzureMLflowArtifactRepository for path {}".format(path))
+        artifacts_client = RunArtifactRepositoryClient(service_context, experiment_name, run_id, path)
+    else:
+        origin = parsed_artifacts_path[_ORIGIN]
+        container = parsed_artifacts_path[_CONTAINER]
+        path = parsed_artifacts_path[_ARTIFACT_PATH]
+        artifacts_client = LocalArtifactRepositoryClient(service_context, origin, container, path)
+    return artifacts_client
```

## Comparing `azureml/mlflow/_internal/run_artifacts_extension_client.py` & `azureml/mlflow/_internal/run_artifact_repository_client.py`

 * *Files 17% similar despite different names*

```diff
@@ -12,110 +12,119 @@
 from azureml._file_utils import upload_blob_from_stream
 from azureml.exceptions import UserErrorException, AzureMLException
 
 from azureml._restclient.models.artifact_path_dto import ArtifactPathDto
 from azureml._restclient.models.batch_artifact_create_command import BatchArtifactCreateCommand
 
 from azureml._restclient.run_artifacts_client import RunArtifactsClient, SUPPORTED_NUM_EMPTY_ARTIFACTS
+from .artifact_repository_client import ArtifactRepositoryClient
 
 
-class RunArtifactsExtensionClient(RunArtifactsClient):
-    """Run Artifacts Extension client class."""
+class RunArtifactRepositoryClient(ArtifactRepositoryClient):
+    """Run Artifact Repository client class."""
 
-    def download_artifact(self, run_id, path, output_file_path):
+    def __init__(self, service_context, experiment_name, run_id, path):
+        super(RunArtifactRepositoryClient, self).__init__(path)
+        self._run_artifacts_client = RunArtifactsClient(service_context, experiment_name)
+        self._run_id = run_id
+
+    def download_artifact(self, path, output_file_path):
         """
         Download a single artifact from artifact service.
 
-        :param run_id: the run id of the run that's associated with the artifact
-        :type run_id: str
         :param path: the filepath within the container of the artifact to be downloaded
         :type path: str
         :param output_file_path: filepath in which to store the downloaded artifact locally
         :rtype: None
         """
         filename = os.path.basename(path)  # save outputs/filename.txt as filename.txt
         if os.path.isdir(output_file_path):
             self._logger.debug("output_file_path for download_artifact is a directory.")
             output_file_path = os.path.join(output_file_path, filename)
         else:
             self._logger.debug("output_file_path for download_artifact is not a directory.")
-        super(RunArtifactsExtensionClient, self).download_artifact(run_id, path, output_file_path)
+        self._run_artifacts_client.download_artifact(self._run_id, path, output_file_path)
 
-    def create_empty_artifacts(self, run_id, paths):
+    def _create_empty_artifacts(self, paths):
         """Create empty artifacts."""
-        if run_id is None:
+        if self._run_id is None:
             raise UserErrorException("run_id cannot be null when creating empty artifacts")
 
         if isinstance(paths, str):
             paths = [paths]
         artifacts = [ArtifactPathDto(path) for path in paths]
         batch_create_command = BatchArtifactCreateCommand(artifacts)
-        res = self._execute_with_experiment_arguments(
-            self._client.run_artifact.batch_create_empty_artifacts,
-            run_id,
+        res = self._run_artifacts_client._execute_with_experiment_arguments(
+            self._run_artifacts_client._client.run_artifact.batch_create_empty_artifacts,
+            self._run_id,
             batch_create_command)
 
         if res.errors:
             error_messages = []
             for artifact_name in res.errors:
                 error = res.errors[artifact_name].error
                 error_messages.append("{}: {}".format(error.code,
                                                       error.message))
             raise AzureMLException("\n".join(error_messages))
 
         return res
 
-    def upload_stream_to_existing_artifact(self, stream, artifact, content_information,
-                                           content_type=None, session=None):
+    def _upload_stream_to_existing_artifact(
+            self, stream, artifact, content_information, content_type=None):
         """Upload a stream to existring artifact."""
         artifact = artifact
         artifact_uri = content_information.content_uri
-        session = session if session is not None else self.session
-        res = upload_blob_from_stream(stream, artifact_uri, content_type=content_type, session=session)
+        res = upload_blob_from_stream(
+            stream,
+            artifact_uri,
+            content_type=content_type,
+            session=self._run_artifacts_client.session)
         return res
 
-    def upload_artifact_from_stream(self, stream, run_id, name, content_type=None, session=None):
+    def _upload_artifact_from_stream(
+            self, stream, name, content_type=None):
         """Upload a stream to a new artifact."""
-        if run_id is None:
+        if self._run_id is None:
             raise UserErrorException("Cannot upload artifact when run_id is None")
         # Construct body
-        res = self.create_empty_artifacts(run_id, name)
+        res = self._create_empty_artifacts(name)
         artifact = res.artifacts[name]
         content_information = res.artifact_content_information[name]
-        self.upload_stream_to_existing_artifact(stream, artifact, content_information,
-                                                content_type=content_type, session=session)
+        self._upload_stream_to_existing_artifact(
+            stream, artifact, content_information,
+            content_type=content_type)
         return res
 
-    def upload_artifact_from_path(self, path, *args, **kwargs):
+    def _upload_artifact_from_path(self, path, *args, **kwargs):
         """Upload a local file to a new artifact."""
         path = os.path.normpath(path)
         path = os.path.abspath(path)
         with open(path, "rb") as stream:
-            return self.upload_artifact_from_stream(stream, *args, **kwargs)
+            return self._upload_artifact_from_stream(stream, *args, **kwargs)
 
     def upload_artifact(self, artifact, *args, **kwargs):
         """Upload local file or stream to a new artifact."""
         self._logger.debug("Called upload_artifact")
         if isinstance(artifact, str):
             self._logger.debug("Uploading path artifact")
-            return self.upload_artifact_from_path(artifact, *args, **kwargs)
+            return self._upload_artifact_from_path(artifact, *args, **kwargs)
         elif isinstance(artifact, IOBase):
             self._logger.debug("Uploading io artifact")
-            return self.upload_artifact_from_stream(artifact, *args, **kwargs)
+            return self._upload_artifact_from_stream(artifact, *args, **kwargs)
         else:
             raise UserErrorException("UnsupportedType: type {} is invalid, "
                                      "supported input types: file path or file".format(type(artifact)))
 
-    def upload_files(self, paths, run_id, names=None):
+    def upload_files(self, paths, names=None):
         """
         Upload files to artifact service.
 
         :rtype: list[BatchArtifactContentInformationDto]
         """
-        if run_id is None:
+        if self._run_id is None:
             raise UserErrorException("run_id cannot be null when uploading artifact")
 
         names = names if names is not None else paths
         path_to_name = {}
         paths_and_names = []
         # Check for duplicates, this removes possible interdependencies
         # during parallel uploads
@@ -128,40 +137,43 @@
                                      "Uploading file {} to the original name "
                                      "{}.".format(path, name, path, path_to_name[path]))
 
         batch_size = SUPPORTED_NUM_EMPTY_ARTIFACTS
 
         results = []
         for i in range(0, len(names), batch_size):
-            with TaskQueue(worker_pool=self._pool, _ident="upload_files", _parent_logger=self._logger) as task_queue:
+            with TaskQueue(
+                    worker_pool=self._run_artifacts_client._pool,
+                    _ident="upload_files",
+                    _parent_logger=self._logger) as task_queue:
                 batch_names = names[i:i + batch_size]
                 batch_paths = paths[i:i + batch_size]
 
-                content_information = self.create_empty_artifacts(run_id, batch_names)
+                content_information = self._create_empty_artifacts(batch_names)
 
-                def perform_upload(path, artifact, artifact_content_info, session):
+                def perform_upload(path, artifact, artifact_content_info):
                     with open(path, "rb") as stream:
-                        return self.upload_stream_to_existing_artifact(stream, artifact, artifact_content_info,
-                                                                       session=session)
+                        return self._upload_stream_to_existing_artifact(stream, artifact, artifact_content_info)
 
                 for path, name in zip(batch_paths, batch_names):
                     artifact = content_information.artifacts[name]
                     artifact_content_info = content_information.artifact_content_information[name]
-                    task = task_queue.add(perform_upload, path, artifact, artifact_content_info, self.session)
+                    task = task_queue.add(perform_upload, path, artifact, artifact_content_info)
                     results.append(task)
 
         return map(lambda task: task.wait(), results)
 
-    def upload_dir(self, dir_path, run_id, path_to_name_fn=None, skip_first_level=False):
+    def upload_dir(
+            self, dir_path, path_to_name_fn=None, skip_first_level=False):
         """
         Upload all files in path.
 
         :rtype: list[BatchArtifactContentInformationDto]
         """
-        if run_id is None:
+        if self._run_id is None:
             raise UserErrorException("Cannot upload when run_id is None")
         paths_to_upload = []
         names = []
         for pathl, _subdirs, files in os.walk(dir_path):
             for _file in files:
                 fpath = os.path.join(pathl, _file)
                 paths_to_upload.append(fpath)
@@ -170,9 +182,12 @@
                 elif skip_first_level:
                     subDir = pathl.split("/", 1)[1]
                     name = os.path.join(subDir, _file)
                 else:
                     name = fpath
                 names.append(name)
         self._logger.debug("Uploading {}".format(names))
-        result = self.upload_files(paths_to_upload, run_id, names)
+        result = self.upload_files(paths_to_upload, names)
         return result
+
+    def get_file_paths(self):
+        return self._run_artifacts_client.get_file_paths(self._run_id)
```

## Comparing `azureml_mlflow-1.8.0.dist-info/LICENSE.txt` & `azureml_mlflow-1.9.0.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `azureml_mlflow-1.8.0.dist-info/metadata.json` & `azureml_mlflow-1.9.0.dist-info/metadata.json`

 * *Files 12% similar despite different names*

### Pretty-printed

 * *Similarity: 0.9403409090909091%*

 * *Differences: {"'extensions'": "{'python.exports': {'mlflow.artifact_repository': {'adbazureml': "*

 * *                 "'azureml.mlflow.entry_point_loaders:adb_azureml_artifacts_builder', 'azureml': "*

 * *                 "'azureml.mlflow.entry_point_loaders:azureml_artifacts_builder'}, "*

 * *                 "'mlflow.model_registry_store': {'azureml': "*

 * *                 "'azureml.mlflow.entry_point_loaders:azureml_model_registry_builder'}, "*

 * *                 "'mlflow.tracking_store': {'adbazureml': "*

 * *                 "'azureml.mlf […]*

```diff
@@ -27,36 +27,36 @@
             },
             "project_urls": {
                 "Home": "https://docs.microsoft.com/en-us/azure/machine-learning/service/"
             }
         },
         "python.exports": {
             "mlflow.artifact_repository": {
-                "adbazureml": "azureml.mlflow:adb_azureml_artifacts_builder",
-                "azureml": "azureml.mlflow:azureml_artifacts_builder"
+                "adbazureml": "azureml.mlflow.entry_point_loaders:adb_azureml_artifacts_builder",
+                "azureml": "azureml.mlflow.entry_point_loaders:azureml_artifacts_builder"
             },
             "mlflow.model_registry_store": {
-                "azureml": "azureml.mlflow:azureml_model_registry_builder"
+                "azureml": "azureml.mlflow.entry_point_loaders:azureml_model_registry_builder"
             },
             "mlflow.tracking_store": {
-                "adbazureml": "azureml.mlflow:adb_azureml_store_builder",
-                "azureml": "azureml.mlflow:azureml_store_builder"
+                "adbazureml": "azureml.mlflow.entry_point_loaders:adb_azureml_store_builder",
+                "azureml": "azureml.mlflow.entry_point_loaders:azureml_store_builder"
             }
         }
     },
     "extras": [],
     "generator": "bdist_wheel (0.30.0)",
     "license": "Proprietary https://aka.ms/azureml-preview-sdk-license ",
     "metadata_version": "2.0",
     "name": "azureml-mlflow",
     "run_requires": [
         {
             "requires": [
-                "azureml-core (~=1.8.0)",
+                "azureml-core (~=1.9.0)",
                 "jsonpickle",
                 "mlflow (>=1.0.0)"
             ]
         }
     ],
     "summary": "UNKNOWN",
-    "version": "1.8.0"
+    "version": "1.9.0"
 }
```

## Comparing `azureml_mlflow-1.8.0.dist-info/RECORD` & `azureml_mlflow-1.9.0.dist-info/RECORD`

 * *Files 18% similar despite different names*

```diff
@@ -1,20 +1,24 @@
 azureml/__init__.py,sha256=n0xtZ3iWcoVg5Qognsb7InYAUVAK8s3iaVeHB5GOaNA,251
-azureml/mlflow/__init__.py,sha256=YOS-ydV83BjtXBW9LovI3jRhsFTm-o8DnvDHsxne6ZY,15026
-azureml/mlflow/_version.py,sha256=8R-mO3NZEmK5bw1PXdk3467o_3bhqbSsXyC8e5lgi2Y,19
+azureml/mlflow/__init__.py,sha256=xz-i7jJ6xVK-6i-6bMkVktCfyuv-OvvtMt0BlyLX-qM,9392
+azureml/mlflow/_version.py,sha256=sI1DTpr91cSf6NbDPrvUz8A2ENd3HBi6me7t7FTQhiw,19
+azureml/mlflow/entry_point_loaders.py,sha256=ui-5rUpzA5p94TVh30LAt-6a-0HsXDbdor5MSdt1EzA,2993
 azureml/mlflow/_internal/__init__.py,sha256=JlCCObuOLZyNhIZlsoL51KtFxiY4xKIIzIRDBcdeu6Y,183
-azureml/mlflow/_internal/artifact_repo.py,sha256=JKkzN2FGmM2u_hkoX81yL15g8cmhA6Ficl11lxx0UWM,12673
+azureml/mlflow/_internal/artifact_repo.py,sha256=YKjYQFyWRTXNcIDD9Ac200aOet7GycFd5zov1qdiycQ,10817
+azureml/mlflow/_internal/artifact_repository_client.py,sha256=LYzD-Q_0zII3agwCFQK5eK5No_9lCs804LNdYvOAUM4,883
 azureml/mlflow/_internal/authentication.py,sha256=wR0D5wRMaSSfyQWasgzwBE4MczIYdFcDdaIrYektBnM,1408
-azureml/mlflow/_internal/azureml_reststore.py,sha256=eq1YDXMURO52nw09HDJ0wCyD5Xl4JczzjyHFUoQiC8M,4232
+azureml/mlflow/_internal/azureml_reststore.py,sha256=0Y3PF5FWfjGmCXj3g22iZCcYJoXjgkPSWhuEnltLKcw,4147
 azureml/mlflow/_internal/hybrid_service_context.py,sha256=SPnHdfw00nRbd7IMO-4PR6NOTM8kE4bi8-zV8gUXEPc,892
-azureml/mlflow/_internal/model_registry.py,sha256=IjUNim_iMlcja4bViFJQH6enAFDnnxKC3NjeubACKQc,1637
-azureml/mlflow/_internal/run_artifacts_extension_client.py,sha256=mWAkEPgcfnotNnx6i1zUoLEPrmjQkJ2kaGf4aGUbxy8,8289
-azureml/mlflow/_internal/store.py,sha256=7iHi_PuROxMQ_7gkCxeCUTIH7x0kfxSmAnF2Hr3iVfI,14031
-azureml/mlflow/_internal/utils.py,sha256=JNbhMyJifiwjztwx7KfL2RNnslcxed52iMHrYMaOJCk,9247
-azureml_mlflow-1.8.0.dist-info/DESCRIPTION.rst,sha256=r_jXzwLYXzSJmLmnvXzIar_x4z8TZlqS0-W0Bpbhdp4,71
-azureml_mlflow-1.8.0.dist-info/LICENSE.txt,sha256=FOfkEEz4uS7g278F9Rq12rqKF3lLyBUZhVpLetuZrTg,1021
-azureml_mlflow-1.8.0.dist-info/METADATA,sha256=kjyrstkH8i7KctO73qdkhB-79BMqw3lTmkTAv7cl7Yo,1005
-azureml_mlflow-1.8.0.dist-info/RECORD,,
-azureml_mlflow-1.8.0.dist-info/WHEEL,sha256=Vlaj2XNMTTJ893zWX-JvKeZUIs7q5E7d7Gise2Vouzc,97
-azureml_mlflow-1.8.0.dist-info/entry_points.txt,sha256=M9ff9UcJ3IEH-t6mA-lsbhAlJvI3TZ8_3d2G4EXAZ7Q,352
-azureml_mlflow-1.8.0.dist-info/metadata.json,sha256=U5X4IbMfGHZUZdmGBHZ0goUPsRBuMnaLsoqYNo9dh84,1481
-azureml_mlflow-1.8.0.dist-info/top_level.txt,sha256=ZOeEa0TAXo6i5wOjwBoqfIGEuxOcKuscGgNSpizqREY,8
+azureml/mlflow/_internal/local_artifact_repository_client.py,sha256=kpBaqkKLC1Ni0UkYkCFd4r-69sFY7ctekEUI81tMbSk,2445
+azureml/mlflow/_internal/model_registry.py,sha256=xNaa6fnno_Sk1tdZ-ccarC2-GAxg5HIHyvTR1tcMC94,2465
+azureml/mlflow/_internal/run_artifact_repository_client.py,sha256=NodvKBwDmUKTbP-8qtUXox-DTFM5sajM09qVLPSHZPw,8549
+azureml/mlflow/_internal/store.py,sha256=I-9fRWs0RD6g70Af9ZFozpuo5Y8pFmL6-2ITnUXGSQY,13960
+azureml/mlflow/_internal/store_loader.py,sha256=mvJurNuaOaQbHLvR-7-k8jN1g7eiqR8HOS8Un8YKj4A,4364
+azureml/mlflow/_internal/utils.py,sha256=1z_a3KyOjKpPrF1CcGf-eiLjzlHW-Om3fGgKGdf8bxk,14018
+azureml_mlflow-1.9.0.dist-info/DESCRIPTION.rst,sha256=FU-qQaZOoVw2zlhUiyrcu1RM2Md7TqDVEJxdUj4p1Nk,899
+azureml_mlflow-1.9.0.dist-info/LICENSE.txt,sha256=FOfkEEz4uS7g278F9Rq12rqKF3lLyBUZhVpLetuZrTg,1021
+azureml_mlflow-1.9.0.dist-info/METADATA,sha256=XvDqKvXkNA8JvgDOESzWakjEuXk5CIYsHidlu7M_PSg,1833
+azureml_mlflow-1.9.0.dist-info/RECORD,,
+azureml_mlflow-1.9.0.dist-info/WHEEL,sha256=Vlaj2XNMTTJ893zWX-JvKeZUIs7q5E7d7Gise2Vouzc,97
+azureml_mlflow-1.9.0.dist-info/entry_points.txt,sha256=L7sN4rwNe4mDFi2pEC4zfQQoXXfcnodtBA1tXbdNvpQ,452
+azureml_mlflow-1.9.0.dist-info/metadata.json,sha256=ywE_on1k4vSKCuEvp2jzrEcYtf-1Ouxalihk89IP9BU,1581
+azureml_mlflow-1.9.0.dist-info/top_level.txt,sha256=ZOeEa0TAXo6i5wOjwBoqfIGEuxOcKuscGgNSpizqREY,8
```

