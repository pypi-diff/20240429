# Comparing `tmp/octopize_avatar-0.7.2.tar.gz` & `tmp/octopize_avatar-0.7.3.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "octopize_avatar-0.7.2.tar", max compression
+gzip compressed data, was "octopize_avatar-0.7.3.tar", max compression
```

## Comparing `octopize_avatar-0.7.2.tar` & `octopize_avatar-0.7.3.tar`

### file list

```diff
@@ -1,50 +1,51 @@
--rw-r--r--   0        0        0     9861 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/LICENSE.txt
--rw-r--r--   0        0        0       54 2024-04-19 09:00:37.220103 octopize_avatar-0.7.2/avatars/__init__.py
--rw-r--r--   0        0        0      672 2024-04-18 14:17:54.946931 octopize_avatar-0.7.2/avatars/_typing.py
--rw-r--r--   0        0        0    64791 2024-04-19 08:58:26.096775 octopize_avatar-0.7.2/avatars/api.py
--rw-r--r--   0        0        0    32701 2024-04-19 08:58:26.096775 octopize_avatar-0.7.2/avatars/api_autogenerated.py
--rw-r--r--   0        0        0     2640 2024-04-19 07:20:16.937747 octopize_avatar-0.7.2/avatars/api_batch_test.py
--rw-r--r--   0        0        0    20331 2024-04-19 07:20:16.937747 octopize_avatar-0.7.2/avatars/api_custom_dataset_methods_test.py
--rw-r--r--   0        0        0    19942 2024-04-19 07:20:16.937747 octopize_avatar-0.7.2/avatars/base_client.py
--rw-r--r--   0        0        0     4788 2024-04-19 08:58:26.096775 octopize_avatar-0.7.2/avatars/client.py
--rw-r--r--   0        0        0     3897 2024-04-19 08:58:26.096775 octopize_avatar-0.7.2/avatars/client_test.py
--rw-r--r--   0        0        0      991 2024-04-19 08:58:26.096775 octopize_avatar-0.7.2/avatars/conftest.py
--rw-r--r--   0        0        0       20 2024-04-19 07:20:16.937747 octopize_avatar-0.7.2/avatars/constants.py
--rw-r--r--   0        0        0      122 2024-04-19 07:20:16.937747 octopize_avatar-0.7.2/avatars/exceptions.py
--rw-r--r--   0        0        0        0 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/lib/__init__.py
--rw-r--r--   0        0        0      450 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/lib/continuous_threshold.py
--rw-r--r--   0        0        0      410 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/lib/continuous_threshold_test.py
--rw-r--r--   0        0        0     6543 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/lib/saferound.py
--rw-r--r--   0        0        0     3557 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/lib/saferound_test.py
--rw-r--r--   0        0        0     3170 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/lib/split.py
--rw-r--r--   0        0        0     2253 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/lib/split_column_types_test.py
--rw-r--r--   0        0        0      931 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/lib/split_columns_types.py
--rw-r--r--   0        0        0     2618 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/lib/split_test.py
--rw-r--r--   0        0        0    80463 2024-04-19 08:58:26.096775 octopize_avatar-0.7.2/avatars/models.py
--rw-r--r--   0        0        0     1361 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/processors/__init__.py
--rw-r--r--   0        0        0     1125 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/processors/conftest.py
--rw-r--r--   0        0        0     2153 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/processors/datetime.py
--rw-r--r--   0        0        0     1327 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/processors/datetime_test.py
--rw-r--r--   0        0        0     8419 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/processors/expected_mean.py
--rw-r--r--   0        0        0     6313 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/processors/expected_mean_test.py
--rw-r--r--   0        0        0     4645 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/processors/group_modalities.py
--rw-r--r--   0        0        0     3839 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/processors/group_modalities_test.py
--rw-r--r--   0        0        0    10267 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/processors/inter_record_bounded_cumulated_difference.py
--rw-r--r--   0        0        0     7906 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/processors/inter_record_bounded_cumulated_difference_test.py
--rw-r--r--   0        0        0    14028 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/processors/inter_record_bounded_range_difference.py
--rw-r--r--   0        0        0     6297 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/processors/inter_record_bounded_range_difference_test.py
--rw-r--r--   0        0        0     8500 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/processors/inter_record_cumulated_difference.py
--rw-r--r--   0        0        0     9393 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/processors/inter_record_cumulated_difference_test.py
--rw-r--r--   0        0        0    11464 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/processors/inter_record_range_difference.py
--rw-r--r--   0        0        0     8349 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/processors/inter_record_range_difference_test.py
--rw-r--r--   0        0        0     3309 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/processors/perturbation.py
--rw-r--r--   0        0        0     3457 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/processors/perturbation_test.py
--rw-r--r--   0        0        0     6494 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/processors/proportions.py
--rw-r--r--   0        0        0     6894 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/processors/proportions_test.py
--rw-r--r--   0        0        0     6941 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/processors/relative_difference.py
--rw-r--r--   0        0        0     7089 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/processors/relative_difference_test.py
--rw-r--r--   0        0        0     4645 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/processors/to_categorical.py
--rw-r--r--   0        0        0     2991 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/processors/to_categorical_test.py
--rw-r--r--   0        0        0     1112 2024-04-16 15:34:49.846563 octopize_avatar-0.7.2/avatars/utils.py
--rw-r--r--   0        0        0     1912 2024-04-19 10:25:28.439846 octopize_avatar-0.7.2/pyproject.toml
--rw-r--r--   0        0        0      712 1970-01-01 00:00:00.000000 octopize_avatar-0.7.2/PKG-INFO
+-rw-r--r--   0        0        0     9861 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/LICENSE.txt
+-rw-r--r--   0        0        0      420 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/README.md
+-rw-r--r--   0        0        0       54 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/__init__.py
+-rw-r--r--   0        0        0      673 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/_typing.py
+-rw-r--r--   0        0        0    64564 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/api.py
+-rw-r--r--   0        0        0    34599 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/api_autogenerated.py
+-rw-r--r--   0        0        0     2640 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/api_batch_test.py
+-rw-r--r--   0        0        0    20298 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/api_custom_dataset_methods_test.py
+-rw-r--r--   0        0        0    19921 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/base_client.py
+-rw-r--r--   0        0        0     4423 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/client.py
+-rw-r--r--   0        0        0     3845 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/client_test.py
+-rw-r--r--   0        0        0     1005 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/conftest.py
+-rw-r--r--   0        0        0       20 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/constants.py
+-rw-r--r--   0        0        0      122 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/exceptions.py
+-rw-r--r--   0        0        0        0 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/lib/__init__.py
+-rw-r--r--   0        0        0      450 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/lib/continuous_threshold.py
+-rw-r--r--   0        0        0      410 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/lib/continuous_threshold_test.py
+-rw-r--r--   0        0        0     6543 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/lib/saferound.py
+-rw-r--r--   0        0        0     3552 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/lib/saferound_test.py
+-rw-r--r--   0        0        0     3182 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/lib/split.py
+-rw-r--r--   0        0        0     2253 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/lib/split_column_types_test.py
+-rw-r--r--   0        0        0      931 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/lib/split_columns_types.py
+-rw-r--r--   0        0        0     2624 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/lib/split_test.py
+-rw-r--r--   0        0        0    80551 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/models.py
+-rw-r--r--   0        0        0     1361 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/processors/__init__.py
+-rw-r--r--   0        0        0     1125 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/processors/conftest.py
+-rw-r--r--   0        0        0     2153 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/processors/datetime.py
+-rw-r--r--   0        0        0     1327 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/processors/datetime_test.py
+-rw-r--r--   0        0        0     8419 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/processors/expected_mean.py
+-rw-r--r--   0        0        0     6313 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/processors/expected_mean_test.py
+-rw-r--r--   0        0        0     4691 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/group_modalities.py
+-rw-r--r--   0        0        0     3839 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/group_modalities_test.py
+-rw-r--r--   0        0        0    10381 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/inter_record_bounded_cumulated_difference.py
+-rw-r--r--   0        0        0     7906 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/inter_record_bounded_cumulated_difference_test.py
+-rw-r--r--   0        0        0    14028 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/inter_record_bounded_range_difference.py
+-rw-r--r--   0        0        0     6297 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/inter_record_bounded_range_difference_test.py
+-rw-r--r--   0        0        0     8633 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/inter_record_cumulated_difference.py
+-rw-r--r--   0        0        0     9393 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/inter_record_cumulated_difference_test.py
+-rw-r--r--   0        0        0    11673 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/inter_record_range_difference.py
+-rw-r--r--   0        0        0     8349 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/inter_record_range_difference_test.py
+-rw-r--r--   0        0        0     3309 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/perturbation.py
+-rw-r--r--   0        0        0     3457 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/perturbation_test.py
+-rw-r--r--   0        0        0     6497 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/proportions.py
+-rw-r--r--   0        0        0     6894 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/proportions_test.py
+-rw-r--r--   0        0        0     6945 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/relative_difference.py
+-rw-r--r--   0        0        0     7089 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/relative_difference_test.py
+-rw-r--r--   0        0        0     4632 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/to_categorical.py
+-rw-r--r--   0        0        0     2979 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/to_categorical_test.py
+-rw-r--r--   0        0        0     1113 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/utils.py
+-rw-r--r--   0        0        0     1912 2024-04-29 15:40:22.330004 octopize_avatar-0.7.3/pyproject.toml
+-rw-r--r--   0        0        0     1183 1970-01-01 00:00:00.000000 octopize_avatar-0.7.3/PKG-INFO
```

### Comparing `octopize_avatar-0.7.2/LICENSE.txt` & `octopize_avatar-0.7.3/LICENSE.txt`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.2/avatars/_typing.py` & `octopize_avatar-0.7.3/avatars/_typing.py`

 * *Files 23% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 
 from typing_extensions import TypeAlias, TypeGuard
 
 if TYPE_CHECKING:
     from _typeshed import SupportsRead, SupportsWrite
 
     class FileLikeInterface(
-        #! Make sure you modify is_file_like too.
+        # ! Make sure you modify is_file_like too.
         SupportsRead[AnyStr],
         SupportsWrite[AnyStr],
         Protocol,
     ):
         def seek(self, offset: int, whence: int = 0) -> int:
             pass
```

### Comparing `octopize_avatar-0.7.2/avatars/api.py` & `octopize_avatar-0.7.3/avatars/api.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,42 +1,20 @@
 # This file has been generated - DO NOT MODIFY
-# API Version : 1.1.2-fa473ed683daf6b108b4771008001137bcd8073d
+# API Version : 1.1.2-cc65e4fa4e46fdc93e7a375ce87a8202f1f59c1a
 
 
 import logging
 import os
 import shutil
 import tempfile
-import time
 import warnings
 from contextlib import ExitStack
 from io import BytesIO
 from pathlib import Path
-from typing import (
-    IO,
-    TYPE_CHECKING,
-    Any,
-    AnyStr,
-    BinaryIO,
-    Callable,
-    Dict,
-    Iterator,
-    List,
-    Literal,
-    NoReturn,
-    Optional,
-    Protocol,
-    Sequence,
-    TextIO,
-    Tuple,
-    TypeVar,
-    Union,
-    cast,
-    overload,
-)
+from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, TypeVar, Union
 from uuid import UUID
 
 import numpy as np
 import pandas as pd
 import pyarrow
 from pyarrow import parquet as pq
 
@@ -46,90 +24,86 @@
 from avatars.api_autogenerated import Health as _Health
 from avatars.api_autogenerated import Jobs as _Jobs
 from avatars.api_autogenerated import Metrics as _Metrics
 from avatars.api_autogenerated import Reports as _Reports
 from avatars.api_autogenerated import Stats as _Stats
 from avatars.api_autogenerated import Users as _Users
 from avatars.constants import DEFAULT_TIMEOUT
-from avatars.exceptions import FileTooLarge, InvalidFileType, Timeout
+from avatars.exceptions import InvalidFileType, Timeout
+from avatars.models import AvatarizationBatchJob  # noqa: F401
+from avatars.models import AvatarizationBatchJobCreate  # noqa: F401
+from avatars.models import AvatarizationJob  # noqa: F401
+from avatars.models import AvatarizationJobCreate  # noqa: F401
+from avatars.models import AvatarizationMultiTableJob  # noqa: F401
+from avatars.models import AvatarizationMultiTableJobCreate  # noqa: F401
+from avatars.models import AvatarizationWithTimeSeriesJob  # noqa: F401
+from avatars.models import AvatarizationWithTimeSeriesJobCreate  # noqa: F401
+from avatars.models import ClusterStats  # noqa: F401
+from avatars.models import CompatibilityResponse  # noqa: F401
+from avatars.models import Contributions  # noqa: F401
+from avatars.models import CreateDataset  # noqa: F401
+from avatars.models import CreateUser  # noqa: F401
+from avatars.models import Dataset  # noqa: F401
+from avatars.models import ExplainedVariance  # noqa: F401
+from avatars.models import ForgottenPasswordRequest  # noqa: F401
+from avatars.models import GenericJob  # noqa: F401
+from avatars.models import Login  # noqa: F401
+from avatars.models import LoginResponse  # noqa: F401
+from avatars.models import PatchDataset  # noqa: F401
+from avatars.models import PrivacyMetricsBatchJob  # noqa: F401
+from avatars.models import PrivacyMetricsBatchJobCreate  # noqa: F401
+from avatars.models import PrivacyMetricsGeolocationJob  # noqa: F401
+from avatars.models import PrivacyMetricsGeolocationJobCreate  # noqa: F401
+from avatars.models import PrivacyMetricsJob  # noqa: F401
+from avatars.models import PrivacyMetricsJobCreate  # noqa: F401
+from avatars.models import PrivacyMetricsMultiTableJob  # noqa: F401
+from avatars.models import PrivacyMetricsMultiTableJobCreate  # noqa: F401
+from avatars.models import PrivacyMetricsWithTimeSeriesJob  # noqa: F401
+from avatars.models import PrivacyMetricsWithTimeSeriesJobCreate  # noqa: F401
+from avatars.models import Projections  # noqa: F401
+from avatars.models import Report  # noqa: F401
+from avatars.models import ReportCreate  # noqa: F401
+from avatars.models import ReportFromBatchCreate  # noqa: F401
+from avatars.models import ReportFromDataCreate  # noqa: F401
+from avatars.models import ReportGeolocationPrivacyCreate  # noqa: F401
+from avatars.models import ResetPasswordRequest  # noqa: F401
+from avatars.models import SignalMetricsBatchJob  # noqa: F401
+from avatars.models import SignalMetricsBatchJobCreate  # noqa: F401
+from avatars.models import SignalMetricsJob  # noqa: F401
+from avatars.models import SignalMetricsJobCreate  # noqa: F401
+from avatars.models import SignalMetricsWithTimeSeriesJob  # noqa: F401
+from avatars.models import SignalMetricsWithTimeSeriesJobCreate  # noqa: F401
+from avatars.models import User  # noqa: F401
 from avatars.models import (
-    AvatarizationBatchJob,
-    AvatarizationBatchJobCreate,
     AvatarizationBatchResult,
-    AvatarizationJob,
-    AvatarizationJobCreate,
-    AvatarizationMultiTableJob,
-    AvatarizationMultiTableJobCreate,
     AvatarizationPipelineCreate,
     AvatarizationPipelineResult,
-    AvatarizationWithTimeSeriesJob,
-    AvatarizationWithTimeSeriesJobCreate,
-    ClusterStats,
     ColumnDetail,
     ColumnType,
-    CompatibilityResponse,
-    Contributions,
-    CreateDataset,
-    CreateUser,
-    Dataset,
-    ExplainedVariance,
     FileType,
-    ForgottenPasswordRequest,
-    GenericJob,
     JobStatus,
-    Login,
-    LoginResponse,
-    PatchDataset,
-    PrivacyMetrics,
-    PrivacyMetricsBatchJob,
-    PrivacyMetricsBatchJobCreate,
-    PrivacyMetricsGeolocationJob,
-    PrivacyMetricsGeolocationJobCreate,
-    PrivacyMetricsJob,
-    PrivacyMetricsJobCreate,
-    PrivacyMetricsMultiTableJob,
-    PrivacyMetricsMultiTableJobCreate,
     PrivacyMetricsParameters,
-    PrivacyMetricsWithTimeSeriesJob,
-    PrivacyMetricsWithTimeSeriesJobCreate,
-    Processor,
-    Projections,
-    Report,
-    ReportCreate,
-    ReportFromBatchCreate,
-    ReportFromDataCreate,
-    ReportGeolocationPrivacyCreate,
-    ResetPasswordRequest,
-    SignalMetrics,
-    SignalMetricsBatchJob,
-    SignalMetricsBatchJobCreate,
-    SignalMetricsJob,
-    SignalMetricsJobCreate,
     SignalMetricsParameters,
-    SignalMetricsWithTimeSeriesJob,
-    SignalMetricsWithTimeSeriesJobCreate,
-    User,
 )
 
 if TYPE_CHECKING:
     from avatars.client import ApiClient
     from avatars._typing import FileLikeInterface, HttpxFile
 
 from avatars._typing import is_file_like
 
 logger = logging.getLogger(__name__)
 logger.addHandler(logging.NullHandler())
 DEFAULT_RETRY_TIMEOUT = 60
-DEFAULT_TIMEOUT = 60
 MAX_ROWS_PER_FILE = 1_000_000
 MAX_BYTES_PER_FILE = 100 * 1024 * 1024  # 100 MB
 
 PARQUET_MAGIC_BYTES = b"PAR1"
 
-T = TypeVar('T')
+T = TypeVar("T")
 
 
 def to_column_type(s: str) -> ColumnType:
     if "float" in s:
         return ColumnType.float
     if "int" in s:
         return ColumnType.int
@@ -152,121 +126,87 @@
     raise TypeError(f"Unknown column type: '{s}'")
 
 
 class Auth:
     def __init__(self, client: "ApiClient") -> None:
         self.client = client
 
-
-
-
     def login(
         self,
-                request: Login,
-
-
-
+        request: Login,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  LoginResponse :
-        """Login the user.
-        """
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> LoginResponse:
+        """Login the user."""
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
             request,
         ]
 
-
         return _Auth(self.client).login(*args, **kwargs)
 
-
-
     def forgotten_password(
         self,
-                request: ForgottenPasswordRequest,
-
-
-
+        request: ForgottenPasswordRequest,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  Any :
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> Any:
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
             request,
         ]
 
-
         return _Auth(self.client).forgotten_password(*args, **kwargs)
 
-
-
     def reset_password(
         self,
-                request: ResetPasswordRequest,
-
-
-
+        request: ResetPasswordRequest,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  Any :
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> Any:
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
             request,
         ]
 
-
         return _Auth(self.client).reset_password(*args, **kwargs)
 
 
-
 class Compatibility:
     def __init__(self, client: "ApiClient") -> None:
         self.client = client
 
-
-
-
     def is_client_compatible(
         self,
-
-
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  CompatibilityResponse :
-        """Verify if the client is compatible with the API.
-        """
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> CompatibilityResponse:
+        """Verify if the client is compatible with the API."""
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
-        args: List[Any] = [
-        ]
-
+        args: List[Any] = []
 
         return _Compatibility(self.client).is_client_compatible(*args, **kwargs)
 
 
-
 class Datasets:
     def __init__(self, client: "ApiClient") -> None:
         self.client = client
 
     def create_dataset_from_stream(
         self,
         request: Optional[
@@ -291,15 +231,14 @@
         )
 
         _source: Optional[
             Union[str, "FileLikeInterface[bytes]", "FileLikeInterface[str]"]
         ] = (request or source)
         return self.create_dataset(source=_source, name=name, timeout=timeout)  # type: ignore[arg-type]
 
-
     def create_dataset(
         self,
         request: Optional[
             Union["FileLikeInterface[str]", "FileLikeInterface[bytes]"]
         ] = None,  # TODO: Remove once deprecated
         name: Optional[str] = None,
         source: Optional[
@@ -311,15 +250,17 @@
         ] = None,  # optional because we still have to support the old way
         *,
         timeout: Optional[int] = DEFAULT_TIMEOUT,
     ) -> Dataset:
         """Create a dataset from file upload."""
 
         if request:
-            warnings.warn(DeprecationWarning("request is deprecated. Use file instead."))
+            warnings.warn(
+                DeprecationWarning("request is deprecated. Use file instead.")
+            )
 
         if request is not None and source is not None:
             raise ValueError("You cannot pass both request and source.")
 
         if request is None and source is None:
             raise ValueError("You need to pass in a source.")
 
@@ -329,26 +270,25 @@
             raise ValueError("You need to pass in a source.")
 
         with ExitStack() as stack:
             file_arguments = self._create_httpx_file_argument(_source, stack)
 
             kwargs = {
                 "method": "post",
-                "url": f"/datasets/stream",
+                "url": "/datasets/stream",
                 "timeout": timeout,
                 "file": file_arguments,
                 "params": dict(
                     name=name,
                 ),
             }
 
             result = self.client.request(**kwargs)  # type: ignore[arg-type]
         return Dataset(**result)
 
-
     def download_dataset_as_stream(
         self,
         id: str,
         destination: Optional[
             Union[str, "FileLikeInterface[bytes]", "FileLikeInterface[str]"]
         ] = None,
         *,
@@ -373,15 +313,14 @@
             id,
             destination=destination,
             timeout=timeout,
             filetype=filetype,
             from_download_as_stream=True,
         )
 
-
     def download_dataset(
         self,
         id: str,
         destination: Optional[
             Union[str, "FileLikeInterface[bytes]", "FileLikeInterface[str]"]
         ] = None,
         filetype: Optional[FileType] = None,
@@ -681,15 +620,16 @@
         if isinstance(source, str):
             source = [source]
 
         if isinstance(source, list):
             # List of files to upload as one dataset.
             # This is especially useful for large parquet files.
             return [
-                ("file", stack.enter_context(open(file_path, "rb"))) for file_path in source
+                ("file", stack.enter_context(open(file_path, "rb")))
+                for file_path in source
             ]
 
         raise TypeError(
             f"Expected source to be a string or a buffer, got {type(source)} instead."
         )
 
     def _split_parquet_file(self, content: bytes, parquet_dir: Path, count: int) -> int:
@@ -739,1364 +679,1001 @@
         if after:
             count = self._split_parquet_file(after, parquet_dir, count + 1)
 
         return count  # to be able to continue on the same file
 
     def find_all_datasets_by_user(
         self,
-
-
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  List[Dataset] :
-        """List all datasets of the current_user.
-        """
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> List[Dataset]:
+        """List all datasets of the current_user."""
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
-        args: List[Any] = [
-        ]
-
+        args: List[Any] = []
 
         return _Datasets(self.client).find_all_datasets_by_user(*args, **kwargs)
 
-
-
-
     def get_dataset(
         self,
-            id: str,
-
-
+        id: str,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  Dataset :
-        """Get a dataset.
-        """
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> Dataset:
+        """Get a dataset."""
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
-                id,
+            id,
         ]
 
-
         return _Datasets(self.client).get_dataset(*args, **kwargs)
 
-
-
     def patch_dataset(
         self,
-                request: PatchDataset,
-
-            id: str,
-
-
+        request: PatchDataset,
+        id: str,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  Dataset :
-        """Modify a dataset.
-        """
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> Dataset:
+        """Modify a dataset."""
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
             request,
-                id,
+            id,
         ]
 
-
         return _Datasets(self.client).patch_dataset(*args, **kwargs)
 
-
-
     def analyze_dataset(
         self,
-            id: str,
-
-
+        id: str,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  Dataset :
-        """Start the analysis of a dataset.
-        """
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> Dataset:
+        """Start the analysis of a dataset."""
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
-                id,
+            id,
         ]
 
-
         return _Datasets(self.client).analyze_dataset(*args, **kwargs)
 
-
-
     def get_dataset_correlations(
         self,
-            id: str,
-
-
+        id: str,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  Any :
-        """Get a dataset's correlations.
-        """
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> Any:
+        """Get a dataset's correlations."""
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
-                id,
+            id,
         ]
 
-
         return _Datasets(self.client).get_dataset_correlations(*args, **kwargs)
 
 
-
-
-
 class Health:
     def __init__(self, client: "ApiClient") -> None:
         self.client = client
 
-
-
-
     def get_root(
         self,
-
-
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  Any :
-        """Verify server health.
-        """
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> Any:
+        """Verify server health."""
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
-        args: List[Any] = [
-        ]
-
+        args: List[Any] = []
 
         return _Health(self.client).get_root(*args, **kwargs)
 
-
-
     def get_health(
         self,
-
-
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  Any :
-        """Verify server health.
-        """
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> Any:
+        """Verify server health."""
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
-        args: List[Any] = [
-        ]
-
+        args: List[Any] = []
 
         return _Health(self.client).get_health(*args, **kwargs)
 
-
-
     def get_health_db(
         self,
-
-
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  Any :
-        """Verify connection to the db health.
-        """
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> Any:
+        """Verify connection to the db health."""
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
-        args: List[Any] = [
-        ]
-
+        args: List[Any] = []
 
         return _Health(self.client).get_health_db(*args, **kwargs)
 
 
-
 class Jobs:
     def __init__(self, client: "ApiClient") -> None:
         self.client = client
 
-
-
-
     def find_all_jobs_by_user(
         self,
-
-            nb_days: Optional[int] = None,
-
+        nb_days: Optional[int] = None,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  List[GenericJob] :
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> List[GenericJob]:
         """Retrieve all jobs executed by the current user.
 
         Jobs are filtered by execution date, by default only the last 5 days are displayed,
         a parameter can be provided to go further back in time.
         """
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
-                nb_days,
+            nb_days,
         ]
 
-
         return _Jobs(self.client).find_all_jobs_by_user(*args, **kwargs)
 
-
-
     def create_full_avatarization_job(
         self,
-                request: AvatarizationJobCreate,
-
-
-
+        request: AvatarizationJobCreate,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  AvatarizationJob :
-        """Create an avatarization job, then calculate metrics.
-        """
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> AvatarizationJob:
+        """Create an avatarization job, then calculate metrics."""
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
             request,
         ]
 
-
         return _Jobs(self.client).create_full_avatarization_job(*args, **kwargs)
 
-
-
     def cancel_job(
         self,
-            id: str,
-
-
+        id: str,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  GenericJob :
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> GenericJob:
         """Cancel any kind of job.
 
         If the job hadn't been started yet, revoke it.
         If the job is ongoing, gently kill it.
         If the job is done, do nothing.
         """
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
-                id,
+            id,
         ]
 
-
         return _Jobs(self.client).cancel_job(*args, **kwargs)
 
-
-
     def create_avatarization_job(
         self,
-                request: AvatarizationJobCreate,
-
-
-
+        request: AvatarizationJobCreate,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  AvatarizationJob :
-        """Create an avatarization job.
-        """
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> AvatarizationJob:
+        """Create an avatarization job."""
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
             request,
         ]
 
-
         return _Jobs(self.client).create_avatarization_job(*args, **kwargs)
 
-
-
     def create_avatarization_batch_job(
         self,
-                request: AvatarizationBatchJobCreate,
-
-
-
+        request: AvatarizationBatchJobCreate,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  AvatarizationBatchJob :
-        """Create an avatarization batch job.
-        """
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> AvatarizationBatchJob:
+        """Create an avatarization batch job."""
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
             request,
         ]
 
-
         return _Jobs(self.client).create_avatarization_batch_job(*args, **kwargs)
 
-
-
     def create_avatarization_with_time_series_job(
         self,
-                request: AvatarizationWithTimeSeriesJobCreate,
-
-
-
+        request: AvatarizationWithTimeSeriesJobCreate,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  AvatarizationWithTimeSeriesJob :
-        """Create an avatarization with time series job.
-        """
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> AvatarizationWithTimeSeriesJob:
+        """Create an avatarization with time series job."""
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
             request,
         ]
 
-
-        return _Jobs(self.client).create_avatarization_with_time_series_job(*args, **kwargs)
-
-
+        return _Jobs(self.client).create_avatarization_with_time_series_job(
+            *args, **kwargs
+        )
 
     def create_avatarization_multi_table_job(
         self,
-                request: AvatarizationMultiTableJobCreate,
-
-
-
+        request: AvatarizationMultiTableJobCreate,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  AvatarizationMultiTableJob :
-        """Create an avatarization for relational data.
-        """
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> AvatarizationMultiTableJob:
+        """Create an avatarization for relational data."""
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
             request,
         ]
 
-
         return _Jobs(self.client).create_avatarization_multi_table_job(*args, **kwargs)
 
-
-
     def create_signal_metrics_job(
         self,
-                request: SignalMetricsJobCreate,
-
-
-
+        request: SignalMetricsJobCreate,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  SignalMetricsJob :
-        """Create a signal metrics job.
-        """
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> SignalMetricsJob:
+        """Create a signal metrics job."""
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
             request,
         ]
 
-
         return _Jobs(self.client).create_signal_metrics_job(*args, **kwargs)
 
-
-
     def create_privacy_metrics_job(
         self,
-                request: PrivacyMetricsJobCreate,
-
-
-
+        request: PrivacyMetricsJobCreate,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  PrivacyMetricsJob :
-        """Create a privacy metrics job.
-        """
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> PrivacyMetricsJob:
+        """Create a privacy metrics job."""
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
             request,
         ]
 
-
         return _Jobs(self.client).create_privacy_metrics_job(*args, **kwargs)
 
-
-
     def create_privacy_metrics_batch_job(
         self,
-                request: PrivacyMetricsBatchJobCreate,
-
-
-
+        request: PrivacyMetricsBatchJobCreate,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  PrivacyMetricsBatchJob :
-        """Create a privacy metrics batch job.
-        """
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> PrivacyMetricsBatchJob:
+        """Create a privacy metrics batch job."""
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
             request,
         ]
 
-
         return _Jobs(self.client).create_privacy_metrics_batch_job(*args, **kwargs)
 
-
-
     def create_privacy_metrics_time_series_job(
         self,
-                request: PrivacyMetricsWithTimeSeriesJobCreate,
-
-
-
+        request: PrivacyMetricsWithTimeSeriesJobCreate,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  PrivacyMetricsWithTimeSeriesJob :
-        """Create a privacy metrics with time series job.
-        """
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> PrivacyMetricsWithTimeSeriesJob:
+        """Create a privacy metrics with time series job."""
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
             request,
         ]
 
-
-        return _Jobs(self.client).create_privacy_metrics_time_series_job(*args, **kwargs)
-
-
+        return _Jobs(self.client).create_privacy_metrics_time_series_job(
+            *args, **kwargs
+        )
 
     def create_signal_metrics_time_series_job(
         self,
-                request: SignalMetricsWithTimeSeriesJobCreate,
-
-
-
+        request: SignalMetricsWithTimeSeriesJobCreate,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  SignalMetricsWithTimeSeriesJob :
-        """Create a signal metrics with time series job.
-        """
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> SignalMetricsWithTimeSeriesJob:
+        """Create a signal metrics with time series job."""
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
             request,
         ]
 
-
         return _Jobs(self.client).create_signal_metrics_time_series_job(*args, **kwargs)
 
-
-
     def create_signal_metrics_batch_job(
         self,
-                request: SignalMetricsBatchJobCreate,
-
-
-
+        request: SignalMetricsBatchJobCreate,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  SignalMetricsBatchJob :
-        """Create a signal metrics batch job.
-        """
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> SignalMetricsBatchJob:
+        """Create a signal metrics batch job."""
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
             request,
         ]
 
-
         return _Jobs(self.client).create_signal_metrics_batch_job(*args, **kwargs)
 
-
-
     def create_privacy_metrics_multi_table_job(
         self,
-                request: PrivacyMetricsMultiTableJobCreate,
-
-
-
+        request: PrivacyMetricsMultiTableJobCreate,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  PrivacyMetricsMultiTableJob :
-        """Create a privacy metrics job.
-        """
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> PrivacyMetricsMultiTableJob:
+        """Create a privacy metrics job."""
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
             request,
         ]
 
-
-        return _Jobs(self.client).create_privacy_metrics_multi_table_job(*args, **kwargs)
-
-
+        return _Jobs(self.client).create_privacy_metrics_multi_table_job(
+            *args, **kwargs
+        )
 
     def create_privacy_metrics_geolocation_job(
         self,
-                request: PrivacyMetricsGeolocationJobCreate,
-
-
-
+        request: PrivacyMetricsGeolocationJobCreate,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  PrivacyMetricsGeolocationJob :
-        """Create a geolocation privacy metrics job.
-        """
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> PrivacyMetricsGeolocationJob:
+        """Create a geolocation privacy metrics job."""
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
             request,
         ]
 
-
-        return _Jobs(self.client).create_privacy_metrics_geolocation_job(*args, **kwargs)
-
-
+        return _Jobs(self.client).create_privacy_metrics_geolocation_job(
+            *args, **kwargs
+        )
 
     def get_privacy_metrics_geolocation_job(
         self,
-            id: str,
-
-
+        id: str,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  PrivacyMetricsGeolocationJob :
-        """Get a geolocation privacy metrics job.
-        """
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> PrivacyMetricsGeolocationJob:
+        """Get a geolocation privacy metrics job."""
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
-                id,
+            id,
         ]
 
-
         return _Jobs(self.client).get_privacy_metrics_geolocation_job(*args, **kwargs)
 
-
-
     def get_avatarization_job(
         self,
-            id: str,
-
-
+        id: str,
         *,
-        per_request_timeout : Optional[int] = DEFAULT_TIMEOUT,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  AvatarizationJob :
-        """Get an avatarization job.
-        """
+        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> AvatarizationJob:
+        """Get an avatarization job."""
 
         kwargs: Dict[str, Any] = {
-            "per_request_timeout" : per_request_timeout,
+            "per_request_timeout": per_request_timeout,
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
-                id,
+            id,
         ]
 
-
         return _Jobs(self.client).get_avatarization_job(*args, **kwargs)
 
-
-
     def get_avatarization_batch_job(
         self,
-            id: str,
-
-
+        id: str,
         *,
-        per_request_timeout : Optional[int] = DEFAULT_TIMEOUT,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  AvatarizationBatchJob :
-        """Get an avatarization batch job.
-        """
+        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> AvatarizationBatchJob:
+        """Get an avatarization batch job."""
 
         kwargs: Dict[str, Any] = {
-            "per_request_timeout" : per_request_timeout,
+            "per_request_timeout": per_request_timeout,
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
-                id,
+            id,
         ]
 
-
         return _Jobs(self.client).get_avatarization_batch_job(*args, **kwargs)
 
-
-
     def get_avatarization_time_series_job(
         self,
-            id: str,
-
-
+        id: str,
         *,
-        per_request_timeout : Optional[int] = DEFAULT_TIMEOUT,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  AvatarizationWithTimeSeriesJob :
-        """Get an avatarization time series job.
-        """
+        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> AvatarizationWithTimeSeriesJob:
+        """Get an avatarization time series job."""
 
         kwargs: Dict[str, Any] = {
-            "per_request_timeout" : per_request_timeout,
+            "per_request_timeout": per_request_timeout,
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
-                id,
+            id,
         ]
 
-
         return _Jobs(self.client).get_avatarization_time_series_job(*args, **kwargs)
 
-
-
     def get_avatarization_multi_table_job(
         self,
-            id: str,
-
-
+        id: str,
         *,
-        per_request_timeout : Optional[int] = DEFAULT_TIMEOUT,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  AvatarizationMultiTableJob :
-        """Get a multi table avatarization job.
-        """
+        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> AvatarizationMultiTableJob:
+        """Get a multi table avatarization job."""
 
         kwargs: Dict[str, Any] = {
-            "per_request_timeout" : per_request_timeout,
+            "per_request_timeout": per_request_timeout,
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
-                id,
+            id,
         ]
 
-
         return _Jobs(self.client).get_avatarization_multi_table_job(*args, **kwargs)
 
-
-
     def get_signal_metrics(
         self,
-            id: str,
-
-
+        id: str,
         *,
-        per_request_timeout : Optional[int] = DEFAULT_TIMEOUT,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  SignalMetricsJob :
-        """Get a signal metrics job.
-        """
+        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> SignalMetricsJob:
+        """Get a signal metrics job."""
 
         kwargs: Dict[str, Any] = {
-            "per_request_timeout" : per_request_timeout,
+            "per_request_timeout": per_request_timeout,
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
-                id,
+            id,
         ]
 
-
         return _Jobs(self.client).get_signal_metrics(*args, **kwargs)
 
-
-
     def get_signal_metrics_batch_job(
         self,
-            id: str,
-
-
+        id: str,
         *,
-        per_request_timeout : Optional[int] = DEFAULT_TIMEOUT,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  SignalMetricsBatchJob :
-        """Get a signal metrics batch job.
-        """
+        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> SignalMetricsBatchJob:
+        """Get a signal metrics batch job."""
 
         kwargs: Dict[str, Any] = {
-            "per_request_timeout" : per_request_timeout,
+            "per_request_timeout": per_request_timeout,
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
-                id,
+            id,
         ]
 
-
         return _Jobs(self.client).get_signal_metrics_batch_job(*args, **kwargs)
 
-
-
     def get_signal_metrics_time_series_job(
         self,
-            id: str,
-
-
+        id: str,
         *,
-        per_request_timeout : Optional[int] = DEFAULT_TIMEOUT,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  SignalMetricsWithTimeSeriesJob :
-        """Get a signal metrics time series job.
-        """
+        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> SignalMetricsWithTimeSeriesJob:
+        """Get a signal metrics time series job."""
 
         kwargs: Dict[str, Any] = {
-            "per_request_timeout" : per_request_timeout,
+            "per_request_timeout": per_request_timeout,
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
-                id,
+            id,
         ]
 
-
         return _Jobs(self.client).get_signal_metrics_time_series_job(*args, **kwargs)
 
-
-
     def get_privacy_metrics(
         self,
-            id: str,
-
-
+        id: str,
         *,
-        per_request_timeout : Optional[int] = DEFAULT_TIMEOUT,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  PrivacyMetricsJob :
-        """Get a privacy metrics job.
-        """
+        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> PrivacyMetricsJob:
+        """Get a privacy metrics job."""
 
         kwargs: Dict[str, Any] = {
-            "per_request_timeout" : per_request_timeout,
+            "per_request_timeout": per_request_timeout,
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
-                id,
+            id,
         ]
 
-
         return _Jobs(self.client).get_privacy_metrics(*args, **kwargs)
 
-
-
     def get_privacy_metrics_batch_job(
         self,
-            id: str,
-
-
+        id: str,
         *,
-        per_request_timeout : Optional[int] = DEFAULT_TIMEOUT,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  PrivacyMetricsBatchJob :
-        """Get a privacy metrics batch job.
-        """
+        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> PrivacyMetricsBatchJob:
+        """Get a privacy metrics batch job."""
 
         kwargs: Dict[str, Any] = {
-            "per_request_timeout" : per_request_timeout,
+            "per_request_timeout": per_request_timeout,
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
-                id,
+            id,
         ]
 
-
         return _Jobs(self.client).get_privacy_metrics_batch_job(*args, **kwargs)
 
-
-
     def get_privacy_metrics_time_series_job(
         self,
-            id: str,
-
-
+        id: str,
         *,
-        per_request_timeout : Optional[int] = DEFAULT_TIMEOUT,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  PrivacyMetricsWithTimeSeriesJob :
-        """Get a privacy metrics time series job.
-        """
+        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> PrivacyMetricsWithTimeSeriesJob:
+        """Get a privacy metrics time series job."""
 
         kwargs: Dict[str, Any] = {
-            "per_request_timeout" : per_request_timeout,
+            "per_request_timeout": per_request_timeout,
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
-                id,
+            id,
         ]
 
-
         return _Jobs(self.client).get_privacy_metrics_time_series_job(*args, **kwargs)
 
-
-
     def get_privacy_metrics_multi_table_job(
         self,
-            id: str,
-
-
+        id: str,
         *,
-        per_request_timeout : Optional[int] = DEFAULT_TIMEOUT,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  PrivacyMetricsMultiTableJob :
-        """Get a privacy metrics multi table job.
-        """
+        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> PrivacyMetricsMultiTableJob:
+        """Get a privacy metrics multi table job."""
 
         kwargs: Dict[str, Any] = {
-            "per_request_timeout" : per_request_timeout,
+            "per_request_timeout": per_request_timeout,
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
-                id,
+            id,
         ]
 
-
         return _Jobs(self.client).get_privacy_metrics_multi_table_job(*args, **kwargs)
 
 
-
 class Metrics:
     def __init__(self, client: "ApiClient") -> None:
         self.client = client
 
-
-
-
     def get_job_projections(
         self,
-            job_id: str,
-
-
+        job_id: str,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  Projections :
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> Projections:
         """Get the projections of records and avatars in 3D.
 
         See https://saiph.readthedocs.io/en/latest/ for more information.
 
         Arguments
         ---------
             job_id:
                 avatarization or privacy job id used to fit the model
         """
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
-                job_id,
+            job_id,
         ]
 
-
         return _Metrics(self.client).get_job_projections(*args, **kwargs)
 
-
-
     def get_variable_contributions(
         self,
-
-            job_id: str,
-            dataset_id: str,
-
+        job_id: str,
+        dataset_id: str,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  Contributions :
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> Contributions:
         """Get the contributions of the dataset variables within the fitted space.
 
         See https://saiph.readthedocs.io/en/latest for more information.
 
         Arguments
         ---------
             job_id:
                 avatarization or privacy job id used to fit the model
         """
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
-                job_id,
-                dataset_id,
+            job_id,
+            dataset_id,
         ]
 
-
         return _Metrics(self.client).get_variable_contributions(*args, **kwargs)
 
-
-
     def get_explained_variance(
         self,
-            job_id: str,
-
-
+        job_id: str,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  ExplainedVariance :
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> ExplainedVariance:
         """Get the explained variance of records.
 
         See https://saiph.readthedocs.io/en/latest/ for more information.
 
         Arguments
         ---------
             job_id:
                 avatarization or privacy job id used to fit the model
         """
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
-                job_id,
+            job_id,
         ]
 
-
         return _Metrics(self.client).get_explained_variance(*args, **kwargs)
 
 
-
 class Reports:
     def __init__(self, client: "ApiClient") -> None:
         self.client = client
 
-
-
-
     def create_report(
         self,
-                request: ReportCreate,
-
-
-
+        request: ReportCreate,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  Report :
-        """Create an anonymization report.
-        """
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> Report:
+        """Create an anonymization report."""
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
             request,
         ]
 
-
         return _Reports(self.client).create_report(*args, **kwargs)
 
-
-
     def get_report(
         self,
-            id: str,
-
-
+        id: str,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  Report :
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> Report:
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
-                id,
+            id,
         ]
 
-
         return _Reports(self.client).get_report(*args, **kwargs)
 
-
-
     def download_report(
         self,
-            id: str,
-
-
+        id: str,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  Any :
-        """Download a report.
-        """
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> Any:
+        """Download a report."""
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
-                id,
+            id,
         ]
 
-
         return _Reports(self.client).download_report(*args, **kwargs)
 
-
-
     def create_report_from_data(
         self,
-                request: ReportFromDataCreate,
-
-
-
+        request: ReportFromDataCreate,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  Report :
-        """Create an anonymization report without avatarization job.
-        """
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> Report:
+        """Create an anonymization report without avatarization job."""
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
             request,
         ]
 
-
         return _Reports(self.client).create_report_from_data(*args, **kwargs)
 
-
-
     def create_report_from_batch(
         self,
-                request: ReportFromBatchCreate,
-
-
-
+        request: ReportFromBatchCreate,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  Report :
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> Report:
         """Create an anonymization report from batch job identifiers.
 
         The report will be generated with the worst privacy_metrics and the mean signal_metrics.
         """
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
             request,
         ]
 
-
         return _Reports(self.client).create_report_from_batch(*args, **kwargs)
 
-
-
     def create_geolocation_privacy_report(
         self,
-                request: ReportGeolocationPrivacyCreate,
-
-
-
+        request: ReportGeolocationPrivacyCreate,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  Report :
-        """Create an anonymization report without avatarization job.
-        """
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> Report:
+        """Create an anonymization report without avatarization job."""
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
             request,
         ]
 
-
         return _Reports(self.client).create_geolocation_privacy_report(*args, **kwargs)
 
 
-
 class Stats:
     def __init__(self, client: "ApiClient") -> None:
         self.client = client
 
-
-
-
     def get_cluster_stats(
         self,
-
-
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  ClusterStats :
-        """Get insights into the cluster's usage.
-        """
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> ClusterStats:
+        """Get insights into the cluster's usage."""
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
-        args: List[Any] = [
-        ]
-
+        args: List[Any] = []
 
         return _Stats(self.client).get_cluster_stats(*args, **kwargs)
 
 
-
 class Users:
     def __init__(self, client: "ApiClient") -> None:
         self.client = client
 
-
-
-
     def find_users(
         self,
-
-            email: Optional[str] = None,
-            username: Optional[str] = None,
-
+        email: Optional[str] = None,
+        username: Optional[str] = None,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  List[User] :
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> List[User]:
         """Get users, optionally filtering them by username or email.
 
         This endpoint is protected with rate limiting.
         """
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
-                email,
-                username,
+            email,
+            username,
         ]
 
-
         return _Users(self.client).find_users(*args, **kwargs)
 
-
-
     def create_user(
         self,
-                request: CreateUser,
-
-
-
+        request: CreateUser,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  User :
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> User:
         """Create a user.
 
         This endpoint is protected with rate limiting.
         """
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
             request,
         ]
 
-
         return _Users(self.client).create_user(*args, **kwargs)
 
-
-
     def get_me(
         self,
-
-
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  User :
-        """Get my own user.
-        """
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> User:
+        """Get my own user."""
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
-        args: List[Any] = [
-        ]
-
+        args: List[Any] = []
 
         return _Users(self.client).get_me(*args, **kwargs)
 
-
-
     def get_user(
         self,
-            id: str,
-
-
+        id: str,
         *,
-        timeout : Optional[int] = DEFAULT_TIMEOUT,
-            ) ->  User :
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> User:
         """Get a user by id.
 
         This endpoint is protected with rate limiting.
         """
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
-
         }
 
         args: List[Any] = [
-                id,
+            id,
         ]
 
-
         return _Users(self.client).get_user(*args, **kwargs)
 
 
-
 class PandasIntegration:
     def __init__(self, client: "ApiClient") -> None:
         self.client = client
 
     def upload_dataframe(
         self,
         request: "pd.DataFrame",
         name: Optional[str] = None,
         *,
         timeout: Optional[int] = DEFAULT_TIMEOUT,
         identifier_variables: List[str] = [],
         **kwargs: Dict[str, Any],  # to collect should_stream
     ) -> Dataset:
+
         if "should_stream" in kwargs:
             warnings.warn(
                 "The `should_stream` parameter is deprecated and will be removed in a future version. "
                 "All uploads are now streamed by default.",
                 DeprecationWarning,
             )
 
@@ -2193,16 +1770,14 @@
             df[name] = df[name].astype(dtype)
 
         df[datetime_columns] = df[datetime_columns].astype("datetime64[ns]")
 
         return df
 
 
-
-
 class Pipelines:
     def __init__(self, client: "ApiClient") -> None:
         self.client = client
 
     def avatarization_pipeline_with_processors(
         self,
         request: AvatarizationPipelineCreate,
@@ -2245,16 +1820,19 @@
                 f"Got error during the avatarization job: {avatarization_job.error_message}"
             )
 
         if (
             avatarization_job.status == JobStatus.pending
             or not avatarization_job.result
         ):
-            raise Timeout(f"The avatarization job '{avatarization_job.id}' timed out."""
-                          """Try increasing the timeout with the `timeout` parameter.""")
+            raise Timeout(
+                f"The avatarization job '{avatarization_job.id}' timed out."
+                ""
+                """Try increasing the timeout with the `timeout` parameter."""
+            )
 
         # Download the dataframe, postprocess it and upload the new dataframe
         sensitive_unshuffled_avatars = (
             self.client.pandas_integration.download_dataframe(
                 str(avatarization_job.result.sensitive_unshuffled_avatars_datasets.id),
                 timeout=timeout,
             )
@@ -2291,32 +1869,40 @@
         print(f"launching signal metrics job with id={signal_job.id}")
 
         # Get the job results
         signal_job = self.client.jobs.get_signal_metrics(
             str(signal_job.id), timeout=timeout, per_request_timeout=per_request_timeout
         )
         privacy_job = self.client.jobs.get_privacy_metrics(
-            str(privacy_job.id), timeout=timeout, per_request_timeout=per_request_timeout
+            str(privacy_job.id),
+            timeout=timeout,
+            per_request_timeout=per_request_timeout,
         )
         if signal_job.status == JobStatus.failure:
             raise Exception(
                 f"Got error during the signal metrics job: {signal_job.error_message}"
             )
         if privacy_job.status == JobStatus.failure:
             raise Exception(
                 f"Got error during the privacy metrics job: {privacy_job.error_message}"
             )
 
         if signal_job.status == JobStatus.pending or not signal_job.result:
-            raise Timeout(f"The signal metrics job '{signal_job.id}' timed out."""
-                          """Try increasing the timeout with the `timeout` parameter.""")
+            raise Timeout(
+                f"The signal metrics job '{signal_job.id}' timed out."
+                ""
+                """Try increasing the timeout with the `timeout` parameter."""
+            )
 
         if privacy_job.status == JobStatus.pending or not privacy_job.result:
-            raise Timeout(f"The privacy metrics job '{privacy_job.id}' timed out."""
-                          """Try increasing the timeout with the `timeout` parameter.""")
+            raise Timeout(
+                f"The privacy metrics job '{privacy_job.id}' timed out."
+                ""
+                """Try increasing the timeout with the `timeout` parameter."""
+            )
 
         # Shuffle sensitive_unshuffled_avatars for security reasons
         random_gen = np.random.default_rng()
         map = random_gen.permutation(sensitive_unshuffled_avatars.index.values).tolist()
         post_processed_avatars = sensitive_unshuffled_avatars.iloc[map].reset_index(
             drop=True
         )
@@ -2326,14 +1912,15 @@
             signal_metrics=signal_job.result,
             post_processed_avatars=post_processed_avatars,
             avatarization_job_id=avatarization_job.id,
             signal_job_id=signal_job.id,
             privacy_job_id=privacy_job.id,
         )
 
+
 def upload_batch_and_get_order(
     client: "ApiClient",
     training: pd.DataFrame,
     splits: List[pd.DataFrame],
     timeout: int = DEFAULT_TIMEOUT,
 ) -> Tuple[UUID, List[UUID], Dict[UUID, pd.Index]]:
     """Upload batches to the server
@@ -2444,8 +2031,9 @@
         )
         split.index = order[original_dataset_id]
         split_dfs.append(split)
 
     concatenated = pd.concat([training_df] + split_dfs).sort_index()
     return concatenated
 
+
 # This file has been generated - DO NOT MODIFY
```

### Comparing `octopize_avatar-0.7.2/avatars/api_batch_test.py` & `octopize_avatar-0.7.3/avatars/api_batch_test.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.2/avatars/api_custom_dataset_methods_test.py` & `octopize_avatar-0.7.3/avatars/api_custom_dataset_methods_test.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,29 +1,17 @@
 import io
 import tempfile
 from pathlib import Path
-from typing import (
-    IO,
-    Any,
-    Dict,
-    Iterator,
-    Literal,
-    Optional,
-    Sequence,
-    Tuple,
-    Union,
-    cast,
-)
+from typing import IO, Any, Dict, Iterator, Union
 from unittest.mock import patch
 from uuid import uuid4
 
 import httpx
 import pandas as pd
 import pytest
-from pydantic import BaseModel
 
 from avatars.api import Datasets, PandasIntegration
 from avatars.conftest import RequestHandle, api_client_factory
 from avatars.exceptions import InvalidFileType
 from avatars.models import Dataset, FileType
 
 TEST_MAX_BYTES_PER_FILE = 1 * 1024  # 1 KB
@@ -31,15 +19,15 @@
 
 @pytest.fixture(scope="session")
 def dataset_json() -> dict[str, Any]:
     return {
         "id": "5e27da4a-a5c5-458b-988e-ef2c66f545d6",
         "hash": "aeedab1ee7a1043753c9ab768594bc8420d7b85491d0be9421edc3813c237f4c",
         "name": "upload",
-        "download_url": "http://localhost:8000/datasets/5e27da4a-a5c5-458b-988e-ef2c66f545d6/download",
+        "download_url": "http://localhost:8000/datasets/5e27da4a-a5c5-458b-988e-ef2c66f545d6/download",  # noqa: E501
         "nb_lines": 1,
         "nb_dimensions": 2,
         "filetype": "csv",
     }
 
 
 @pytest.fixture(scope="session")
@@ -158,16 +146,17 @@
             content = large_csv if buffer == io.BytesIO else large_csv.decode()
             filled_buffer = buffer(content)
         else:
             filled_buffer = buffer(pd.read_csv(io.BytesIO(large_csv)).to_parquet())
 
         filled_buffer.seek(0)
         # TODO: Remove the type ignore when request is deprecated
-        # error: Argument "source" to "create_dataset" of "Datasets" has incompatible type "Union[BytesIO, StringIO]"; expected
-        # "Union[str, list[str], FileLikeInterface[bytes], None]"  [arg-type]
+        # error: Argument "source" to "create_dataset" of "Datasets"
+        #        has incompatible type "Union[BytesIO, StringIO]"; expected
+        #        "Union[str, list[str], FileLikeInterface[bytes], None]"  [arg-type]
         res = Datasets(client).create_dataset(source=filled_buffer)  # type: ignore[arg-type]
         assert res.id
 
     @patch("avatars.api.MAX_BYTES_PER_FILE", TEST_MAX_BYTES_PER_FILE)
     @pytest.mark.parametrize(
         "filetype, mode",
         [
@@ -477,15 +466,18 @@
         download_dataset_response: RequestHandle,
     ) -> None:
         """Verify that the method raises an error with an invalid destination."""
         client = api_client_factory(download_dataset_response)
         with pytest.raises(
             TypeError, match="Expected destination to be a string or a buffer"
         ):
-            Datasets(client).download_dataset(str(uuid4()), destination=123)  # type: ignore[arg-type]
+            Datasets(client).download_dataset(
+                str(uuid4()),
+                destination=123,  # type: ignore[arg-type]
+            )
 
     @pytest.mark.parametrize(
         "destination",
         [
             pytest.param(io.StringIO(), id="io.StringIO"),
             pytest.param("file_handle"),
         ],
```

### Comparing `octopize_avatar-0.7.2/avatars/base_client.py` & `octopize_avatar-0.7.3/avatars/base_client.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 from __future__ import annotations
 
 import itertools
 import logging
 import time
-from datetime import datetime
 from contextlib import contextmanager
 from copy import deepcopy
 from dataclasses import dataclass, field
+from datetime import datetime
 from io import BytesIO
 from json import loads as json_loads
 from typing import (
     TYPE_CHECKING,
     Any,
     Callable,
     Dict,
@@ -27,21 +27,15 @@
 )
 
 import httpx
 from httpx import ReadTimeout, Request, Response, WriteTimeout
 from pydantic import BaseModel
 
 from avatars.models import JobStatus
-from avatars.utils import (
-    ensure_valid,
-    pop_or,
-    validated,
-    remove_optionals,
-)
-
+from avatars.utils import ensure_valid, pop_or, remove_optionals, validated
 
 if TYPE_CHECKING:
     from avatars._typing import FileLikeInterface, HttpxFile
 
 logger = logging.getLogger(__name__)
 
 DEFAULT_RETRY_TIMEOUT = 60
@@ -639,9 +633,9 @@
             return cast(ResponseClass, info.response)
 
     def __str__(self) -> str:
         return ", ".join(
             f"ApiClient(base_url={self.base_url}"
             f"timeout={self.timeout}"
             f"should_verify_ssl={self.should_verify_ssl}"
-            "verify_auth={self.verify_auth})"
+            f"verify_auth={self.verify_auth})"
         )
```

### Comparing `octopize_avatar-0.7.2/avatars/client_test.py` & `octopize_avatar-0.7.3/avatars/client_test.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,9 @@
-import logging
 import unittest
-from typing import Any, Callable, Dict, Optional
+from typing import Any
 from unittest.mock import Mock, patch
 
 import httpx
 import pytest
 
 from avatars.client import ApiClient
 from avatars.conftest import RequestHandle, api_client_factory, mock_httpx_client
@@ -18,26 +17,26 @@
         api_client.request("GET", base_url, **kwargs)
 
     # Verify default is set to True
     api_client = ApiClient(
         base_url=base_url, verify_auth=False, should_verify_compatibility=False
     )
     do_request(api_client)
-    assert mock_client.call_args.kwargs["verify"] == True
+    assert mock_client.call_args.kwargs["verify"] is True
     mock_client.reset_mock()
 
     # Verify that the should_verify_ssl parameter is passed to the httpx.Client
     api_client = ApiClient(
         base_url=base_url,
         should_verify_ssl=False,
         verify_auth=False,
         should_verify_compatibility=False,
     )
     do_request(api_client)
-    assert mock_client.call_args.kwargs["verify"] == False
+    assert mock_client.call_args.kwargs["verify"] is False
     mock_client.reset_mock()
 
 
 @pytest.mark.parametrize(
     "base_url",
     [
         '"https://test.com"',
@@ -64,15 +63,15 @@
         "most_recent_compatible_client": "1.0.0",
     }
 
     http_client = mock_httpx_client(
         handler=lambda request: httpx.Response(200, json=json)
     )
 
-    with pytest.warns(match="Client is not compatible") as checker:
+    with pytest.warns(match="Client is not compatible"):
         ApiClient(
             base_url="http://localhost:8000",
             http_client=http_client,
             verify_auth=False,
             should_verify_compatibility=True,
         )
```

### Comparing `octopize_avatar-0.7.2/avatars/conftest.py` & `octopize_avatar-0.7.3/avatars/conftest.py`

 * *Files 6% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 RequestHandle = Callable[[httpx.Request], httpx.Response]
 
 
 def mock_httpx_client(handler: Optional[RequestHandle] = None) -> httpx.Client:
     """Generate a HTTPX client with a MockTransport."""
 
     if handler is None:
-        handler = lambda request: httpx.Response(200, json={})
+        handler = lambda request: httpx.Response(200, json={})  # noqa: E731
 
     transport = httpx.MockTransport(handler)
     return httpx.Client(base_url="http://localhost:8000", transport=transport)
 
 
 def api_client_factory(handler: Optional[RequestHandle] = None) -> ApiClient:
     """Generate an API client with a mock transport.
```

### Comparing `octopize_avatar-0.7.2/avatars/lib/saferound.py` & `octopize_avatar-0.7.3/avatars/lib/saferound.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.2/avatars/lib/saferound_test.py` & `octopize_avatar-0.7.3/avatars/lib/saferound_test.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 """Original source: https://github.com/cgdeboer/iteround."""
 
 from collections import OrderedDict
-from typing import Any, NamedTuple
+from typing import NamedTuple
 from typing import OrderedDict as OrderedDictType
 from typing import Tuple
 
 import pytest
 
 from avatars.lib.saferound import RoundingStrategy, saferound
```

### Comparing `octopize_avatar-0.7.2/avatars/lib/split.py` & `octopize_avatar-0.7.3/avatars/lib/split.py`

 * *Files 1% similar despite different names*

```diff
@@ -14,15 +14,16 @@
 
 
     Arguments
     ---------
         df:
             dataframe to split into batch
         row_limit:
-            maximum number of rows per batch, this parameter determines the number of batches that will be created
+            maximum number of rows per batch, this parameter determines the
+            number of batches that will be created
 
     Keyword Arguments
     -----------------
         seed:
             seed of the random split for the batch
 
     Returns
```

### Comparing `octopize_avatar-0.7.2/avatars/lib/split_column_types_test.py` & `octopize_avatar-0.7.3/avatars/lib/split_column_types_test.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.2/avatars/lib/split_columns_types.py` & `octopize_avatar-0.7.3/avatars/lib/split_columns_types.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.2/avatars/lib/split_test.py` & `octopize_avatar-0.7.3/avatars/lib/split_test.py`

 * *Files 7% similar despite different names*

```diff
@@ -36,15 +36,16 @@
             "a": ["a", "b", "b", "a", "a", "b", "b", "a", "a", "b", "b", "a"],
             "b": ["A", "B", "B", "B", "A", "B", "B", "B", "A", "B", "B", "B"],
             "c": ["1", "2", "1", "1", "1", "2", "1", "1", "1", "2", "1", "1"],
         }
     )
     training, splits = get_split_for_batch(df, row_limit=6, seed=42)
 
-    # the two first record should be the same of the original data and the other ones should be randomly selected
+    # the two first record should be the same of the original data and the other ones
+    # should be randomly selected
     expected_training = pd.DataFrame(
         data={
             "a": ["a", "b", "b", "a", "a", "b"],
             "b": ["A", "B", "B", "B", "B", "B"],
             "c": ["1", "2", "1", "1", "1", "1"],
         },
         index=[0, 1, 10, 3, 7, 2],
```

### Comparing `octopize_avatar-0.7.2/avatars/models.py` & `octopize_avatar-0.7.3/avatars/models.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 # This file has been generated - DO NOT MODIFY
-# API Version : 1.1.2-fa473ed683daf6b108b4771008001137bcd8073d
+# API Version : 1.1.2-cc65e4fa4e46fdc93e7a375ce87a8202f1f59c1a
 
 
 from __future__ import annotations
 
 from datetime import datetime
 from enum import Enum
 from typing import Dict, List, Optional, Protocol, Union, runtime_checkable
@@ -11,337 +11,337 @@
 
 import pandas as pd
 from pydantic import BaseModel, ConfigDict, Field, RootModel
 from typing_extensions import Annotated, Literal
 
 # generated by datamodel-codegen:
 #   filename:  input.json
-#   timestamp: 2024-04-18T12:52:59+00:00
+#   timestamp: 2024-04-29T09:36:19+00:00
 
 
 class AnalysisStatus(Enum):
-    started = 'started'
-    done = 'done'
+    started = "started"
+    done = "done"
 
 
 class ClosestRatePercentageThresholdItem(RootModel[float]):
     root: Annotated[float, Field(ge=0.0, le=100.0)]
 
 
 class ClosestRateRatioThresholdItem(RootModel[float]):
     root: Annotated[float, Field(ge=0.0, le=1.0)]
 
 
 class ClusterStats(BaseModel):
-    n_active_tasks: Annotated[Optional[int], Field(title='N Active Tasks')] = None
+    n_active_tasks: Annotated[Optional[int], Field(title="N Active Tasks")] = None
     available_concurrency: Annotated[
-        Optional[int], Field(title='Available Concurrency')
+        Optional[int], Field(title="Available Concurrency")
     ] = None
     utilization_rate_100: Annotated[
-        Optional[int], Field(title='Utilization Rate 100')
+        Optional[int], Field(title="Utilization Rate 100")
     ] = None
-    status_message: Annotated[Optional[str], Field(title='Status Message')] = ''
+    status_message: Annotated[Optional[str], Field(title="Status Message")] = ""
 
 
 class ColumnStats(BaseModel):
-    label: Annotated[str, Field(title='Label')]
-    dtype: Annotated[str, Field(title='Dtype')]
-    missing_number: Annotated[int, Field(ge=0, title='Missing Number')]
+    label: Annotated[str, Field(title="Label")]
+    dtype: Annotated[str, Field(title="Dtype")]
+    missing_number: Annotated[int, Field(ge=0, title="Missing Number")]
     missing_proportion: Annotated[
-        float, Field(ge=0.0, le=1.0, title='Missing Proportion')
+        float, Field(ge=0.0, le=1.0, title="Missing Proportion")
     ]
-    unique_number: Annotated[int, Field(ge=0, title='Unique Number')]
+    unique_number: Annotated[int, Field(ge=0, title="Unique Number")]
     unique_proportion: Annotated[
-        float, Field(ge=0.0, le=1.0, title='Unique Proportion')
+        float, Field(ge=0.0, le=1.0, title="Unique Proportion")
     ]
-    mean: Annotated[Optional[float], Field(title='Mean')] = None
-    median: Annotated[Optional[float], Field(title='Median')] = None
+    mean: Annotated[Optional[float], Field(title="Mean")] = None
+    median: Annotated[Optional[float], Field(title="Median")] = None
     standard_deviation: Annotated[
-        Optional[float], Field(title='Standard Deviation')
+        Optional[float], Field(title="Standard Deviation")
     ] = None
-    mode: Annotated[Optional[Union[str, int, float]], Field(title='Mode')] = None
-    is_skewed: Annotated[Optional[bool], Field(title='Is Skewed')] = None
-    is_rare: Annotated[Optional[bool], Field(title='Is Rare')] = None
+    mode: Annotated[Optional[Union[str, int, float]], Field(title="Mode")] = None
+    is_skewed: Annotated[Optional[bool], Field(title="Is Skewed")] = None
+    is_rare: Annotated[Optional[bool], Field(title="Is Rare")] = None
 
 
 class ColumnType(Enum):
-    int = 'int'
-    bool = 'bool'
-    category = 'category'
-    float = 'float'
-    datetime = 'datetime'
+    int = "int"
+    bool = "bool"
+    category = "category"
+    float = "float"
+    datetime = "datetime"
 
 
 class CommonSignalMetricsParameters(BaseModel):
     pass
 
 
 class CompatibilityStatus(Enum):
-    compatible = 'compatible'
-    incompatible = 'incompatible'
-    unknown = 'unknown'
+    compatible = "compatible"
+    incompatible = "incompatible"
+    unknown = "unknown"
 
 
 class Contributions(BaseModel):
-    data: Annotated[Dict[str, Dict[str, float]], Field(title='Data')]
+    data: Annotated[Dict[str, Dict[str, float]], Field(title="Data")]
 
 
 class DatasetSummary(BaseModel):
-    stats: Annotated[List[ColumnStats], Field(title='Stats')]
-    probable_ids: Annotated[List[str], Field(title='Probable Ids')]
+    stats: Annotated[List[ColumnStats], Field(title="Stats")]
+    probable_ids: Annotated[List[str], Field(title="Probable Ids")]
 
 
 class ExcludeCategoricalMethod(Enum):
     """
     The method to exclude categorical column.
 
     There are several possible choices:
 
     - ``row_order``: **SENSITIVE** The excluded column will be linked to the original row order.
     This is a violation of privacy.
 
     - ``coordinate_similarity``: The excluded column will be linked by individual similarity.
     """
 
-    row_order = 'row_order'
-    coordinate_similarity = 'coordinate_similarity'
+    row_order = "row_order"
+    coordinate_similarity = "coordinate_similarity"
 
 
 class ExplainedVariance(BaseModel):
-    raw: Annotated[List[float], Field(title='Raw')]
-    ratio: Annotated[List[float], Field(title='Ratio')]
+    raw: Annotated[List[float], Field(title="Raw")]
+    ratio: Annotated[List[float], Field(title="Ratio")]
 
 
 class FPCAParameters(BaseModel):
     """
     Parameters of a FPCA projection.
     """
 
     nf: Annotated[
         int,
         Field(
-            description='Number of degrees of freedom kept by the projection.',
-            title='Nf',
+            description="Number of degrees of freedom kept by the projection.",
+            title="Nf",
         ),
     ]
-    projection_type: Annotated[Literal['fpca'], Field(title='Projection Type')] = 'fpca'
+    projection_type: Annotated[Literal["fpca"], Field(title="Projection Type")] = "fpca"
 
 
 class FileType(Enum):
-    csv = 'csv'
-    parquet = 'parquet'
+    csv = "csv"
+    parquet = "parquet"
 
 
 class FlattenParameters(BaseModel):
     """
     Parameters of a Flatten projection.
     """
 
-    projection_type: Annotated[Literal['flatten'], Field(title='Projection Type')] = (
-        'flatten'
+    projection_type: Annotated[Literal["flatten"], Field(title="Projection Type")] = (
+        "flatten"
     )
 
 
 class ForgottenPasswordRequest(BaseModel):
-    email: Annotated[str, Field(title='Email')]
+    email: Annotated[str, Field(title="Email")]
 
 
 class GenericParameters(BaseModel):
     pass
     model_config = ConfigDict(
-        extra='allow',
+        extra="allow",
     )
 
 
 class GenericResult(BaseModel):
     pass
 
 
 class GeolocationDensityParameters(BaseModel):
     """
     Parameters of a Geolocation Density projection.
     """
 
     projection_type: Annotated[
-        Literal['geolocation_density'], Field(title='Projection Type')
-    ] = 'geolocation_density'
+        Literal["geolocation_density"], Field(title="Projection Type")
+    ] = "geolocation_density"
     n_lon_to_compute: Annotated[
         Optional[int],
-        Field(description='Number of longitude to compute.', title='N Lon To Compute'),
+        Field(description="Number of longitude to compute.", title="N Lon To Compute"),
     ] = 100
     n_lat_to_compute: Annotated[
         Optional[int],
-        Field(description='Number of latitude to compute.', title='N Lat To Compute'),
+        Field(description="Number of latitude to compute.", title="N Lat To Compute"),
     ] = 100
 
 
 class GeolocationFeatures(Enum):
     """
     Features available for geolocation feature projection and metrics.
     """
 
-    length = 'length'
-    duration = 'duration'
-    speed = 'speed'
+    length = "length"
+    duration = "duration"
+    speed = "speed"
 
 
 class GeolocationFeaturesParameters(BaseModel):
     """
     Parameters of a Geolocation Features projection.
     """
 
     projection_type: Annotated[
-        Literal['geolocation_features'], Field(title='Projection Type')
-    ] = 'geolocation_features'
+        Literal["geolocation_features"], Field(title="Projection Type")
+    ] = "geolocation_features"
 
 
 class GeolocationPrivacyMetrics(BaseModel):
-    local_cloaking: Annotated[float, Field(title='Local Cloaking')]
-    hidden_rate: Annotated[float, Field(title='Hidden Rate')]
+    local_cloaking: Annotated[float, Field(title="Local Cloaking")]
+    hidden_rate: Annotated[float, Field(title="Hidden Rate")]
     specific_hidden_rate: Annotated[
-        Optional[float], Field(title='Specific Hidden Rate')
+        Optional[float], Field(title="Specific Hidden Rate")
     ] = None
     distance_to_closest: Annotated[
-        Optional[float], Field(title='Distance To Closest')
+        Optional[float], Field(title="Distance To Closest")
     ] = None
     closest_distances_ratio: Annotated[
-        Optional[float], Field(title='Closest Distances Ratio')
+        Optional[float], Field(title="Closest Distances Ratio")
     ] = None
-    closest_rate: Annotated[Optional[float], Field(title='Closest Rate')] = None
+    closest_rate: Annotated[Optional[float], Field(title="Closest Rate")] = None
     features_inference_rate: Annotated[
-        Optional[float], Field(title='Features Inference Rate')
+        Optional[float], Field(title="Features Inference Rate")
     ] = None
     features_correlation_protection_rate: Annotated[
-        Optional[float], Field(title='Features Correlation Protection Rate')
+        Optional[float], Field(title="Features Correlation Protection Rate")
     ] = None
     thresholded_signal_inference_rate: Annotated[
-        Optional[float], Field(title='Thresholded Signal Inference Rate')
+        Optional[float], Field(title="Thresholded Signal Inference Rate")
     ] = None
     trace_correlation_protection_rate: Annotated[
-        Optional[float], Field(title='Trace Correlation Protection Rate')
+        Optional[float], Field(title="Trace Correlation Protection Rate")
     ] = None
     inference_distances: Annotated[
-        Optional[List[float]], Field(title='Inference Distances')
+        Optional[List[float]], Field(title="Inference Distances")
     ] = None
 
 
 class TrainingFractionItem(RootModel[float]):
     root: Annotated[float, Field(ge=0.0, le=1.0)]
 
 
 class ImputeMethod(Enum):
-    knn = 'knn'
-    mode = 'mode'
-    median = 'median'
-    mean = 'mean'
-    fast_knn = 'fast_knn'
+    knn = "knn"
+    mode = "mode"
+    median = "median"
+    mean = "mean"
+    fast_knn = "fast_knn"
 
 
 class JobKind(Enum):
-    avatarization = 'avatarization'
-    privacy_metrics = 'privacy_metrics'
-    signal_metrics = 'signal_metrics'
-    avatarization_batch = 'avatarization_batch'
-    privacy_metrics_batch = 'privacy_metrics_batch'
-    signal_metrics_batch = 'signal_metrics_batch'
-    avatarization_with_time_series = 'avatarization_with_time_series'
-    avatarization_multi_table = 'avatarization_multi_table'
-    privacy_metrics_with_time_series = 'privacy_metrics_with_time_series'
-    privacy_metrics_multi_table = 'privacy_metrics_multi_table'
-    signal_metrics_with_time_series = 'signal_metrics_with_time_series'
-    privacy_metrics_geolocation = 'privacy_metrics_geolocation'
-    generic = 'generic'
+    avatarization = "avatarization"
+    privacy_metrics = "privacy_metrics"
+    signal_metrics = "signal_metrics"
+    avatarization_batch = "avatarization_batch"
+    privacy_metrics_batch = "privacy_metrics_batch"
+    signal_metrics_batch = "signal_metrics_batch"
+    avatarization_with_time_series = "avatarization_with_time_series"
+    avatarization_multi_table = "avatarization_multi_table"
+    privacy_metrics_with_time_series = "privacy_metrics_with_time_series"
+    privacy_metrics_multi_table = "privacy_metrics_multi_table"
+    signal_metrics_with_time_series = "signal_metrics_with_time_series"
+    privacy_metrics_geolocation = "privacy_metrics_geolocation"
+    generic = "generic"
 
 
 class JobProgress(BaseModel):
-    completion_rate_100: Annotated[int, Field(title='Completion Rate 100')]
-    name: Annotated[Optional[str], Field(title='Name')] = None
-    created_at: Annotated[datetime, Field(title='Created At')]
+    completion_rate_100: Annotated[int, Field(title="Completion Rate 100")]
+    name: Annotated[Optional[str], Field(title="Name")] = None
+    created_at: Annotated[datetime, Field(title="Created At")]
 
 
 class JobStatus(Enum):
-    pending = 'pending'
-    started = 'started'
-    success = 'success'
-    failure = 'failure'
-    killed = 'killed'
-    unknown = 'unknown'
+    pending = "pending"
+    started = "started"
+    success = "success"
+    failure = "failure"
+    killed = "killed"
+    unknown = "unknown"
 
 
 class LinkMethod(Enum):
-    bottom_projection = 'bottom_projection'
-    linear_sum_assignement = 'linear_sum_assignement'
+    bottom_projection = "bottom_projection"
+    linear_sum_assignement = "linear_sum_assignement"
 
 
 class LoginResponse(BaseModel):
-    access_token: Annotated[str, Field(title='Access Token')]
-    token_type: Annotated[str, Field(title='Token Type')]
+    access_token: Annotated[str, Field(title="Access Token")]
+    token_type: Annotated[str, Field(title="Token Type")]
 
 
 class PointOfInterest(Enum):
     """
     Available target points of interest (POI) for geolocation signal-based metrics.
     """
 
-    start = 'start'
-    end = 'end'
+    start = "start"
+    end = "end"
 
 
 class PrivacyBatchDatasetMapping(BaseModel):
-    original_id: Annotated[UUID, Field(title='Original Id')]
-    unshuffled_avatars_id: Annotated[UUID, Field(title='Unshuffled Avatars Id')]
+    original_id: Annotated[UUID, Field(title="Original Id")]
+    unshuffled_avatars_id: Annotated[UUID, Field(title="Unshuffled Avatars Id")]
 
 
 class PrivacyMetricsComputationType(Enum):
-    standalone = 'standalone'
-    to_bottom_id_propagated = 'to_bottom_id_propagated'
-    to_top_enriched = 'to_top_enriched'
-    full_enriched = 'full_enriched'
+    standalone = "standalone"
+    to_bottom_id_propagated = "to_bottom_id_propagated"
+    to_top_enriched = "to_top_enriched"
+    full_enriched = "full_enriched"
 
 
 class PrivacyMetricsTargets(BaseModel):
-    hidden_rate: Annotated[str, Field(title='Hidden Rate')]
-    local_cloaking: Annotated[str, Field(title='Local Cloaking')]
-    distance_to_closest: Annotated[str, Field(title='Distance To Closest')]
-    closest_distances_ratio: Annotated[str, Field(title='Closest Distances Ratio')]
+    hidden_rate: Annotated[str, Field(title="Hidden Rate")]
+    local_cloaking: Annotated[str, Field(title="Local Cloaking")]
+    distance_to_closest: Annotated[str, Field(title="Distance To Closest")]
+    closest_distances_ratio: Annotated[str, Field(title="Closest Distances Ratio")]
     column_direct_match_protection: Annotated[
-        str, Field(title='Column Direct Match Protection')
+        str, Field(title="Column Direct Match Protection")
     ]
-    categorical_hidden_rate: Annotated[str, Field(title='Categorical Hidden Rate')]
+    categorical_hidden_rate: Annotated[str, Field(title="Categorical Hidden Rate")]
     row_direct_match_protection: Annotated[
-        str, Field(title='Row Direct Match Protection')
+        str, Field(title="Row Direct Match Protection")
     ]
     correlation_protection_rate: Annotated[
-        str, Field(title='Correlation Protection Rate')
+        str, Field(title="Correlation Protection Rate")
     ]
-    inference_continuous: Annotated[str, Field(title='Inference Continuous')]
-    inference_categorical: Annotated[str, Field(title='Inference Categorical')]
-    closest_rate: Annotated[str, Field(title='Closest Rate')]
+    inference_continuous: Annotated[str, Field(title="Inference Continuous")]
+    inference_categorical: Annotated[str, Field(title="Inference Categorical")]
+    closest_rate: Annotated[str, Field(title="Closest Rate")]
 
 
 class PrivacyMetricsTimeSeriesParameters(BaseModel):
-    dataset_id: Annotated[UUID, Field(title='Dataset Id')]
+    dataset_id: Annotated[UUID, Field(title="Dataset Id")]
     unshuffled_avatar_dataset_id: Annotated[
-        UUID, Field(title='Unshuffled Avatar Dataset Id')
+        UUID, Field(title="Unshuffled Avatar Dataset Id")
     ]
     projection_parameters: Annotated[
         Optional[Union[FPCAParameters, FlattenParameters]],
         Field(
-            description='Parameters of the time series projection.',
-            title='Projection Parameters',
+            description="Parameters of the time series projection.",
+            title="Projection Parameters",
         ),
     ] = None
     parameter_type: Annotated[
-        Literal['privacy_metrics_time_series'], Field(title='Parameter Type')
-    ] = 'privacy_metrics_time_series'
+        Literal["privacy_metrics_time_series"], Field(title="Parameter Type")
+    ] = "privacy_metrics_time_series"
 
 
 class Projections(BaseModel):
-    records: Annotated[List[List[float]], Field(title='Records')]
-    avatars: Annotated[List[List[float]], Field(title='Avatars')]
+    records: Annotated[List[List[float]], Field(title="Records")]
+    avatars: Annotated[List[List[float]], Field(title="Avatars")]
 
 
 class RareCategoricalMethod(Enum):
     """
     The method to replace rare modalities.
 
     Available rare modality replacement strategies are:
@@ -352,64 +352,64 @@
     - ``probabilistic``: Probabilities will be defined for each potential replacement value and
         the rare modalities will be replaced following these probabilities in a
         non-deterministic way.
 
     - ``missing``: Rare modalities will be replace with missing values.
     """
 
-    most_similar = 'most_similar'
-    probabilistic = 'probabilistic'
-    missing = 'missing'
+    most_similar = "most_similar"
+    probabilistic = "probabilistic"
+    missing = "missing"
 
 
 class Report(BaseModel):
-    id: Annotated[UUID, Field(title='Id')]
-    user_id: Annotated[UUID, Field(title='User Id')]
-    job_id: Annotated[UUID, Field(title='Job Id')]
-    created_at: Annotated[Optional[datetime], Field(title='Created At')] = None
-    download_url: Annotated[str, Field(title='Download Url')]
+    id: Annotated[UUID, Field(title="Id")]
+    user_id: Annotated[UUID, Field(title="User Id")]
+    job_id: Annotated[UUID, Field(title="Job Id")]
+    created_at: Annotated[Optional[datetime], Field(title="Created At")] = None
+    download_url: Annotated[str, Field(title="Download Url")]
 
 
 class ReportCreate(BaseModel):
-    avatarization_job_id: Annotated[UUID, Field(title='Avatarization Job Id')]
-    privacy_job_id: Annotated[UUID, Field(title='Privacy Job Id')]
-    signal_job_id: Annotated[UUID, Field(title='Signal Job Id')]
+    avatarization_job_id: Annotated[UUID, Field(title="Avatarization Job Id")]
+    privacy_job_id: Annotated[UUID, Field(title="Privacy Job Id")]
+    signal_job_id: Annotated[UUID, Field(title="Signal Job Id")]
 
 
 class ReportFromBatchCreate(BaseModel):
     avatarization_batch_job_id: Annotated[
-        UUID, Field(title='Avatarization Batch Job Id')
+        UUID, Field(title="Avatarization Batch Job Id")
     ]
-    privacy_batch_job_id: Annotated[UUID, Field(title='Privacy Batch Job Id')]
-    signal_batch_job_id: Annotated[UUID, Field(title='Signal Batch Job Id')]
+    privacy_batch_job_id: Annotated[UUID, Field(title="Privacy Batch Job Id")]
+    signal_batch_job_id: Annotated[UUID, Field(title="Signal Batch Job Id")]
 
 
 class ReportFromDataCreate(BaseModel):
-    dataset_id: Annotated[UUID, Field(title='Dataset Id')]
-    avatars_dataset_id: Annotated[UUID, Field(title='Avatars Dataset Id')]
-    privacy_job_id: Annotated[UUID, Field(title='Privacy Job Id')]
-    signal_job_id: Annotated[UUID, Field(title='Signal Job Id')]
+    dataset_id: Annotated[UUID, Field(title="Dataset Id")]
+    avatars_dataset_id: Annotated[UUID, Field(title="Avatars Dataset Id")]
+    privacy_job_id: Annotated[UUID, Field(title="Privacy Job Id")]
+    signal_job_id: Annotated[UUID, Field(title="Signal Job Id")]
 
 
 class ReportGeolocationPrivacyCreate(BaseModel):
-    dataset_id: Annotated[UUID, Field(title='Dataset Id')]
-    avatars_dataset_id: Annotated[UUID, Field(title='Avatars Dataset Id')]
-    privacy_job_id: Annotated[UUID, Field(title='Privacy Job Id')]
+    dataset_id: Annotated[UUID, Field(title="Dataset Id")]
+    avatars_dataset_id: Annotated[UUID, Field(title="Avatars Dataset Id")]
+    privacy_job_id: Annotated[UUID, Field(title="Privacy Job Id")]
 
 
 class ResetPasswordRequest(BaseModel):
-    email: Annotated[str, Field(title='Email')]
-    new_password: Annotated[str, Field(title='New Password')]
-    new_password_repeated: Annotated[str, Field(title='New Password Repeated')]
-    token: Annotated[UUID, Field(title='Token')]
+    email: Annotated[str, Field(title="Email")]
+    new_password: Annotated[str, Field(title="New Password")]
+    new_password_repeated: Annotated[str, Field(title="New Password Repeated")]
+    token: Annotated[UUID, Field(title="Token")]
 
 
 class SignalBatchDatasetMapping(BaseModel):
-    original_id: Annotated[UUID, Field(title='Original Id')]
-    avatars_id: Annotated[UUID, Field(title='Avatars Id')]
+    original_id: Annotated[UUID, Field(title="Original Id")]
+    avatars_id: Annotated[UUID, Field(title="Avatars Id")]
 
 
 class SignalMetricsBatchParameters(BaseModel):
     """
     Parameters to configure a SignalMetricsBatchJob.
 
     There are two main use-cases:
@@ -487,240 +487,240 @@
     ...         original_id=training_original_id,
     ...     ),
     ...     batch_dataset_mappings=batch_dataset_mappings
     ... )
     """
 
     model_config = ConfigDict(
-        extra='allow',
+        extra="allow",
     )
     avatarization_batch_job_id: Annotated[
         Optional[UUID],
         Field(
-            description='Identifier of the avatarization batch job that was launched prior to this signal metrics batch job. This has to be set to None if you are computing signal metrics without applying the avatar method beforehand. Setting it to ``None`` requires ``training_dataset_mapping`` and ``batch_dataset_mappings`` to be set.',
-            title='Avatarization Batch Job Id',
+            description="Identifier of the avatarization batch job that was launched prior to this signal metrics batch job. This has to be set to None if you are computing signal metrics without applying the avatar method beforehand. Setting it to ``None`` requires ``training_dataset_mapping`` and ``batch_dataset_mappings`` to be set.",
+            title="Avatarization Batch Job Id",
         ),
     ] = None
     training_dataset_mapping: Annotated[
         Optional[SignalBatchDatasetMapping],
         Field(
-            description='Dataset identifiers for the training batch. The dataset identifiers specified here will be used to fit the anonymization.'
+            description="Dataset identifiers for the training batch. The dataset identifiers specified here will be used to fit the anonymization."
         ),
     ] = None
     common_parameters: Annotated[
         Optional[CommonSignalMetricsParameters],
         Field(
-            description='Parameters to use during the computation of the privacy metrics. These will be applied on all the batches, including the training batch.'
+            description="Parameters to use during the computation of the privacy metrics. These will be applied on all the batches, including the training batch."
         ),
     ] = None
     seed: Annotated[
         Optional[int],
-        Field(description='Seed used to generate the random state.', title='Seed'),
+        Field(description="Seed used to generate the random state.", title="Seed"),
     ] = None
     batch_dataset_mappings: Annotated[
         Optional[List[SignalBatchDatasetMapping]],
         Field(
-            description='List of pairs of dataset identifiers. You should not specify again the dataset identifiers that were specified in ``training_dataset_mapping``.',
-            title='Batch Dataset Mappings',
+            description="List of pairs of dataset identifiers. You should not specify again the dataset identifiers that were specified in ``training_dataset_mapping``.",
+            title="Batch Dataset Mappings",
         ),
     ] = None
 
 
 class SignalMetricsParameters(BaseModel):
     """
     Parameters to configure a SignalMetricsJob.
     """
 
     model_config = ConfigDict(
-        extra='allow',
+        extra="allow",
     )
-    original_id: Annotated[UUID, Field(title='Original Id')]
-    persistance_job_id: Annotated[Optional[UUID], Field(title='Persistance Job Id')] = (
+    original_id: Annotated[UUID, Field(title="Original Id")]
+    persistance_job_id: Annotated[Optional[UUID], Field(title="Persistance Job Id")] = (
         None
     )
     avatarization_job_id: Annotated[
-        Optional[UUID], Field(title='Avatarization Job Id')
+        Optional[UUID], Field(title="Avatarization Job Id")
     ] = None
-    avatars_id: Annotated[UUID, Field(title='Avatars Id')]
+    avatars_id: Annotated[UUID, Field(title="Avatars Id")]
 
 
 class SignalMetricsTargets(BaseModel):
-    hellinger_mean: Annotated[str, Field(title='Hellinger Mean')]
+    hellinger_mean: Annotated[str, Field(title="Hellinger Mean")]
     correlation_difference_ratio: Annotated[
-        str, Field(title='Correlation Difference Ratio')
+        str, Field(title="Correlation Difference Ratio")
     ]
 
 
 class SignalMetricsTimeSeriesParameters(BaseModel):
-    dataset_id: Annotated[UUID, Field(title='Dataset Id')]
-    avatar_dataset_id: Annotated[UUID, Field(title='Avatar Dataset Id')]
+    dataset_id: Annotated[UUID, Field(title="Dataset Id")]
+    avatar_dataset_id: Annotated[UUID, Field(title="Avatar Dataset Id")]
 
 
 class SignalMetricsWithTimeSeriesParameters(BaseModel):
     model_config = ConfigDict(
-        extra='allow',
+        extra="allow",
     )
     vanilla_original_id: Annotated[
         Optional[UUID],
         Field(
-            description='Identifier of the original vanilla dataset.',
-            title='Vanilla Original Id',
+            description="Identifier of the original vanilla dataset.",
+            title="Vanilla Original Id",
         ),
     ] = None
     vanilla_avatars_id: Annotated[
         Optional[UUID],
         Field(
-            description='Identifier of the avatar vanilla dataset.',
-            title='Vanilla Avatars Id',
+            description="Identifier of the avatar vanilla dataset.",
+            title="Vanilla Avatars Id",
         ),
     ] = None
     time_series: Annotated[
         List[SignalMetricsTimeSeriesParameters],
         Field(
-            description='Privacy metrics parameters of the time series. Each time series can have a different set of parameters.',
-            title='Time Series',
+            description="Privacy metrics parameters of the time series. Each time series can have a different set of parameters.",
+            title="Time Series",
         ),
     ]
     seed: Annotated[
         Optional[int],
-        Field(description='Seed used to generate the random state.', title='Seed'),
+        Field(description="Seed used to generate the random state.", title="Seed"),
     ] = None
 
 
 class SignalPosition(Enum):
     """
     Available known signal positions for geolocation signal-based metrics.
     """
 
-    start = 'start'
-    middle = 'middle'
-    end = 'end'
+    start = "start"
+    middle = "middle"
+    end = "end"
 
 
 class TableReference(BaseModel):
-    dataset_id: Annotated[UUID, Field(title='Dataset Id')]
+    dataset_id: Annotated[UUID, Field(title="Dataset Id")]
     is_individual_level: Annotated[
         bool,
         Field(
-            description='Whether the table contains rows at the individual level. Individual level means that each row contains information about a distinct physical person.',
-            title='Is Individual Level',
+            description="Whether the table contains rows at the individual level. Individual level means that each row contains information about a distinct physical person.",
+            title="Is Individual Level",
         ),
     ]
 
 
 class TimeSeriesSignalMetricsPerDataset(BaseModel):
-    pointwise_hellinger_mean: Annotated[float, Field(title='Pointwise Hellinger Mean')]
-    pointwise_hellinger_std: Annotated[float, Field(title='Pointwise Hellinger Std')]
-    density_hellinger_mean: Annotated[float, Field(title='Density Hellinger Mean')]
-    density_hellinger_std: Annotated[float, Field(title='Density Hellinger Std')]
-    original_id: Annotated[UUID, Field(title='Original Id')]
-    avatars_id: Annotated[UUID, Field(title='Avatars Id')]
+    pointwise_hellinger_mean: Annotated[float, Field(title="Pointwise Hellinger Mean")]
+    pointwise_hellinger_std: Annotated[float, Field(title="Pointwise Hellinger Std")]
+    density_hellinger_mean: Annotated[float, Field(title="Density Hellinger Mean")]
+    density_hellinger_std: Annotated[float, Field(title="Density Hellinger Std")]
+    original_id: Annotated[UUID, Field(title="Original Id")]
+    avatars_id: Annotated[UUID, Field(title="Avatars Id")]
 
 
 class UserRole(Enum):
-    admin = 'admin'
-    user = 'user'
+    admin = "admin"
+    user = "user"
 
 
 class ValidationError(BaseModel):
-    loc: Annotated[List[Union[str, int]], Field(title='Location')]
-    msg: Annotated[str, Field(title='Message')]
-    type: Annotated[str, Field(title='Error Type')]
+    loc: Annotated[List[Union[str, int]], Field(title="Location")]
+    msg: Annotated[str, Field(title="Message")]
+    type: Annotated[str, Field(title="Error Type")]
 
 
 class GrantTypeItem(RootModel[str]):
-    root: Annotated[str, Field(pattern='password')]
+    root: Annotated[str, Field(pattern="password")]
 
 
 class Login(BaseModel):
-    grant_type: Annotated[Optional[GrantTypeItem], Field(title='Grant Type')] = None
-    username: Annotated[str, Field(title='Username')]
-    password: Annotated[str, Field(title='Password')]
-    scope: Annotated[Optional[str], Field(title='Scope')] = ''
-    client_id: Annotated[Optional[str], Field(title='Client Id')] = None
-    client_secret: Annotated[Optional[str], Field(title='Client Secret')] = None
+    grant_type: Annotated[Optional[GrantTypeItem], Field(title="Grant Type")] = None
+    username: Annotated[str, Field(title="Username")]
+    password: Annotated[str, Field(title="Password")]
+    scope: Annotated[Optional[str], Field(title="Scope")] = ""
+    client_id: Annotated[Optional[str], Field(title="Client Id")] = None
+    client_secret: Annotated[Optional[str], Field(title="Client Secret")] = None
 
 
 class CreateDataset(BaseModel):
-    file: Annotated[bytes, Field(title='File')]
-    name: Annotated[Optional[str], Field(title='Name')] = None
+    file: Annotated[bytes, Field(title="File")]
+    name: Annotated[Optional[str], Field(title="Name")] = None
 
 
 class GenericJob(BaseModel):
-    id: Annotated[UUID, Field(title='Id')]
+    id: Annotated[UUID, Field(title="Id")]
     kind: JobKind
-    created_at: Annotated[datetime, Field(title='Created At')]
+    created_at: Annotated[datetime, Field(title="Created At")]
     status: JobStatus
-    error_message: Annotated[Optional[str], Field(title='Error Message')] = None
-    traceback: Annotated[Optional[str], Field(title='Traceback')] = None
+    error_message: Annotated[Optional[str], Field(title="Error Message")] = None
+    traceback: Annotated[Optional[str], Field(title="Traceback")] = None
     result: Optional[GenericResult] = None
     parameters: GenericParameters
     current_progress: Optional[JobProgress] = None
 
 
 class AvatarizationTimeSeriesParameters(BaseModel):
     """
     Parameters for the anonymization of a Time Series.
     """
 
     dataset_id: Annotated[
         UUID,
         Field(
-            description='Identifier of the dataset containing the time series.',
-            title='Dataset Id',
+            description="Identifier of the dataset containing the time series.",
+            title="Dataset Id",
         ),
     ]
     projection_parameters: Annotated[
         Union[FPCAParameters, FlattenParameters],
         Field(
-            description='Parameters of the time series projection.',
-            title='Projection Parameters',
+            description="Parameters of the time series projection.",
+            title="Projection Parameters",
         ),
     ]
 
 
 class ColumnDetail(BaseModel):
     type: ColumnType
-    label: Annotated[str, Field(title='Label')]
-    is_identifier: Annotated[Optional[bool], Field(title='Is Identifier')] = False
+    label: Annotated[str, Field(title="Label")]
+    is_identifier: Annotated[Optional[bool], Field(title="Is Identifier")] = False
 
 
 class CompatibilityResponse(BaseModel):
-    message: Annotated[str, Field(title='Message')]
+    message: Annotated[str, Field(title="Message")]
     most_recent_compatible_client: Annotated[
-        Optional[str], Field(title='Most Recent Compatible Client')
+        Optional[str], Field(title="Most Recent Compatible Client")
     ] = None
     status: CompatibilityStatus
 
 
 class CreateUser(BaseModel):
     """
     Create a user, either with an email, or a username.
 
     The choice will depend on how your server is setup.
     """
 
-    username: Annotated[Optional[str], Field(title='Username')] = None
-    email: Annotated[Optional[str], Field(title='Email')] = None
+    username: Annotated[Optional[str], Field(title="Username")] = None
+    email: Annotated[Optional[str], Field(title="Email")] = None
     role: Optional[UserRole] = UserRole.user
-    password: Annotated[Optional[str], Field(title='Password')] = None
+    password: Annotated[Optional[str], Field(title="Password")] = None
 
 
 class Dataset(BaseModel):
-    id: Annotated[UUID, Field(title='Id')]
-    hash: Annotated[str, Field(title='Hash')]
-    name: Annotated[Optional[str], Field(title='Name')] = None
-    columns: Annotated[Optional[List[ColumnDetail]], Field(title='Columns')] = None
-    download_url: Annotated[str, Field(title='Download Url')]
+    id: Annotated[UUID, Field(title="Id")]
+    hash: Annotated[str, Field(title="Hash")]
+    name: Annotated[Optional[str], Field(title="Name")] = None
+    columns: Annotated[Optional[List[ColumnDetail]], Field(title="Columns")] = None
+    download_url: Annotated[str, Field(title="Download Url")]
     analysis_status: Optional[AnalysisStatus] = None
-    analysis_duration: Annotated[Optional[float], Field(title='Analysis Duration')] = (
+    analysis_duration: Annotated[Optional[float], Field(title="Analysis Duration")] = (
         None
     )
-    nb_lines: Annotated[Optional[int], Field(title='Nb Lines')] = None
-    nb_dimensions: Annotated[int, Field(title='Nb Dimensions')]
+    nb_lines: Annotated[Optional[int], Field(title="Nb Lines")] = None
+    nb_dimensions: Annotated[int, Field(title="Nb Dimensions")]
     summary: Optional[DatasetSummary] = None
     filetype: FileType
 
 
 class ExcludeCategoricalParameters(BaseModel):
     """
     Parameters to exclude some variables from the anonymization before re-assigning.
@@ -728,112 +728,112 @@
     The use of this parameter is recommended when the data contains categorical variables
     with a large number of modalities.
     """
 
     exclude_cardinality_threshold: Annotated[
         int,
         Field(
-            description='Threshold defining the minimum cardinality of a variable to be excluded. Any categorical variable with a number of modalities greater or equal to this threshold will be excluded from the anonymization and re-assigned probabilistically.',
+            description="Threshold defining the minimum cardinality of a variable to be excluded. Any categorical variable with a number of modalities greater or equal to this threshold will be excluded from the anonymization and re-assigned probabilistically.",
             ge=1,
-            title='Exclude Cardinality Threshold',
+            title="Exclude Cardinality Threshold",
         ),
     ]
     exclude_replacement_strategy: Annotated[
         ExcludeCategoricalMethod,
-        Field(description='See :class:`ExcludeCategoricalMethod`.'),
+        Field(description="See :class:`ExcludeCategoricalMethod`."),
     ]
     rare_occurence_threshold: Annotated[
         int,
         Field(
-            description='Maximum number of occurrences for a modality in an excluded variable to be considered as rare. Modalities considered as rare will be removed and replaced by other modalities prior to being re-assigned. This prevents the risk of re-identification based on rare modalities.',
+            description="Maximum number of occurrences for a modality in an excluded variable to be considered as rare. Modalities considered as rare will be removed and replaced by other modalities prior to being re-assigned. This prevents the risk of re-identification based on rare modalities.",
             ge=0,
-            title='Rare Occurence Threshold',
+            title="Rare Occurence Threshold",
         ),
     ]
     rare_replacement_strategy: Annotated[
         Optional[RareCategoricalMethod],
-        Field(description='See :class:`RareCategoricalMethod`.'),
+        Field(description="See :class:`RareCategoricalMethod`."),
     ] = None
     number_reference_records: Annotated[
         Optional[int],
         Field(
-            description='Number of training records to use to compute inter-records distances. Set to ``None`` to use all records, beware this may lead to long computational time. If ``number_reference_records`` is too low, it will be corrected to ensure each value combination to be replaced is represented in the sample. If ``number_reference_records`` is larger than the number of records, it will be corrected to the number of records.',
-            title='Number Reference Records',
+            description="Number of training records to use to compute inter-records distances. Set to ``None`` to use all records, beware this may lead to long computational time. If ``number_reference_records`` is too low, it will be corrected to ensure each value combination to be replaced is represented in the sample. If ``number_reference_records`` is larger than the number of records, it will be corrected to the number of records.",
+            title="Number Reference Records",
         ),
     ] = None
 
 
 class HTTPValidationError(BaseModel):
-    detail: Annotated[Optional[List[ValidationError]], Field(title='Detail')] = None
+    detail: Annotated[Optional[List[ValidationError]], Field(title="Detail")] = None
 
 
 class ImputationParameters(BaseModel):
     method: Optional[ImputeMethod] = None
-    k: Annotated[Optional[int], Field(title='K')] = None
+    k: Annotated[Optional[int], Field(title="K")] = None
     training_fraction: Annotated[
-        Optional[TrainingFractionItem], Field(title='Training Fraction')
+        Optional[TrainingFractionItem], Field(title="Training Fraction")
     ] = None
 
 
 class PrivacyMetrics(BaseModel):
-    hidden_rate: Annotated[float, Field(title='Hidden Rate')]
-    local_cloaking: Annotated[float, Field(title='Local Cloaking')]
+    hidden_rate: Annotated[float, Field(title="Hidden Rate")]
+    local_cloaking: Annotated[float, Field(title="Local Cloaking")]
     distance_to_closest: Annotated[
-        Optional[float], Field(title='Distance To Closest')
+        Optional[float], Field(title="Distance To Closest")
     ] = None
     closest_distances_ratio: Annotated[
-        Optional[float], Field(title='Closest Distances Ratio')
+        Optional[float], Field(title="Closest Distances Ratio")
     ] = None
     column_direct_match_protection: Annotated[
-        float, Field(title='Column Direct Match Protection')
+        float, Field(title="Column Direct Match Protection")
     ]
-    categorical_hidden_rate: Annotated[float, Field(title='Categorical Hidden Rate')]
+    categorical_hidden_rate: Annotated[float, Field(title="Categorical Hidden Rate")]
     row_direct_match_protection: Annotated[
-        float, Field(title='Row Direct Match Protection')
+        float, Field(title="Row Direct Match Protection")
     ]
     correlation_protection_rate: Annotated[
-        Optional[float], Field(title='Correlation Protection Rate')
+        Optional[float], Field(title="Correlation Protection Rate")
     ] = None
     inference_continuous: Annotated[
-        Optional[float], Field(title='Inference Continuous')
+        Optional[float], Field(title="Inference Continuous")
     ] = None
     inference_categorical: Annotated[
-        Optional[float], Field(title='Inference Categorical')
+        Optional[float], Field(title="Inference Categorical")
     ] = None
-    closest_rate: Annotated[Optional[float], Field(title='Closest Rate')] = None
+    closest_rate: Annotated[Optional[float], Field(title="Closest Rate")] = None
     targets: PrivacyMetricsTargets
 
 
 class PrivacyMetricsGeolocationScenario(BaseModel):
     projection_parameters: Annotated[
         Union[GeolocationDensityParameters, GeolocationFeaturesParameters],
         Field(
-            description='Parameters of the geolocation projection.',
-            title='Projection Parameters',
+            description="Parameters of the geolocation projection.",
+            title="Projection Parameters",
         ),
     ]
     known_features: Annotated[
         Optional[List[GeolocationFeatures]],
-        Field(description='Known geolocation features', title='Known Features'),
+        Field(description="Known geolocation features", title="Known Features"),
     ] = None
     target_feature: Annotated[
-        Optional[GeolocationFeatures], Field(description='Target geolocation features')
+        Optional[GeolocationFeatures], Field(description="Target geolocation features")
     ] = None
     known_signal_position: Annotated[
-        Optional[SignalPosition], Field(description='Position of the known signal')
+        Optional[SignalPosition], Field(description="Position of the known signal")
     ] = None
     target_signal_poi: Annotated[
         Optional[PointOfInterest],
-        Field(description='Target point of interest (POI) to infer'),
+        Field(description="Target point of interest (POI) to infer"),
     ] = None
     inference_metric_threshold: Annotated[
         Optional[float],
         Field(
-            description='Threshold (in meters) for the inference metric',
-            title='Inference Metric Threshold',
+            description="Threshold (in meters) for the inference metric",
+            title="Inference Metric Threshold",
         ),
     ] = 500
 
 
 class PrivacyMetricsGeolocationScenarioResult(BaseModel):
     privacy_metrics: GeolocationPrivacyMetrics
     parameters: PrivacyMetricsGeolocationScenario
@@ -841,266 +841,266 @@
 
 class PrivacyMetricsParameters(BaseModel):
     """
     Parameters to configure a PrivacyMetricsJob.
     """
 
     model_config = ConfigDict(
-        extra='allow',
+        extra="allow",
     )
-    original_id: Annotated[UUID, Field(title='Original Id')]
-    persistance_job_id: Annotated[Optional[UUID], Field(title='Persistance Job Id')] = (
+    original_id: Annotated[UUID, Field(title="Original Id")]
+    persistance_job_id: Annotated[Optional[UUID], Field(title="Persistance Job Id")] = (
         None
     )
     avatarization_job_id: Annotated[
-        Optional[UUID], Field(title='Avatarization Job Id')
+        Optional[UUID], Field(title="Avatarization Job Id")
     ] = None
     imputation: Annotated[
         Optional[ImputationParameters],
         Field(
-            description='Imputation parameters used to preprocess avatars data for metrics computation.'
+            description="Imputation parameters used to preprocess avatars data for metrics computation."
         ),
     ] = None
     ncp: Annotated[
         Optional[int],
         Field(
-            description='Number of components used for distance based metrics computation. Default behavior similar to avatarization. If unspecified, ncp is set to the minimum value between 5 and the number of variables after one hot encoding. Note: we recommend using to use the same ncp as during the avatarization to check the worst-case scenario.',
-            title='Ncp',
+            description="Number of components used for distance based metrics computation. Default behavior similar to avatarization. If unspecified, ncp is set to the minimum value between 5 and the number of variables after one hot encoding. Note: we recommend using to use the same ncp as during the avatarization to check the worst-case scenario.",
+            title="Ncp",
         ),
     ] = None
     use_categorical_reduction: Annotated[
         Optional[bool],
         Field(
-            description='Parameter to vectorize categorical variables using cat2vec. See :class:`AvatarizationParameters` for more details. Note: we recommend using the same parameter as during the avatarization.',
-            title='Use Categorical Reduction',
+            description="Parameter to vectorize categorical variables using cat2vec. See :class:`AvatarizationParameters` for more details. Note: we recommend using the same parameter as during the avatarization.",
+            title="Use Categorical Reduction",
         ),
     ] = None
     known_variables: Annotated[
         Optional[List[str]],
         Field(
-            description='Variables considered to be known by the attacker. Used to measure some specific privacy metrics. Note: we recommend using demographic variables as they are the most likely to be known by an attacker.',
-            title='Known Variables',
+            description="Variables considered to be known by the attacker. Used to measure some specific privacy metrics. Note: we recommend using demographic variables as they are the most likely to be known by an attacker.",
+            title="Known Variables",
         ),
     ] = None
     target: Annotated[
         Optional[str],
         Field(
-            description='The target variable to be inferred. Used to measure some specific privacy metrics. Must be used together with ``known_variables``. Note: we recommend using a sensitive variable',
-            title='Target',
+            description="The target variable to be inferred. Used to measure some specific privacy metrics. Must be used together with ``known_variables``. Note: we recommend using a sensitive variable",
+            title="Target",
         ),
     ] = None
     closest_rate_percentage_threshold: Annotated[
         Optional[ClosestRatePercentageThresholdItem],
         Field(
-            description='Threshold of closest original individuals to define a distance threshold. Any avatar data generated closer from an original than the distance threshold, is considered at risk.',
-            title='Closest Rate Percentage Threshold',
+            description="Threshold of closest original individuals to define a distance threshold. Any avatar data generated closer from an original than the distance threshold, is considered at risk.",
+            title="Closest Rate Percentage Threshold",
         ),
     ] = None
     closest_rate_ratio_threshold: Annotated[
         Optional[ClosestRateRatioThresholdItem],
         Field(
-            description='Closest distance ratio threshold below which an avatar data is considered too close to an original and too isolated from the rest to be safe.',
-            title='Closest Rate Ratio Threshold',
+            description="Closest distance ratio threshold below which an avatar data is considered too close to an original and too isolated from the rest to be safe.",
+            title="Closest Rate Ratio Threshold",
         ),
     ] = None
-    unshuffled_avatars_id: Annotated[UUID, Field(title='Unshuffled Avatars Id')]
+    unshuffled_avatars_id: Annotated[UUID, Field(title="Unshuffled Avatars Id")]
     seed: Annotated[
         Optional[int],
-        Field(description='Seed used to generate the random state.', title='Seed'),
+        Field(description="Seed used to generate the random state.", title="Seed"),
     ] = None
 
 
 class PrivacyMetricsPerBatchResult(BaseModel):
-    hidden_rate: Annotated[float, Field(title='Hidden Rate')]
-    local_cloaking: Annotated[float, Field(title='Local Cloaking')]
+    hidden_rate: Annotated[float, Field(title="Hidden Rate")]
+    local_cloaking: Annotated[float, Field(title="Local Cloaking")]
     distance_to_closest: Annotated[
-        Optional[float], Field(title='Distance To Closest')
+        Optional[float], Field(title="Distance To Closest")
     ] = None
     closest_distances_ratio: Annotated[
-        Optional[float], Field(title='Closest Distances Ratio')
+        Optional[float], Field(title="Closest Distances Ratio")
     ] = None
     column_direct_match_protection: Annotated[
-        float, Field(title='Column Direct Match Protection')
+        float, Field(title="Column Direct Match Protection")
     ]
-    categorical_hidden_rate: Annotated[float, Field(title='Categorical Hidden Rate')]
+    categorical_hidden_rate: Annotated[float, Field(title="Categorical Hidden Rate")]
     row_direct_match_protection: Annotated[
-        float, Field(title='Row Direct Match Protection')
+        float, Field(title="Row Direct Match Protection")
     ]
     correlation_protection_rate: Annotated[
-        Optional[float], Field(title='Correlation Protection Rate')
+        Optional[float], Field(title="Correlation Protection Rate")
     ] = None
     inference_continuous: Annotated[
-        Optional[float], Field(title='Inference Continuous')
+        Optional[float], Field(title="Inference Continuous")
     ] = None
     inference_categorical: Annotated[
-        Optional[float], Field(title='Inference Categorical')
+        Optional[float], Field(title="Inference Categorical")
     ] = None
-    closest_rate: Annotated[Optional[float], Field(title='Closest Rate')] = None
+    closest_rate: Annotated[Optional[float], Field(title="Closest Rate")] = None
     targets: PrivacyMetricsTargets
-    original_id: Annotated[UUID, Field(title='Original Id')]
-    unshuffled_avatars_id: Annotated[UUID, Field(title='Unshuffled Avatars Id')]
+    original_id: Annotated[UUID, Field(title="Original Id")]
+    unshuffled_avatars_id: Annotated[UUID, Field(title="Unshuffled Avatars Id")]
 
 
 class PrivacyMetricsPerMultiTableDataset(BaseModel):
-    hidden_rate: Annotated[float, Field(title='Hidden Rate')]
-    local_cloaking: Annotated[float, Field(title='Local Cloaking')]
+    hidden_rate: Annotated[float, Field(title="Hidden Rate")]
+    local_cloaking: Annotated[float, Field(title="Local Cloaking")]
     distance_to_closest: Annotated[
-        Optional[float], Field(title='Distance To Closest')
+        Optional[float], Field(title="Distance To Closest")
     ] = None
     closest_distances_ratio: Annotated[
-        Optional[float], Field(title='Closest Distances Ratio')
+        Optional[float], Field(title="Closest Distances Ratio")
     ] = None
     column_direct_match_protection: Annotated[
-        float, Field(title='Column Direct Match Protection')
+        float, Field(title="Column Direct Match Protection")
     ]
-    categorical_hidden_rate: Annotated[float, Field(title='Categorical Hidden Rate')]
+    categorical_hidden_rate: Annotated[float, Field(title="Categorical Hidden Rate")]
     row_direct_match_protection: Annotated[
-        float, Field(title='Row Direct Match Protection')
+        float, Field(title="Row Direct Match Protection")
     ]
     correlation_protection_rate: Annotated[
-        Optional[float], Field(title='Correlation Protection Rate')
+        Optional[float], Field(title="Correlation Protection Rate")
     ] = None
     inference_continuous: Annotated[
-        Optional[float], Field(title='Inference Continuous')
+        Optional[float], Field(title="Inference Continuous")
     ] = None
     inference_categorical: Annotated[
-        Optional[float], Field(title='Inference Categorical')
+        Optional[float], Field(title="Inference Categorical")
     ] = None
-    closest_rate: Annotated[Optional[float], Field(title='Closest Rate')] = None
+    closest_rate: Annotated[Optional[float], Field(title="Closest Rate")] = None
     targets: PrivacyMetricsTargets
-    original_id: Annotated[UUID, Field(title='Original Id')]
-    unshuffled_avatars_id: Annotated[UUID, Field(title='Unshuffled Avatars Id')]
-    metrics_type: Annotated[Literal['multi_table'], Field(title='Metrics Type')] = (
-        'multi_table'
+    original_id: Annotated[UUID, Field(title="Original Id")]
+    unshuffled_avatars_id: Annotated[UUID, Field(title="Unshuffled Avatars Id")]
+    metrics_type: Annotated[Literal["multi_table"], Field(title="Metrics Type")] = (
+        "multi_table"
     )
     individual_identifier_variable: Annotated[
-        str, Field(title='Individual Identifier Variable')
+        str, Field(title="Individual Identifier Variable")
     ]
     computation_type: PrivacyMetricsComputationType
-    dataset_name: Annotated[str, Field(title='Dataset Name')]
+    dataset_name: Annotated[str, Field(title="Dataset Name")]
 
 
 class PrivacyMetricsPerTimeSeriesDataset(BaseModel):
-    hidden_rate: Annotated[float, Field(title='Hidden Rate')]
-    local_cloaking: Annotated[float, Field(title='Local Cloaking')]
+    hidden_rate: Annotated[float, Field(title="Hidden Rate")]
+    local_cloaking: Annotated[float, Field(title="Local Cloaking")]
     distance_to_closest: Annotated[
-        Optional[float], Field(title='Distance To Closest')
+        Optional[float], Field(title="Distance To Closest")
     ] = None
     closest_distances_ratio: Annotated[
-        Optional[float], Field(title='Closest Distances Ratio')
+        Optional[float], Field(title="Closest Distances Ratio")
     ] = None
     column_direct_match_protection: Annotated[
-        float, Field(title='Column Direct Match Protection')
+        float, Field(title="Column Direct Match Protection")
     ]
-    categorical_hidden_rate: Annotated[float, Field(title='Categorical Hidden Rate')]
+    categorical_hidden_rate: Annotated[float, Field(title="Categorical Hidden Rate")]
     row_direct_match_protection: Annotated[
-        float, Field(title='Row Direct Match Protection')
+        float, Field(title="Row Direct Match Protection")
     ]
     correlation_protection_rate: Annotated[
-        Optional[float], Field(title='Correlation Protection Rate')
+        Optional[float], Field(title="Correlation Protection Rate")
     ] = None
     inference_continuous: Annotated[
-        Optional[float], Field(title='Inference Continuous')
+        Optional[float], Field(title="Inference Continuous")
     ] = None
     inference_categorical: Annotated[
-        Optional[float], Field(title='Inference Categorical')
+        Optional[float], Field(title="Inference Categorical")
     ] = None
-    closest_rate: Annotated[Optional[float], Field(title='Closest Rate')] = None
+    closest_rate: Annotated[Optional[float], Field(title="Closest Rate")] = None
     targets: PrivacyMetricsTargets
-    original_id: Annotated[UUID, Field(title='Original Id')]
-    unshuffled_avatars_id: Annotated[UUID, Field(title='Unshuffled Avatars Id')]
-    metrics_type: Annotated[Literal['time_series'], Field(title='Metrics Type')] = (
-        'time_series'
+    original_id: Annotated[UUID, Field(title="Original Id")]
+    unshuffled_avatars_id: Annotated[UUID, Field(title="Unshuffled Avatars Id")]
+    metrics_type: Annotated[Literal["time_series"], Field(title="Metrics Type")] = (
+        "time_series"
     )
 
 
 class PrivacyMetricsWithTimeSeriesParameters(BaseModel):
     model_config = ConfigDict(
-        extra='allow',
+        extra="allow",
     )
     imputation: Annotated[
         Optional[ImputationParameters],
         Field(
-            description='Imputation parameters used to preprocess avatars data for metrics computation.'
+            description="Imputation parameters used to preprocess avatars data for metrics computation."
         ),
     ] = None
     ncp: Annotated[
         Optional[int],
         Field(
-            description='Number of components used for distance based metrics computation. Default behavior similar to avatarization. If unspecified, ncp is set to the minimum value between 5 and the number of variables after one hot encoding. Note: we recommend using to use the same ncp as during the avatarization to check the worst-case scenario.',
-            title='Ncp',
+            description="Number of components used for distance based metrics computation. Default behavior similar to avatarization. If unspecified, ncp is set to the minimum value between 5 and the number of variables after one hot encoding. Note: we recommend using to use the same ncp as during the avatarization to check the worst-case scenario.",
+            title="Ncp",
         ),
     ] = None
     use_categorical_reduction: Annotated[
         Optional[bool],
         Field(
-            description='Parameter to vectorize categorical variables using cat2vec. See :class:`AvatarizationParameters` for more details. Note: we recommend using the same parameter as during the avatarization.',
-            title='Use Categorical Reduction',
+            description="Parameter to vectorize categorical variables using cat2vec. See :class:`AvatarizationParameters` for more details. Note: we recommend using the same parameter as during the avatarization.",
+            title="Use Categorical Reduction",
         ),
     ] = None
     known_variables: Annotated[
         Optional[List[str]],
         Field(
-            description='Variables considered to be known by the attacker. Used to measure some specific privacy metrics. Note: we recommend using demographic variables as they are the most likely to be known by an attacker.',
-            title='Known Variables',
+            description="Variables considered to be known by the attacker. Used to measure some specific privacy metrics. Note: we recommend using demographic variables as they are the most likely to be known by an attacker.",
+            title="Known Variables",
         ),
     ] = None
     target: Annotated[
         Optional[str],
         Field(
-            description='The target variable to be inferred. Used to measure some specific privacy metrics. Must be used together with ``known_variables``. Note: we recommend using a sensitive variable',
-            title='Target',
+            description="The target variable to be inferred. Used to measure some specific privacy metrics. Must be used together with ``known_variables``. Note: we recommend using a sensitive variable",
+            title="Target",
         ),
     ] = None
     closest_rate_percentage_threshold: Annotated[
         Optional[ClosestRatePercentageThresholdItem],
         Field(
-            description='Threshold of closest original individuals to define a distance threshold. Any avatar data generated closer from an original than the distance threshold, is considered at risk.',
-            title='Closest Rate Percentage Threshold',
+            description="Threshold of closest original individuals to define a distance threshold. Any avatar data generated closer from an original than the distance threshold, is considered at risk.",
+            title="Closest Rate Percentage Threshold",
         ),
     ] = None
     closest_rate_ratio_threshold: Annotated[
         Optional[ClosestRateRatioThresholdItem],
         Field(
-            description='Closest distance ratio threshold below which an avatar data is considered too close to an original and too isolated from the rest to be safe.',
-            title='Closest Rate Ratio Threshold',
+            description="Closest distance ratio threshold below which an avatar data is considered too close to an original and too isolated from the rest to be safe.",
+            title="Closest Rate Ratio Threshold",
         ),
     ] = None
     vanilla_original_id: Annotated[
         Optional[UUID],
         Field(
-            description='Identifier of the original vanilla dataset.',
-            title='Vanilla Original Id',
+            description="Identifier of the original vanilla dataset.",
+            title="Vanilla Original Id",
         ),
     ] = None
     vanilla_unshuffled_avatars_id: Annotated[
         Optional[UUID],
         Field(
-            description='Identifier of the avatar vanilla dataset.',
-            title='Vanilla Unshuffled Avatars Id',
+            description="Identifier of the avatar vanilla dataset.",
+            title="Vanilla Unshuffled Avatars Id",
         ),
     ] = None
     time_series: Annotated[
         List[PrivacyMetricsTimeSeriesParameters],
         Field(
-            description='Privacy metrics parameters of the time series. Each time series can have a different set of parameters.',
-            title='Time Series',
+            description="Privacy metrics parameters of the time series. Each time series can have a different set of parameters.",
+            title="Time Series",
         ),
     ]
     seed: Annotated[
         Optional[int],
-        Field(description='Seed used to generate the random state.', title='Seed'),
+        Field(description="Seed used to generate the random state.", title="Seed"),
     ] = None
 
 
 class SignalMetrics(BaseModel):
-    hellinger_mean: Annotated[float, Field(title='Hellinger Mean')]
-    hellinger_std: Annotated[float, Field(title='Hellinger Std')]
+    hellinger_mean: Annotated[float, Field(title="Hellinger Mean")]
+    hellinger_std: Annotated[float, Field(title="Hellinger Std")]
     correlation_difference_ratio: Annotated[
-        Optional[float], Field(title='Correlation Difference Ratio')
+        Optional[float], Field(title="Correlation Difference Ratio")
     ] = None
     targets: SignalMetricsTargets
 
 
 class SignalMetricsBatchJobCreate(BaseModel):
     kind: Optional[JobKind] = JobKind.signal_metrics_batch
     parameters: SignalMetricsBatchParameters
@@ -1108,500 +1108,500 @@
 
 class SignalMetricsJobCreate(BaseModel):
     kind: Optional[JobKind] = JobKind.signal_metrics
     parameters: SignalMetricsParameters
 
 
 class SignalMetricsPerBatchResult(BaseModel):
-    hellinger_mean: Annotated[float, Field(title='Hellinger Mean')]
-    hellinger_std: Annotated[float, Field(title='Hellinger Std')]
+    hellinger_mean: Annotated[float, Field(title="Hellinger Mean")]
+    hellinger_std: Annotated[float, Field(title="Hellinger Std")]
     correlation_difference_ratio: Annotated[
-        Optional[float], Field(title='Correlation Difference Ratio')
+        Optional[float], Field(title="Correlation Difference Ratio")
     ] = None
     targets: SignalMetricsTargets
-    original_id: Annotated[UUID, Field(title='Original Id')]
-    avatars_id: Annotated[UUID, Field(title='Avatars Id')]
+    original_id: Annotated[UUID, Field(title="Original Id")]
+    avatars_id: Annotated[UUID, Field(title="Avatars Id")]
 
 
 class SignalMetricsPerDataset(BaseModel):
-    hellinger_mean: Annotated[float, Field(title='Hellinger Mean')]
-    hellinger_std: Annotated[float, Field(title='Hellinger Std')]
+    hellinger_mean: Annotated[float, Field(title="Hellinger Mean")]
+    hellinger_std: Annotated[float, Field(title="Hellinger Std")]
     correlation_difference_ratio: Annotated[
-        Optional[float], Field(title='Correlation Difference Ratio')
+        Optional[float], Field(title="Correlation Difference Ratio")
     ] = None
     targets: SignalMetricsTargets
-    original_id: Annotated[UUID, Field(title='Original Id')]
-    avatars_id: Annotated[UUID, Field(title='Avatars Id')]
+    original_id: Annotated[UUID, Field(title="Original Id")]
+    avatars_id: Annotated[UUID, Field(title="Avatars Id")]
 
 
 class SignalMetricsWithTimeSeriesJobCreate(BaseModel):
     kind: Optional[JobKind] = JobKind.signal_metrics_with_time_series
     parameters: SignalMetricsWithTimeSeriesParameters
 
 
 class TableLink(BaseModel):
     parent_table: Annotated[
-        TableReference, Field(description='Reference to the parent table.')
+        TableReference, Field(description="Reference to the parent table.")
     ]
     child_table: Annotated[
-        TableReference, Field(description='Reference to the child table.')
+        TableReference, Field(description="Reference to the child table.")
     ]
     parent_link_key: Annotated[
         str,
         Field(
-            description='Variable to be used for linking with the child table. The link_key specified here must contain unique values only. It is most often the\n        primary key. It needs to have the is_identifier property set to True when creating\n        the dataset.',
-            title='Parent Link Key',
+            description="Variable to be used for linking with the child table. The link_key specified here must contain unique values only. It is most often the\n        primary key. It needs to have the is_identifier property set to True when creating\n        the dataset.",
+            title="Parent Link Key",
         ),
     ]
     child_link_key: Annotated[
         str,
         Field(
-            description='Variable to be used for linking with the parent table. It is most often a foreign key. It needs to have the property is_identifier set to True\n        when creating the dataset.',
-            title='Child Link Key',
+            description="Variable to be used for linking with the parent table. It is most often a foreign key. It needs to have the property is_identifier set to True\n        when creating the dataset.",
+            title="Child Link Key",
         ),
     ]
     link_method: Optional[LinkMethod] = LinkMethod.bottom_projection
 
 
 class User(BaseModel):
-    id: Annotated[UUID, Field(title='Id')]
-    organization_id: Annotated[UUID, Field(title='Organization Id')]
-    username: Annotated[Optional[str], Field(title='Username')] = None
-    email: Annotated[Optional[str], Field(title='Email')] = None
+    id: Annotated[UUID, Field(title="Id")]
+    organization_id: Annotated[UUID, Field(title="Organization Id")]
+    username: Annotated[Optional[str], Field(title="Username")] = None
+    email: Annotated[Optional[str], Field(title="Email")] = None
     role: Optional[UserRole] = UserRole.user
 
 
 class PatchDataset(BaseModel):
-    columns: Annotated[Optional[List[ColumnDetail]], Field(title='Columns')] = None
+    columns: Annotated[Optional[List[ColumnDetail]], Field(title="Columns")] = None
 
 
 class PrivacyMetricsJob(BaseModel):
-    id: Annotated[UUID, Field(title='Id')]
+    id: Annotated[UUID, Field(title="Id")]
     kind: JobKind
-    created_at: Annotated[datetime, Field(title='Created At')]
+    created_at: Annotated[datetime, Field(title="Created At")]
     status: JobStatus
-    error_message: Annotated[Optional[str], Field(title='Error Message')] = None
-    traceback: Annotated[Optional[str], Field(title='Traceback')] = None
+    error_message: Annotated[Optional[str], Field(title="Error Message")] = None
+    traceback: Annotated[Optional[str], Field(title="Traceback")] = None
     result: Optional[PrivacyMetrics] = None
     parameters: PrivacyMetricsParameters
     current_progress: Optional[JobProgress] = None
 
 
 class SignalMetricsJob(BaseModel):
-    id: Annotated[UUID, Field(title='Id')]
+    id: Annotated[UUID, Field(title="Id")]
     kind: JobKind
-    created_at: Annotated[datetime, Field(title='Created At')]
+    created_at: Annotated[datetime, Field(title="Created At")]
     status: JobStatus
-    error_message: Annotated[Optional[str], Field(title='Error Message')] = None
-    traceback: Annotated[Optional[str], Field(title='Traceback')] = None
+    error_message: Annotated[Optional[str], Field(title="Error Message")] = None
+    traceback: Annotated[Optional[str], Field(title="Traceback")] = None
     result: Optional[SignalMetrics] = None
     parameters: SignalMetricsParameters
     current_progress: Optional[JobProgress] = None
 
 
 class AvatarizationBatchParameters(BaseModel):
     """
     Parameters to create a batch anonymization job.
 
     It contains the seed parameter.
     """
 
     model_config = ConfigDict(
-        extra='allow',
+        extra="allow",
     )
-    k: Annotated[int, Field(title='K')]
+    k: Annotated[int, Field(title="K")]
     column_weights: Annotated[
-        Optional[Dict[str, float]], Field(title='Column Weights')
+        Optional[Dict[str, float]], Field(title="Column Weights")
     ] = None
-    ncp: Annotated[Optional[int], Field(title='Ncp')] = None
+    ncp: Annotated[Optional[int], Field(title="Ncp")] = None
     imputation: Optional[ImputationParameters] = None
     use_categorical_reduction: Annotated[
-        Optional[bool], Field(title='Use Categorical Reduction')
+        Optional[bool], Field(title="Use Categorical Reduction")
     ] = None
     to_categorical_threshold: Annotated[
-        Optional[int], Field(title='To Categorical Threshold')
+        Optional[int], Field(title="To Categorical Threshold")
     ] = None
     exclude_categorical: Optional[ExcludeCategoricalParameters] = None
-    training_dataset_id: Annotated[UUID, Field(title='Training Dataset Id')]
-    dataset_ids: Annotated[List[UUID], Field(min_length=1, title='Dataset Ids')]
-    seed: Annotated[Optional[int], Field(title='Seed')] = None
+    training_dataset_id: Annotated[UUID, Field(title="Training Dataset Id")]
+    dataset_ids: Annotated[List[UUID], Field(min_length=1, title="Dataset Ids")]
+    seed: Annotated[Optional[int], Field(title="Seed")] = None
 
 
 class AvatarizationParameters(BaseModel):
     """
     Parameters to create an anonymization job.
 
     It contains the seed parameter.
     """
 
     model_config = ConfigDict(
-        extra='allow',
+        extra="allow",
     )
-    k: Annotated[int, Field(title='K')]
+    k: Annotated[int, Field(title="K")]
     column_weights: Annotated[
-        Optional[Dict[str, float]], Field(title='Column Weights')
+        Optional[Dict[str, float]], Field(title="Column Weights")
     ] = None
-    ncp: Annotated[Optional[int], Field(title='Ncp')] = None
+    ncp: Annotated[Optional[int], Field(title="Ncp")] = None
     imputation: Optional[ImputationParameters] = None
     use_categorical_reduction: Annotated[
-        Optional[bool], Field(title='Use Categorical Reduction')
+        Optional[bool], Field(title="Use Categorical Reduction")
     ] = None
     to_categorical_threshold: Annotated[
-        Optional[int], Field(title='To Categorical Threshold')
+        Optional[int], Field(title="To Categorical Threshold")
     ] = None
     exclude_categorical: Optional[ExcludeCategoricalParameters] = None
-    dataset_id: Annotated[UUID, Field(title='Dataset Id')]
-    seed: Annotated[Optional[int], Field(title='Seed')] = None
+    dataset_id: Annotated[UUID, Field(title="Dataset Id")]
+    seed: Annotated[Optional[int], Field(title="Seed")] = None
 
 
 class AvatarizationPerBatchResult(BaseModel):
     privacy_metrics: Optional[PrivacyMetrics] = None
     signal_metrics: Optional[SignalMetrics] = None
     avatars_dataset: Dataset
     sensitive_unshuffled_avatars_datasets: Dataset
-    original_id: Annotated[UUID, Field(title='Original Id')]
+    original_id: Annotated[UUID, Field(title="Original Id")]
 
 
 class AvatarizationResult(BaseModel):
     privacy_metrics: Optional[PrivacyMetrics] = None
     signal_metrics: Optional[SignalMetrics] = None
     avatars_dataset: Dataset
     sensitive_unshuffled_avatars_datasets: Dataset
 
 
 class AvatarizationResultPerDataset(BaseModel):
     privacy_metrics: Optional[PrivacyMetrics] = None
     signal_metrics: Optional[SignalMetrics] = None
     avatars_dataset: Dataset
     sensitive_unshuffled_avatars_datasets: Dataset
-    original_id: Annotated[UUID, Field(title='Original Id')]
+    original_id: Annotated[UUID, Field(title="Original Id")]
 
 
 class AvatarizationWithTimeSeriesParameters(BaseModel):
     """
     Parameters to create an time series anonymization job.
 
     It contains the seed parameter.
     """
 
     model_config = ConfigDict(
-        extra='allow',
+        extra="allow",
     )
-    k: Annotated[int, Field(title='K')]
+    k: Annotated[int, Field(title="K")]
     column_weights: Annotated[
-        Optional[Dict[str, float]], Field(title='Column Weights')
+        Optional[Dict[str, float]], Field(title="Column Weights")
     ] = None
-    ncp: Annotated[Optional[int], Field(title='Ncp')] = None
+    ncp: Annotated[Optional[int], Field(title="Ncp")] = None
     imputation: Optional[ImputationParameters] = None
     use_categorical_reduction: Annotated[
-        Optional[bool], Field(title='Use Categorical Reduction')
+        Optional[bool], Field(title="Use Categorical Reduction")
     ] = None
     to_categorical_threshold: Annotated[
-        Optional[int], Field(title='To Categorical Threshold')
+        Optional[int], Field(title="To Categorical Threshold")
     ] = None
     exclude_categorical: Optional[ExcludeCategoricalParameters] = None
     vanilla_dataset_id: Annotated[
         Optional[UUID],
         Field(
-            description='Dataset identifier of the vanilla data. ',
-            title='Vanilla Dataset Id',
+            description="Dataset identifier of the vanilla data. ",
+            title="Vanilla Dataset Id",
         ),
     ] = None
     time_series: Annotated[
         List[AvatarizationTimeSeriesParameters],
         Field(
-            description='Parameters of the time series. Each time series can have a different set of parameters.',
-            title='Time Series',
+            description="Parameters of the time series. Each time series can have a different set of parameters.",
+            title="Time Series",
         ),
     ]
-    seed: Annotated[Optional[int], Field(title='Seed')] = None
+    seed: Annotated[Optional[int], Field(title="Seed")] = None
 
 
 class AvatarizationWithTimeSeriesResult(BaseModel):
-    datasets: Annotated[List[AvatarizationResultPerDataset], Field(title='Datasets')]
+    datasets: Annotated[List[AvatarizationResultPerDataset], Field(title="Datasets")]
 
 
 class BaseAvatarizationParameters(BaseModel):
     """
     Parameters to create an anonymization with vanilla dataset.
 
     This class is used in the multi table job. It does not contain the seed.
     """
 
-    k: Annotated[int, Field(title='K')]
+    k: Annotated[int, Field(title="K")]
     column_weights: Annotated[
-        Optional[Dict[str, float]], Field(title='Column Weights')
+        Optional[Dict[str, float]], Field(title="Column Weights")
     ] = None
-    ncp: Annotated[Optional[int], Field(title='Ncp')] = None
+    ncp: Annotated[Optional[int], Field(title="Ncp")] = None
     imputation: Optional[ImputationParameters] = None
     use_categorical_reduction: Annotated[
-        Optional[bool], Field(title='Use Categorical Reduction')
+        Optional[bool], Field(title="Use Categorical Reduction")
     ] = None
     to_categorical_threshold: Annotated[
-        Optional[int], Field(title='To Categorical Threshold')
+        Optional[int], Field(title="To Categorical Threshold")
     ] = None
     exclude_categorical: Optional[ExcludeCategoricalParameters] = None
-    dataset_id: Annotated[UUID, Field(title='Dataset Id')]
+    dataset_id: Annotated[UUID, Field(title="Dataset Id")]
 
 
 class BaseAvatarizationWithTimeSeriesParameters(BaseModel):
     """
     Parameters to configure an anonymization with a time series dataset.
 
     This class is used in the multi table job. It does not contain the seed.
     """
 
-    k: Annotated[int, Field(title='K')]
+    k: Annotated[int, Field(title="K")]
     column_weights: Annotated[
-        Optional[Dict[str, float]], Field(title='Column Weights')
+        Optional[Dict[str, float]], Field(title="Column Weights")
     ] = None
-    ncp: Annotated[Optional[int], Field(title='Ncp')] = None
+    ncp: Annotated[Optional[int], Field(title="Ncp")] = None
     imputation: Optional[ImputationParameters] = None
     use_categorical_reduction: Annotated[
-        Optional[bool], Field(title='Use Categorical Reduction')
+        Optional[bool], Field(title="Use Categorical Reduction")
     ] = None
     to_categorical_threshold: Annotated[
-        Optional[int], Field(title='To Categorical Threshold')
+        Optional[int], Field(title="To Categorical Threshold")
     ] = None
     exclude_categorical: Optional[ExcludeCategoricalParameters] = None
     vanilla_dataset_id: Annotated[
         Optional[UUID],
         Field(
-            description='Dataset identifier of the vanilla data. ',
-            title='Vanilla Dataset Id',
+            description="Dataset identifier of the vanilla data. ",
+            title="Vanilla Dataset Id",
         ),
     ] = None
     time_series: Annotated[
         List[AvatarizationTimeSeriesParameters],
         Field(
-            description='Parameters of the time series. Each time series can have a different set of parameters.',
-            title='Time Series',
+            description="Parameters of the time series. Each time series can have a different set of parameters.",
+            title="Time Series",
         ),
     ]
 
 
 class BasePrivacyMetricsParameters(BaseModel):
     """
     Parameters to configure privacy metrics.
 
     This class is used in the multi table job. It does not contain the seed.
     """
 
     model_config = ConfigDict(
-        extra='allow',
+        extra="allow",
     )
-    original_id: Annotated[UUID, Field(title='Original Id')]
-    persistance_job_id: Annotated[Optional[UUID], Field(title='Persistance Job Id')] = (
+    original_id: Annotated[UUID, Field(title="Original Id")]
+    persistance_job_id: Annotated[Optional[UUID], Field(title="Persistance Job Id")] = (
         None
     )
     avatarization_job_id: Annotated[
-        Optional[UUID], Field(title='Avatarization Job Id')
+        Optional[UUID], Field(title="Avatarization Job Id")
     ] = None
     imputation: Annotated[
         Optional[ImputationParameters],
         Field(
-            description='Imputation parameters used to preprocess avatars data for metrics computation.'
+            description="Imputation parameters used to preprocess avatars data for metrics computation."
         ),
     ] = None
     ncp: Annotated[
         Optional[int],
         Field(
-            description='Number of components used for distance based metrics computation. Default behavior similar to avatarization. If unspecified, ncp is set to the minimum value between 5 and the number of variables after one hot encoding. Note: we recommend using to use the same ncp as during the avatarization to check the worst-case scenario.',
-            title='Ncp',
+            description="Number of components used for distance based metrics computation. Default behavior similar to avatarization. If unspecified, ncp is set to the minimum value between 5 and the number of variables after one hot encoding. Note: we recommend using to use the same ncp as during the avatarization to check the worst-case scenario.",
+            title="Ncp",
         ),
     ] = None
     use_categorical_reduction: Annotated[
         Optional[bool],
         Field(
-            description='Parameter to vectorize categorical variables using cat2vec. See :class:`AvatarizationParameters` for more details. Note: we recommend using the same parameter as during the avatarization.',
-            title='Use Categorical Reduction',
+            description="Parameter to vectorize categorical variables using cat2vec. See :class:`AvatarizationParameters` for more details. Note: we recommend using the same parameter as during the avatarization.",
+            title="Use Categorical Reduction",
         ),
     ] = None
     known_variables: Annotated[
         Optional[List[str]],
         Field(
-            description='Variables considered to be known by the attacker. Used to measure some specific privacy metrics. Note: we recommend using demographic variables as they are the most likely to be known by an attacker.',
-            title='Known Variables',
+            description="Variables considered to be known by the attacker. Used to measure some specific privacy metrics. Note: we recommend using demographic variables as they are the most likely to be known by an attacker.",
+            title="Known Variables",
         ),
     ] = None
     target: Annotated[
         Optional[str],
         Field(
-            description='The target variable to be inferred. Used to measure some specific privacy metrics. Must be used together with ``known_variables``. Note: we recommend using a sensitive variable',
-            title='Target',
+            description="The target variable to be inferred. Used to measure some specific privacy metrics. Must be used together with ``known_variables``. Note: we recommend using a sensitive variable",
+            title="Target",
         ),
     ] = None
     closest_rate_percentage_threshold: Annotated[
         Optional[ClosestRatePercentageThresholdItem],
         Field(
-            description='Threshold of closest original individuals to define a distance threshold. Any avatar data generated closer from an original than the distance threshold, is considered at risk.',
-            title='Closest Rate Percentage Threshold',
+            description="Threshold of closest original individuals to define a distance threshold. Any avatar data generated closer from an original than the distance threshold, is considered at risk.",
+            title="Closest Rate Percentage Threshold",
         ),
     ] = None
     closest_rate_ratio_threshold: Annotated[
         Optional[ClosestRateRatioThresholdItem],
         Field(
-            description='Closest distance ratio threshold below which an avatar data is considered too close to an original and too isolated from the rest to be safe.',
-            title='Closest Rate Ratio Threshold',
+            description="Closest distance ratio threshold below which an avatar data is considered too close to an original and too isolated from the rest to be safe.",
+            title="Closest Rate Ratio Threshold",
         ),
     ] = None
-    unshuffled_avatars_id: Annotated[UUID, Field(title='Unshuffled Avatars Id')]
+    unshuffled_avatars_id: Annotated[UUID, Field(title="Unshuffled Avatars Id")]
 
 
 class BasePrivacyMetricsWithTimeSeriesParameters(BaseModel):
     """
     Parameters to configure a privacy metric job with a time series dataset.
 
     This class is used in the multi table job. It does not contain the seed.
     """
 
     model_config = ConfigDict(
-        extra='allow',
+        extra="allow",
     )
     imputation: Annotated[
         Optional[ImputationParameters],
         Field(
-            description='Imputation parameters used to preprocess avatars data for metrics computation.'
+            description="Imputation parameters used to preprocess avatars data for metrics computation."
         ),
     ] = None
     ncp: Annotated[
         Optional[int],
         Field(
-            description='Number of components used for distance based metrics computation. Default behavior similar to avatarization. If unspecified, ncp is set to the minimum value between 5 and the number of variables after one hot encoding. Note: we recommend using to use the same ncp as during the avatarization to check the worst-case scenario.',
-            title='Ncp',
+            description="Number of components used for distance based metrics computation. Default behavior similar to avatarization. If unspecified, ncp is set to the minimum value between 5 and the number of variables after one hot encoding. Note: we recommend using to use the same ncp as during the avatarization to check the worst-case scenario.",
+            title="Ncp",
         ),
     ] = None
     use_categorical_reduction: Annotated[
         Optional[bool],
         Field(
-            description='Parameter to vectorize categorical variables using cat2vec. See :class:`AvatarizationParameters` for more details. Note: we recommend using the same parameter as during the avatarization.',
-            title='Use Categorical Reduction',
+            description="Parameter to vectorize categorical variables using cat2vec. See :class:`AvatarizationParameters` for more details. Note: we recommend using the same parameter as during the avatarization.",
+            title="Use Categorical Reduction",
         ),
     ] = None
     known_variables: Annotated[
         Optional[List[str]],
         Field(
-            description='Variables considered to be known by the attacker. Used to measure some specific privacy metrics. Note: we recommend using demographic variables as they are the most likely to be known by an attacker.',
-            title='Known Variables',
+            description="Variables considered to be known by the attacker. Used to measure some specific privacy metrics. Note: we recommend using demographic variables as they are the most likely to be known by an attacker.",
+            title="Known Variables",
         ),
     ] = None
     target: Annotated[
         Optional[str],
         Field(
-            description='The target variable to be inferred. Used to measure some specific privacy metrics. Must be used together with ``known_variables``. Note: we recommend using a sensitive variable',
-            title='Target',
+            description="The target variable to be inferred. Used to measure some specific privacy metrics. Must be used together with ``known_variables``. Note: we recommend using a sensitive variable",
+            title="Target",
         ),
     ] = None
     closest_rate_percentage_threshold: Annotated[
         Optional[ClosestRatePercentageThresholdItem],
         Field(
-            description='Threshold of closest original individuals to define a distance threshold. Any avatar data generated closer from an original than the distance threshold, is considered at risk.',
-            title='Closest Rate Percentage Threshold',
+            description="Threshold of closest original individuals to define a distance threshold. Any avatar data generated closer from an original than the distance threshold, is considered at risk.",
+            title="Closest Rate Percentage Threshold",
         ),
     ] = None
     closest_rate_ratio_threshold: Annotated[
         Optional[ClosestRateRatioThresholdItem],
         Field(
-            description='Closest distance ratio threshold below which an avatar data is considered too close to an original and too isolated from the rest to be safe.',
-            title='Closest Rate Ratio Threshold',
+            description="Closest distance ratio threshold below which an avatar data is considered too close to an original and too isolated from the rest to be safe.",
+            title="Closest Rate Ratio Threshold",
         ),
     ] = None
     vanilla_original_id: Annotated[
         Optional[UUID],
         Field(
-            description='Identifier of the original vanilla dataset.',
-            title='Vanilla Original Id',
+            description="Identifier of the original vanilla dataset.",
+            title="Vanilla Original Id",
         ),
     ] = None
     vanilla_unshuffled_avatars_id: Annotated[
         Optional[UUID],
         Field(
-            description='Identifier of the avatar vanilla dataset.',
-            title='Vanilla Unshuffled Avatars Id',
+            description="Identifier of the avatar vanilla dataset.",
+            title="Vanilla Unshuffled Avatars Id",
         ),
     ] = None
     time_series: Annotated[
         List[PrivacyMetricsTimeSeriesParameters],
         Field(
-            description='Privacy metrics parameters of the time series. Each time series can have a different set of parameters.',
-            title='Time Series',
+            description="Privacy metrics parameters of the time series. Each time series can have a different set of parameters.",
+            title="Time Series",
         ),
     ]
 
 
 class CommonPrivacyMetricsParameters(BaseModel):
     """
     Parameters to configure common privacy metrics parameters.
     """
 
     imputation: Annotated[
         Optional[ImputationParameters],
         Field(
-            description='Imputation parameters used to preprocess avatars data for metrics computation.'
+            description="Imputation parameters used to preprocess avatars data for metrics computation."
         ),
     ] = None
     ncp: Annotated[
         Optional[int],
         Field(
-            description='Number of components used for distance based metrics computation. Default behavior similar to avatarization. If unspecified, ncp is set to the minimum value between 5 and the number of variables after one hot encoding. Note: we recommend using to use the same ncp as during the avatarization to check the worst-case scenario.',
-            title='Ncp',
+            description="Number of components used for distance based metrics computation. Default behavior similar to avatarization. If unspecified, ncp is set to the minimum value between 5 and the number of variables after one hot encoding. Note: we recommend using to use the same ncp as during the avatarization to check the worst-case scenario.",
+            title="Ncp",
         ),
     ] = None
     use_categorical_reduction: Annotated[
         Optional[bool],
         Field(
-            description='Parameter to vectorize categorical variables using cat2vec. See :class:`AvatarizationParameters` for more details. Note: we recommend using the same parameter as during the avatarization.',
-            title='Use Categorical Reduction',
+            description="Parameter to vectorize categorical variables using cat2vec. See :class:`AvatarizationParameters` for more details. Note: we recommend using the same parameter as during the avatarization.",
+            title="Use Categorical Reduction",
         ),
     ] = None
     known_variables: Annotated[
         Optional[List[str]],
         Field(
-            description='Variables considered to be known by the attacker. Used to measure some specific privacy metrics. Note: we recommend using demographic variables as they are the most likely to be known by an attacker.',
-            title='Known Variables',
+            description="Variables considered to be known by the attacker. Used to measure some specific privacy metrics. Note: we recommend using demographic variables as they are the most likely to be known by an attacker.",
+            title="Known Variables",
         ),
     ] = None
     target: Annotated[
         Optional[str],
         Field(
-            description='The target variable to be inferred. Used to measure some specific privacy metrics. Must be used together with ``known_variables``. Note: we recommend using a sensitive variable',
-            title='Target',
+            description="The target variable to be inferred. Used to measure some specific privacy metrics. Must be used together with ``known_variables``. Note: we recommend using a sensitive variable",
+            title="Target",
         ),
     ] = None
     closest_rate_percentage_threshold: Annotated[
         Optional[ClosestRatePercentageThresholdItem],
         Field(
-            description='Threshold of closest original individuals to define a distance threshold. Any avatar data generated closer from an original than the distance threshold, is considered at risk.',
-            title='Closest Rate Percentage Threshold',
+            description="Threshold of closest original individuals to define a distance threshold. Any avatar data generated closer from an original than the distance threshold, is considered at risk.",
+            title="Closest Rate Percentage Threshold",
         ),
     ] = None
     closest_rate_ratio_threshold: Annotated[
         Optional[ClosestRateRatioThresholdItem],
         Field(
-            description='Closest distance ratio threshold below which an avatar data is considered too close to an original and too isolated from the rest to be safe.',
-            title='Closest Rate Ratio Threshold',
+            description="Closest distance ratio threshold below which an avatar data is considered too close to an original and too isolated from the rest to be safe.",
+            title="Closest Rate Ratio Threshold",
         ),
     ] = None
 
 
 class MetaPrivacyMetrics(BaseModel):
     details: Annotated[
         List[
             Union[
                 PrivacyMetricsPerMultiTableDataset, PrivacyMetricsPerTimeSeriesDataset
             ]
         ],
-        Field(title='Details'),
+        Field(title="Details"),
     ]
 
 
 class MetaSignalMetrics(BaseModel):
     details: Annotated[
         List[Union[SignalMetricsPerDataset, TimeSeriesSignalMetricsPerDataset]],
-        Field(title='Details'),
+        Field(title="Details"),
     ]
 
 
 class PrivacyMetricsBatchParameters(BaseModel):
     """
     Parameters to configure a PrivacyMetricsBatchJob.
 
@@ -1683,83 +1683,83 @@
     ...         original_id=training_original_id
     ...        ),
     ...     batch_dataset_mappings=batch_dataset_mappings
     ... )
     """
 
     model_config = ConfigDict(
-        extra='allow',
+        extra="allow",
     )
     avatarization_batch_job_id: Annotated[
         Optional[UUID],
         Field(
-            description='Identifier of the avatarization batch job that was launched prior to this privacy metrics batch job. This has to be set to None if you are computing privacy metrics without applying the avatar method beforehand. Setting it to ``None`` requires ``training_dataset_mapping`` and ``batch_dataset_mappings`` to be set.',
-            title='Avatarization Batch Job Id',
+            description="Identifier of the avatarization batch job that was launched prior to this privacy metrics batch job. This has to be set to None if you are computing privacy metrics without applying the avatar method beforehand. Setting it to ``None`` requires ``training_dataset_mapping`` and ``batch_dataset_mappings`` to be set.",
+            title="Avatarization Batch Job Id",
         ),
     ] = None
     training_dataset_mapping: Annotated[
         Optional[PrivacyBatchDatasetMapping],
         Field(
-            description='Dataset identifiers for the training batch. The dataset identifiers specified here will be used to fit the anonymization.'
+            description="Dataset identifiers for the training batch. The dataset identifiers specified here will be used to fit the anonymization."
         ),
     ] = None
     common_parameters: Annotated[
         Optional[CommonPrivacyMetricsParameters],
         Field(
-            description='Parameters to use during the computation of the privacy metrics. These will be applied on all the batches, including the training batch.'
+            description="Parameters to use during the computation of the privacy metrics. These will be applied on all the batches, including the training batch."
         ),
     ] = None
     seed: Annotated[
         Optional[int],
-        Field(description='Seed used to generate the random state.', title='Seed'),
+        Field(description="Seed used to generate the random state.", title="Seed"),
     ] = None
     batch_dataset_mappings: Annotated[
         Optional[List[PrivacyBatchDatasetMapping]],
         Field(
-            description='List of pairs of dataset identifiers. You should not specify again the dataset identifiers that were specified in ``training_dataset_mapping``.',
-            title='Batch Dataset Mappings',
+            description="List of pairs of dataset identifiers. You should not specify again the dataset identifiers that were specified in ``training_dataset_mapping``.",
+            title="Batch Dataset Mappings",
         ),
     ] = None
 
 
 class PrivacyMetricsBatchResult(BaseModel):
     worst_metrics: PrivacyMetrics
     mean_metrics: PrivacyMetrics
     training_metrics: PrivacyMetricsPerBatchResult
     batch_metrics: Annotated[
-        List[PrivacyMetricsPerBatchResult], Field(title='Batch Metrics')
+        List[PrivacyMetricsPerBatchResult], Field(title="Batch Metrics")
     ]
 
 
 class PrivacyMetricsGeolocationParameters(BaseModel):
     """
     Parameters to configure a PrivacyMetricsGeolocationJob.
 
     It enables to compute privacy metrics for multiple scenarios (with no limit in number).
     """
 
     model_config = ConfigDict(
-        extra='allow',
+        extra="allow",
     )
-    original_dataset_id: Annotated[UUID, Field(title='Original Dataset Id')]
+    original_dataset_id: Annotated[UUID, Field(title="Original Dataset Id")]
     unshuffled_avatar_dataset_id: Annotated[
-        UUID, Field(title='Unshuffled Avatar Dataset Id')
+        UUID, Field(title="Unshuffled Avatar Dataset Id")
     ]
     scenarios: Annotated[
-        List[PrivacyMetricsGeolocationScenario], Field(title='Scenarios')
+        List[PrivacyMetricsGeolocationScenario], Field(title="Scenarios")
     ]
     parameter_type: Annotated[
-        Literal['privacy_metrics_geolocation'], Field(title='Parameter Type')
-    ] = 'privacy_metrics_geolocation'
+        Literal["privacy_metrics_geolocation"], Field(title="Parameter Type")
+    ] = "privacy_metrics_geolocation"
 
 
 class PrivacyMetricsGeolocationResult(BaseModel):
     privacy_metrics_per_scenario: Annotated[
         List[PrivacyMetricsGeolocationScenarioResult],
-        Field(title='Privacy Metrics Per Scenario'),
+        Field(title="Privacy Metrics Per Scenario"),
     ]
 
 
 class PrivacyMetricsJobCreate(BaseModel):
     kind: Optional[JobKind] = JobKind.privacy_metrics
     parameters: PrivacyMetricsParameters
 
@@ -1799,144 +1799,146 @@
       ------------------------
 
       - The avatar datasets referenced by the avatars_id in table_parameters need to be
         identical in columns, shape, and dtype to their corresponding original dataset.
     """
 
     model_config = ConfigDict(
-        extra='allow',
+        extra="allow",
     )
     table_links: Annotated[
         List[TableLink],
         Field(
-            description='Description of each relational link between tables.',
-            title='Table Links',
+            description="Description of each relational link between tables.",
+            min_length=1,
+            title="Table Links",
         ),
     ]
     table_parameters: Annotated[
         List[
             Union[
                 BasePrivacyMetricsParameters, BasePrivacyMetricsWithTimeSeriesParameters
             ]
         ],
         Field(
-            description='Privacy metrics parameters for each table. ',
-            title='Table Parameters',
+            description="Privacy metrics parameters for each table. ",
+            min_length=1,
+            title="Table Parameters",
         ),
     ]
     seed: Annotated[
         Optional[int],
-        Field(description='Seed used to generate the random state.', title='Seed'),
+        Field(description="Seed used to generate the random state.", title="Seed"),
     ] = None
 
 
 class PrivacyMetricsWithTimeSeriesJobCreate(BaseModel):
     kind: Optional[JobKind] = JobKind.privacy_metrics_with_time_series
     parameters: PrivacyMetricsWithTimeSeriesParameters
 
 
 class SignalMetricsBatchResult(BaseModel):
     mean_metrics: SignalMetrics
     training_metrics: SignalMetricsPerBatchResult
     batch_metrics: Annotated[
-        List[SignalMetricsPerBatchResult], Field(title='Batch Metrics')
+        List[SignalMetricsPerBatchResult], Field(title="Batch Metrics")
     ]
 
 
 class AvatarizationJob(BaseModel):
-    id: Annotated[UUID, Field(title='Id')]
+    id: Annotated[UUID, Field(title="Id")]
     kind: JobKind
-    created_at: Annotated[datetime, Field(title='Created At')]
+    created_at: Annotated[datetime, Field(title="Created At")]
     status: JobStatus
-    error_message: Annotated[Optional[str], Field(title='Error Message')] = None
-    traceback: Annotated[Optional[str], Field(title='Traceback')] = None
+    error_message: Annotated[Optional[str], Field(title="Error Message")] = None
+    traceback: Annotated[Optional[str], Field(title="Traceback")] = None
     result: Optional[AvatarizationResult] = None
     parameters: AvatarizationParameters
     current_progress: Optional[JobProgress] = None
 
 
 class AvatarizationWithTimeSeriesJob(BaseModel):
-    id: Annotated[UUID, Field(title='Id')]
+    id: Annotated[UUID, Field(title="Id")]
     kind: JobKind
-    created_at: Annotated[datetime, Field(title='Created At')]
+    created_at: Annotated[datetime, Field(title="Created At")]
     status: JobStatus
-    error_message: Annotated[Optional[str], Field(title='Error Message')] = None
-    traceback: Annotated[Optional[str], Field(title='Traceback')] = None
+    error_message: Annotated[Optional[str], Field(title="Error Message")] = None
+    traceback: Annotated[Optional[str], Field(title="Traceback")] = None
     result: Optional[AvatarizationWithTimeSeriesResult] = None
     parameters: AvatarizationWithTimeSeriesParameters
     current_progress: Optional[JobProgress] = None
 
 
 class PrivacyMetricsBatchJob(BaseModel):
-    id: Annotated[UUID, Field(title='Id')]
+    id: Annotated[UUID, Field(title="Id")]
     kind: JobKind
-    created_at: Annotated[datetime, Field(title='Created At')]
+    created_at: Annotated[datetime, Field(title="Created At")]
     status: JobStatus
-    error_message: Annotated[Optional[str], Field(title='Error Message')] = None
-    traceback: Annotated[Optional[str], Field(title='Traceback')] = None
+    error_message: Annotated[Optional[str], Field(title="Error Message")] = None
+    traceback: Annotated[Optional[str], Field(title="Traceback")] = None
     result: Optional[PrivacyMetricsBatchResult] = None
     parameters: PrivacyMetricsBatchParameters
     current_progress: Optional[JobProgress] = None
 
 
 class PrivacyMetricsGeolocationJob(BaseModel):
-    id: Annotated[UUID, Field(title='Id')]
+    id: Annotated[UUID, Field(title="Id")]
     kind: JobKind
-    created_at: Annotated[datetime, Field(title='Created At')]
+    created_at: Annotated[datetime, Field(title="Created At")]
     status: JobStatus
-    error_message: Annotated[Optional[str], Field(title='Error Message')] = None
-    traceback: Annotated[Optional[str], Field(title='Traceback')] = None
+    error_message: Annotated[Optional[str], Field(title="Error Message")] = None
+    traceback: Annotated[Optional[str], Field(title="Traceback")] = None
     result: Optional[PrivacyMetricsGeolocationResult] = None
     parameters: PrivacyMetricsGeolocationParameters
     current_progress: Optional[JobProgress] = None
 
 
 class PrivacyMetricsMultiTableJob(BaseModel):
-    id: Annotated[UUID, Field(title='Id')]
+    id: Annotated[UUID, Field(title="Id")]
     kind: JobKind
-    created_at: Annotated[datetime, Field(title='Created At')]
+    created_at: Annotated[datetime, Field(title="Created At")]
     status: JobStatus
-    error_message: Annotated[Optional[str], Field(title='Error Message')] = None
-    traceback: Annotated[Optional[str], Field(title='Traceback')] = None
+    error_message: Annotated[Optional[str], Field(title="Error Message")] = None
+    traceback: Annotated[Optional[str], Field(title="Traceback")] = None
     result: Optional[MetaPrivacyMetrics] = None
     parameters: PrivacyMetricsMultiTableParameters
     current_progress: Optional[JobProgress] = None
 
 
 class PrivacyMetricsWithTimeSeriesJob(BaseModel):
-    id: Annotated[UUID, Field(title='Id')]
+    id: Annotated[UUID, Field(title="Id")]
     kind: JobKind
-    created_at: Annotated[datetime, Field(title='Created At')]
+    created_at: Annotated[datetime, Field(title="Created At")]
     status: JobStatus
-    error_message: Annotated[Optional[str], Field(title='Error Message')] = None
-    traceback: Annotated[Optional[str], Field(title='Traceback')] = None
+    error_message: Annotated[Optional[str], Field(title="Error Message")] = None
+    traceback: Annotated[Optional[str], Field(title="Traceback")] = None
     result: Optional[MetaPrivacyMetrics] = None
     parameters: PrivacyMetricsWithTimeSeriesParameters
     current_progress: Optional[JobProgress] = None
 
 
 class SignalMetricsBatchJob(BaseModel):
-    id: Annotated[UUID, Field(title='Id')]
+    id: Annotated[UUID, Field(title="Id")]
     kind: JobKind
-    created_at: Annotated[datetime, Field(title='Created At')]
+    created_at: Annotated[datetime, Field(title="Created At")]
     status: JobStatus
-    error_message: Annotated[Optional[str], Field(title='Error Message')] = None
-    traceback: Annotated[Optional[str], Field(title='Traceback')] = None
+    error_message: Annotated[Optional[str], Field(title="Error Message")] = None
+    traceback: Annotated[Optional[str], Field(title="Traceback")] = None
     result: Optional[SignalMetricsBatchResult] = None
     parameters: SignalMetricsBatchParameters
     current_progress: Optional[JobProgress] = None
 
 
 class SignalMetricsWithTimeSeriesJob(BaseModel):
-    id: Annotated[UUID, Field(title='Id')]
+    id: Annotated[UUID, Field(title="Id")]
     kind: JobKind
-    created_at: Annotated[datetime, Field(title='Created At')]
+    created_at: Annotated[datetime, Field(title="Created At")]
     status: JobStatus
-    error_message: Annotated[Optional[str], Field(title='Error Message')] = None
-    traceback: Annotated[Optional[str], Field(title='Traceback')] = None
+    error_message: Annotated[Optional[str], Field(title="Error Message")] = None
+    traceback: Annotated[Optional[str], Field(title="Traceback")] = None
     result: Optional[MetaSignalMetrics] = None
     parameters: SignalMetricsWithTimeSeriesParameters
     current_progress: Optional[JobProgress] = None
 
 
 class AvatarizationBatchJobCreate(BaseModel):
     kind: Optional[JobKind] = JobKind.avatarization_batch
@@ -1944,15 +1946,15 @@
 
 
 class AvatarizationBatchResult(BaseModel):
     privacy_metrics: Optional[PrivacyMetrics] = None
     signal_metrics: Optional[SignalMetrics] = None
     training_result: AvatarizationPerBatchResult
     batch_results: Annotated[
-        List[AvatarizationPerBatchResult], Field(title='Batch Results')
+        List[AvatarizationPerBatchResult], Field(title="Batch Results")
     ]
 
 
 class AvatarizationJobCreate(BaseModel):
     kind: Optional[JobKind] = JobKind.avatarization
     parameters: AvatarizationParameters
 
@@ -1984,45 +1986,47 @@
     - Every table can only reference another table once. This means that A -> B <- A is
       not possible.
     - Top level tables need to be individual_level tables. For example, A -> B <- C , A and C need
       to be individual_level tables.
     """
 
     model_config = ConfigDict(
-        extra='allow',
+        extra="allow",
     )
     table_links: Annotated[
         List[TableLink],
         Field(
-            description='Description of each relational link between tables.',
-            title='Table Links',
+            description="Description of each relational link between tables.",
+            min_length=1,
+            title="Table Links",
         ),
     ]
     table_parameters: Annotated[
         List[
             Union[
                 BaseAvatarizationParameters, BaseAvatarizationWithTimeSeriesParameters
             ]
         ],
         Field(
-            description='Avatarization parameters for each table. ',
-            title='Table Parameters',
+            description="Avatarization parameters for each table. ",
+            min_length=1,
+            title="Table Parameters",
         ),
     ]
     seed: Annotated[
         Optional[int],
         Field(
-            description='Seed used to generate the random numbers.If not provided, a random seed will be generated.',
-            title='Seed',
+            description="Seed used to generate the random numbers.If not provided, a random seed will be generated.",
+            title="Seed",
         ),
     ] = None
 
 
 class AvatarizationMultiTableResult(BaseModel):
-    datasets: Annotated[List[AvatarizationResultPerDataset], Field(title='Datasets')]
+    datasets: Annotated[List[AvatarizationResultPerDataset], Field(title="Datasets")]
 
 
 class AvatarizationWithTimeSeriesJobCreate(BaseModel):
     kind: Optional[JobKind] = JobKind.avatarization_with_time_series
     parameters: AvatarizationWithTimeSeriesParameters
 
 
@@ -2038,49 +2042,47 @@
 
 class PrivacyMetricsMultiTableJobCreate(BaseModel):
     kind: Optional[JobKind] = JobKind.privacy_metrics_multi_table
     parameters: PrivacyMetricsMultiTableParameters
 
 
 class AvatarizationBatchJob(BaseModel):
-    id: Annotated[UUID, Field(title='Id')]
+    id: Annotated[UUID, Field(title="Id")]
     kind: JobKind
-    created_at: Annotated[datetime, Field(title='Created At')]
+    created_at: Annotated[datetime, Field(title="Created At")]
     status: JobStatus
-    error_message: Annotated[Optional[str], Field(title='Error Message')] = None
-    traceback: Annotated[Optional[str], Field(title='Traceback')] = None
+    error_message: Annotated[Optional[str], Field(title="Error Message")] = None
+    traceback: Annotated[Optional[str], Field(title="Traceback")] = None
     result: Optional[AvatarizationBatchResult] = None
     parameters: AvatarizationBatchParameters
     current_progress: Optional[JobProgress] = None
 
 
 class AvatarizationMultiTableJob(BaseModel):
-    id: Annotated[UUID, Field(title='Id')]
+    id: Annotated[UUID, Field(title="Id")]
     kind: JobKind
-    created_at: Annotated[datetime, Field(title='Created At')]
+    created_at: Annotated[datetime, Field(title="Created At")]
     status: JobStatus
-    error_message: Annotated[Optional[str], Field(title='Error Message')] = None
-    traceback: Annotated[Optional[str], Field(title='Traceback')] = None
+    error_message: Annotated[Optional[str], Field(title="Error Message")] = None
+    traceback: Annotated[Optional[str], Field(title="Traceback")] = None
     result: Optional[AvatarizationMultiTableResult] = None
     parameters: AvatarizationMultiTableParameters
     current_progress: Optional[JobProgress] = None
 
 
 class AvatarizationMultiTableJobCreate(BaseModel):
     kind: Optional[JobKind] = JobKind.avatarization_multi_table
     parameters: AvatarizationMultiTableParameters
 
 
 @runtime_checkable
 class Processor(Protocol):
-    def preprocess(self, df: pd.DataFrame) -> pd.DataFrame:
-        ...
+    def preprocess(self, df: pd.DataFrame) -> pd.DataFrame: ...
 
-    def postprocess(self, source: pd.DataFrame, dest: pd.DataFrame) -> pd.DataFrame:
-        ...
+    def postprocess(self, source: pd.DataFrame, dest: pd.DataFrame) -> pd.DataFrame: ...
 
 
 class AvatarizationPipelineCreate(BaseModel):
     model_config = ConfigDict(arbitrary_types_allowed=True)
 
     avatarization_job_create: AvatarizationJobCreate
     processors: List[Processor] = []
```

### Comparing `octopize_avatar-0.7.2/avatars/processors/__init__.py` & `octopize_avatar-0.7.3/avatars/processors/__init__.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.2/avatars/processors/conftest.py` & `octopize_avatar-0.7.3/avatars/processors/conftest.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.2/avatars/processors/datetime.py` & `octopize_avatar-0.7.3/avatars/processors/datetime.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.2/avatars/processors/datetime_test.py` & `octopize_avatar-0.7.3/avatars/processors/datetime_test.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.2/avatars/processors/expected_mean.py` & `octopize_avatar-0.7.3/avatars/processors/expected_mean.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.2/avatars/processors/expected_mean_test.py` & `octopize_avatar-0.7.3/avatars/processors/expected_mean_test.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.2/avatars/processors/group_modalities.py` & `octopize_avatar-0.7.3/avatars/processors/group_modalities.py`

 * *Files 2% similar despite different names*

```diff
@@ -34,15 +34,19 @@
     ... )
     >>> df
       variable_1 variable_2 variable_3
     0        red        red      green
     1       blue       blue      green
     2       blue       blue      green
     3      green        red      green
-    >>> processor = GroupModalitiesProcessor(min_unique=2, global_threshold=1, new_category="other")
+    >>> processor = GroupModalitiesProcessor(
+    ...     min_unique=2,
+    ...     global_threshold=1,
+    ...     new_category="other"
+    ... )
     >>> processor.preprocess(df)
       variable_1 variable_2 variable_3
     0      other        red      green
     1       blue       blue      green
     2       blue       blue      green
     3      other        red      green
     """
```

### Comparing `octopize_avatar-0.7.2/avatars/processors/group_modalities_test.py` & `octopize_avatar-0.7.3/avatars/processors/group_modalities_test.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.2/avatars/processors/inter_record_bounded_cumulated_difference.py` & `octopize_avatar-0.7.3/avatars/processors/inter_record_bounded_cumulated_difference.py`

 * *Files 1% similar despite different names*

```diff
@@ -169,40 +169,46 @@
         working = working.drop(columns=[self.target_variable])
 
         return working
 
     def postprocess(self, source: pd.DataFrame, dest: pd.DataFrame) -> pd.DataFrame:
         if self.new_first_variable_name not in dest.columns.values:
             raise ValueError(
-                f"Expected a valid `new_first_variable_name`, got {self.new_first_variable_name} instead"
+                "Expected a valid `new_first_variable_name`, "
+                f"got {self.new_first_variable_name} instead"
             )
 
         if self.new_difference_variable_name not in dest.columns.values:
             raise ValueError(
-                f"Expected a valid `new_difference_variable_name`, got {self.new_difference_variable_name} instead"
+                "Expected a valid `new_difference_variable_name`, "
+                f"got {self.new_difference_variable_name} instead"
             )
 
         if source[self.id_variable].isnull().values.any():
             raise ValueError(
-                "Expected no missing values for id variable in source, got column with nulls instead"
+                "Expected no missing values for id variable in source, "
+                "got column with nulls instead"
             )
 
         if source[self.target_variable].isnull().values.any():
             raise ValueError(
-                "Expected no missing values for target variable in source, got column with nulls instead"
+                "Expected no missing values for target variable in source, "
+                "got column with nulls instead"
             )
 
         if dest[self.new_difference_variable_name].isnull().values.any():
             raise ValueError(
-                "Expected no missing values for `new_difference_variable_name`, got column with nulls instead"
+                "Expected no missing values for `new_difference_variable_name`, "
+                "got column with nulls instead"
             )
 
         if dest[self.new_first_variable_name].isnull().values.any():
             raise ValueError(
-                "Expected no missing values for `new_first_variable_name`, got column with nulls instead"
+                "Expected no missing values for `new_first_variable_name`, "
+                "got column with nulls instead"
             )
 
         working = dest.copy()
 
         vals = []
         sorted_indices = []
```

### Comparing `octopize_avatar-0.7.2/avatars/processors/inter_record_bounded_cumulated_difference_test.py` & `octopize_avatar-0.7.3/avatars/processors/inter_record_bounded_cumulated_difference_test.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.2/avatars/processors/inter_record_bounded_range_difference.py` & `octopize_avatar-0.7.3/avatars/processors/inter_record_bounded_range_difference.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.2/avatars/processors/inter_record_bounded_range_difference_test.py` & `octopize_avatar-0.7.3/avatars/processors/inter_record_bounded_range_difference_test.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.2/avatars/processors/inter_record_cumulated_difference.py` & `octopize_avatar-0.7.3/avatars/processors/inter_record_cumulated_difference.py`

 * *Files 2% similar despite different names*

```diff
@@ -155,48 +155,55 @@
         df = df.sort_index()
 
         return df
 
     def postprocess(self, source: pd.DataFrame, dest: pd.DataFrame) -> pd.DataFrame:
         if self.new_first_variable_name not in dest.columns.values:
             raise ValueError(
-                f"Expected a valid `new_first_variable_name`, got {self.new_first_variable_name} instead"
+                "Expected a valid `new_first_variable_name`, "
+                f"got {self.new_first_variable_name} instead"
             )
 
         if self.new_difference_variable_name not in dest.columns.values:
             raise ValueError(
-                f"Expected a valid `new_difference_variable_name`, got {self.new_difference_variable_name} instead"
+                "Expected a valid `new_difference_variable_name`, "
+                f"got {self.new_difference_variable_name} instead"
             )
 
         if source[self.id_variable].isnull().values.any():
             raise ValueError(
-                "Expected no missing values for id variable in source, got column with nulls instead"
+                "Expected no missing values for id variable in source, "
+                "got column with nulls instead"
             )
 
         if source[self.target_variable].isnull().values.any():
             raise ValueError(
-                "Expected no missing values for target variable in source, got column with nulls instead"
+                "Expected no missing values for target variable in source, "
+                "got column with nulls instead"
             )
 
         if dest[self.new_difference_variable_name].isnull().values.any():
             raise ValueError(
-                "Expected no missing values for `new_difference_variable_name`, got column with nulls instead"
+                "Expected no missing values for `new_difference_variable_name`, "
+                "got column with nulls instead"
             )
 
         if dest[self.new_first_variable_name].isnull().values.any():
             raise ValueError(
-                "Expected no missing values for `new_first_variable_name`, got column with nulls instead"
+                "Expected no missing values for `new_first_variable_name`, "
+                "got column with nulls instead"
             )
 
         if (
             self.keep_record_order
             and len(set(source.index).symmetric_difference(dest.index)) > 0
         ):
             raise ValueError(
-                "Expected `keep_record_order` to be `True` only if source and dest have same indices, got source and dest with different indices"
+                "Expected `keep_record_order` to be `True` only if source and dest "
+                "have same indices, got source and dest with different indices"
             )
 
         df = dest.copy()
 
         # sort values in the same way as they were ordered in preprocess
         if self.keep_record_order:
             ordered_indices = source.sort_values(
```

### Comparing `octopize_avatar-0.7.2/avatars/processors/inter_record_cumulated_difference_test.py` & `octopize_avatar-0.7.3/avatars/processors/inter_record_cumulated_difference_test.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.2/avatars/processors/inter_record_range_difference.py` & `octopize_avatar-0.7.3/avatars/processors/inter_record_range_difference.py`

 * *Files 2% similar despite different names*

```diff
@@ -149,20 +149,24 @@
             self.id_variable,
             self.target_start_variable,
             self.target_end_variable,
             self.sort_by_variable,
         ]
         if len(set(variables_to_check).difference(df.columns.values)) > 0:
             raise ValueError(
-                f"Expected valid variable names for `id_variable`, `target_start_variable`, `target_end_variable` and `sort_by_variable`, got '{self.id_variable}', '{self.target_start_variable}', '{self.target_end_variable}' and '{self.sort_by_variable}' instead"
+                "Expected valid variable names for `id_variable`, `target_start_variable`, "
+                "`target_end_variable` and `sort_by_variable`, got "
+                f"'{self.id_variable}', '{self.target_start_variable}', "
+                f"'{self.target_end_variable}' and '{self.sort_by_variable}' instead"
             )
 
         if df[variables_to_check].isnull().values.any():
             raise ValueError(
-                f"Expected no missing values for `id_variable`, `target_start_variable`, `target_end_variable` and `sort_by_variable`, got columns with nulls instead"
+                "Expected no missing values for `id_variable`, `target_start_variable`, "
+                "`target_end_variable` and `sort_by_variable`, got columns with nulls instead"
             )
 
         df = df.copy()
 
         # data needs to be sorted
         df = df.sort_values([self.id_variable, self.sort_by_variable])
 
@@ -206,38 +210,45 @@
             self.id_variable,
             self.new_first_variable,
             self.new_range_variable,
             self.new_difference_variable,
         ]
         if len(set(variables_to_check).difference(dest.columns.values)) > 0:
             raise ValueError(
-                f"Expected valid variable names for `id_variable`, `new_first_variable`, `new_range_variable` and `new_difference_variable`, got '{self.id_variable}', '{self.new_first_variable}', '{self.new_range_variable}' and '{self.new_difference_variable}' instead"
+                "Expected valid variable names for `id_variable`, `new_first_variable`, "
+                "`new_range_variable` and `new_difference_variable`, "
+                f"got '{self.id_variable}', '{self.new_first_variable}', "
+                f"'{self.new_range_variable}' and '{self.new_difference_variable}' instead"
             )
 
         if self.sort_by_variable not in source.columns.values:
             raise ValueError(
                 f"Expected a valid `sort_by_variable`, got '{self.sort_by_variable}' instead"
             )
 
         if dest[variables_to_check].isnull().values.any():
             raise ValueError(
-                f"Expected no missing values for `id_variable`, `new_first_variable`, `new_range_variable` and `new_difference_variable`, got columns with nulls instead"
+                "Expected no missing values for `id_variable`, "
+                "`new_first_variable`, `new_range_variable` and `new_difference_variable`, "
+                "got columns with nulls instead"
             )
 
         if source[self.sort_by_variable].isnull().values.any():
             raise ValueError(
-                "Expected no missing values for `sort_by_variable` in source, got column with nulls instead"
+                "Expected no missing values for `sort_by_variable` in source, "
+                "got column with nulls instead"
             )
 
         if (
             self.keep_record_order
             and len(set(source.index).symmetric_difference(dest.index)) > 0
         ):
             raise ValueError(
-                "Expected `keep_record_order` to be `True` only if source and dest have same indices, got source and dest with different indices"
+                "Expected `keep_record_order` to be `True` only if source and dest "
+                "have same indices, got source and dest with different indices"
             )
 
         df = dest.copy()
 
         # sort values in the same way as they were ordered in preprocess
         if self.keep_record_order:
             ordered_indices = source.sort_values(
```

### Comparing `octopize_avatar-0.7.2/avatars/processors/inter_record_range_difference_test.py` & `octopize_avatar-0.7.3/avatars/processors/inter_record_range_difference_test.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.2/avatars/processors/perturbation.py` & `octopize_avatar-0.7.3/avatars/processors/perturbation.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.2/avatars/processors/perturbation_test.py` & `octopize_avatar-0.7.3/avatars/processors/perturbation_test.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.2/avatars/processors/proportions.py` & `octopize_avatar-0.7.3/avatars/processors/proportions.py`

 * *Files 0% similar despite different names*

```diff
@@ -5,15 +5,16 @@
 
 from avatars.lib.saferound import saferound
 
 
 class ProportionProcessor:
     """Processor to express numeric variables as a proportion of another variable.
 
-    By this transformation, we keep the addition and substraction relations such as variable_1 = variable_2 + variable_3.
+    By this transformation, we keep the addition and subtraction relations
+    such as variable_1 = variable_2 + variable_3.
 
     Arguments
     ---------
         variable_names:
             variables to transform
         reference:
             the variable of reference
```

### Comparing `octopize_avatar-0.7.2/avatars/processors/proportions_test.py` & `octopize_avatar-0.7.3/avatars/processors/proportions_test.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.2/avatars/processors/relative_difference.py` & `octopize_avatar-0.7.3/avatars/processors/relative_difference.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,17 @@
 from typing import List, Optional
 
 import pandas as pd
 
 
 class RelativeDifferenceProcessor:
-    """Processor to express numeric variables as a difference relative to the sum of other variables.
+    """Express numeric variables as a difference relative to the sum of other variables.
 
-    Even if the avatarization is keeping relation and correlation, it will not guarantee mathematical relation retention.
+    Even if the avatarization is keeping relation and correlation,
+    it will not guarantee mathematical relation retention.
     You can apply the RelativeDifferenceProcessor to retain this relation between variables.
 
     Arguments
     ---------
         target:
             variables to transform
         references:
@@ -165,25 +166,26 @@
         if wrong_variables:
             raise ValueError(
                 "Expected all reference variables in dataset columns, "
                 f"got {wrong_variables} instead."
             )
         if df[self.references].isnull().values.any():
             raise ValueError(
-                "Expected no missing values for `references`, got column with missing values instead"
+                "Expected no missing values for `references`, "
+                "got column with missing values instead"
             )
         df[self.target_rename] = (
             df[self.target].sub(df[self.references].sum(axis=1))
         ) / self.scaling_unit
         if self.drop_original_target:
             df = df.drop(columns=[self.target])
         return df
 
     def postprocess(self, source: pd.DataFrame, dest: pd.DataFrame) -> pd.DataFrame:
-        """Transform a difference relative to the sum of other variables into an absolute numeric value.
+        """Transform a difference relative to the sum of variables into an absolute numeric value.
 
         Arguments
         ---------
             source: not used
             dest: dataframe to transform
 
         Returns
```

### Comparing `octopize_avatar-0.7.2/avatars/processors/relative_difference_test.py` & `octopize_avatar-0.7.3/avatars/processors/relative_difference_test.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.2/avatars/processors/to_categorical.py` & `octopize_avatar-0.7.3/avatars/processors/to_categorical.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,22 +1,21 @@
-from typing import List
-
 import numpy as np
 import pandas as pd
 
 from avatars.lib.continuous_threshold import get_continuous_under_threshold
 
 
 class ToCategoricalProcessor:
     """Processor to model selected numeric variables as categorical variables.
 
     Arguments
     ---------
         to_categorical_threshold:
-            threshold of the number of distinct value to consider a continuous variable as categorical.
+            threshold of the number of distinct value to consider
+            a continuous variable as categorical.
 
     Keyword Arguments
     -----------------
         keep_continuous:
             if `True`, continuous variables will be kept and
         suffixed with `continuous_suffix`.
         continuous_suffix:
```

### Comparing `octopize_avatar-0.7.2/avatars/processors/to_categorical_test.py` & `octopize_avatar-0.7.3/avatars/processors/to_categorical_test.py`

 * *Files 1% similar despite different names*

```diff
@@ -58,15 +58,15 @@
     processor = ToCategoricalProcessor(to_categorical_threshold=2, keep_continuous=True)
     processed_df = processor.preprocess(df=original)
     df = processor.postprocess(source=original, dest=processed_df)
     pd_testing.assert_frame_equal(df, expected)
 
 
 def test_postprocessed_with_category(original: pd.DataFrame) -> None:
-    """Check post processor with a transformed variable changed the categorical variable as expected.
+    """Check post processor with a transformed variable changed the categorical variable.
 
     Transformed data frame could be obtained with another processor such as GroupModalities().
     """
     transformed = pd.DataFrame(
         {
             "variable_1": [1, 2, 3, np.nan],
             "variable_2": ["other", "other", "other", np.nan],
```

### Comparing `octopize_avatar-0.7.2/avatars/utils.py` & `octopize_avatar-0.7.3/avatars/utils.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 from __future__ import annotations
 
 from contextlib import contextmanager
 from enum import Enum
 from typing import Any, Generator, Optional, TypeVar
+
 from toolz.dicttoolz import valfilter, valmap
 
 T = TypeVar("T")
 U = TypeVar("U")
 R = TypeVar("R")
```

### Comparing `octopize_avatar-0.7.2/pyproject.toml` & `octopize_avatar-0.7.3/pyproject.toml`

 * *Files 5% similar despite different names*

```diff
@@ -1,21 +1,21 @@
 [build-system]
 requires = ["poetry-core>=1.0.0"]
 build-backend = "poetry.core.masonry.api"
 
 [tool.poetry]
 name = "octopize.avatar"
 # Also modify avatars/__init__.py
-version = "0.7.2"
+version = "0.7.3"
 description = "Python client for Octopize's avatar API"
-authors = ["Octopize <help@octopize.io>"]
+authors = ["Octopize <pypi-octopize@octopize.io>"]
 license = "Apache-2.0"
-packages = [
-    { include = "avatars"},
-]
+packages = [{ include = "avatars" }]
+readme = "README.md"
+
 
 [tool.poetry.dependencies]
 python = "^3.9"
 httpx = "^0.23.0"
 pydantic = "^2"
 pandas = "^1.4.2"
 toolz = "^0.12.0"
@@ -30,15 +30,15 @@
 sphinx-multiversion = "^0.2.4"
 autodoc-pydantic = "^2"
 
 
 [tool.poetry.group.dev.dependencies]
 blacken-docs = "^1.12.1"
 typer = "^0.4.1"
-black = {extras = ["jupyter"], version = "^24.3.0" }
+black = { extras = ["jupyter"], version = "^24.3.0" }
 isort = "^5.10.1"
 bandit = "^1.7.4"
 mypy = "^1.4"
 pytest = "^7.2.0"
 jupytext = "^1.14.2"
 nbconvert = "^7.2.6"
 flake8 = "^6.0.0"
@@ -48,39 +48,34 @@
 seaborn = "^0.12.1"
 matplotlib = "^3.6.2"
 ipykernel = "^6.19.2"
 missingno = "^0.5.1"
 notebook = "^6.5.6"
 
 
-
 [tool.isort]
 # https://pycqa.github.io/isort/docs/configuration/black_compatibility.html
 profile = "black"
 
 [tool.mypy]
 python_version = "3.9"
-plugins = [
-  "pydantic.mypy"
-]
+plugins = ["pydantic.mypy"]
 strict = true
 exclude = "(tmp|.venv)"
 
 
 [tool.coverage.report]
 exclude_lines = [
-    "pragma: no cover",
-    "def __repr__",
-    "if __name__ == .__main__.:",
-    "nocov",
-    "if TYPE_CHECKING:",
+  "pragma: no cover",
+  "def __repr__",
+  "if __name__ == .__main__.:",
+  "nocov",
+  "if TYPE_CHECKING:",
 ]
-omit = [
-    "**/*_test.py",
-    ]
+omit = ["**/*_test.py"]
 
 [tool.coverage.run]
 branch = true
 
 [tool.pydantic-mypy]
 init_forbid_extra = true
 init_typed = true
```

