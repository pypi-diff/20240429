# Comparing `tmp/OmniGenome-0.0.1a0-py3-none-any.whl.zip` & `tmp/OmniGenome-0.0.2a0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,99 +1,52 @@
-Zip file size: 98256 bytes, number of entries: 97
--rw-rw-rw-  2.0 fat     4823 b- defN 24-Apr-14 15:50 omnigenome/__init__.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-16 16:29 omnigenome/_src/__init__.py
--rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-06 13:21 omnigenome/_src/abc/__init__.py
--rw-rw-rw-  2.0 fat    10028 b- defN 24-Apr-14 13:16 omnigenome/_src/abc/abstract_dataset.py
--rw-rw-rw-  2.0 fat     1773 b- defN 24-Apr-13 10:46 omnigenome/_src/abc/abstract_metric.py
--rw-rw-rw-  2.0 fat    10266 b- defN 24-Apr-14 13:16 omnigenome/_src/abc/abstract_model.py
--rw-rw-rw-  2.0 fat     1884 b- defN 24-Apr-14 13:16 omnigenome/_src/abc/abstract_tokenizer.py
--rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-06 13:42 omnigenome/_src/config/__init__.py
--rw-rw-rw-  2.0 fat     7654 b- defN 24-Apr-09 18:34 omnigenome/_src/config/config.py
--rw-rw-rw-  2.0 fat      971 b- defN 24-Apr-06 13:44 omnigenome/_src/config/config_check.py
--rw-rw-rw-  2.0 fat      689 b- defN 24-Apr-14 13:16 omnigenome/_src/dataset/__init__.py
--rw-rw-rw-  2.0 fat     8801 b- defN 24-Apr-14 10:40 omnigenome/_src/dataset/omnigenome_dataset.py
--rw-rw-rw-  2.0 fat      493 b- defN 24-Apr-14 10:52 omnigenome/_src/metric/__init__.py
--rw-rw-rw-  2.0 fat     2658 b- defN 24-Apr-12 21:15 omnigenome/_src/metric/classification_metric.py
--rw-rw-rw-  2.0 fat     2248 b- defN 24-Apr-12 21:15 omnigenome/_src/metric/ranking_metric.py
--rw-rw-rw-  2.0 fat     2626 b- defN 24-Apr-12 21:15 omnigenome/_src/metric/regression_metric.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-16 16:34 omnigenome/_src/misc/__init__.py
--rw-rw-rw-  2.0 fat     5497 b- defN 24-Apr-14 13:16 omnigenome/_src/misc/utils.py
--rw-rw-rw-  2.0 fat      444 b- defN 24-Apr-14 10:45 omnigenome/_src/model/__init__.py
--rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-08 20:27 omnigenome/_src/model/classiifcation/__init__.py
--rw-rw-rw-  2.0 fat    18734 b- defN 24-Apr-14 13:16 omnigenome/_src/model/classiifcation/model.py
--rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-10 12:33 omnigenome/_src/model/mlm/__init__.py
--rw-rw-rw-  2.0 fat     4136 b- defN 24-Apr-14 12:29 omnigenome/_src/model/mlm/model.py
--rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-08 20:27 omnigenome/_src/model/regression/__init__.py
--rw-rw-rw-  2.0 fat    18876 b- defN 24-Apr-14 12:29 omnigenome/_src/model/regression/model.py
--rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-08 21:23 omnigenome/_src/model/seq2seq/__init__.py
--rw-rw-rw-  2.0 fat      339 b- defN 24-Apr-14 10:41 omnigenome/_src/model/seq2seq/model.py
--rw-rw-rw-  2.0 fat      512 b- defN 24-Apr-14 10:45 omnigenome/_src/tokenizer/__init__.py
--rw-rw-rw-  2.0 fat     2534 b- defN 24-Apr-14 10:41 omnigenome/_src/tokenizer/bpe_tokenizer.py
--rw-rw-rw-  2.0 fat     3857 b- defN 24-Apr-14 10:42 omnigenome/_src/tokenizer/kmers_tokenizer.py
--rw-rw-rw-  2.0 fat     3786 b- defN 24-Apr-14 10:43 omnigenome/_src/tokenizer/single_nucleotide_tokenizer.py
--rw-rw-rw-  2.0 fat      409 b- defN 24-Apr-14 10:45 omnigenome/_src/trainer/__init__.py
--rw-rw-rw-  2.0 fat     1092 b- defN 24-Apr-09 16:39 omnigenome/_src/trainer/hf_trainer.py
--rw-rw-rw-  2.0 fat     5958 b- defN 24-Apr-14 13:16 omnigenome/_src/trainer/trainer.py
+Zip file size: 52114 bytes, number of entries: 50
+-rw-rw-rw-  2.0 fat     3712 b- defN 24-Apr-29 14:11 omnigenome/__init__.py
 -rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-16 16:24 omnigenome/bench/__init__.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-11 17:29 omnigenome/bench/auto_bench/__init__.py
--rw-rw-rw-  2.0 fat      472 b- defN 24-Apr-14 10:55 omnigenome/bench/auto_bench/auto_bench.py
+-rw-rw-rw-  2.0 fat     7289 b- defN 24-Apr-29 14:58 omnigenome/bench/auto_bench/auto_bench.py
+-rw-rw-rw-  2.0 fat     7366 b- defN 24-Apr-29 14:11 omnigenome/bench/auto_bench/auto_bench_config.py
+-rw-rw-rw-  2.0 fat      971 b- defN 24-Apr-06 13:44 omnigenome/bench/auto_bench/config_check.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-11 17:29 omnigenome/bench/bench_hub/__init__.py
 -rw-rw-rw-  2.0 fat      439 b- defN 24-Apr-14 13:16 omnigenome/bench/bench_hub/bench_hub.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-16 16:29 omnigenome/src/__init__.py
+-rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-27 14:21 omnigenome/src/__init__.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-06 13:21 omnigenome/src/abc/__init__.py
--rw-rw-rw-  2.0 fat    10028 b- defN 24-Apr-14 13:16 omnigenome/src/abc/abstract_dataset.py
+-rw-rw-rw-  2.0 fat    10343 b- defN 24-Apr-29 14:11 omnigenome/src/abc/abstract_dataset.py
 -rw-rw-rw-  2.0 fat     1773 b- defN 24-Apr-13 10:46 omnigenome/src/abc/abstract_metric.py
--rw-rw-rw-  2.0 fat    10238 b- defN 24-Apr-14 15:39 omnigenome/src/abc/abstract_model.py
--rw-rw-rw-  2.0 fat     1884 b- defN 24-Apr-14 13:16 omnigenome/src/abc/abstract_tokenizer.py
--rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-06 13:42 omnigenome/src/config/__init__.py
--rw-rw-rw-  2.0 fat     7654 b- defN 24-Apr-09 18:34 omnigenome/src/config/config.py
--rw-rw-rw-  2.0 fat      971 b- defN 24-Apr-06 13:44 omnigenome/src/config/config_check.py
--rw-rw-rw-  2.0 fat      689 b- defN 24-Apr-14 13:16 omnigenome/src/dataset/__init__.py
--rw-rw-rw-  2.0 fat     8801 b- defN 24-Apr-14 10:40 omnigenome/src/dataset/omnigenome_dataset.py
--rw-rw-rw-  2.0 fat      493 b- defN 24-Apr-14 10:52 omnigenome/src/metric/__init__.py
--rw-rw-rw-  2.0 fat     2658 b- defN 24-Apr-12 21:15 omnigenome/src/metric/classification_metric.py
--rw-rw-rw-  2.0 fat     2248 b- defN 24-Apr-12 21:15 omnigenome/src/metric/ranking_metric.py
--rw-rw-rw-  2.0 fat     2626 b- defN 24-Apr-12 21:15 omnigenome/src/metric/regression_metric.py
+-rw-rw-rw-  2.0 fat    13841 b- defN 24-Apr-29 14:58 omnigenome/src/abc/abstract_model.py
+-rw-rw-rw-  2.0 fat     3175 b- defN 24-Apr-29 14:58 omnigenome/src/abc/abstract_tokenizer.py
+-rw-rw-rw-  2.0 fat      634 b- defN 24-Apr-29 14:11 omnigenome/src/dataset/__init__.py
+-rw-rw-rw-  2.0 fat     9110 b- defN 24-Apr-29 14:11 omnigenome/src/dataset/omnigenome_dataset.py
+-rw-rw-rw-  2.0 fat      493 b- defN 24-Apr-29 14:11 omnigenome/src/metric/__init__.py
+-rw-rw-rw-  2.0 fat     2660 b- defN 24-Apr-29 14:11 omnigenome/src/metric/classification_metric.py
+-rw-rw-rw-  2.0 fat     2250 b- defN 24-Apr-29 14:11 omnigenome/src/metric/ranking_metric.py
+-rw-rw-rw-  2.0 fat     2628 b- defN 24-Apr-29 14:11 omnigenome/src/metric/regression_metric.py
 -rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-16 16:34 omnigenome/src/misc/__init__.py
--rw-rw-rw-  2.0 fat     5497 b- defN 24-Apr-14 13:16 omnigenome/src/misc/utils.py
--rw-rw-rw-  2.0 fat      470 b- defN 24-Apr-14 14:46 omnigenome/src/model/__init__.py
+-rw-rw-rw-  2.0 fat     6784 b- defN 24-Apr-29 14:11 omnigenome/src/misc/utils.py
+-rw-rw-rw-  2.0 fat      470 b- defN 24-Apr-29 14:11 omnigenome/src/model/__init__.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-08 20:27 omnigenome/src/model/classiifcation/__init__.py
--rw-rw-rw-  2.0 fat    18734 b- defN 24-Apr-14 13:16 omnigenome/src/model/classiifcation/model.py
+-rw-rw-rw-  2.0 fat    11809 b- defN 24-Apr-29 12:13 omnigenome/src/model/classiifcation/model.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-10 12:33 omnigenome/src/model/mlm/__init__.py
--rw-rw-rw-  2.0 fat     4136 b- defN 24-Apr-14 12:29 omnigenome/src/model/mlm/model.py
+-rw-rw-rw-  2.0 fat     4521 b- defN 24-Apr-27 20:58 omnigenome/src/model/mlm/model.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-08 20:27 omnigenome/src/model/regression/__init__.py
--rw-rw-rw-  2.0 fat    18876 b- defN 24-Apr-14 12:29 omnigenome/src/model/regression/model.py
+-rw-rw-rw-  2.0 fat    10948 b- defN 24-Apr-29 14:58 omnigenome/src/model/regression/model.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-08 21:23 omnigenome/src/model/seq2seq/__init__.py
--rw-rw-rw-  2.0 fat      601 b- defN 24-Apr-14 14:48 omnigenome/src/model/seq2seq/model.py
--rw-rw-rw-  2.0 fat      512 b- defN 24-Apr-14 10:45 omnigenome/src/tokenizer/__init__.py
--rw-rw-rw-  2.0 fat     2534 b- defN 24-Apr-14 10:41 omnigenome/src/tokenizer/bpe_tokenizer.py
--rw-rw-rw-  2.0 fat     3857 b- defN 24-Apr-14 10:42 omnigenome/src/tokenizer/kmers_tokenizer.py
--rw-rw-rw-  2.0 fat     3786 b- defN 24-Apr-14 10:43 omnigenome/src/tokenizer/single_nucleotide_tokenizer.py
+-rw-rw-rw-  2.0 fat      607 b- defN 24-Apr-27 14:47 omnigenome/src/model/seq2seq/model.py
+-rw-rw-rw-  2.0 fat      512 b- defN 24-Apr-29 14:11 omnigenome/src/tokenizer/__init__.py
+-rw-rw-rw-  2.0 fat     2996 b- defN 24-Apr-29 14:11 omnigenome/src/tokenizer/bpe_tokenizer.py
+-rw-rw-rw-  2.0 fat     4598 b- defN 24-Apr-29 14:11 omnigenome/src/tokenizer/kmers_tokenizer.py
+-rw-rw-rw-  2.0 fat     3798 b- defN 24-Apr-29 14:11 omnigenome/src/tokenizer/single_nucleotide_tokenizer.py
 -rw-rw-rw-  2.0 fat      409 b- defN 24-Apr-14 10:45 omnigenome/src/trainer/__init__.py
--rw-rw-rw-  2.0 fat     1092 b- defN 24-Apr-09 16:39 omnigenome/src/trainer/hf_trainer.py
--rw-rw-rw-  2.0 fat     5400 b- defN 24-Apr-14 13:25 omnigenome/src/trainer/trainer.py
+-rw-rw-rw-  2.0 fat     1092 b- defN 24-Apr-29 14:11 omnigenome/src/trainer/hf_trainer.py
+-rw-rw-rw-  2.0 fat    10163 b- defN 24-Apr-29 14:58 omnigenome/src/trainer/trainer.py
 -rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-16 16:25 omnigenome/utility/__init__.py
--rw-rw-rw-  2.0 fat    10682 b- defN 24-Apr-14 14:32 omnigenome/utility/hub_utils.py
+-rw-rw-rw-  2.0 fat     9554 b- defN 24-Apr-24 20:40 omnigenome/utility/ensemble.py
+-rw-rw-rw-  2.0 fat    10629 b- defN 24-Apr-29 14:11 omnigenome/utility/hub_utils.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-11 17:29 omnigenome/utility/model_hub/__init__.py
--rw-rw-rw-  2.0 fat     3689 b- defN 24-Apr-14 15:39 omnigenome/utility/model_hub/model_hub.py
+-rw-rw-rw-  2.0 fat     2993 b- defN 24-Apr-27 21:32 omnigenome/utility/model_hub/model_hub.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-06 13:21 omnigenome/utility/pipeline_hub/__init__.py
--rw-rw-rw-  2.0 fat     6155 b- defN 24-Apr-14 15:42 omnigenome/utility/pipeline_hub/pipeline.py
+-rw-rw-rw-  2.0 fat     5603 b- defN 24-Apr-29 14:11 omnigenome/utility/pipeline_hub/pipeline.py
 -rw-rw-rw-  2.0 fat      883 b- defN 24-Apr-14 15:43 omnigenome/utility/pipeline_hub/pipeline_hub.py
--rw-rw-rw-  2.0 fat      342 b- defN 24-Jan-20 14:56 tutorials/__init__.py
--rw-rw-rw-  2.0 fat     1660 b- defN 24-Feb-11 02:51 tutorials/codonbert_adapter.py
--rw-rw-rw-  2.0 fat     4710 b- defN 24-Apr-14 14:32 tutorials/mRNA_test.py
--rw-rw-rw-  2.0 fat     3463 b- defN 24-Apr-14 14:32 tutorials/mlm_service_test.py
--rw-rw-rw-  2.0 fat     3496 b- defN 24-Apr-14 14:32 tutorials/pc_test.py
--rw-rw-rw-  2.0 fat     3400 b- defN 24-Apr-14 14:32 tutorials/psp_test.py
--rw-rw-rw-  2.0 fat     4408 b- defN 24-Apr-14 14:32 tutorials/snmd_test.py
--rw-rw-rw-  2.0 fat     4610 b- defN 24-Apr-14 14:32 tutorials/snmr_test.py
--rw-rw-rw-  2.0 fat     3471 b- defN 24-Apr-14 14:32 tutorials/snp_test.py
--rw-rw-rw-  2.0 fat     4049 b- defN 24-Apr-14 14:32 tutorials/ssp_test.py
--rw-rw-rw-  2.0 fat     8409 b- defN 24-Apr-09 16:39 tutorials/ssp_validation.py
--rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-14 02:15 tutorials/utility/__init__.py
--rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-14 02:15 tutorials/utility/pipeline_examples/__init__.py
--rw-rw-rw-  2.0 fat     1408 b- defN 24-Apr-14 13:16 tutorials/utility/pipeline_examples/ssp_pipeline.py
--rw-rw-rw-  2.0 fat      835 b- defN 24-Apr-14 15:50 OmniGenome-0.0.1a0.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 24-Apr-14 15:50 OmniGenome-0.0.1a0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       21 b- defN 24-Apr-14 15:50 OmniGenome-0.0.1a0.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     8941 b- defN 24-Apr-14 15:50 OmniGenome-0.0.1a0.dist-info/RECORD
-97 files, 318846 bytes uncompressed, 83830 bytes compressed:  73.7%
+-rw-rw-rw-  2.0 fat      861 b- defN 24-Apr-29 14:59 OmniGenome-0.0.2a0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 24-Apr-29 14:59 OmniGenome-0.0.2a0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       11 b- defN 24-Apr-29 14:59 OmniGenome-0.0.2a0.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     4654 b- defN 24-Apr-29 14:59 OmniGenome-0.0.2a0.dist-info/RECORD
+50 files, 164091 bytes uncompressed, 44522 bytes compressed:  72.9%
```

## zipnote {}

```diff
@@ -1,116 +1,23 @@
 Filename: omnigenome/__init__.py
 Comment: 
 
-Filename: omnigenome/_src/__init__.py
-Comment: 
-
-Filename: omnigenome/_src/abc/__init__.py
-Comment: 
-
-Filename: omnigenome/_src/abc/abstract_dataset.py
-Comment: 
-
-Filename: omnigenome/_src/abc/abstract_metric.py
-Comment: 
-
-Filename: omnigenome/_src/abc/abstract_model.py
-Comment: 
-
-Filename: omnigenome/_src/abc/abstract_tokenizer.py
-Comment: 
-
-Filename: omnigenome/_src/config/__init__.py
-Comment: 
-
-Filename: omnigenome/_src/config/config.py
-Comment: 
-
-Filename: omnigenome/_src/config/config_check.py
-Comment: 
-
-Filename: omnigenome/_src/dataset/__init__.py
-Comment: 
-
-Filename: omnigenome/_src/dataset/omnigenome_dataset.py
-Comment: 
-
-Filename: omnigenome/_src/metric/__init__.py
-Comment: 
-
-Filename: omnigenome/_src/metric/classification_metric.py
-Comment: 
-
-Filename: omnigenome/_src/metric/ranking_metric.py
-Comment: 
-
-Filename: omnigenome/_src/metric/regression_metric.py
-Comment: 
-
-Filename: omnigenome/_src/misc/__init__.py
-Comment: 
-
-Filename: omnigenome/_src/misc/utils.py
-Comment: 
-
-Filename: omnigenome/_src/model/__init__.py
-Comment: 
-
-Filename: omnigenome/_src/model/classiifcation/__init__.py
-Comment: 
-
-Filename: omnigenome/_src/model/classiifcation/model.py
-Comment: 
-
-Filename: omnigenome/_src/model/mlm/__init__.py
-Comment: 
-
-Filename: omnigenome/_src/model/mlm/model.py
-Comment: 
-
-Filename: omnigenome/_src/model/regression/__init__.py
-Comment: 
-
-Filename: omnigenome/_src/model/regression/model.py
-Comment: 
-
-Filename: omnigenome/_src/model/seq2seq/__init__.py
-Comment: 
-
-Filename: omnigenome/_src/model/seq2seq/model.py
-Comment: 
-
-Filename: omnigenome/_src/tokenizer/__init__.py
-Comment: 
-
-Filename: omnigenome/_src/tokenizer/bpe_tokenizer.py
-Comment: 
-
-Filename: omnigenome/_src/tokenizer/kmers_tokenizer.py
-Comment: 
-
-Filename: omnigenome/_src/tokenizer/single_nucleotide_tokenizer.py
-Comment: 
-
-Filename: omnigenome/_src/trainer/__init__.py
-Comment: 
-
-Filename: omnigenome/_src/trainer/hf_trainer.py
+Filename: omnigenome/bench/__init__.py
 Comment: 
 
-Filename: omnigenome/_src/trainer/trainer.py
+Filename: omnigenome/bench/auto_bench/__init__.py
 Comment: 
 
-Filename: omnigenome/bench/__init__.py
+Filename: omnigenome/bench/auto_bench/auto_bench.py
 Comment: 
 
-Filename: omnigenome/bench/auto_bench/__init__.py
+Filename: omnigenome/bench/auto_bench/auto_bench_config.py
 Comment: 
 
-Filename: omnigenome/bench/auto_bench/auto_bench.py
+Filename: omnigenome/bench/auto_bench/config_check.py
 Comment: 
 
 Filename: omnigenome/bench/bench_hub/__init__.py
 Comment: 
 
 Filename: omnigenome/bench/bench_hub/bench_hub.py
 Comment: 
@@ -129,23 +36,14 @@
 
 Filename: omnigenome/src/abc/abstract_model.py
 Comment: 
 
 Filename: omnigenome/src/abc/abstract_tokenizer.py
 Comment: 
 
-Filename: omnigenome/src/config/__init__.py
-Comment: 
-
-Filename: omnigenome/src/config/config.py
-Comment: 
-
-Filename: omnigenome/src/config/config_check.py
-Comment: 
-
 Filename: omnigenome/src/dataset/__init__.py
 Comment: 
 
 Filename: omnigenome/src/dataset/omnigenome_dataset.py
 Comment: 
 
 Filename: omnigenome/src/metric/__init__.py
@@ -213,14 +111,17 @@
 
 Filename: omnigenome/src/trainer/trainer.py
 Comment: 
 
 Filename: omnigenome/utility/__init__.py
 Comment: 
 
+Filename: omnigenome/utility/ensemble.py
+Comment: 
+
 Filename: omnigenome/utility/hub_utils.py
 Comment: 
 
 Filename: omnigenome/utility/model_hub/__init__.py
 Comment: 
 
 Filename: omnigenome/utility/model_hub/model_hub.py
@@ -231,62 +132,20 @@
 
 Filename: omnigenome/utility/pipeline_hub/pipeline.py
 Comment: 
 
 Filename: omnigenome/utility/pipeline_hub/pipeline_hub.py
 Comment: 
 
-Filename: tutorials/__init__.py
-Comment: 
-
-Filename: tutorials/codonbert_adapter.py
-Comment: 
-
-Filename: tutorials/mRNA_test.py
-Comment: 
-
-Filename: tutorials/mlm_service_test.py
-Comment: 
-
-Filename: tutorials/pc_test.py
-Comment: 
-
-Filename: tutorials/psp_test.py
-Comment: 
-
-Filename: tutorials/snmd_test.py
-Comment: 
-
-Filename: tutorials/snmr_test.py
-Comment: 
-
-Filename: tutorials/snp_test.py
-Comment: 
-
-Filename: tutorials/ssp_test.py
-Comment: 
-
-Filename: tutorials/ssp_validation.py
-Comment: 
-
-Filename: tutorials/utility/__init__.py
-Comment: 
-
-Filename: tutorials/utility/pipeline_examples/__init__.py
-Comment: 
-
-Filename: tutorials/utility/pipeline_examples/ssp_pipeline.py
-Comment: 
-
-Filename: OmniGenome-0.0.1a0.dist-info/METADATA
+Filename: OmniGenome-0.0.2a0.dist-info/METADATA
 Comment: 
 
-Filename: OmniGenome-0.0.1a0.dist-info/WHEEL
+Filename: OmniGenome-0.0.2a0.dist-info/WHEEL
 Comment: 
 
-Filename: OmniGenome-0.0.1a0.dist-info/top_level.txt
+Filename: OmniGenome-0.0.2a0.dist-info/top_level.txt
 Comment: 
 
-Filename: OmniGenome-0.0.1a0.dist-info/RECORD
+Filename: OmniGenome-0.0.2a0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## omnigenome/__init__.py

```diff
@@ -4,79 +4,60 @@
 # author: YANG, HENG <hy345@exeter.ac.uk> (杨恒)
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 
 __name__ = "OmniGenome"
-__version__ = "0.0.1alpha0"
+__version__ = "0.0.2alpha"
 __author__ = "YANG, HENG"
 __email__ = "yangheng2021@gmail.com"
 __license__ = "MIT"
 
 
+from .bench.auto_bench.auto_bench import AutoBench
+from .bench.auto_bench.auto_bench_config import AutoBenchConfig
+from .bench.bench_hub.bench_hub import BenchHub
+from .src import dataset as dataset
+from .src import metric as metric
+from .src import model as model
+from .src import tokenizer as tokenizer
 from .src.abc.abstract_dataset import OmniGenomeDataset
-from .src.abc.abstract_model import OmniGenomeModel
 from .src.abc.abstract_metric import OmniGenomeMetric
+from .src.abc.abstract_model import OmniGenomeModel
 from .src.abc.abstract_tokenizer import OmniGenomeTokenizer
-
+from .src.dataset.omnigenome_dataset import OmniGenomeDatasetForSequenceClassification
+from .src.dataset.omnigenome_dataset import OmniGenomeDatasetForSequenceRegression
+from .src.dataset.omnigenome_dataset import OmniGenomeDatasetForTokenClassification
+from .src.dataset.omnigenome_dataset import OmniGenomeDatasetForTokenRegression
+from .src.metric import ClassificationMetric, RegressionMetric, RankingMetric
+from .src.misc import utils as utils
+from .src.model import (
+    OmniGenomeModelForSequenceClassification,
+    OmniGenomeModelForTokenClassification,
+    OmniGenomeModelForSequenceClassificationWith2DStructure,
+    OmniGenomeModelForTokenClassificationWith2DStructure,
+    OmniGenomeModelForSequenceRegression,
+    OmniGenomeModelForTokenRegression,
+    OmniGenomeModelForSequenceRegressionWith2DStructure,
+    OmniGenomeModelForTokenRegressionWith2DStructure,
+    OmniGenomeEncoderModelForMLM,
+    OmniGenomeEncoderModelForSeq2Seq,
+)
+from .src.tokenizer import OmniBPETokenizer
 from .src.tokenizer import OmniKmersTokenizer
 from .src.tokenizer import OmniSingleNucleotideTokenizer
-from .src.tokenizer import OmniBPETokenizer
-
-# from .src import config as config  # no development yet
-
+from .src.trainer.hf_trainer import HFTrainer
+from .src.trainer.trainer import Trainer
 from .utility import hub_utils as hub_utils
 from .utility.model_hub.model_hub import ModelHub
-
 from .utility.pipeline_hub.pipeline import Pipeline
 from .utility.pipeline_hub.pipeline_hub import PipelineHub
 
-from .bench.bench_hub.bench_hub import BenchHub
-from .bench.auto_bench.auto_bench import AutoBench
-from .bench.auto_bench.auto_bench import AutoBenchConfig
-
-from .src.misc import utils as utils
-
-from .src.trainer.trainer import Trainer
-from .src.trainer.hf_trainer import HFTrainer
-
-from .src import config as config
-from .src import metric as metric
-from .src import model as model
-from .src import tokenizer as tokenizer
-from .src import dataset as dataset
-
-from .src.model import (
-    OmniGenomeEncoderModelForSequenceClassification,
-    OmniGenomeEncoderModelForTokenClassification,
-    OmniGenomeEncoderModelForSequenceClassificationWith2DStructure,
-    OmniGenomeEncoderModelForTokenClassificationWith2DStructure,
-    OmniGenomeDecoderModelForSequenceClassification,
-    OmniGenomeDecoderModelForTokenClassification,
-    OmniGenomeDecoderModelForSequenceClassificationWith2DStructure,
-    OmniGenomeDecoderModelForTokenClassificationWith2DStructure,
-    OmniGenomeEncoderModelForSequenceRegression,
-    OmniGenomeEncoderModelForTokenRegression,
-    OmniGenomeEncoderModelForSequenceRegressionWith2DStructure,
-    OmniGenomeEncoderModelForTokenRegressionWith2DStructure,
-    OmniGenomeDecoderModelForSequenceRegression,
-    OmniGenomeDecoderModelForTokenRegression,
-    OmniGenomeDecoderModelForSequenceRegressionWith2DStructure,
-    OmniGenomeDecoderModelForTokenRegressionWith2DStructure,
-    OmniGenomeEncoderModelForMLM,
-    OmniGenomeEncoderModelForSeq2Seq,
-)
-
-from .src.dataset.omnigenome_dataset import OmniGenomeDatasetForTokenClassification
-from .src.dataset.omnigenome_dataset import OmniGenomeDatasetForTokenRegression
-from .src.dataset.omnigenome_dataset import OmniGenomeDatasetForSequenceClassification
-from .src.dataset.omnigenome_dataset import OmniGenomeDatasetForSequenceRegression
-
-from .src.metric import ClassificationMetric, RegressionMetric, RankingMetric
+# from .src import config as config  # no development yet
 
 __all__ = [
     "OmniGenomeDataset",
     "OmniGenomeModel",
     "OmniGenomeMetric",
     "OmniGenomeTokenizer",
     "OmniKmersTokenizer",
@@ -85,40 +66,31 @@
     "ModelHub",
     "Pipeline",
     "PipelineHub",
     "BenchHub",
     "AutoBench",
     "AutoBenchConfig",
     "utils",
-    "config",
-    "metric",
     "model",
     "tokenizer",
     "dataset",
-    "OmniGenomeEncoderModelForSequenceClassification",
-    "OmniGenomeEncoderModelForTokenClassification",
-    "OmniGenomeEncoderModelForSequenceClassificationWith2DStructure",
-    "OmniGenomeEncoderModelForTokenClassificationWith2DStructure",
-    "OmniGenomeDecoderModelForSequenceClassification",
-    "OmniGenomeDecoderModelForTokenClassification",
-    "OmniGenomeDecoderModelForSequenceClassificationWith2DStructure",
-    "OmniGenomeDecoderModelForTokenClassificationWith2DStructure",
-    "OmniGenomeEncoderModelForSequenceRegression",
-    "OmniGenomeEncoderModelForTokenRegression",
-    "OmniGenomeEncoderModelForSequenceRegressionWith2DStructure",
-    "OmniGenomeEncoderModelForTokenRegressionWith2DStructure",
-    "OmniGenomeDecoderModelForSequenceRegression",
-    "OmniGenomeDecoderModelForTokenRegression",
-    "OmniGenomeDecoderModelForSequenceRegressionWith2DStructure",
-    "OmniGenomeDecoderModelForTokenRegressionWith2DStructure",
+    "OmniGenomeModelForSequenceClassification",
+    "OmniGenomeModelForTokenClassification",
+    "OmniGenomeModelForSequenceClassificationWith2DStructure",
+    "OmniGenomeModelForTokenClassificationWith2DStructure",
+    "OmniGenomeModelForSequenceRegression",
+    "OmniGenomeModelForTokenRegression",
+    "OmniGenomeModelForSequenceRegressionWith2DStructure",
+    "OmniGenomeModelForTokenRegressionWith2DStructure",
     "OmniGenomeEncoderModelForMLM",
     "OmniGenomeEncoderModelForSeq2Seq",
     "OmniGenomeDatasetForTokenClassification",
     "OmniGenomeDatasetForTokenRegression",
     "OmniGenomeDatasetForSequenceClassification",
     "OmniGenomeDatasetForSequenceRegression",
     "ClassificationMetric",
     "RegressionMetric",
     "RankingMetric",
     "Trainer",
     "HFTrainer",
+    "AutoBenchConfig",
 ]
```

## omnigenome/bench/auto_bench/auto_bench.py

```diff
@@ -3,16 +3,178 @@
 # time: 11:54 14/04/2024
 # author: YANG, HENG <hy345@exeter.ac.uk> (杨恒)
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 
+import importlib
+import os
 
-class AutoBenchConfig:
-    def __init__(self):
-        pass
+import autocuda
+import findfile
+import torch
+from metric_visualizer import MetricVisualizer
+
+from ...src.abc.abstract_tokenizer import OmniGenomeTokenizer
+from ...src.misc.utils import seed_everything, fprint
+from ...src.trainer.trainer import Trainer
 
 
 class AutoBench:
-    def __init__(self):
-        pass
+    def __init__(
+        self, bench_root, model_name_or_path, tokenizer=None, device=None, **kwargs
+    ):
+        self.bench_root = bench_root.rstrip("/")
+        self.model_name_or_path = model_name_or_path.rstrip("/")
+        self.tokenizer = tokenizer.rstrip("/") if tokenizer else None
+        self.device = device if device else autocuda.auto_cuda()
+
+        # Import benchmark list
+        self.bench_metadata = importlib.import_module(f"{self.bench_root}.metadata")
+        fprint("Loaded benchmarks: ", self.bench_metadata.bench_list)
+
+        self.mv_path = f"{self.bench_root}-{self.model_name_or_path}.mv".replace(
+            "/", "-"
+        )
+        if os.path.exists(self.mv_path):
+            self.mv = MetricVisualizer.load(self.mv_path)
+            self.mv.summary()
+        else:
+            self.mv = MetricVisualizer(f"{self.bench_root}-{self.model_name_or_path}")
+
+        for key, value in kwargs.items():
+            if key in self.bench_metadata:
+                self.bench_metadata[key] = value
+
+        self.bench_info()
+
+    def bench_info(self):
+        info = f"Benchmark Root: {self.bench_root}\n"
+        info += f"Benchmark List: {self.bench_metadata.bench_list}\n"
+        info += f"Model Name or Path: {self.model_name_or_path}\n"
+        info += f"Tokenizer: {self.tokenizer}\n"
+        info += f"Device: {self.device}\n"
+        info += f"Metric Visualizer Path: {self.mv_path}\n"
+        info += f"BenchConfig Details: {self.bench_metadata}\n"
+        fprint(info)
+        return info
+
+    def run(self, **kwargs):
+        """
+
+        :param kwargs: parameters in kwargs will be used to overwrite the default parameters in the benchmark config
+        :return:
+        """
+
+        # Import benchmark config
+        for bench in self.bench_metadata.bench_list:
+            bench_config_path = findfile.find_file(
+                self.bench_root, f"{self.bench_root}.{bench}.config".split(".")
+            )
+            config = importlib.import_module(
+                bench_config_path.rstrip("/").replace(os.sep, ".").replace(".py", "")
+            )
+            bench_config = config.bench_config
+
+            for key, value in kwargs.items():
+                if key in bench_config:
+                    bench_config[key] = value
+
+            # Init Tokenizer and Model
+            if self.tokenizer:
+                tokenizer = OmniGenomeTokenizer.from_pretrained(
+                    self.tokenizer, trust_remote_code=True
+                )
+            else:
+                tokenizer = OmniGenomeTokenizer.from_pretrained(
+                    self.model_name_or_path, trust_remote_code=True
+                )
+            # Run Benchmarks
+            for seed in bench_config["seeds"]:
+                record_name = f"{self.bench_root}-{self.model_name_or_path}-{bench}"
+                # check if the record exists
+                if record_name in self.mv.transpose() and len(
+                    list(self.mv.transpose()[record_name].values())[0]
+                ) >= len(bench_config["seeds"]):
+                    continue
+
+                seed_everything(seed)
+                model_cls = bench_config["model_cls"]
+                model = model_cls(
+                    self.model_name_or_path,
+                    tokenizer=tokenizer,
+                    label2id=bench_config.label2id,
+                    num_labels=bench_config["num_labels"],
+                    trust_remote_code=True,
+                )
+
+                # Init Trainer
+                dataset_cls = bench_config["dataset_cls"]
+
+                train_set = dataset_cls(
+                    data_source=bench_config["train_file"],
+                    tokenizer=tokenizer,
+                    label2id=bench_config["label2id"],
+                    max_length=bench_config["max_length"],
+                )
+                test_set = dataset_cls(
+                    data_source=bench_config["test_file"],
+                    tokenizer=tokenizer,
+                    label2id=bench_config["label2id"],
+                    max_length=bench_config["max_length"],
+                )
+                valid_set = dataset_cls(
+                    data_source=bench_config["valid_file"],
+                    tokenizer=tokenizer,
+                    label2id=bench_config["label2id"],
+                    max_length=bench_config["max_length"],
+                )
+                batch_size = (
+                    bench_config["batch_size"] if "batch_size" in bench_config else 8
+                )
+
+                train_loader = torch.utils.data.DataLoader(
+                    train_set, batch_size=batch_size, shuffle=True
+                )
+                valid_loader = torch.utils.data.DataLoader(
+                    valid_set, batch_size=batch_size
+                )
+                test_loader = torch.utils.data.DataLoader(
+                    test_set, batch_size=batch_size
+                )
+
+                optimizer = torch.optim.AdamW(
+                    model.parameters(),
+                    lr=bench_config["learning_rate"]
+                    if "learning_rate" in bench_config
+                    else 2e-5,
+                    weight_decay=bench_config["weight_decay"]
+                    if "weight_decay" in bench_config
+                    else 0,
+                )
+                trainer = Trainer(
+                    model=model,
+                    train_loader=train_loader,
+                    eval_loader=valid_loader,
+                    test_loader=test_loader,
+                    batch_size=batch_size,
+                    epochs=bench_config["epochs"],
+                    patience=bench_config["patience"]
+                    if "patience" in bench_config
+                    else 3,
+                    optimizer=optimizer,
+                    loss_fn=bench_config["loss_fn"]
+                    if "loss_fn" in bench_config
+                    else None,
+                    compute_metrics=bench_config["compute_metrics"],
+                    seed=seed,
+                    device=self.device,
+                )
+                metrics = trainer.train()
+                for key, value in metrics["test"][-1].items():
+                    self.mv.log(record_name, key, value)
+                fprint(metrics)
+                self.mv.summary(round=4)
+                self.mv.dump(self.mv_path)
+                del model, trainer, optimizer, train_loader, valid_loader, test_loader
+                torch.cuda.empty_cache()
```

## omnigenome/src/__init__.py

```diff
@@ -0,0 +1,22 @@
+00000000: 2320 2d2a 2d20 636f 6469 6e67 3a20 7574  # -*- coding: ut
+00000010: 662d 3820 2d2a 2d0d 0a23 2066 696c 653a  f-8 -*-..# file:
+00000020: 205f 5f69 6e69 745f 5f2e 7079 0d0a 2320   __init__.py..# 
+00000030: 7469 6d65 3a20 3231 3a31 3120 3038 2f30  time: 21:11 08/0
+00000040: 342f 3230 3234 0d0a 2320 6175 7468 6f72  4/2024..# author
+00000050: 3a20 5941 4e47 2c20 4845 4e47 203c 6879  : YANG, HENG <hy
+00000060: 3334 3540 6578 6574 6572 2e61 632e 756b  345@exeter.ac.uk
+00000070: 3e20 28e6 9da8 e681 9229 0d0a 2320 6769  > (......)..# gi
+00000080: 7468 7562 3a20 6874 7470 733a 2f2f 6769  thub: https://gi
+00000090: 7468 7562 2e63 6f6d 2f79 616e 6768 656e  thub.com/yanghen
+000000a0: 6739 350d 0a23 2068 7567 6769 6e67 6661  g95..# huggingfa
+000000b0: 6365 3a20 6874 7470 733a 2f2f 6875 6767  ce: https://hugg
+000000c0: 696e 6766 6163 652e 636f 2f79 616e 6768  ingface.co/yangh
+000000d0: 656e 670d 0a23 2067 6f6f 676c 6520 7363  eng..# google sc
+000000e0: 686f 6c61 723a 2068 7474 7073 3a2f 2f73  holar: https://s
+000000f0: 6368 6f6c 6172 2e67 6f6f 676c 652e 636f  cholar.google.co
+00000100: 6d2f 6369 7461 7469 6f6e 733f 7573 6572  m/citations?user
+00000110: 3d4e 5071 3561 5f30 4141 4141 4a26 686c  =NPq5a_0AAAAJ&hl
+00000120: 3d65 6e0d 0a23 2043 6f70 7972 6967 6874  =en..# Copyright
+00000130: 2028 4329 2032 3031 392d 3230 3234 2e20   (C) 2019-2024. 
+00000140: 416c 6c20 5269 6768 7473 2052 6573 6572  All Rights Reser
+00000150: 7665 642e 0d0a                           ved...
```

## omnigenome/src/abc/abstract_dataset.py

```diff
@@ -4,15 +4,14 @@
 # author: YANG, HENG <hy345@exeter.ac.uk> (杨恒)
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 import os.path
 import random
-import warnings
 
 import numpy as np
 import torch
 import tqdm
 from transformers import BatchEncoding
 
 from ..misc.utils import fprint, env_meta_info
@@ -52,16 +51,14 @@
 
 
 class OmniGenomeDataset(torch.utils.data.Dataset):
     def __init__(self, data_source, tokenizer, max_length=None, **kwargs):
         super(OmniGenomeDataset, self).__init__()
         self.metadata = env_meta_info()
 
-        for key, value in kwargs.items():
-            self.metadata[key] = value
         self.tokenizer = tokenizer
 
         if max_length is not None:
             fprint(
                 f"Detected max_length={max_length} in the dataset, using it as the max_length."
             )
             self.max_length = max_length
@@ -72,44 +69,51 @@
             fprint(
                 f"Detected max_length={self.tokenizer.max_length} from the tokenizer."
             )
             self.max_length = self.tokenizer.max_length
         else:
             raise ValueError("max_length must be provided in the dataset or tokenizer.")
 
+        self.tokenizer.max_length = self.max_length
         self.examples = []
         self.data = []
 
-        if os.path.exists(data_source):
+        if data_source is not None and os.path.exists(data_source):
             fprint(f"Loading data from {data_source}...")
             self.load_data_source(data_source, **kwargs)
-        else:
-            raise FileNotFoundError(f"The file {data_source} does not exist.")
 
-        for example in tqdm.tqdm(self.examples):
-            self.data.append(self.prepare_input(example))
+            for example in tqdm.tqdm(self.examples):
+                self.data.append(self.prepare_input(example))
 
-        self._pad_and_truncate()
-        self.data = covert_input_to_tensor(self.data)
-        self._post_processing()
+            if self.examples:
+                self._post_processing()
+                self._pad_and_truncate()
+                self.data = covert_input_to_tensor(self.data)
 
     def to(self, device):
         for data_item in self.data:
             for key, value in data_item.items():
                 if isinstance(value, torch.Tensor):
                     data_item[key] = value.to(device)
         return self
 
     def _pad_and_truncate(self, pad_value=0):
         if hasattr(self.tokenizer, "pad_token_id"):
             pad_token_id = self.tokenizer.pad_token_id
         else:
             pad_token_id = self.tokenizer.base_tokenizer.pad_token_id
-        max_length = max(
-            torch.sum(data_item["input_ids"] != pad_token_id) for data_item in self.data
+        max_length = min(
+            max(
+                max(
+                    torch.sum(data_item["input_ids"] != pad_token_id)
+                    for data_item in self.data
+                ),
+                max(torch.sum(data_item["labels"] != -100) for data_item in self.data),
+            ),
+            self.max_length,
         )
 
         for data_item in self.data:
             for key, value in data_item.items():
                 value = torch.tensor(np.array(value))
                 dtype = value.dtype
                 if isinstance(value, torch.Tensor) and value.dim() == 2:
@@ -128,15 +132,17 @@
                         elif key == "attention_mask":
                             _pad_value = torch.zeros((padding_length, value.size(1)))
                         elif "label" in key or "labels" in key:
                             _pad_value = -100 * torch.ones(
                                 (padding_length, value.size(1))
                             )
                         else:
-                            _pad_value = pad_value
+                            _pad_value = pad_value * torch.ones(
+                                (padding_length, value.size(1))
+                            )
                         data_item[key] = torch.cat([value, _pad_value], dim=0)
                     elif padding_length < 0:
                         data_item[key] = value[:max_length]
                     data_item[key] = data_item[key].to(dtype)
 
                 elif isinstance(value, torch.Tensor) and value.dim() == 1:
                     padding_length = max_length - value.size(0)
@@ -152,15 +158,15 @@
                                     * torch.ones((padding_length,))
                                 )
                         elif key == "attention_mask":
                             _pad_value = torch.zeros((padding_length,))
                         elif "label" in key or "labels" in key:
                             _pad_value = -100 * torch.ones((padding_length,))
                         else:
-                            _pad_value = pad_value
+                            _pad_value = pad_value * torch.ones((padding_length,))
                         data_item[key] = torch.cat([value, _pad_value], dim=0)
                     elif padding_length < 0:
                         data_item[key] = value[:max_length]
 
                     data_item[key] = data_item[key].to(dtype)
 
     def load_data_source(self, data_source, **kwargs):
```

## omnigenome/src/abc/abstract_model.py

```diff
@@ -4,33 +4,30 @@
 # author: YANG, HENG <hy345@exeter.ac.uk> (杨恒)
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 import json
 import os
+import shutil
 import warnings
 
+import findfile
 import torch
-from ViennaRNA import RNA
-from transformers import AutoModel, AutoConfig, AutoTokenizer
-
-from ..misc.utils import fprint, env_meta_info
-
+from transformers import AutoModel, AutoConfig, AutoTokenizer, BatchEncoding
 
 from ..misc.utils import RNA2StructureCache
-
-rna2structure = RNA2StructureCache()
+from ..misc.utils import fprint, env_meta_info
 
 
 def count_parameters(model):
     return sum(p.numel() for p in model.parameters() if p.requires_grad)
 
 
-def extract_last_hidden_state(model, inputs, ss=None, tokenizer=None):
+def last_hidden_state_forward(model, inputs, ss=None, tokenizer=None):
     """
 
     :param model: The model to extract the last hidden state from
     :param inputs: The inputs to the model
     :param ss: We can use the secondary structure information to help the model to learn better representations,
                   if ss is not None, the model will NOT use the secondary structure information. If ss is 'viennarna',
                     the model will use the secondary structure information generated by the ViennaRNA package. If ss is
@@ -39,56 +36,87 @@
     :return: The last hidden state of the model and the secondary structure information if ss is not None
     """
     assert ss in [
         None,
         "viennarna",
         "model",
     ], f'ss should be one of [None, "viennarna", "model"], got {ss}'
+
     if isinstance(inputs, tuple):
         input_ids = inputs[0]
-        attention_mask = inputs[1]
-    else:
+        attention_mask = inputs[1] if len(inputs) > 1 else None
+    elif isinstance(inputs, BatchEncoding) or isinstance(inputs, dict):
         input_ids = inputs["input_ids"]
-        attention_mask = inputs["attention_mask"]
+        attention_mask = (
+            inputs["attention_mask"] if "attention_mask" in inputs else None
+        )
+    else:
+        raise ValueError(
+            f"The inputs should be a tuple, BatchEncoding or a dictionary-like object, got {type(inputs)}."
+        )
 
     try:
         outputs = model(
             input_ids,
             attention_mask=attention_mask,
             output_hidden_states=True,
         )
+
     except Exception as e:
         # For autoregressive models, the attention_mask is not required
         outputs = model(
             input_ids,
             output_hidden_states=True,
         )
 
-    assert (
-        "last_hidden_state" in outputs
-    ), f"last_hidden_state not found in the outputs from the {model.__class__.__name__}"
-    last_hidden_state = outputs["last_hidden_state"]
+    if not hasattr(outputs, "last_hidden_state"):
+        warnings.warn(
+            f"last_hidden_state not found in the outputs from the {model.__class__.__name__} model."
+        )
+
+    if hasattr(outputs, "last_hidden_state"):
+        last_hidden_state = outputs.last_hidden_state
+    elif hasattr(outputs, "hidden_states"):
+        last_hidden_state = outputs.hidden_states[-1]
+    elif (
+        isinstance(outputs, list)
+        or isinstance(outputs, tuple)
+        or isinstance(outputs, torch.Tensor)
+    ):
+        # For some models like DNABERT-2, the outputs is a list, tuple of tensors
+        last_hidden_state = outputs[-1] if len(outputs[-1].shape) == 3 else outputs[0]
+    else:
+        raise ValueError(
+            f"Cannot find the last hidden state in the outputs from the {model.__class__.__name__} \
+            model, please check the model architecture."
+        )
 
     if ss == "viennarna":
+        if not hasattr(model, "rna2structure"):
+            model.rna2structure = RNA2StructureCache()
+
+        if hasattr(tokenizer, "base_tokenizer"):
+            tokenizer = tokenizer.base_tokenizer
         sequences = tokenizer.batch_decode(input_ids, skip_special_tokens=True)
-        structures = [rna2structure.fold(seq)[0] for seq in sequences]
+        structures = model.rna2structure.fold(
+            [seq.replace(" ", "") for seq in sequences]
+        )
         tokenized_struct = tokenizer(
             structures,
             padding="max_length",
             max_length=input_ids.shape[1],
             truncation=True,
             return_tensors="pt",
-            add_special_tokens=False,
+            add_special_tokens=True,
         )
         tokenized_struct.to(input_ids.device)
         ss_last_hidden_state = model(
             **tokenized_struct,
             output_hidden_states=True,
         )["last_hidden_state"]
-        rna2structure.update_cache_file()
     elif ss == "model":
         raise NotImplementedError(
             "The model-based secondary structure information is not implemented yet."
         )
         # tokenized_struct = last_hidden_state.argmax(-1)
         # ss_last_hidden_state = model(
         #     tokenized_struct,
@@ -98,72 +126,101 @@
     else:
         return last_hidden_state
 
     return last_hidden_state, ss_last_hidden_state
 
 
 class OmniGenomeModel(torch.nn.Module):
-    def __init__(self, config, base_model, tokenizer, *args, **kwargs):
-        label2id = kwargs.pop("label2id", None)
-        if label2id is not None:
-            config.label2id = label2id
-            config.id2label = {v: k for k, v in config.label2id.items()}
-            self.num_labels = len(config.label2id)
+    def __init__(self, config_or_model_model, tokenizer, *args, **kwargs):
+        self.loss_fn = None
 
+        label2id = kwargs.pop("label2id", None)
         trust_remote_code = kwargs.pop("trust_remote_code", True)
+        num_labels = kwargs.pop("num_labels", None)
+        num_labels = num_labels if num_labels is not None else len(label2id)
 
+        # do not change the order of the following lines
         super().__init__(*args, **kwargs)
 
+        if isinstance(config_or_model_model, str):
+            config = AutoConfig.from_pretrained(
+                config_or_model_model,
+                num_labels=num_labels,
+                label2id=label2id,
+                trust_remote_code=trust_remote_code,
+            )
+            self.model = AutoModel.from_pretrained(
+                config_or_model_model,
+                config=config,
+                trust_remote_code=trust_remote_code,
+            )
+            self.model.config = config
+        elif isinstance(config_or_model_model, torch.nn.Module):
+            self.model = config_or_model_model
+        elif isinstance(config_or_model_model, AutoConfig):
+            config = config_or_model_model
+            self.model = AutoModel.from_config(config)
+            self.model.config = config
+        else:
+            raise ValueError(
+                "The config_or_model_model should be either a string, a torch.nn.Module or a AutoConfig object."
+            )
+
+        # Update the config
+        self.config = self.model.config
+        if isinstance(label2id, dict):
+            self.config.label2id = label2id
+            self.config.id2label = {v: k for k, v in label2id.items()}
+
+        # The metadata of the model
         self.metadata = env_meta_info()
         self.metadata["model_cls"] = self.__class__.__name__
-        self.config = config
 
-        if isinstance(base_model, torch.nn.Module):
-            self.model = base_model
+        # The config of the model
+        if hasattr(self.config, "n_embd"):
+            self.config.hidden_size = self.config.n_embd
+        elif hasattr(self.config, "d_model"):
+            self.config.hidden_size = self.config.d_model
+        elif hasattr(self.config, "hidden_size"):
+            self.config.hidden_size = self.config.hidden_size
         else:
-            self.model = AutoModel.from_pretrained(
-                base_model, trust_remote_code=trust_remote_code
+            raise RuntimeError(
+                "The hidden size of the model is not found in the config."
             )
 
-        self.model.config = config
-
+        # The tokenizer of the model
         self.tokenizer = tokenizer
         if hasattr(self.tokenizer, "base_tokenizer"):
             self.pad_token_id = self.tokenizer.base_tokenizer.pad_token_id
         else:
             self.pad_token_id = self.tokenizer.pad_token_id
 
-        self.num_labels = config.num_labels
         self.dropout = torch.nn.Dropout(kwargs.get("dropout", 0.0))
         self.activation = torch.nn.Tanh()
 
-        for key, value in kwargs.items():
-            self.metadata[key] = value
-
-        fprint(
-            f"The trainable parameters of the model are: {count_parameters(self.model) / 1e6:.2f} Millions"
-        )
-
     def loss_function(self, logits, labels):
         raise NotImplementedError(
             "The loss_function() function should be implemented for your model."
         )
 
+    def set_loss_fn(self, loss_function):
+        self.loss_fn = loss_function
+
     def predict(self, inputs, **kwargs):
         raise NotImplementedError(
             "The predict() function should be implemented for your model."
         )
 
     def inference(self, inputs, **kwargs):
         raise NotImplementedError(
             "The inference() function should be implemented for your model."
         )
 
     def forward(self, inputs):
-        last_hidden_state = extract_last_hidden_state(self.model, inputs)
+        last_hidden_state = last_hidden_state_forward(self.model, inputs)
         last_hidden_state = self.dropout(last_hidden_state)
         last_hidden_state = self.activation(last_hidden_state)
         outputs = {"last_hidden_state": last_hidden_state}
         return outputs
 
     def __call__(self, inputs, labels=None, *args, **kwargs):
         if isinstance(inputs, dict):
@@ -199,73 +256,97 @@
             return loss
         else:
             raise RuntimeError(
                 "The output of the forward() function should be a dictionary-like objective"
                 " and have either 'loss', or 'logits' and 'labels' attribute."
             )
 
-    def save(self, path, overwrite=False, **kwargs):
+    def save(self, path, overwrite=False, dtype=torch.float16, **kwargs):
+        self.eval()
         import dill
 
         if os.path.exists(path) and not overwrite:
             raise FileExistsError(
                 f"The path {path} already exists, please set overwrite=True to overwrite it."
             )
 
         if not os.path.exists(path):
             os.makedirs(path)
 
-        device = self.model.device
-
-        self.model.to("cpu")
+        for file in findfile.find_files(
+            self.config.name_or_path,
+            and_key=[],
+            exclude_key=["pytorch_model", "model", "safetensors"],
+        ):
+            shutil.copyfile(file, f"{path}/{os.path.basename(file)}")
+
+        _device = self.model.device
+        _dtype = self.model.dtype
+        self.model.to(dtype).to("cpu")
         with open(f"{path}/tokenizer.pkl", "wb") as f:
             dill.dump(self.tokenizer, f)
         with open(f"{path}/metadata.json", "w", encoding="utf8") as f:
             json.dump(self.metadata, f)
-        self.model.save_pretrained(path, safe_serialization=False)
-        self.config.save_pretrained(path)
-        self.model.to(device)
+        self.model.save_pretrained(
+            f"{path}", safe_serialization=False
+        )  # do not remove this line, used to save customed model scripts
+        with open(f"{path}/pytorch_model.bin", "wb") as f:
+            torch.save(self.state_dict(), f)
+
+        self.model.to(_dtype).to(_device)
+        fprint(f"The model is saved to {path}.")
 
     def load(self, path, **kwargs):
         with open(f"{path}/metadata.json", "r", encoding="utf8") as f:
             metadata = json.load(f)
 
         if metadata["model_cls"] != self.__class__.__name__:  # Check the model class
             raise ValueError(
                 f"The model class in the loaded model is {metadata['model_cls']}, "
                 f"but the current model class is {self.__class__.__name__}."
             )
         config = AutoConfig.from_pretrained(path, trust_remote_code=True, **kwargs)
-        self.config.from_pretrained(path, trust_remote_code=True, **kwargs)
-        with open(f"{path}/config.json", "r", encoding="utf8") as f:
-            config.__dict__ = json.load(f)
 
         for key, value in config.__dict__.items():
             if key not in self.config.__dict__ or self.config.__dict__[key] != value:
                 fprint(
                     f"Warning: The value of the key {key} in the loaded model is {value}, "
                     f"but the current value is {self.config.__dict__.get(key, None)}."
                 )
 
         with open(f"{path}/pytorch_model.bin", "rb") as f:
-            self.model.load_state_dict(
-                torch.load(f, map_location=self.model.device), strict=False
+            self.load_state_dict(
+                torch.load(f, map_location=kwargs.get("device", "cpu")), strict=True
             )
-
         return self
 
-    def load_tokenizer(self, path):
-        import dill
-
-        with open(f"{path}/tokenizer.pkl", "rb") as f:
-            tokenizer = dill.load(f)
-        return tokenizer
+    def _is_causal_lm(self):
+        if (
+            hasattr(self.config, "architectures")
+            and "CausalLM" in str(self.config.architectures)
+        ) or (
+            hasattr(self.config, "auto_map") and "CausalLM" in str(self.config.auto_map)
+        ):
+            return True
+        else:
+            return False
 
     @staticmethod
     def from_pretrained(model_name_or_path, tokenizer, *args, **kwargs):
         config = kwargs.pop("config", None)
         if config is None:
             config = AutoConfig.from_pretrained(model_name_or_path, **kwargs)
         base_model = AutoModel.from_pretrained(model_name_or_path, **kwargs)
         if tokenizer is None:
             tokenizer = AutoTokenizer.from_pretrained(base_model, **kwargs)
         return OmniGenomeModel(config, base_model, tokenizer, *args, **kwargs)
+
+    def model_info(self):
+        info = f"Model Name: {self.__class__.__name__}\n"
+        info += f"Model Metadata: {self.metadata}\n"
+        info += f"Base Model Name: {self.config.name_or_path}\n"
+        info += f"Model Type: {self.config.model_type}\n"
+        info += f"Model Architecture: {self.config.architectures}\n"
+        info += f"Model Parameters: {count_parameters(self.model) / 1e6} M\n"
+        info += f"Model Config: {self.config}\n"
+        fprint(info)
+        return info
```

## omnigenome/src/abc/abstract_tokenizer.py

```diff
@@ -1,45 +1,79 @@
 # -*- coding: utf-8 -*-
-# file: tokenizer_wrapper.py
+# file: omnigenome_wrapper.py
 # time: 18:37 06/04/2024
 # author: YANG, HENG <hy345@exeter.ac.uk> (杨恒)
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
+import warnings
 
 from transformers import AutoTokenizer
 
 from ..misc.utils import env_meta_info
 
 
 class OmniGenomeTokenizer:
-    def __init__(self, base_tokenizer=None, max_length=None, **kwargs):
-        super().__init__(**kwargs)
-
+    def __init__(self, base_tokenizer=None, max_length=512, **kwargs):
         self.metadata = env_meta_info()
 
         self.base_tokenizer = base_tokenizer
         self.max_length = max_length
 
         for key, value in kwargs.items():
             self.metadata[key] = value
 
+        self.u2t = kwargs.get("u2t", False)
+        self.t2u = kwargs.get("t2u", False)
+        self.add_whitespace = kwargs.get("add_whitespace", False)
+
+        for key, value in base_tokenizer.__dict__.items():
+            self.key = value
+
     @staticmethod
     def from_pretrained(model_name_or_path, **kwargs):
-        self = OmniGenomeTokenizer(
-            AutoTokenizer.from_pretrained(model_name_or_path, **kwargs)
-        )
-        return self
+        import importlib
+
+        try:
+            wrapper_name = (
+                f"{model_name_or_path.rstrip('/')}.omnigenome_wrapper".replace("/", ".")
+            )
+            wrapper = importlib.import_module(wrapper_name)
+            tokenizer = wrapper.Tokenizer(
+                AutoTokenizer.from_pretrained(model_name_or_path, **kwargs)
+            )
+        except ImportError:
+            warnings.warn(
+                f"Cannot find the tokenizer wrapper from {wrapper_name},"
+                " using the default OmniGenomeTokenizer."
+            )
+            tokenizer = OmniGenomeTokenizer(
+                AutoTokenizer.from_pretrained(model_name_or_path, **kwargs)
+            )
+        return tokenizer
 
     def save_pretrained(self, save_directory):
         self.base_tokenizer.save_pretrained(save_directory)
 
     def __call__(self, *args, **kwargs):
-        return self.base_tokenizer(*args, **kwargs)
+        padding = kwargs.pop("padding", True)
+        truncation = kwargs.pop("truncation", True)
+        max_length = kwargs.pop(
+            "max_length", self.max_length if self.max_length else 512
+        )
+        return_tensor = kwargs.pop("return_tensors", "pt")
+        return self.base_tokenizer(
+            padding=padding,
+            truncation=truncation,
+            max_length=max_length,
+            return_tensors=return_tensor,
+            *args,
+            **kwargs,
+        )
 
     def tokenize(self, sequence, **kwargs):
         raise NotImplementedError(
             "The tokenize() function should be adapted for different models,"
             " please implement it for your model."
         )
```

## omnigenome/src/dataset/__init__.py

```diff
@@ -3,14 +3,11 @@
 # time: 22:33 08/04/2024
 # author: YANG, HENG <hy345@exeter.ac.uk> (杨恒)
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 
-from .omnigenome_dataset import OmniGenomeDataset
-
-from .omnigenome_dataset import OmniGenomeDatasetForTokenClassification
-from .omnigenome_dataset import OmniGenomeDatasetForTokenRegression
-
 from .omnigenome_dataset import OmniGenomeDatasetForSequenceClassification
 from .omnigenome_dataset import OmniGenomeDatasetForSequenceRegression
+from .omnigenome_dataset import OmniGenomeDatasetForTokenClassification
+from .omnigenome_dataset import OmniGenomeDatasetForTokenRegression
```

## omnigenome/src/dataset/omnigenome_dataset.py

```diff
@@ -7,17 +7,16 @@
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 import json
 
 import numpy as np
 import torch
 
-from ... import __name__, __version__
-
 from ..abc.abstract_dataset import OmniGenomeDataset
+from ... import __name__, __version__
 
 
 class OmniGenomeDatasetForTokenClassification(OmniGenomeDataset):
     def __init__(self, data_source, tokenizer, label2id, max_length=None, **kwargs):
         self.label2id = label2id
         self.id2label = {v: k for k, v in label2id.items()}
 
@@ -53,15 +52,15 @@
                 raise Exception(
                     "The input instance must contain a 'seq' or 'sequence' key."
                 )
         else:
             raise Exception("Unknown instance format.")
 
         tokenized_inputs = self.tokenizer(
-            " ".join(list(sequence)),
+            sequence,
             padding="do_not_pad",
             truncation=True,
             max_length=self.max_length,
             return_tensors="pt",
             **kwargs
         )
         for col in tokenized_inputs:
@@ -95,23 +94,27 @@
             self.metadata[key] = value
 
     def prepare_input(self, instance, **kwargs):
         labels = None
         if isinstance(instance, str):
             sequence = instance
         elif isinstance(instance, dict):
-            sequence = instance.get("seq", None)
+            sequence = (
+                instance.get("seq", None)
+                if "seq" in instance
+                else instance.get("sequence", None)
+            )
             label = instance.get("label", None)
             labels = instance.get("labels", None)
             labels = labels if labels is not None else label
         else:
             raise Exception("Unknown instance format.")
 
         tokenized_inputs = self.tokenizer(
-            " ".join(list(sequence)),
+            sequence,
             padding="do_not_pad",
             truncation=True,
             max_length=self.max_length,
             return_tensors="pt",
             **kwargs
         )
         for col in tokenized_inputs:
@@ -141,23 +144,27 @@
             self.metadata[key] = value
 
     def prepare_input(self, instance, **kwargs):
         labels = None
         if isinstance(instance, str):
             sequence = instance
         elif isinstance(instance, dict):
-            sequence = instance.get("seq", None)
+            sequence = (
+                instance.get("seq", None)
+                if "seq" in instance
+                else instance.get("sequence", None)
+            )
             label = instance.get("label", None)
             labels = instance.get("labels", None)
             labels = labels if labels is not None else label
         else:
             raise Exception("Unknown instance format.")
 
         tokenized_inputs = self.tokenizer(
-            " ".join(list(sequence)),
+            sequence,
             padding="do_not_pad",
             truncation=True,
             max_length=self.max_length,
             return_tensors="pt",
             **kwargs
         )
         for col in tokenized_inputs:
@@ -210,23 +217,27 @@
             self.metadata[key] = value
 
     def prepare_input(self, instance, **kwargs):
         labels = None
         if isinstance(instance, str):
             sequence = instance
         elif isinstance(instance, dict):
-            sequence = instance.get("seq", None)
+            sequence = (
+                instance.get("seq", None)
+                if "seq" in instance
+                else instance.get("sequence", None)
+            )
             label = instance.get("label", None)
             labels = instance.get("labels", None)
             labels = labels if labels is not None else label
         else:
             raise Exception("Unknown instance format.")
 
         tokenized_inputs = self.tokenizer(
-            " ".join(list(sequence)),
+            sequence,
             padding="do_not_pad",
             truncation=True,
             max_length=self.max_length,
             return_tensors="pt",
             **kwargs
         )
         for col in tokenized_inputs:
```

## omnigenome/src/metric/__init__.py

 * *Ordering differences only*

```diff
@@ -4,9 +4,9 @@
 # author: YANG, HENG <hy345@exeter.ac.uk> (杨恒)
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 
 from .classification_metric import ClassificationMetric
-from .regression_metric import RegressionMetric
 from .ranking_metric import RankingMetric
+from .regression_metric import RegressionMetric
```

## omnigenome/src/metric/classification_metric.py

```diff
@@ -4,14 +4,15 @@
 # author: YANG, HENG <hy345@exeter.ac.uk> (杨恒)
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 
 import types
+
 import numpy as np
 import sklearn.metrics as metrics
 
 from ..abc.abstract_metric import OmniGenomeMetric
 
 
 class ClassificationMetric(OmniGenomeMetric):
```

## omnigenome/src/metric/ranking_metric.py

```diff
@@ -5,14 +5,15 @@
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 
 
 import types
+
 import numpy as np
 import sklearn.metrics as metrics
 
 from ..abc.abstract_metric import OmniGenomeMetric
 
 
 class RankingMetric(OmniGenomeMetric):
```

## omnigenome/src/metric/regression_metric.py

```diff
@@ -5,14 +5,15 @@
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 
 
 import types
+
 import numpy as np
 import sklearn.metrics as metrics
 
 from ..abc.abstract_metric import OmniGenomeMetric
 
 
 class RegressionMetric(OmniGenomeMetric):
```

## omnigenome/src/misc/utils.py

```diff
@@ -2,19 +2,22 @@
 # file: utils.py
 # time: 14:45 06/04/2024
 # author: YANG, HENG <hy345@exeter.ac.uk> (杨恒)
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
+import multiprocessing
 import os
 import pickle
 import sys
 import time
 
+import RNA
+
 
 def seed_everything(seed=42):
     import random
     import numpy as np
     import torch
 
     random.seed(seed)
@@ -29,26 +32,23 @@
     def __init__(self, cache_file=None, *args, **kwargs):
         import RNA
 
         self.RNA = RNA
         super().__init__(*args, **kwargs)
 
         if not cache_file:
-            cache_file = "__OMNIGENOME_DATA__/rna2stucture.cache.pkl"
-
-        self.cache_file = cache_file
-
-        if not os.path.exists(os.path.dirname(cache_file)):
-            os.makedirs(os.path.dirname(cache_file))
+            self.cache_file = "__OMNIGENOME_DATA__/rna2stucture.cache.pkl"
+        else:
+            self.cache_file = cache_file
 
-        if not os.path.exists(cache_file):
+        if self.cache_file is None or not os.path.exists(self.cache_file):
             self.cache = {}
         else:
-            print(f"Loading cache from {cache_file}...")
-            with open(cache_file, "rb") as f:
+            print(f"Initialize sequence to structure cache from {self.cache_file}...")
+            with open(self.cache_file, "rb") as f:
                 self.cache = pickle.load(f)
 
         self.queue_num = 0
 
     def __getitem__(self, key):
         return self.cache[key]
 
@@ -57,34 +57,68 @@
 
     def __str__(self):
         return str(self.cache)
 
     def __repr__(self):
         return str(self.cache)
 
-    def fold(self, sequence):
-        if sequence in self.cache:
-            return self.cache[sequence]
+    def fold(self, sequence, return_mfe=False, num_workers=1):
+        if not isinstance(sequence, list):
+            sequence = [sequence]
+
+        structures = []
+
+        if not all([seq in self.cache for seq in sequence]):
+            if num_workers == 1:
+                for seq in sequence:
+                    if seq not in self.cache:
+                        self.cache[seq] = RNA.fold(seq)
+            else:
+                if num_workers is None:
+                    num_workers = min(os.cpu_count(), len(sequence))
+
+                with multiprocessing.Pool(num_workers) as pool:
+                    for seq in sequence:
+                        if seq in self.cache:
+                            continue
+                        self.queue_num += 1
+                        async_result = pool.apply_async(RNA.fold, args=(seq,))
+                        structures.append(async_result)
+
+                    for seq, result in zip(sequence, structures):
+                        self.cache[seq] = result.get()  # result is a tuple
+
+        if return_mfe:
+            structures = [self.cache[seq] for seq in sequence]
         else:
-            self.queue_num += 1
-            fold = self.RNA.fold(sequence)
-            self.cache[sequence] = fold
-            return fold
+            structures = [self.cache[seq][0] for seq in sequence]
+
+        self.update_cache_file(self.cache_file)
+
+        if len(structures) == 1:
+            return structures[0]
+        else:
+            return structures
 
     def update_cache_file(self, cache_file=None):
         if self.queue_num < 100:
             return
 
         if cache_file is None:
             cache_file = self.cache_file
 
+        if not os.path.exists(os.path.dirname(cache_file)):
+            os.makedirs(os.path.dirname(cache_file))
+
         print(f"Updating cache file {cache_file}...")
-        with open(self.cache_file, "wb") as f:
+        with open(cache_file, "wb") as f:
             pickle.dump(self.cache, f)
 
+        self.queue_num = 0
+
 
 def env_meta_info():
     from torch.version import __version__ as torch_version
     from torch.version import cuda as torch_cuda_version
     from torch.version import git_version
     from transformers import __version__ as transformers_version
     from ... import __version__ as omnigenome_version
```

## omnigenome/src/model/__init__.py

 * *Ordering differences only*

```diff
@@ -4,10 +4,10 @@
 # author: YANG, HENG <hy345@exeter.ac.uk> (杨恒)
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 
 from .classiifcation.model import *
-from .regression.model import *
 from .mlm.model import *
+from .regression.model import *
 from .seq2seq.model import *
```

## omnigenome/src/model/classiifcation/model.py

```diff
@@ -4,91 +4,87 @@
 # author: YANG, HENG <hy345@exeter.ac.uk> (杨恒)
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 
 import torch
+from transformers import BatchEncoding
 from transformers.models.bert.modeling_bert import BertPooler
 
 from ...abc.abstract_model import OmniGenomeModel
-from ...abc.abstract_model import extract_last_hidden_state
+from ...abc.abstract_model import last_hidden_state_forward
 
 
-class OmniGenomeEncoderModelForTokenClassification(OmniGenomeModel):
-    def __init__(self, config, base_model, tokenizer, *args, **kwargs):
-        super().__init__(config, base_model, tokenizer, *args, **kwargs)
-        self.metadata["model_name"] = "OmniGenomeEncoderModelForTokenClassification"
+class OmniGenomeModelForTokenClassification(OmniGenomeModel):
+    def __init__(self, config_or_model_model, tokenizer, *args, **kwargs):
+        super().__init__(config_or_model_model, tokenizer, *args, **kwargs)
+        self.metadata["model_name"] = self.__class__.__name__
         self.softmax = torch.nn.Softmax(dim=-1)
-        self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)
+        self.classifier = torch.nn.Linear(
+            self.config.hidden_size, self.config.num_labels
+        )
+        self.loss_fn = torch.nn.CrossEntropyLoss()
+        self.model_info()
 
     def forward(self, inputs):
-        last_hidden_state = extract_last_hidden_state(self.model, inputs)
+        last_hidden_state = last_hidden_state_forward(self.model, inputs)
         last_hidden_state = self.dropout(last_hidden_state)
         last_hidden_state = self.activation(last_hidden_state)
         logits = self.classifier(last_hidden_state)
         logits = self.softmax(logits)
         outputs = {"logits": logits, "last_hidden_state": last_hidden_state}
         return outputs
 
     def predict(self, sequence_or_inputs, **kwargs):
-        if isinstance(sequence_or_inputs, str):
+        if not isinstance(sequence_or_inputs, BatchEncoding) and not isinstance(
+            sequence_or_inputs, dict
+        ):
             inputs = self.tokenizer(sequence_or_inputs, return_tensors="pt", **kwargs)
         else:
             inputs = sequence_or_inputs
+        inputs = inputs.to(self.model.device)
 
-        if len(inputs["input_ids"].shape) == 1:
-            for col in inputs:
-                inputs[col] = inputs[col].to(self.model.device).unsqueeze(0)
-        else:
-            for col in inputs:
-                inputs[col] = inputs[col].to(self.model.device)
-
-        outputs = self(inputs)
+        with torch.no_grad():
+            outputs = self(inputs)
         logits = outputs["logits"]
         last_hidden_state = outputs["last_hidden_state"]
 
         predictions = []
         for i in range(logits.shape[0]):
             predictions.append(logits[i].argmax(dim=-1).detach().cpu().numpy())
 
-        if len(predictions) == 1:
-            outputs = {
-                "predictions": predictions[0],
-                "logits": logits[0],
-                "last_hidden_state": last_hidden_state[0],
-            }
-        else:
-            outputs = {
-                "predictions": predictions,
-                "logits": logits,
-                "last_hidden_state": last_hidden_state,
-            }
+        outputs = {
+            "predictions": predictions,
+            "logits": logits,
+            "last_hidden_state": last_hidden_state,
+        }
 
         return outputs
 
     def inference(self, sequence_or_inputs, **kwargs):
         inputs = self.tokenizer(sequence_or_inputs, return_tensors="pt", **kwargs)
         inputs = inputs.to(self.model.device)
 
-        outputs = self(inputs)
-        logits = outputs["logits"]
-        last_hidden_state = outputs["last_hidden_state"]
+        with torch.no_grad():
+            outputs = self(inputs)
+        logits = outputs["logits"][:, 1:-1:, :]
+        last_hidden_state = outputs["last_hidden_state"][:, 1:-1:, :]
 
         predictions = []
         for i in range(logits.shape[0]):
             i_logit = logits[i][
-                : inputs["input_ids"][0].ne(self.config.pad_token_id).sum(dim=-1)
-            ][1:-1]
+                : inputs["input_ids"][i].ne(self.config.pad_token_id).sum(dim=-1)
+            ]
             prediction = [
                 self.config.id2label.get(x.item(), "") for x in i_logit.argmax(dim=-1)
             ]
             predictions.append(prediction)
 
-        if len(predictions) == 1:
+        if not isinstance(sequence_or_inputs, list):
             outputs = {
                 "predictions": predictions[0],
                 "logits": logits[0],
                 "last_hidden_state": last_hidden_state[0],
             }
         else:
             outputs = {
@@ -96,88 +92,113 @@
                 "logits": logits,
                 "last_hidden_state": last_hidden_state,
             }
 
         return outputs
 
     def loss_function(self, logits, labels):
-        loss_fn = torch.nn.CrossEntropyLoss()
-        loss = loss_fn(logits.view(-1, self.config.num_labels), labels.view(-1))
+        loss = self.loss_fn(logits.view(-1, self.config.num_labels), labels.view(-1))
         return loss
 
 
-class OmniGenomeEncoderModelForSequenceClassification(OmniGenomeModel):
-    def __init__(self, config, base_model, tokenizer, *args, **kwargs):
-        super().__init__(config, base_model, tokenizer, *args, **kwargs)
-        self.metadata["model_name"] = "OmniGenomeEncoderModelForSequenceClassification"
-        self.pooler = BertPooler(config)
+class OmniGenomeModelForSequenceClassification(OmniGenomeModel):
+    def __init__(self, config_or_model, tokenizer, *args, **kwargs):
+        super().__init__(config_or_model, tokenizer, *args, **kwargs)
+        self.metadata["model_name"] = self.__class__.__name__
+        self.pooler = BertPooler(self.config)
         self.softmax = torch.nn.Softmax(dim=-1)
-        self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)
+        self.classifier = torch.nn.Linear(
+            self.config.hidden_size, self.config.num_labels
+        )
+        self.loss_fn = torch.nn.CrossEntropyLoss()
+        self.model_info()
 
     def forward(self, inputs):
-        last_hidden_state = extract_last_hidden_state(self.model, inputs)
+        last_hidden_state = last_hidden_state_forward(self.model, inputs)
         last_hidden_state = self.dropout(last_hidden_state)
         last_hidden_state = self.activation(last_hidden_state)
-        pooled_output = self.pooler(last_hidden_state)
-        logits = self.classifier(pooled_output)
-        logits = self.softmax(logits)
+
+        if self._is_causal_lm():
+            logits = self.classifier(last_hidden_state)
+            pad_token_id = getattr(self.config, "pad_token_id", -100)
+            sequence_lengths = inputs["input_ids"].ne(pad_token_id).sum(dim=1) - 1
+            logits = logits[
+                torch.arange(inputs["input_ids"].size(0), device=logits.device),
+                sequence_lengths,
+            ]
+        else:
+            last_hidden_state = self.pooler(last_hidden_state)
+            logits = self.classifier(last_hidden_state)
+            logits = self.activation(logits)
+            logits = self.softmax(logits)
+
         outputs = {"logits": logits, "last_hidden_state": last_hidden_state}
         return outputs
 
     def predict(self, sequence_or_inputs, **kwargs):
-        if isinstance(sequence_or_inputs, str):
-            inputs = self.tokenizer(sequence_or_inputs, return_tensors="pt", **kwargs)
+        if not isinstance(sequence_or_inputs, BatchEncoding) and not isinstance(
+            sequence_or_inputs, dict
+        ):
+            inputs = self.tokenizer(
+                sequence_or_inputs,
+                padding=kwargs.pop("padding", "max_length"),
+                max_length=kwargs.pop("max_length", 512),
+                truncation=True,
+                return_tensors="pt",
+                **kwargs
+            )
         else:
             inputs = sequence_or_inputs
+        inputs = inputs.to(self.model.device)
 
-        if len(inputs["input_ids"].shape) == 1:
-            for col in inputs:
-                inputs[col] = inputs[col].to(self.model.device).unsqueeze(0)
-        else:
-            for col in inputs:
-                inputs[col] = inputs[col].to(self.model.device)
-
-        outputs = self(inputs)
+        with torch.no_grad():
+            outputs = self(inputs)
         logits = outputs["logits"]
         last_hidden_state = outputs["last_hidden_state"]
 
         predictions = []
         for i in range(logits.shape[0]):
             predictions.append(logits[i].argmax(dim=-1).item())
 
-        if len(predictions) == 1:
-            outputs = {
-                "predictions": predictions[0],
-                "logits": logits[0],
-                "last_hidden_state": last_hidden_state[0],
-            }
-        else:
-            outputs = {
-                "predictions": predictions,
-                "logits": logits,
-                "last_hidden_state": last_hidden_state,
-            }
+        outputs = {
+            "predictions": predictions,
+            "logits": logits,
+            "last_hidden_state": last_hidden_state,
+        }
 
         return outputs
 
     def inference(self, sequence_or_inputs, **kwargs):
-        inputs = self.tokenizer(sequence_or_inputs, return_tensors="pt", **kwargs)
+        if not isinstance(sequence_or_inputs, BatchEncoding) and not isinstance(
+            sequence_or_inputs, dict
+        ):
+            inputs = self.tokenizer(
+                sequence_or_inputs,
+                padding=kwargs.pop("padding", "max_length"),
+                max_length=kwargs.pop("max_length", 512),
+                truncation=True,
+                return_tensors="pt",
+                **kwargs
+            )
+        else:
+            inputs = sequence_or_inputs
         inputs = inputs.to(self.model.device)
 
-        outputs = self(inputs)
+        with torch.no_grad():
+            outputs = self(inputs)
         logits = outputs["logits"]
         last_hidden_state = outputs["last_hidden_state"]
 
         predictions = []
         for i in range(logits.shape[0]):
             predictions.append(
                 self.config.id2label.get(logits[i].argmax(dim=-1).item(), "")
             )
 
-        if len(predictions) == 1:
+        if not isinstance(sequence_or_inputs, list):
             outputs = {
                 "predictions": predictions[0],
                 "logits": logits[0],
                 "last_hidden_state": last_hidden_state[0],
             }
         else:
             outputs = {
@@ -185,39 +206,39 @@
                 "logits": logits,
                 "last_hidden_state": last_hidden_state,
             }
 
         return outputs
 
     def loss_function(self, logits, labels):
-        loss_fn = torch.nn.CrossEntropyLoss()
-        loss = loss_fn(logits.view(-1, self.config.num_labels), labels.view(-1))
+        loss = self.loss_fn(logits.view(-1, self.config.num_labels), labels.view(-1))
         return loss
 
 
-class OmniGenomeEncoderModelForTokenClassificationWith2DStructure(
-    OmniGenomeEncoderModelForSequenceClassification
+class OmniGenomeModelForTokenClassificationWith2DStructure(
+    OmniGenomeModelForTokenClassification
 ):
-    def __init__(self, config, base_model, tokenizer, *args, **kwargs):
-        super().__init__(config, base_model, tokenizer, *args, **kwargs)
-        self.metadata[
-            "model_name"
-        ] = "OmniGenomeEncoderModelForTokenClassificationWith2DStructure"
+    def __init__(self, config_or_model_model, tokenizer, *args, **kwargs):
+        super().__init__(config_or_model_model, tokenizer, *args, **kwargs)
+        self.metadata["model_name"] = self.__class__.__name__
 
-        self.cat_layer = torch.nn.Linear(config.hidden_size * 2, config.hidden_size)
+        self.cat_layer = torch.nn.Linear(
+            self.config.hidden_size * 2, self.config.hidden_size
+        )
         self.conv1d = torch.nn.Conv1d(
-            in_channels=config.hidden_size * 2,
-            out_channels=config.hidden_size,
+            in_channels=self.config.hidden_size * 2,
+            out_channels=self.config.hidden_size,
             kernel_size=1,
             stride=1,
             padding=0,
         )
+        self.model_info()
 
     def forward(self, inputs):
-        last_hidden_state, ss_last_hidden_state = extract_last_hidden_state(
+        last_hidden_state, ss_last_hidden_state = last_hidden_state_forward(
             self.model, inputs, ss="viennarna", tokenizer=self.tokenizer
         )
 
         cat_last_hidden_state = torch.cat(
             [last_hidden_state, ss_last_hidden_state], dim=-1
         )
         conv_output = self.conv1d(cat_last_hidden_state.transpose(1, 2)).transpose(1, 2)
@@ -232,34 +253,35 @@
             "logits": logits,
             "last_hidden_state": last_hidden_state,
             "ss_last_hidden_state": ss_last_hidden_state,
         }
         return outputs
 
 
-class OmniGenomeEncoderModelForSequenceClassificationWith2DStructure(
-    OmniGenomeEncoderModelForSequenceClassification
+class OmniGenomeModelForSequenceClassificationWith2DStructure(
+    OmniGenomeModelForSequenceClassification
 ):
-    def __init__(self, config, base_model, tokenizer, *args, **kwargs):
-        super().__init__(config, base_model, tokenizer, *args, **kwargs)
-        self.metadata[
-            "model_name"
-        ] = "OmniGenomeEncoderModelForSequenceClassificationWith2DStructure"
+    def __init__(self, config_or_model_model, tokenizer, *args, **kwargs):
+        super().__init__(config_or_model_model, tokenizer, *args, **kwargs)
+        self.metadata["model_name"] = self.__class__.__name__
 
-        self.cat_layer = torch.nn.Linear(config.hidden_size * 2, config.hidden_size)
+        self.cat_layer = torch.nn.Linear(
+            self.config.hidden_size * 2, self.config.hidden_size
+        )
         self.conv1d = torch.nn.Conv1d(
-            in_channels=config.hidden_size * 2,
-            out_channels=config.hidden_size,
+            in_channels=self.config.hidden_size * 2,
+            out_channels=self.config.hidden_size,
             kernel_size=1,
             stride=1,
             padding=0,
         )
+        self.model_info()
 
     def forward(self, inputs):
-        last_hidden_state, ss_last_hidden_state = extract_last_hidden_state(
+        last_hidden_state, ss_last_hidden_state = last_hidden_state_forward(
             self.model, inputs, ss="viennarna", tokenizer=self.tokenizer
         )
 
         cat_last_hidden_state = torch.cat(
             [last_hidden_state, ss_last_hidden_state], dim=-1
         )
         conv_output = self.conv1d(cat_last_hidden_state.transpose(1, 2)).transpose(1, 2)
@@ -267,214 +289,26 @@
         last_hidden_state = self.cat_layer(
             torch.cat([last_hidden_state, conv_output], dim=-1)
         )
         last_hidden_state = self.dropout(last_hidden_state)
         last_hidden_state = self.activation(last_hidden_state)
         last_hidden_state = self.pooler(last_hidden_state)
         logits = self.classifier(last_hidden_state)
-        logits = self.softmax(logits)
-        outputs = {
-            "logits": logits,
-            "last_hidden_state": last_hidden_state,
-            "ss_last_hidden_state": ss_last_hidden_state,
-        }
-        return outputs
-
-
-class OmniGenomeDecoderModelForTokenClassification(
-    OmniGenomeEncoderModelForTokenClassification
-):
-    def __init__(self, config, base_model, tokenizer, *args, **kwargs):
-        if hasattr(config, "n_embd"):
-            config.hidden_size = config.n_embd
-        elif hasattr(config, "d_model"):
-            config.hidden_size = config.d_model
-        elif hasattr(config, "hidden_size"):
-            config.hidden_size = config.hidden_size
-        else:
-            raise RuntimeError(
-                "The hidden size of the model is not found in the config."
-            )
-
-        super().__init__(config, base_model, tokenizer, *args, **kwargs)
-        self.metadata["model_name"] = "OmniGenomeDecoderModelForTokenClassification"
-
-    def forward(self, inputs):
-        input_ids = inputs["input_ids"]
-        last_hidden_state = extract_last_hidden_state(self.model, (input_ids, None))
-        last_hidden_state = self.dropout(last_hidden_state)
-        last_hidden_state = self.activation(last_hidden_state)
-        logits = self.classifier(last_hidden_state)
-        logits = self.softmax(logits)
-        outputs = {"logits": logits, "last_hidden_state": last_hidden_state}
-        return outputs
-
-
-class OmniGenomeDecoderModelForSequenceClassification(
-    OmniGenomeEncoderModelForSequenceClassification
-):
-    def __init__(self, config, base_model, tokenizer, *args, **kwargs):
-        if hasattr(config, "n_embd"):
-            config.hidden_size = config.n_embd
-        elif hasattr(config, "d_model"):
-            config.hidden_size = config.d_model
-        elif hasattr(config, "hidden_size"):
-            config.hidden_size = config.hidden_size
-        else:
-            raise RuntimeError(
-                "The hidden size of the model is not found in the config."
-            )
-
-        super().__init__(config, base_model, tokenizer, *args, **kwargs)
-        self.metadata["model_name"] = "OmniGenomeDecoderModelForSequenceClassification"
-
-        self.pooler = BertPooler(config)
-
-    def forward(self, inputs):
-        input_ids = inputs["input_ids"]
-        last_hidden_state = extract_last_hidden_state(self.model, (input_ids, None))
-        last_hidden_state = self.dropout(last_hidden_state)
-        last_hidden_state = self.activation(last_hidden_state)
-        last_hidden_state = self.pooler(last_hidden_state)
-        logits = self.classifier(last_hidden_state)
-        logits = self.softmax(logits)
-        sequence_lengths = input_ids.ne(self.config.pad_token_id).sum(dim=-1) - 1
-        pooled_logits = logits[
-            torch.arange(input_ids.size(0), device=logits.device), sequence_lengths
-        ]
-        outputs = {"logits": pooled_logits, "last_hidden_state": last_hidden_state}
-        return outputs
-
 
-class OmniGenomeDecoderModelForTokenClassificationWith2DStructure(
-    OmniGenomeEncoderModelForTokenClassification
-):
-    def __init__(self, config, base_model, tokenizer, *args, **kwargs):
-        if hasattr(config, "n_embd"):
-            config.hidden_size = config.n_embd
-        elif hasattr(config, "d_model"):
-            config.hidden_size = config.d_model
-        elif hasattr(config, "hidden_size"):
-            config.hidden_size = config.hidden_size
+        if self._is_causal_lm():
+            pad_token_id = getattr(self.config, "pad_token_id", -100)
+            sequence_lengths = inputs["input_ids"].ne(pad_token_id).sum(dim=1) - 1
+            logits = logits[
+                torch.arange(inputs["input_ids"].size(0), device=logits.device),
+                sequence_lengths,
+            ]
         else:
-            raise RuntimeError(
-                "The hidden size of the model is not found in the config."
-            )
-
-        super().__init__(config, base_model, tokenizer, *args, **kwargs)
-
-        self.metadata[
-            "model_name"
-        ] = "OmniGenomeDecoderModelForTokenClassificationWith2DStructure"
-
-        self.cat_layer = torch.nn.Linear(config.hidden_size * 2, config.hidden_size)
-        self.conv1d = torch.nn.Conv1d(
-            in_channels=config.hidden_size * 2,
-            out_channels=config.hidden_size,
-            kernel_size=1,
-            stride=1,
-            padding=0,
-        )
-
-        self.pooler = BertPooler(config)
-
-    def forward(self, inputs):
-        input_ids = inputs["input_ids"]
-        last_hidden_state, ss_last_hidden_state = extract_last_hidden_state(
-            self.model, (input_ids, None), ss="viennarna", tokenizer=self.tokenizer
-        )
-
-        cat_last_hidden_state = torch.cat(
-            [last_hidden_state, ss_last_hidden_state], dim=-1
-        )
-        conv_output = self.conv1d(cat_last_hidden_state.transpose(1, 2)).transpose(1, 2)
-        last_hidden_state = self.cat_layer(
-            torch.cat([last_hidden_state, conv_output], dim=-1)
-        )
-
-        sequence_lengths = input_ids.ne(self.config.pad_token_id).sum(dim=-1) - 1
-        last_hidden_state = last_hidden_state[
-            torch.arange(input_ids.size(0), device=last_hidden_state.device),
-            sequence_lengths,
-        ]
-        last_hidden_state = self.dropout(last_hidden_state)
-        last_hidden_state = self.activation(last_hidden_state)
-        logits = self.classifier(last_hidden_state)
-        logits = self.activation(logits)
-        logits = self.softmax(logits)
+            logits = self.activation(logits)
+            logits = self.softmax(logits)
+            logits = self.pooler(logits)
 
         outputs = {
             "logits": logits,
             "last_hidden_state": last_hidden_state,
             "ss_last_hidden_state": ss_last_hidden_state,
         }
         return outputs
-
-
-class OmniGenomeDecoderModelForSequenceClassificationWith2DStructure(
-    OmniGenomeEncoderModelForSequenceClassification
-):
-    def __init__(self, config, base_model, tokenizer, *args, **kwargs):
-        if hasattr(config, "n_embd"):
-            config.hidden_size = config.n_embd
-        elif hasattr(config, "d_model"):
-            config.hidden_size = config.d_model
-        elif hasattr(config, "hidden_size"):
-            config.hidden_size = config.hidden_size
-        else:
-            raise RuntimeError(
-                "The hidden size of the model is not found in the config."
-            )
-
-        super().__init__(config, base_model, tokenizer, *args, **kwargs)
-
-        self.metadata[
-            "model_name"
-        ] = "OmniGenomeDecoderModelForSequenceClassificationWith2DStructure"
-
-        self.cat_layer = torch.nn.Linear(config.hidden_size * 2, config.hidden_size)
-        self.conv1d = torch.nn.Conv1d(
-            in_channels=config.hidden_size * 2,
-            out_channels=config.hidden_size,
-            kernel_size=1,
-            stride=1,
-            padding=0,
-        )
-
-        self.pooler = BertPooler(config)
-
-    def forward(self, inputs):
-        input_ids = inputs["input_ids"]
-        last_hidden_state, ss_last_hidden_state = extract_last_hidden_state(
-            self.model, (input_ids, None), ss="viennarna", tokenizer=self.tokenizer
-        )
-
-        cat_last_hidden_state = torch.cat(
-            [last_hidden_state, ss_last_hidden_state], dim=-1
-        )
-        conv_output = self.conv1d(cat_last_hidden_state.transpose(1, 2)).transpose(1, 2)
-
-        last_hidden_state = self.cat_layer(
-            torch.cat([last_hidden_state, conv_output], dim=-1)
-        )
-
-        sequence_lengths = input_ids.ne(self.config.pad_token_id).sum(dim=-1) - 1
-        last_hidden_state = last_hidden_state[
-            torch.arange(input_ids.size(0), device=last_hidden_state.device),
-            sequence_lengths,
-        ]
-        last_hidden_state = self.dropout(last_hidden_state)
-        last_hidden_state = self.activation(last_hidden_state)
-        last_hidden_state = self.pooler(last_hidden_state)
-        logits = self.classifier(last_hidden_state)
-        logits = self.activation(logits)
-        logits = self.softmax(logits)
-        sequence_lengths = input_ids.ne(self.config.pad_token_id).sum(dim=-1) - 1
-        pooled_logits = logits[
-            torch.arange(input_ids.size(0), device=logits.device), sequence_lengths
-        ]
-        outputs = {
-            "logits": pooled_logits,
-            "last_hidden_state": last_hidden_state,
-            "ss_last_hidden_state": ss_last_hidden_state,
-        }
-        return outputs
```

## omnigenome/src/model/mlm/model.py

```diff
@@ -4,28 +4,33 @@
 # author: YANG, HENG <hy345@exeter.ac.uk> (杨恒)
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 
 import torch
+from transformers import BatchEncoding
 
 from ...abc.abstract_model import OmniGenomeModel
 
 
 class OmniGenomeEncoderModelForMLM(OmniGenomeModel):
-    def __init__(self, config, base_model, tokenizer, *args, **kwargs):
-        super().__init__(config, base_model, tokenizer, *args, **kwargs)
-        self.metadata["model_name"] = "OmniGenomeEncoderModelForMLM"
-        if not hasattr(base_model, "lm_head"):
+    def __init__(self, config_or_model_model, tokenizer, *args, **kwargs):
+        super().__init__(config_or_model_model, tokenizer, *args, **kwargs)
+        self.metadata["model_name"] = self.__class__.__name__
+        if not hasattr(self.model, "lm_head"):
             raise ValueError(
                 "The model does not have a language model head, which is required for MLM."
                 "Please use a model that supports masked language modeling."
             )
-        self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)
+        self.classifier = torch.nn.Linear(
+            self.config.hidden_size, self.config.num_labels
+        )
+
+        self.loss_fn = torch.nn.CrossEntropyLoss()
 
     def forward(self, inputs):
         outputs = self.model(**inputs, output_hidden_states=True)
         last_hidden_state = (
             outputs["last_hidden_state"]
             if "last_hidden_state" in outputs
             else outputs["hidden_states"][-1]
@@ -36,35 +41,32 @@
             "loss": loss,
             "logits": logits,
             "last_hidden_state": last_hidden_state,
         }
         return outputs
 
     def predict(self, sequence_or_inputs, **kwargs):
-        if isinstance(sequence_or_inputs, str):
+        if not isinstance(sequence_or_inputs, BatchEncoding) and not isinstance(
+            sequence_or_inputs, dict
+        ):
             inputs = self.tokenizer(sequence_or_inputs, return_tensors="pt", **kwargs)
         else:
             inputs = sequence_or_inputs
+        inputs = inputs.to(self.model.device)
 
-        if len(inputs["input_ids"].shape) == 1:
-            for col in inputs:
-                inputs[col] = inputs[col].to(self.model.device).unsqueeze(0)
-        else:
-            for col in inputs:
-                inputs[col] = inputs[col].to(self.model.device)
-
-        outputs = self(inputs)
+        with torch.no_grad():
+            outputs = self(inputs)
         logits = outputs["logits"]
         last_hidden_state = outputs["last_hidden_state"]
 
         predictions = []
         for i in range(logits.shape[0]):
             predictions.append(logits[i].argmax(dim=-1).detach().cpu().numpy())
 
-        if len(predictions) == 1:
+        if not isinstance(sequence_or_inputs, list):
             outputs = {
                 "predictions": predictions[0],
                 "logits": logits[0],
                 "last_hidden_state": last_hidden_state[0],
             }
         else:
             outputs = {
@@ -75,27 +77,35 @@
 
         return outputs
 
     def inference(self, sequence_or_inputs, **kwargs):
         inputs = self.tokenizer(sequence_or_inputs, return_tensors="pt", **kwargs)
         inputs = inputs.to(self.model.device)
 
-        outputs = self(inputs)
-        logits = outputs["logits"]
-        last_hidden_state = outputs["last_hidden_state"]
+        with torch.no_grad():
+            outputs = self(inputs)
+        logits = outputs["logits"][:, 1:-1:, :]
+        last_hidden_state = outputs["last_hidden_state"][:, 1:-1:, :]
 
         predictions = []
         for i in range(logits.shape[0]):
             i_logits = logits[i][
                 : inputs["input_ids"][i].ne(self.tokenizer.pad_token_id).sum().item()
-            ][1:-1]
-            prediction = self.tokenizer.decode(i_logits.argmax(dim=-1))
-            predictions.append(prediction)
+            ]
+            prediction = self.tokenizer.decode(i_logits.argmax(dim=-1)).replace(" ", "")
+            if (
+                torch.sum(
+                    inputs["input_ids"][i] == self.tokenizer.convert_tokens_to_ids("U")
+                )
+                > 0
+            ):
+                prediction = prediction.replace("U", "T")
+            predictions.append(list(prediction))
 
-        if len(predictions) == 1:
+        if not isinstance(sequence_or_inputs, list):
             outputs = {
                 "predictions": predictions[0],
                 "logits": logits[0],
                 "last_hidden_state": last_hidden_state[0],
             }
         else:
             outputs = {
```

## omnigenome/src/model/regression/model.py

```diff
@@ -4,87 +4,82 @@
 # author: YANG, HENG <hy345@exeter.ac.uk> (杨恒)
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 
 import torch
+from transformers import BatchEncoding
 from transformers.models.bert.modeling_bert import BertPooler
 
 from ...abc.abstract_model import OmniGenomeModel
-from ...abc.abstract_model import extract_last_hidden_state
+from ...abc.abstract_model import last_hidden_state_forward
 
 
-class OmniGenomeEncoderModelForTokenRegression(OmniGenomeModel):
-    def __init__(self, config, base_model, tokenizer, *args, **kwargs):
-        super().__init__(config, base_model, tokenizer, *args, **kwargs)
-        self.metadata["model_name"] = "OmniGenomeEncoderModelForTokenRegression"
-
-        self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)
+class OmniGenomeModelForTokenRegression(OmniGenomeModel):
+    def __init__(self, config_or_model_model, tokenizer, *args, **kwargs):
+        super().__init__(config_or_model_model, tokenizer, *args, **kwargs)
+        self.metadata["model_name"] = self.__class__.__name__
+        self.classifier = torch.nn.Linear(
+            self.config.hidden_size, self.config.num_labels
+        )
+        self.loss_fn = torch.nn.MSELoss()
+        self.model_info()
 
     def forward(self, inputs):
-        last_hidden_state = extract_last_hidden_state(self.model, inputs)
+        last_hidden_state = last_hidden_state_forward(self.model, inputs)
         last_hidden_state = self.dropout(last_hidden_state)
         last_hidden_state = self.activation(last_hidden_state)
         logits = self.classifier(last_hidden_state)
         outputs = {"logits": logits, "last_hidden_state": last_hidden_state}
         return outputs
 
     def predict(self, sequence_or_inputs, **kwargs):
-        if isinstance(sequence_or_inputs, str):
+        if not isinstance(sequence_or_inputs, BatchEncoding) and not isinstance(
+            sequence_or_inputs, dict
+        ):
             inputs = self.tokenizer(sequence_or_inputs, return_tensors="pt", **kwargs)
         else:
             inputs = sequence_or_inputs
+        inputs = inputs.to(self.model.device)
 
-        if len(inputs["input_ids"].shape) == 1:
-            for col in inputs:
-                inputs[col] = inputs[col].to(self.model.device).unsqueeze(0)
-        else:
-            for col in inputs:
-                inputs[col] = inputs[col].to(self.model.device)
-
-        outputs = self(inputs)
+        with torch.no_grad():
+            outputs = self(inputs)
         logits = outputs["logits"]
         last_hidden_state = outputs["last_hidden_state"]
 
         predictions = []
         for i in range(logits.shape[0]):
             predictions.append(logits[i].detach().cpu().numpy())
 
-        if len(predictions) == 1:
-            outputs = {
-                "predictions": predictions[0],
-                "logits": logits[0],
-                "last_hidden_state": last_hidden_state[0],
-            }
-        else:
-            outputs = {
-                "predictions": predictions,
-                "logits": logits,
-                "last_hidden_state": last_hidden_state,
-            }
+        outputs = {
+            "predictions": predictions,
+            "logits": logits,
+            "last_hidden_state": last_hidden_state,
+        }
 
         return outputs
 
     def inference(self, sequence_or_inputs, **kwargs):
         inputs = self.tokenizer(sequence_or_inputs, return_tensors="pt", **kwargs)
         inputs = inputs.to(self.model.device)
 
-        outputs = self(inputs)
-        logits = outputs["logits"]
-        last_hidden_state = outputs["last_hidden_state"]
+        with torch.no_grad():
+            outputs = self(inputs)
+        logits = outputs["logits"][:, 1:-1:, :]
+        last_hidden_state = outputs["last_hidden_state"][:, 1:-1:, :]
 
         predictions = []
         for i in range(logits.shape[0]):
             i_logits = logits[i][
                 : inputs["input_ids"][i].ne(self.config.pad_token_id).sum().item()
-            ][1:-1]
+            ]
             predictions.append(i_logits.detach().cpu().numpy())
 
-        if len(predictions) == 1:
+        if not isinstance(sequence_or_inputs, list):
             outputs = {
                 "predictions": predictions[0],
                 "logits": logits[0],
                 "last_hidden_state": last_hidden_state[0],
             }
         else:
             outputs = {
@@ -102,85 +97,89 @@
         logits = logits.view(-1)
         labels = labels.view(-1)
         mask = torch.where(labels != padding_value)
 
         filtered_logits = logits[mask]
         filtered_targets = labels[mask]
 
-        loss = torch.nn.functional.mse_loss(
-            filtered_logits, filtered_targets, reduction="mean"
-        )
-        return loss**0.5
+        loss = self.loss_fn(filtered_logits, filtered_targets)
+        return loss
 
 
-class OmniGenomeEncoderModelForSequenceRegression(OmniGenomeModel):
-    def __init__(self, config, base_model, tokenizer, *args, **kwargs):
-        super().__init__(config, base_model, tokenizer, *args, **kwargs)
-        self.metadata["model_name"] = "OmniGenomeEncoderModelForSequenceRegression"
-        self.pooler = BertPooler(config)
-        self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)
+class OmniGenomeModelForSequenceRegression(OmniGenomeModel):
+    def __init__(self, config_or_model_model, tokenizer, *args, **kwargs):
+        super().__init__(config_or_model_model, tokenizer, *args, **kwargs)
+        self.metadata["model_name"] = self.__class__.__name__
+        self.pooler = BertPooler(self.config)
+        self.classifier = torch.nn.Linear(
+            self.config.hidden_size, self.config.num_labels
+        )
+        self.loss_fn = torch.nn.MSELoss()
+        self.model_info()
 
     def forward(self, inputs):
-        last_hidden_state = extract_last_hidden_state(self.model, inputs)
+        last_hidden_state = last_hidden_state_forward(self.model, inputs)
         last_hidden_state = self.dropout(last_hidden_state)
         last_hidden_state = self.activation(last_hidden_state)
-        pooled_output = self.pooler(last_hidden_state)
-        logits = self.classifier(pooled_output)
+
+        if self._is_causal_lm():
+            logits = self.classifier(last_hidden_state)
+            pad_token_id = getattr(self.config, "pad_token_id", -100)
+            sequence_lengths = inputs["input_ids"].ne(pad_token_id).sum(dim=1) - 1
+            logits = logits[
+                torch.arange(inputs["input_ids"].size(0), device=logits.device),
+                sequence_lengths,
+            ]
+        else:
+            last_hidden_state = self.pooler(last_hidden_state)
+            logits = self.classifier(last_hidden_state)
+
         outputs = {"logits": logits, "last_hidden_state": last_hidden_state}
         return outputs
 
     def predict(self, sequence_or_inputs, **kwargs):
-        if isinstance(sequence_or_inputs, str):
+        if not isinstance(sequence_or_inputs, BatchEncoding) and not isinstance(
+            sequence_or_inputs, dict
+        ):
             inputs = self.tokenizer(sequence_or_inputs, return_tensors="pt", **kwargs)
         else:
             inputs = sequence_or_inputs
+        inputs = inputs.to(self.model.device)
 
-        if len(inputs["input_ids"].shape) == 1:
-            for col in inputs:
-                inputs[col] = inputs[col].to(self.model.device).unsqueeze(0)
-        else:
-            for col in inputs:
-                inputs[col] = inputs[col].to(self.model.device)
-
-        outputs = self(inputs)
+        with torch.no_grad():
+            outputs = self(inputs)
         logits = outputs["logits"]
         last_hidden_state = outputs["last_hidden_state"]
 
         predictions = []
         for i in range(logits.shape[0]):
             predictions.append(logits[i][0].item())
 
-        if len(predictions) == 1:
-            outputs = {
-                "predictions": predictions[0],
-                "logits": logits[0],
-                "last_hidden_state": last_hidden_state[0],
-            }
-        else:
-            outputs = {
-                "predictions": predictions,
-                "logits": logits,
-                "last_hidden_state": last_hidden_state,
-            }
+        outputs = {
+            "predictions": predictions,
+            "logits": logits,
+            "last_hidden_state": last_hidden_state,
+        }
 
         return outputs
 
     def inference(self, sequence_or_inputs, **kwargs):
         inputs = self.tokenizer(sequence_or_inputs, return_tensors="pt", **kwargs)
         inputs = inputs.to(self.model.device)
 
-        outputs = self(inputs)
+        with torch.no_grad():
+            outputs = self(inputs)
         logits = outputs["logits"]
         last_hidden_state = outputs["last_hidden_state"]
 
         predictions = []
         for i in range(logits.shape[0]):
             predictions.append(logits[i][0].item())
 
-        if len(predictions) == 1:
+        if not isinstance(sequence_or_inputs, list):
             outputs = {
                 "predictions": predictions[0],
                 "logits": logits[0],
                 "last_hidden_state": last_hidden_state[0],
             }
         else:
             outputs = {
@@ -198,287 +197,102 @@
         logits = logits.view(-1)
         labels = labels.view(-1)
         mask = torch.where(labels != padding_value)
 
         filtered_logits = logits[mask]
         filtered_targets = labels[mask]
 
-        loss = torch.nn.functional.mse_loss(
-            filtered_logits, filtered_targets, reduction="mean"
-        )
-        return loss**0.5
+        loss = self.loss_fn(filtered_logits, filtered_targets)
+        return loss
 
 
-class OmniGenomeEncoderModelForTokenRegressionWith2DStructure(
-    OmniGenomeEncoderModelForSequenceRegression
+class OmniGenomeModelForTokenRegressionWith2DStructure(
+    OmniGenomeModelForTokenRegression
 ):
-    def __init__(self, config, base_model, tokenizer, *args, **kwargs):
-        super().__init__(config, base_model, tokenizer, *args, **kwargs)
-        self.metadata[
-            "model_name"
-        ] = "OmniGenomeEncoderModelForTokenRegressionWith2DStructure"
-        self.cat_layer = torch.nn.Linear(config.hidden_size * 2, config.hidden_size)
-        self.conv1d = torch.nn.Conv1d(
-            in_channels=config.hidden_size * 2,
-            out_channels=config.hidden_size,
-            kernel_size=1,
-            stride=1,
-            padding=0,
-        )
-
-        self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)
-
-    def forward(self, inputs):
-        last_hidden_state, ss_last_hidden_state = extract_last_hidden_state(
-            self.model, inputs, ss="viennarna", tokenizer=self.tokenizer
-        )
+    def __init__(self, config_or_model_model, tokenizer, *args, **kwargs):
+        super().__init__(config_or_model_model, tokenizer, *args, **kwargs)
+        self.metadata["model_name"] = self.__class__.__name__
 
-        cat_last_hidden_state = torch.cat(
-            [last_hidden_state, ss_last_hidden_state], dim=-1
+        self.cat_layer = torch.nn.Linear(
+            self.config.hidden_size * 2, self.config.hidden_size
         )
-        conv_output = self.conv1d(cat_last_hidden_state.transpose(1, 2)).transpose(1, 2)
-        last_hidden_state = self.cat_layer(
-            torch.cat([last_hidden_state, conv_output], dim=-1)
-        )
-        last_hidden_state = self.dropout(last_hidden_state)
-        last_hidden_state = self.activation(last_hidden_state)
-        logits = self.classifier(last_hidden_state)
-        outputs = {
-            "logits": logits,
-            "last_hidden_state": last_hidden_state,
-            "ss_last_hidden_state": ss_last_hidden_state,
-        }
-        return outputs
-
-
-class OmniGenomeEncoderModelForSequenceRegressionWith2DStructure(
-    OmniGenomeEncoderModelForSequenceRegression
-):
-    def __init__(self, config, base_model, tokenizer, *args, **kwargs):
-        super().__init__(config, base_model, tokenizer, *args, **kwargs)
-        self.metadata[
-            "model_name"
-        ] = "OmniGenomeEncoderModelForSequenceRegressionWith2DStructure"
-        self.cat_layer = torch.nn.Linear(config.hidden_size * 2, config.hidden_size)
         self.conv1d = torch.nn.Conv1d(
-            in_channels=config.hidden_size * 2,
-            out_channels=config.hidden_size,
+            in_channels=self.config.hidden_size * 2,
+            out_channels=self.config.hidden_size,
             kernel_size=1,
             stride=1,
             padding=0,
         )
 
-        self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)
+        self.classifier = torch.nn.Linear(
+            self.config.hidden_size, self.config.num_labels
+        )
+        self.model_info()
 
     def forward(self, inputs):
-        last_hidden_state, ss_last_hidden_state = extract_last_hidden_state(
+        last_hidden_state, ss_last_hidden_state = last_hidden_state_forward(
             self.model, inputs, ss="viennarna", tokenizer=self.tokenizer
         )
 
         cat_last_hidden_state = torch.cat(
             [last_hidden_state, ss_last_hidden_state], dim=-1
         )
         conv_output = self.conv1d(cat_last_hidden_state.transpose(1, 2)).transpose(1, 2)
-
         last_hidden_state = self.cat_layer(
             torch.cat([last_hidden_state, conv_output], dim=-1)
         )
         last_hidden_state = self.dropout(last_hidden_state)
         last_hidden_state = self.activation(last_hidden_state)
         logits = self.classifier(last_hidden_state)
         outputs = {
             "logits": logits,
             "last_hidden_state": last_hidden_state,
             "ss_last_hidden_state": ss_last_hidden_state,
         }
         return outputs
 
 
-class OmniGenomeDecoderModelForTokenRegression(
-    OmniGenomeEncoderModelForTokenRegression
+class OmniGenomeModelForSequenceRegressionWith2DStructure(
+    OmniGenomeModelForSequenceRegression
 ):
-    def __init__(self, config, base_model, tokenizer, *args, **kwargs):
-        if hasattr(config, "n_embd"):
-            config.hidden_size = config.n_embd
-        elif hasattr(config, "d_model"):
-            config.hidden_size = config.d_model
-        elif hasattr(config, "hidden_size"):
-            config.hidden_size = config.hidden_size
-        else:
-            raise RuntimeError(
-                "The hidden size of the model is not found in the config."
-            )
-
-        super().__init__(config, base_model, tokenizer, *args, **kwargs)
-        self.metadata["model_name"] = "OmniGenomeDecoderModelForTokenRegression"
-        self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)
-
-    def forward(self, inputs):
-        input_ids = inputs["input_ids"]
-        last_hidden_state = extract_last_hidden_state(self.model, (input_ids, None))
-        last_hidden_state = self.dropout(last_hidden_state)
-        last_hidden_state = self.activation(last_hidden_state)
-        logits = self.classifier(last_hidden_state)
-        sequence_lengths = input_ids.ne(self.config.pad_token_id).sum(dim=-1) - 1
-        pooled_logits = logits[
-            torch.arange(input_ids.size(0), device=logits.device), sequence_lengths
-        ]
-        outputs = {"logits": pooled_logits, "last_hidden_state": last_hidden_state}
-        return outputs
+    def __init__(self, config_or_model_model, tokenizer, *args, **kwargs):
+        super().__init__(config_or_model_model, tokenizer, *args, **kwargs)
+        self.metadata["model_name"] = self.__class__.__name__
 
-
-class OmniGenomeDecoderModelForSequenceRegression(
-    OmniGenomeEncoderModelForSequenceRegression
-):
-    def __init__(self, config, base_model, tokenizer, *args, **kwargs):
-        if hasattr(config, "n_embd"):
-            config.hidden_size = config.n_embd
-        elif hasattr(config, "d_model"):
-            config.hidden_size = config.d_model
-        elif hasattr(config, "hidden_size"):
-            config.hidden_size = config.hidden_size
-        else:
-            raise RuntimeError(
-                "The hidden size of the model is not found in the config."
-            )
-
-        super().__init__(config, base_model, tokenizer, *args, **kwargs)
-        self.metadata["model_name"] = "OmniGenomeDecoderModelForSequenceRegression"
-        self.pooler = BertPooler(config)
-        self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)
-
-    def forward(self, inputs):
-        input_ids = inputs["input_ids"]
-        last_hidden_state = extract_last_hidden_state(self.model, (input_ids, None))
-        last_hidden_state = self.dropout(last_hidden_state)
-        last_hidden_state = self.activation(last_hidden_state)
-        last_hidden_state = self.pooler(last_hidden_state)
-        logits = self.classifier(last_hidden_state)
-        outputs = {"logits": logits, "last_hidden_state": last_hidden_state}
-        return outputs
-
-
-class OmniGenomeDecoderModelForTokenRegressionWith2DStructure(
-    OmniGenomeEncoderModelForTokenRegression
-):
-    def __init__(self, config, base_model, tokenizer, *args, **kwargs):
-        if hasattr(config, "n_embd"):
-            config.hidden_size = config.n_embd
-        elif hasattr(config, "d_model"):
-            config.hidden_size = config.d_model
-        elif hasattr(config, "hidden_size"):
-            config.hidden_size = config.hidden_size
-        else:
-            raise RuntimeError(
-                "The hidden size of the model is not found in the config."
-            )
-
-        super().__init__(config, base_model, tokenizer, *args, **kwargs)
-        self.metadata[
-            "model_name"
-        ] = "OmniGenomeDecoderModelForTokenRegressionWith2DStructure"
-        self.cat_layer = torch.nn.Linear(config.hidden_size * 2, config.hidden_size)
-        self.conv1d = torch.nn.Conv1d(
-            in_channels=config.hidden_size * 2,
-            out_channels=config.hidden_size,
-            kernel_size=1,
-            stride=1,
-            padding=0,
-        )
-
-        self.pooler = BertPooler(config)
-        self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)
-
-    def forward(self, inputs):
-        input_ids = inputs["input_ids"]
-        last_hidden_state, ss_last_hidden_state = extract_last_hidden_state(
-            self.model, (input_ids, None), ss="viennarna", tokenizer=self.tokenizer
-        )
-
-        cat_last_hidden_state = torch.cat(
-            [last_hidden_state, ss_last_hidden_state], dim=-1
-        )
-        conv_output = self.conv1d(cat_last_hidden_state.transpose(1, 2)).transpose(1, 2)
-        last_hidden_state = self.cat_layer(
-            torch.cat([last_hidden_state, conv_output], dim=-1)
+        self.cat_layer = torch.nn.Linear(
+            self.config.hidden_size * 2, self.config.hidden_size
         )
-
-        sequence_lengths = input_ids.ne(self.config.pad_token_id).sum(dim=-1) - 1
-        last_hidden_state = last_hidden_state[
-            torch.arange(input_ids.size(0), device=last_hidden_state.device),
-            sequence_lengths,
-        ]
-        last_hidden_state = self.dropout(last_hidden_state)
-        last_hidden_state = self.activation(last_hidden_state)
-        logits = self.classifier(last_hidden_state)
-        logits = self.activation(logits)
-
-        outputs = {
-            "logits": logits,
-            "last_hidden_state": last_hidden_state,
-            "ss_last_hidden_state": ss_last_hidden_state,
-        }
-        return outputs
-
-
-class OmniGenomeDecoderModelForSequenceRegressionWith2DStructure(
-    OmniGenomeEncoderModelForSequenceRegression
-):
-    def __init__(self, config, base_model, tokenizer, *args, **kwargs):
-        if hasattr(config, "n_embd"):
-            config.hidden_size = config.n_embd
-        elif hasattr(config, "d_model"):
-            config.hidden_size = config.d_model
-        elif hasattr(config, "hidden_size"):
-            config.hidden_size = config.hidden_size
-        else:
-            raise RuntimeError(
-                "The hidden size of the model is not found in the config."
-            )
-
-        super().__init__(config, base_model, tokenizer, *args, **kwargs)
-        self.metadata[
-            "model_name"
-        ] = "OmniGenomeDecoderModelForSequenceRegressionWith2DStructure"
-        self.cat_layer = torch.nn.Linear(config.hidden_size * 2, config.hidden_size)
         self.conv1d = torch.nn.Conv1d(
-            in_channels=config.hidden_size * 2,
-            out_channels=config.hidden_size,
+            in_channels=self.config.hidden_size * 2,
+            out_channels=self.config.hidden_size,
             kernel_size=1,
             stride=1,
             padding=0,
         )
 
-        self.pooler = BertPooler(config)
-        self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)
+        self.classifier = torch.nn.Linear(
+            self.config.hidden_size, self.config.num_labels
+        )
+        self.model_info()
 
     def forward(self, inputs):
-        input_ids = inputs["input_ids"]
-        last_hidden_state, ss_last_hidden_state = extract_last_hidden_state(
-            self.model, (input_ids, None), ss="viennarna", tokenizer=self.tokenizer
+        last_hidden_state, ss_last_hidden_state = last_hidden_state_forward(
+            self.model, inputs, ss="viennarna", tokenizer=self.tokenizer
         )
 
         cat_last_hidden_state = torch.cat(
             [last_hidden_state, ss_last_hidden_state], dim=-1
         )
         conv_output = self.conv1d(cat_last_hidden_state.transpose(1, 2)).transpose(1, 2)
 
         last_hidden_state = self.cat_layer(
             torch.cat([last_hidden_state, conv_output], dim=-1)
         )
-
-        sequence_lengths = input_ids.ne(self.config.pad_token_id).sum(dim=-1) - 1
-        last_hidden_state = last_hidden_state[
-            torch.arange(input_ids.size(0), device=last_hidden_state.device),
-            sequence_lengths,
-        ]
         last_hidden_state = self.dropout(last_hidden_state)
         last_hidden_state = self.activation(last_hidden_state)
-        last_hidden_state = self.pooler(last_hidden_state)
         logits = self.classifier(last_hidden_state)
-        logits = self.activation(logits)
         outputs = {
             "logits": logits,
             "last_hidden_state": last_hidden_state,
             "ss_last_hidden_state": ss_last_hidden_state,
         }
         return outputs
```

## omnigenome/src/model/seq2seq/model.py

```diff
@@ -7,9 +7,9 @@
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 
 from ...abc.abstract_model import OmniGenomeModel
 
 
 class OmniGenomeEncoderModelForSeq2Seq(OmniGenomeModel):
-    def __init__(self, config, base_model, tokenizer, *args, **kwargs):
-        super().__init__(config, base_model, tokenizer, *args, **kwargs)
+    def __init__(self, config_or_model_model, tokenizer, *args, **kwargs):
+        super().__init__(config_or_model_model, tokenizer, *args, **kwargs)
```

## omnigenome/src/tokenizer/__init__.py

 * *Ordering differences only*

```diff
@@ -5,9 +5,9 @@
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 
 
 from .bpe_tokenizer import OmniBPETokenizer
-from .single_nucleotide_tokenizer import OmniSingleNucleotideTokenizer
 from .kmers_tokenizer import OmniKmersTokenizer
+from .single_nucleotide_tokenizer import OmniSingleNucleotideTokenizer
```

## omnigenome/src/tokenizer/bpe_tokenizer.py

```diff
@@ -2,76 +2,82 @@
 # file: bpe_tokenizer.py
 # time: 18:32 08/04/2024
 # author: YANG, HENG <hy345@exeter.ac.uk> (杨恒)
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
+import numpy as np
+from transformers import AutoTokenizer
 
 from ..abc.abstract_tokenizer import OmniGenomeTokenizer
 
 
 def is_bpe_tokenization(tokens, threshold=0.1):
     if not tokens:
         return False
 
-    bpe_endings_count = sum(
-        1
-        for token in tokens
-        if token.contains("##") or token.contains("@@") or token.contains("▁")
-    )
+    # bpe_endings_count = sum(
+    #     1
+    #     for token in tokens
+    #     if token.startswith("##") or token.startswith("@@") or token.startswith("▁")
+    # )
+    # bpe_ratio = bpe_endings_count / len(tokens)
 
-    bpe_ratio = bpe_endings_count / len(tokens)
+    rmse = np.mean([len(token) ** 2 for token in tokens]) ** 0.5
 
-    return bpe_ratio >= threshold
+    return rmse >= threshold
 
 
 class OmniBPETokenizer(OmniGenomeTokenizer):
     def __init__(self, base_tokenizer=None, **kwargs):
         super(OmniBPETokenizer, self).__init__(base_tokenizer, **kwargs)
-        self.metadata["tokenizer_name"] = "BPETokenizer"
+        self.metadata["tokenizer_name"] = self.__class__.__name__
 
     def __call__(self, sequence, **kwargs):
-        sequences = self.tokenize(sequence)
+        if self.u2t:
+            sequence = sequence.replace("U", "T")
+        if self.add_whitespace:
+            sequence = " ".join(list(sequence))
+
+        sequences = self.tokenize(sequence)[
+            : min(self.max_length, kwargs.get("max_length", 512)) - 2
+        ]
 
         if not is_bpe_tokenization(sequences):
             raise ValueError("The tokenizer seems not to be a BPE tokenizer.")
+        tokenized_inputs = dict()
+        tokenized_inputs["input_ids"] = self.base_tokenizer.convert_tokens_to_ids(
+            sequences
+        )
+        tokenized_inputs["attention_mask"] = [1] * len(tokenized_inputs["input_ids"])
 
-        tokenized_inputs = self.base_tokenizer(
-            sequences,
-            padding="do_not_pad",
-            truncation=True,
-            max_length=self.max_length,
+        tokenized_inputs = self.base_tokenizer.pad(
+            tokenized_inputs,
+            padding=kwargs.get("padding", "max_length"),
+            max_length=min(self.max_length, kwargs.get("max_length", 512)),
+            return_attention_mask=kwargs.get("return_attention_mask", True),
             return_tensors="pt",
-            **kwargs
         )
-
         return tokenized_inputs
 
+    @staticmethod
+    def from_pretrained(model_name_or_path, **kwargs):
+        self = OmniBPETokenizer(
+            AutoTokenizer.from_pretrained(model_name_or_path, **kwargs)
+        )
+        return self
+
     def tokenize(self, sequence, **kwargs):
         return self.base_tokenizer.tokenize(sequence)
 
     def encode(self, sequence, **kwargs):
         assert hasattr(
             self.base_tokenizer, "bpe"
         ), "The base tokenizer must be a BPE tokenizer."
         return self.base_tokenizer.encode(sequence, **kwargs)
 
     def decode(self, sequence, **kwargs):
         assert hasattr(
             self.base_tokenizer, "bpe"
         ), "The base tokenizer must be a BPE tokenizer."
         return self.base_tokenizer.decode(sequence, **kwargs)
-
-
-if __name__ == "__main__":
-    from transformers import AutoTokenizer
-
-    RNA = "ACGUAGGUAUCGUAGA"
-    base_tokenizer_name = "bert-base-cased"
-    # base_tokenizer_name = "facebook/esm2_t12_35M_UR50D"
-    base_tokenizer = AutoTokenizer.from_pretrained(base_tokenizer_name)
-    tokenizer = OmniBPETokenizer(base_tokenizer, max_length=512)
-    tokens = tokenizer.tokenize(RNA)
-    print(tokens)
-    tokenized_inputs = tokenizer(RNA)
-    print(tokenized_inputs)
```

## omnigenome/src/tokenizer/kmers_tokenizer.py

```diff
@@ -13,31 +13,43 @@
 
 class OmniKmersTokenizer(OmniGenomeTokenizer):
     def __init__(self, base_tokenizer=None, k=3, overlap=0, max_length=512, **kwargs):
         super(OmniKmersTokenizer, self).__init__(base_tokenizer, **kwargs)
         self.k = k
         self.overlap = overlap
         self.max_length = max_length
-        self.metadata["tokenizer_name"] = "KmersTokenizer"
+        self.metadata["tokenizer_name"] = self.__class__.__name__
 
     def __call__(self, sequence, **kwargs):
-        sequence_tokens = self.tokenize(sequence)
+        if self.u2t:
+            sequence = "".join([seq.replace("U", "T").upper() for seq in sequence])
+        if self.t2u:
+            sequence = "".join([seq.replace("T", "U").upper() for seq in sequence])
+
+        sequence_tokens = self.tokenize(sequence)[
+            : kwargs.get("max_length", self.max_length) - 2
+        ]
         tokenized_inputs = {
             "input_ids": [],
             "attention_mask": [],
         }
-        bos_id, eos_id = self.base_tokenizer("")["input_ids"]
+        bos_id = (
+            self.base_tokenizer.bos_token_id
+            if self.base_tokenizer.bos_token_id is not None
+            else self.base_tokenizer.cls_token_id
+        )
+        eos_id = (
+            self.base_tokenizer.eos_token_id
+            if self.base_tokenizer.eos_token_id is not None
+            else self.base_tokenizer.sep_token_id
+        )
 
         for tokens in sequence_tokens:
             tokenized_inputs["input_ids"].append(
-                [bos_id]
-                + self.base_tokenizer.convert_tokens_to_ids(
-                    tokens[: self.max_length - 2]
-                )
-                + [eos_id]
+                [bos_id] + self.base_tokenizer.convert_tokens_to_ids(tokens) + [eos_id]
             )
             tokenized_inputs["attention_mask"].append(
                 [1] * len(tokenized_inputs["input_ids"][-1])
             )
 
         for i, ids in enumerate(tokenized_inputs["input_ids"]):
             if ids.count(self.base_tokenizer.unk_token_id) / len(ids) > 0.1:
@@ -50,24 +62,31 @@
             max_length=self.max_length,
             pad_to_multiple_of=self.max_length,
             return_attention_mask=True,
             return_tensors="pt",
         )
         return tokenized_inputs
 
+    @staticmethod
+    def from_pretrained(model_name_or_path, **kwargs):
+        self = OmniKmersTokenizer(
+            AutoTokenizer.from_pretrained(model_name_or_path, **kwargs)
+        )
+        return self
+
     def tokenize(self, sequence, **kwargs):
         if isinstance(sequence, str):
             sequences = [sequence]
         else:
             sequences = sequence
 
         sequence_tokens = []
         for i in range(len(sequences)):
             tokens = []
-            for j in range(0, len(sequences[i]), self.overlap):
+            for j in range(0, len(sequences[i]), self.k - self.overlap):
                 tokens.append(sequences[i][j : j + self.k])
 
             sequence_tokens.append(tokens)
 
         return sequence_tokens
 
     def encode(self, input_ids, **kwargs):
```

## omnigenome/src/tokenizer/single_nucleotide_tokenizer.py

```diff
@@ -2,61 +2,83 @@
 # file: single_nucleotide_tokenizer.py
 # time: 18:05 08/04/2024
 # author: YANG, HENG <hy345@exeter.ac.uk> (杨恒)
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
-import pickle
+
 import warnings
 
+from transformers import AutoTokenizer
+
 from ..abc.abstract_tokenizer import OmniGenomeTokenizer
 
 
 class OmniSingleNucleotideTokenizer(OmniGenomeTokenizer):
     def __init__(self, base_tokenizer=None, **kwargs):
         super(OmniSingleNucleotideTokenizer, self).__init__(base_tokenizer, **kwargs)
-        self.metadata["tokenizer_name"] = "SingleNucleotideTokenizer"
+        self.metadata["tokenizer_name"] = self.__class__.__name__
 
     def __call__(self, sequence, **kwargs):
-        sequence_tokens = self.tokenize(sequence)
+        if self.u2t:
+            sequence = "".join([seq.replace("U", "T").upper() for seq in sequence])
+        if self.t2u:
+            sequence = "".join([seq.replace("T", "U").upper() for seq in sequence])
+        if self.add_whitespace:
+            sequence = " ".join(list(sequence))
+        sequence_tokens = self.tokenize(sequence)[
+            : kwargs.get("max_length", self.max_length) - 2
+        ]
         tokenized_inputs = {
             "input_ids": [],
             "attention_mask": [],
         }
-        bos_id, eos_id = self.base_tokenizer("")["input_ids"]
-
+        bos_id = (
+            self.base_tokenizer.bos_token_id
+            if self.base_tokenizer.bos_token_id is not None
+            else self.base_tokenizer.cls_token_id
+        )
+        eos_id = (
+            self.base_tokenizer.eos_token_id
+            if self.base_tokenizer.eos_token_id is not None
+            else self.base_tokenizer.sep_token_id
+        )
         for tokens in sequence_tokens:
             tokenized_inputs["input_ids"].append(
-                [bos_id]
-                + self.base_tokenizer.convert_tokens_to_ids(
-                    tokens[: self.max_length - 2]
-                )
-                + [eos_id]
+                [bos_id] + self.base_tokenizer.convert_tokens_to_ids(tokens) + [eos_id]
             )
             tokenized_inputs["attention_mask"].append(
                 [1] * len(tokenized_inputs["input_ids"][-1])
             )
 
         for i, ids in enumerate(tokenized_inputs["input_ids"]):
             if ids.count(self.base_tokenizer.unk_token_id) / len(ids) > 0.1:
                 warnings.warn(
-                    f"Unknown tokens are more than 10% in the {i}th sequence, please check the tokenization process."
+                    f"Unknown tokens are more than "
+                    f"{ids.count(self.base_tokenizer.unk_token_id) / len(ids)}% in the {i}-th sequence, "
+                    f"please check the tokenization process."
                 )
-
+        max_length = max(len(ids) for ids in tokenized_inputs["input_ids"])
         tokenized_inputs = self.base_tokenizer.pad(
             tokenized_inputs,
-            padding="max_length",
-            max_length=self.max_length,
-            pad_to_multiple_of=self.max_length,
-            return_attention_mask=True,
+            padding=kwargs.get("padding", "max_length"),
+            max_length=min(max_length, kwargs.get("max_length", 512)),
+            return_attention_mask=kwargs.get("return_attention_mask", True),
             return_tensors="pt",
         )
         return tokenized_inputs
 
+    @staticmethod
+    def from_pretrained(model_name_or_path, **kwargs):
+        self = OmniSingleNucleotideTokenizer(
+            AutoTokenizer.from_pretrained(model_name_or_path, **kwargs)
+        )
+        return self
+
     def tokenize(self, sequence, **kwargs):
         if isinstance(sequence, str):
             sequences = [sequence]
         else:
             sequences = sequence
 
         sequence_tokens = []
@@ -69,34 +91,7 @@
         return self.base_tokenizer.encode(sequence, **kwargs)
 
     def decode(self, sequence, **kwargs):
         return self.base_tokenizer.decode(sequence, **kwargs)
 
     def encode_plus(self, sequence, **kwargs):
         return self.base_tokenizer.encode_plus(sequence, **kwargs)
-
-
-if __name__ == "__main__":
-    from transformers import AutoTokenizer
-
-    # RNA = "ACGUAGGUAUCGUAGA"
-    # # base_tokenizer_name = 'bert-base-cased'
-    # base_tokenizer_name = "facebook/esm2_t12_35M_UR50D"
-    # base_tokenizer = AutoTokenizer.from_pretrained(base_tokenizer_name)
-    # tokenizer = KmersTokenizer(base_tokenizer)
-    # tokens = tokenizer.tokenize(RNA)
-    # print(tokens)
-    # tokenized_inputs = tokenizer(RNA)
-    # print(tokenized_inputs)
-
-    RNA = "ACGUAGGUAUCGUAGA"
-    base_tokenizer_name = "bert-base-cased"
-    # base_tokenizer_name = "facebook/esm2_t12_35M_UR50D"
-    base_tokenizer = AutoTokenizer.from_pretrained(base_tokenizer_name)
-    tokenizer = OmniSingleNucleotideTokenizer(base_tokenizer, max_length=512)
-    tokens = tokenizer.tokenize(RNA)
-    print(tokens)
-    tokenized_inputs = tokenizer(RNA)
-    print(tokenized_inputs)
-    pickle.dump(tokenizer, open("tokenizer.og.pkl", "wb"))
-    tokenizer = pickle.load(open("tokenizer.og.pkl", "rb"))
-    tokenized_inputs = tokenizer(RNA)
```

## omnigenome/src/trainer/hf_trainer.py

 * *Ordering differences only*

```diff
@@ -3,20 +3,20 @@
 # time: 14:40 06/04/2024
 # author: YANG, HENG <hy345@exeter.ac.uk> (杨恒)
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 
-from ... import __version__ as omnigenome_version
-from ... import __name__ as omnigenome_name
-
 from transformers import Trainer
 from transformers import TrainingArguments
 
+from ... import __name__ as omnigenome_name
+from ... import __version__ as omnigenome_version
+
 
 class HFTrainer(Trainer):
     def __init__(self, *args, **kwargs):
         super(HFTrainer, self).__init__(*args, **kwargs)
         self.metadata = {
             "library_name": omnigenome_name,
             "omnigenome_version": omnigenome_version,
```

## omnigenome/src/trainer/trainer.py

```diff
@@ -2,114 +2,208 @@
 # file: trainer.py
 # time: 14:40 06/04/2024
 # author: YANG, HENG <hy345@exeter.ac.uk> (杨恒)
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
+import os
+
+import autocuda
 import numpy as np
 import torch
 from tqdm import tqdm
-import autocuda
 
-from ..misc.utils import seed_everything, env_meta_info
+from ..misc.utils import env_meta_info, fprint, seed_everything
+
+
+def _infer_optimization_direction(metrics, prev_metrics):
+    is_prev_increasing = np.mean(list(prev_metrics[0].values())[0]) < np.mean(
+        list(prev_metrics[-1].values())[0]
+    )
+    is_still_increasing = np.mean(list(prev_metrics[1].values())[0]) < np.mean(
+        list(metrics.values())[0]
+    )
+
+    if is_prev_increasing and is_still_increasing:
+        return "larger_is_better"
+
+    is_prev_decreasing = np.mean(list(prev_metrics[0].values())[0]) > np.mean(
+        list(prev_metrics[-1].values())[0]
+    )
+    is_still_decreasing = np.mean(list(prev_metrics[1].values())[0]) > np.mean(
+        list(metrics.values())
+    )
+
+    if is_prev_decreasing and is_still_decreasing:
+        return "smaller_is_better"
 
 
 class Trainer:
     def __init__(
         self,
         model,
         train_loader: torch.utils.data.DataLoader = None,
         eval_loader: torch.utils.data.DataLoader = None,
         test_loader: torch.utils.data.DataLoader = None,
         epochs: int = 3,
+        patience: int = 3,
         optimizer: torch.optim.Optimizer = None,
+        loss_fn: torch.nn.Module = None,
         compute_metrics: [list, str] = None,
         seed: int = 42,
         device: [torch.device, str] = None,
         *args,
         **kwargs,
     ):
         self.model = model
         self.train_loader = train_loader
         self.eval_loader = eval_loader
         self.test_loader = test_loader
         self.epochs = epochs
+        self.patience = patience
         self.optimizer = optimizer
+        self.loss_fn = loss_fn
         self.compute_metrics = (
             compute_metrics if isinstance(compute_metrics, list) else [compute_metrics]
         )
         self.seed = seed
         self.device = device if device else autocuda.auto_cuda()
-
+        if self.loss_fn is not None:
+            self.model.set_loss_fn(self.loss_fn)
         self.model.to(self.device)
 
         self.metadata = env_meta_info()
         self.metrics = {}
 
         self.trial_name = kwargs.get("trial_name", self.model.__class__.__name__)
 
-    def train(self, path_to_save=None, **kwargs):
+    def _is_metric_better(self, metrics, stage="valid"):
+        assert stage in [
+            "valid",
+            "test",
+        ], "The metrics stage should be either 'valid' or 'test'."
+
+        fprint(metrics)
+
+        prev_metrics = self.metrics.get(stage, None)
+
+        if not prev_metrics or len(prev_metrics) <= 1:
+            if stage not in self.metrics:
+                self.metrics.update({f"{stage}": [metrics]})
+            else:
+                self.metrics[f"{stage}"].append(metrics)
+            return True
+
+        if stage not in self.metrics:
+            self.metrics.update({f"{stage}": [metrics]})
+        else:
+            self.metrics[f"{stage}"].append(metrics)
+
+        if "best_valid" not in self.metrics:
+            self.metrics.update({"best_valid": metrics})
+
+        if _infer_optimization_direction(metrics, prev_metrics) == "larger_is_better":
+            if np.mean(list(metrics.values())[0]) > np.mean(
+                list(self.metrics["best_valid"].values())[0]
+            ):
+                self.metrics.update({"best_valid": metrics})
+        elif (
+            _infer_optimization_direction(metrics, prev_metrics) == "smaller_is_better"
+        ):
+            if np.mean(list(metrics.values())[0]) < np.mean(
+                list(self.metrics["best_valid"].values())[0]
+            ):
+                self.metrics.update({"best_valid": metrics})
+        else:
+            return False
+        if self.metrics["best_valid"] == metrics:
+            return True
+        else:
+            return False
+
+    def train(self, path_to_save=None, autocast=False, **kwargs):
         seed_everything(self.seed)
-        valid_metrics = {}
-        test_metrics = {}
+        patience = 0
+
+        if self.eval_loader is not None and len(self.eval_loader) > 0:
+            valid_metrics = self.evaluate()
+        else:
+            valid_metrics = self.test()
+        if self._is_metric_better(valid_metrics, stage="valid"):
+            self._save_state_dict()
+            patience = 0
+
         for epoch in range(self.epochs):
             self.model.train()
             train_loss = []
             train_it = tqdm(
                 self.train_loader, desc=f"Epoch {epoch + 1}/{self.epochs} Loss:"
             )
             for batch in train_it:
                 batch.to(self.device)
-                loss = self.model(batch)["loss"]
+                if autocast:
+                    with torch.cuda.amp.autocast():
+                        loss = self.model(batch)["loss"]
+                else:
+                    loss = self.model(batch)["loss"]
                 loss.backward()
                 self.optimizer.step()
                 self.optimizer.zero_grad()
                 train_loss.append(loss.item())
                 train_it.set_description(
                     f"Epoch {epoch + 1}/{self.epochs} Loss: {np.average(train_loss):.4f}"
                 )
 
-            if self.eval_loader:
+            if self.eval_loader is not None and len(self.eval_loader) > 0:
                 valid_metrics = self.evaluate()
-                self.metrics[f"validation_metrics"] = valid_metrics
-                print(f"Validation Metrics: {valid_metrics}")
+            else:
+                valid_metrics = self.test()
+
+            if self._is_metric_better(valid_metrics, stage="valid"):
+                self._save_state_dict()
+                patience = 0
+            else:
+                patience += 1
+                if patience >= self.patience:
+                    fprint(f"Early stopping at epoch {epoch + 1}.")
+                    break
 
             if path_to_save:
-                _path_to_save = path_to_save + "_epoch_" + str(epoch)
+                _path_to_save = path_to_save + "_epoch_" + str(epoch + 1)
 
                 if valid_metrics:
                     for key, value in valid_metrics.items():
                         _path_to_save += f"_seed_{self.seed}_{key}_{value:.4f}"
 
                 self.save_model(path_to_save, **kwargs)
 
-        if self.test_loader:
+        if self.test_loader is not None and len(self.test_loader) > 0:
+            self._load_state_dict()
             test_metrics = self.test()
-            self.metrics[f"test_metrics"] = test_metrics
-            print(f"Test Metrics: {test_metrics}")
+            self._is_metric_better(test_metrics, stage="test")
 
         if path_to_save:
             _path_to_save = path_to_save + "_final"
-
-            if test_metrics:
-                for key, value in test_metrics.items():
+            if self.metrics["test_metrics"]:
+                for key, value in self.metrics["test_metrics"][-1].items():
                     _path_to_save += f"_seed_{self.seed}_{key}_{value:.4f}"
 
             self.save_model(path_to_save, **kwargs)
 
         return self.metrics
 
     def evaluate(self):
         valid_metrics = {}
         with torch.no_grad():
             self.model.eval()
             val_truth = []
             val_preds = []
-            for batch in self.eval_loader:
+            it = tqdm(self.eval_loader, desc="Evaluating")
+            for batch in it:
                 batch.to(self.device)
                 predictions = self.model.predict(batch)["predictions"]
                 val_truth.append(batch["labels"].detach().cpu().numpy())
                 val_preds.append(np.array(predictions))
 
             val_truth = np.concatenate(val_truth)
             val_preds = np.concatenate(val_preds)
@@ -145,7 +239,36 @@
         raise NotImplementedError(
             "The compute_metrics() function should be implemented for your model."
             " It should return a dictionary of metrics."
         )
 
     def save_model(self, path, overwrite=False, **kwargs):
         self.model.save(path, overwrite, **kwargs)
+
+    def _load_state_dict(self):
+        model_state_dict_path = (
+            self.model.model.__class__.__name__ + "_init_model_state_dict.pt"
+        )
+        optimizer_state_dict_path = (
+            self.model.model.__class__.__name__ + "_init_optimizer_state_dict.pt"
+        )
+        if os.path.exists(optimizer_state_dict_path):
+            self.optimizer.load_state_dict(torch.load(optimizer_state_dict_path))
+        if os.path.exists(model_state_dict_path):
+            self.model.load_state_dict(torch.load(model_state_dict_path))
+        self.model.to(self.device)
+
+    def _save_state_dict(self):
+        model_state_dict_path = (
+            self.model.model.__class__.__name__ + "_init_model_state_dict.pt"
+        )
+        optimizer_state_dict_path = (
+            self.model.model.__class__.__name__ + "_init_optimizer_state_dict.pt"
+        )
+        if os.path.exists(model_state_dict_path):
+            os.remove(model_state_dict_path)
+        if os.path.exists(optimizer_state_dict_path):
+            os.remove(optimizer_state_dict_path)
+        self.model.to("cpu")
+        torch.save(self.optimizer.state_dict(), optimizer_state_dict_path)
+        torch.save(self.model.state_dict(), model_state_dict_path)
+        self.model.to(self.device)
```

## omnigenome/utility/hub_utils.py

```diff
@@ -11,20 +11,18 @@
 import os
 from distutils.version import StrictVersion
 from typing import Union, Dict, Any
 
 import findfile
 import requests
 import tqdm
-from findfile import find_cwd_files, find_cwd_dir
 from termcolor import colored
 
-from omnigenome.src.misc.utils import fprint
 from omnigenome import __version__ as current_version
-
+from omnigenome.src.misc.utils import fprint
 
 default_repo = "https://huggingface.co/spaces/anonymous8/gfm_hub/"
 
 
 def unzip_checkpoint(checkpoint_path):
     """
     Unzips a checkpoint file.
```

## omnigenome/utility/model_hub/model_hub.py

```diff
@@ -5,14 +5,15 @@
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 import json
 import os
 
+import autocuda
 import torch
 from transformers import AutoConfig, AutoModel
 
 from omnigenome.utility.hub_utils import query_models_info, download_model
 from ...src.misc.utils import env_meta_info, fprint
 
 
@@ -25,15 +26,15 @@
     @staticmethod
     def load_model_and_tokenizer(model_name_or_path, local_only=False, **kwargs):
         model = ModelHub.load(model_name_or_path, local_only=local_only, **kwargs)
         fprint(f"The model and tokenizer has been loaded from {model_name_or_path}.")
         return model, model.tokenizer
 
     @staticmethod
-    def load(model_name_or_path, local_only=False, **kwargs):
+    def load(model_name_or_path, local_only=False, device=None, **kwargs):
         if isinstance(model_name_or_path, str) and os.path.exists(model_name_or_path):
             path = model_name_or_path
         elif isinstance(model_name_or_path, str) and not os.path.exists(
             model_name_or_path
         ):
             path = download_model(model_name_or_path, local_only=local_only, **kwargs)
         else:
@@ -50,49 +51,25 @@
         base_model = AutoModel.from_config(config, trust_remote_code=True, **kwargs)
         model_lib = importlib.import_module(metadata["library_name"].lower()).model
         model_cls = getattr(model_lib, metadata["model_cls"])
 
         with open(f"{path}/tokenizer.pkl", "rb") as f:
             tokenizer = dill.load(f)
 
-        model = model_cls(config, base_model, tokenizer, **kwargs)
-
+        model = model_cls(base_model, tokenizer, label2id=config.label2id, **kwargs)
         with open(f"{path}/pytorch_model.bin", "rb") as f:
             model.load_state_dict(
-                torch.load(f, map_location=kwargs.get("device", "cpu")), strict=False
+                torch.load(f, map_location=kwargs.get("device", "cpu")), strict=True
             )
-            model.metadata.update(metadata)
+        if device is None:
+            model.to(autocuda.auto_cuda())
+        else:
+            model.to(device)
         return model
 
-    @staticmethod
-    def save(self, path, overwrite=False, **kwargs):
-        import dill
-
-        if os.path.exists(path) and not overwrite:
-            raise FileExistsError(
-                f"The path {path} already exists, please set overwrite=True to overwrite it."
-            )
-
-        if not os.path.exists(path):
-            os.makedirs(path)
-
-        device = self.model.device
-
-        self.model.to("cpu")
-        with open(f"{path}/model.pkl", "wb") as f:
-            dill.dump(self, f)
-        with open(f"{path}/tokenizer.pkl", "wb") as f:
-            dill.dump(self.tokenizer, f)
-        self.config.metadata = self.metadata
-        self.config.save_pretrained(path)
-
-        self.model.to(device)
-
-        fprint(f"The model and tokenizer has been saved to {path}.")
-
     def available_models(
         self, model_name_or_path=None, local_only=False, repo="", **kwargs
     ):
         models_info = query_models_info(
             model_name_or_path, local_only=local_only, repo=repo, **kwargs
         )
         return models_info
```

## omnigenome/utility/pipeline_hub/pipeline.py

```diff
@@ -6,23 +6,19 @@
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 import json
 import os
 
 import autocuda
-import torch
 from transformers import AutoConfig, AutoTokenizer
 
 from ..hub_utils import download_pipeline
 from ..model_hub.model_hub import ModelHub
-
-from ...src.abc.abstract_dataset import OmniGenomeDataset
 from ...src.abc.abstract_model import OmniGenomeModel
-from ...src.metric.classification_metric import ClassificationMetric
 from ...src.misc.utils import env_meta_info, fprint
 from ...src.trainer.trainer import Trainer
 
 
 class Pipeline:
     model: OmniGenomeModel = None
     tokenizer = None
@@ -66,32 +62,20 @@
     def to(self, device):
         self.model.to(device)
         self.device = device
 
     def init_pipeline(self, *, model_name_or_path, tokenizer=None, **kwargs):
         trust_remote_code = kwargs.get("trust_remote_code", True)
         try:  # for the models saved by OmniGenome and served by the model hub
-            self.model, self.tokenizer = ModelHub.load(model_name_or_path, **kwargs)
-            return self
+            self.model = ModelHub.load(model_name_or_path, **kwargs)
+            self.tokenizer = self.model.tokenizer
+            self.metadata.update(self.model.metadata)
         except Exception as e:
             print(f"Fail to load the model from the model hub, the error is: {e}")
 
-        try:  # for the models saved by the OmniGenome and located in the local file system
-            self.model, self.tokenizer = ModelHub.load_model_and_tokenizer(
-                model_name_or_path, **kwargs
-            )
-            return self
-        except Exception as e:
-            print(
-                f"Fail to load the model from the local file system, the error is: {e}"
-            )
-
-        if isinstance(
-            model_name_or_path, str
-        ):  # for the models from the Hugging Face model hub
             config = AutoConfig.from_pretrained(
                 model_name_or_path, trust_remote_code=trust_remote_code
             )
             if tokenizer is None:
                 tokenizer = AutoTokenizer.from_pretrained(
                     model_name_or_path, trust_remote_code=trust_remote_code
                 )
@@ -99,16 +83,15 @@
                 model_name_or_path,
                 config=config,
                 tokenizer=tokenizer,
                 trust_remote_code=trust_remote_code,
                 **kwargs,
             )
             self.tokenizer = self.model.tokenizer
-
-        self.metadata = self.model.metadata
+            self.metadata.update(self.model.metadata)
         fprint(f"The pipeline has been initialized from {model_name_or_path}.")
         return self
 
     def train(self, datasets: dict = None, trainer=None, **kwargs):
         if trainer is not None:
             assert isinstance(trainer, Trainer)
             self.trainer = trainer
@@ -139,17 +122,19 @@
         with open(f"{path}/tokenizer.pkl", "rb") as f:
             tokenizer = dill.load(f)
         with open(f"{path}/trainer.pkl", "rb") as f:
             trainer = dill.load(f)
         model = ModelHub.load(path, local_only=local_only, **kwargs)
         model.metadata.update(metadata)
         pipeline = Pipeline(
-            name=pipeline_name_or_path
-            if kwargs.get("name") is None
-            else kwargs.get("name"),
+            name=(
+                pipeline_name_or_path
+                if kwargs.get("name") is None
+                else kwargs.get("name")
+            ),
             model_name_or_path=model,
             tokenizer=tokenizer,
             datasets=datasets,
             trainer=trainer,
             **kwargs,
         )
         return pipeline
```

## Comparing `omnigenome/_src/config/config.py` & `omnigenome/bench/auto_bench/auto_bench_config.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,25 +1,23 @@
 # -*- coding: utf-8 -*-
-# file: checkpoint_template.py
-# time: 02/11/2022 15:44
+# file: auto_bench_config.py
+# time: 14:58 29/04/2024
 # author: YANG, HENG <hy345@exeter.ac.uk> (杨恒)
 # github: https://github.com/yangheng95
-# GScholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
-# ResearchGate: https://www.researchgate.net/profile/Heng-Yang-17/research
-# Copyright (C) 2022. All Rights Reserved.
-
+# huggingface: https://huggingface.co/yangheng
+# google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
+# Copyright (C) 2019-2024. All Rights Reserved.
 from argparse import Namespace
 
-from ..config.config_check import config_check
-from ..misc.utils import fprint
-
 from transformers import PretrainedConfig
 
+from .config_check import config_check
+
 
-class Config(PretrainedConfig):
+class AutoBenchConfig(PretrainedConfig):
     def __init__(self, args=None, **kwargs):
         """
         The Config is a subclass of argparse.Namespace and based on a parameter dict.
         It also counts the call-frequency of each parameter.
 
         :param args: A parameter dict.
         :param kwargs: Same params as Namespace.
@@ -200,18 +198,7 @@
     def __ne__(self, other):
         """
         Check if the parameter dict is not equal to another object.
         :param other: The other object to compare with the parameter dict.
         :return: True if the parameter dict is not equal to the other object, False otherwise.
         """
         return self.args != other
-
-
-if __name__ == "__main__":  # test
-    config = Config({"a": 1, "b": 2})
-    config.a = 2
-    config.b = 3
-    config.c = 4
-    fprint(config.a)
-    fprint(config.b)
-    fprint(config.c)
-    fprint(config.args_call_count)
```

## Comparing `omnigenome/_src/config/config_check.py` & `omnigenome/bench/auto_bench/config_check.py`

 * *Files identical despite different names*

## Comparing `OmniGenome-0.0.1a0.dist-info/METADATA` & `OmniGenome-0.0.2a0.dist-info/METADATA`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: OmniGenome
-Version: 0.0.1a0
+Version: 0.0.2a0
 Summary: OmniGenome: A comprehensive toolkit for genome analysis.
 Home-page: https://github.com/yangheng95/OmniGenome
 Author: Yang, Heng
 Author-email: hy345@exeter.ac.uk
 License: MIT
 Requires-Python: >=3.9
 Description-Content-Type: text/markdown
@@ -15,12 +15,13 @@
 Requires-Dist: termcolor
 Requires-Dist: gitpython
 Requires-Dist: transformers (>=4.37.0)
 Requires-Dist: torch (>=1.0.0)
 Requires-Dist: sentencepiece
 Requires-Dist: protobuf (<4.0.0)
 Requires-Dist: pandas
+Requires-Dist: viennarna
 Provides-Extra: dev
 Requires-Dist: dill ; extra == 'dev'
 Requires-Dist: pytest ; extra == 'dev'
 
 # OmniGenome: A Comprehensive Toolkit of Genomic Modeling and Benchmarking
```

