# Comparing `tmp/sonar_tools-2.8.2-py3-none-any.whl.zip` & `tmp/sonar_tools-2.9-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,81 +1,81 @@
-Zip file size: 208358 bytes, number of entries: 79
--rw-r--r--  2.0 unx      847 b- defN 24-Feb-24 10:27 sonar/__init__.py
--rw-r--r--  2.0 unx     3391 b- defN 24-Feb-24 11:08 sonar/aggregations.py
--rw-r--r--  2.0 unx    20715 b- defN 24-Feb-24 10:27 sonar/applications.py
--rw-r--r--  2.0 unx     7190 b- defN 24-Feb-24 10:27 sonar/components.py
--rw-r--r--  2.0 unx     3015 b- defN 24-Feb-24 10:27 sonar/custom_measures.py
--rw-r--r--  2.0 unx    11148 b- defN 24-Feb-24 10:27 sonar/devops.py
--rw-r--r--  2.0 unx     1544 b- defN 24-Feb-24 10:27 sonar/exceptions.py
--rw-r--r--  2.0 unx    12636 b- defN 24-Feb-24 11:24 sonar/groups.py
--rw-r--r--  2.0 unx     3856 b- defN 24-Feb-24 10:27 sonar/languages.py
--rw-r--r--  2.0 unx     9711 b- defN 24-Feb-24 10:27 sonar/measures.py
--rw-r--r--  2.0 unx     6493 b- defN 24-Feb-24 10:27 sonar/metrics.py
--rw-r--r--  2.0 unx     2229 b- defN 24-Feb-24 10:27 sonar/options.py
--rw-r--r--  2.0 unx    34203 b- defN 24-Feb-24 11:24 sonar/platform.py
--rw-r--r--  2.0 unx    30565 b- defN 24-Feb-24 10:27 sonar/portfolios.py
--rw-r--r--  2.0 unx    18122 b- defN 24-Feb-24 10:27 sonar/qualitygates.py
--rw-r--r--  2.0 unx    29776 b- defN 24-Feb-24 10:27 sonar/qualityprofiles.py
--rw-r--r--  2.0 unx    11184 b- defN 24-Feb-24 10:27 sonar/rules.py
--rw-r--r--  2.0 unx    16616 b- defN 24-Feb-24 10:27 sonar/settings.py
--rw-r--r--  2.0 unx    19366 b- defN 24-Feb-24 10:27 sonar/sif.py
--rw-r--r--  2.0 unx     6123 b- defN 24-Feb-24 10:27 sonar/sqobject.py
--rw-r--r--  2.0 unx     8685 b- defN 24-Feb-24 10:27 sonar/syncer.py
--rw-r--r--  2.0 unx    21302 b- defN 24-Feb-24 10:27 sonar/tasks.py
--rw-r--r--  2.0 unx     3580 b- defN 24-Feb-24 10:27 sonar/tokens.py
--rw-r--r--  2.0 unx    17735 b- defN 24-Feb-24 10:27 sonar/users.py
--rw-r--r--  2.0 unx    15966 b- defN 24-Feb-24 10:27 sonar/utilities.py
--rw-r--r--  2.0 unx      874 b- defN 24-Feb-24 10:27 sonar/version.py
--rw-r--r--  2.0 unx     5470 b- defN 24-Feb-24 10:27 sonar/webhooks.py
--rw-r--r--  2.0 unx     1061 b- defN 24-Feb-24 10:27 sonar/audit/__init__.py
--rw-r--r--  2.0 unx     2935 b- defN 24-Feb-24 10:27 sonar/audit/config.py
--rw-r--r--  2.0 unx     2594 b- defN 24-Feb-24 10:27 sonar/audit/problem.py
--rw-r--r--  2.0 unx    20474 b- defN 24-Feb-24 10:27 sonar/audit/rules.json
--rw-r--r--  2.0 unx     6250 b- defN 24-Feb-24 10:27 sonar/audit/rules.py
--rw-r--r--  2.0 unx     1167 b- defN 24-Feb-24 10:27 sonar/audit/severities.py
--rw-r--r--  2.0 unx    13464 b- defN 24-Feb-24 10:27 sonar/audit/sonar-audit.properties
--rw-r--r--  2.0 unx     1218 b- defN 24-Feb-24 10:27 sonar/audit/types.py
--rw-r--r--  2.0 unx      847 b- defN 24-Feb-24 10:27 sonar/dce/__init__.py
--rw-r--r--  2.0 unx     9962 b- defN 24-Feb-24 10:27 sonar/dce/app_nodes.py
--rw-r--r--  2.0 unx     1123 b- defN 24-Feb-24 10:27 sonar/dce/nodes.py
--rw-r--r--  2.0 unx     4198 b- defN 24-Feb-24 10:27 sonar/dce/search_nodes.py
--rw-r--r--  2.0 unx      847 b- defN 24-Feb-24 10:27 sonar/findings/__init__.py
--rw-r--r--  2.0 unx     7103 b- defN 24-Feb-24 10:27 sonar/findings/changelog.py
--rw-r--r--  2.0 unx    12898 b- defN 24-Feb-24 10:27 sonar/findings/findings.py
--rw-r--r--  2.0 unx    16122 b- defN 24-Feb-24 10:27 sonar/findings/hotspots.py
--rw-r--r--  2.0 unx    32150 b- defN 24-Feb-24 10:27 sonar/findings/issues.py
--rw-r--r--  2.0 unx      847 b- defN 24-Feb-24 10:27 sonar/permissions/__init__.py
--rw-r--r--  2.0 unx     1872 b- defN 24-Feb-24 10:27 sonar/permissions/aggregation_permissions.py
--rw-r--r--  2.0 unx     1048 b- defN 24-Feb-24 10:27 sonar/permissions/application_permissions.py
--rw-r--r--  2.0 unx     3415 b- defN 24-Feb-24 10:27 sonar/permissions/global_permissions.py
--rw-r--r--  2.0 unx     9823 b- defN 24-Feb-24 10:27 sonar/permissions/permission_templates.py
--rw-r--r--  2.0 unx    11688 b- defN 24-Feb-24 10:27 sonar/permissions/permissions.py
--rw-r--r--  2.0 unx     1045 b- defN 24-Feb-24 10:27 sonar/permissions/portfolio_permissions.py
--rw-r--r--  2.0 unx     8358 b- defN 24-Feb-24 10:27 sonar/permissions/project_permissions.py
--rw-r--r--  2.0 unx     4511 b- defN 24-Feb-24 10:27 sonar/permissions/quality_permissions.py
--rw-r--r--  2.0 unx     2246 b- defN 24-Feb-24 10:27 sonar/permissions/qualitygate_permissions.py
--rw-r--r--  2.0 unx     2407 b- defN 24-Feb-24 10:27 sonar/permissions/qualityprofile_permissions.py
--rw-r--r--  2.0 unx     3014 b- defN 24-Feb-24 10:27 sonar/permissions/template_permissions.py
--rw-r--r--  2.0 unx      847 b- defN 24-Feb-24 10:27 sonar/projects/__init__.py
--rw-r--r--  2.0 unx    17320 b- defN 24-Feb-24 10:27 sonar/projects/branches.py
--rw-r--r--  2.0 unx    57922 b- defN 24-Feb-24 10:27 sonar/projects/projects.py
--rw-r--r--  2.0 unx     4600 b- defN 24-Feb-24 10:27 sonar/projects/pull_requests.py
--rwxr-xr-x  2.0 unx     2276 b- defN 24-Feb-24 11:08 sonar_tools-2.8.2.data/scripts/sonar-tools
--rw-r--r--  2.0 unx      847 b- defN 24-Feb-24 10:27 tools/__init__.py
--rw-r--r--  2.0 unx     6870 b- defN 24-Feb-24 10:27 tools/audit.py
--rw-r--r--  2.0 unx     8129 b- defN 24-Feb-24 11:24 tools/config.py
--rw-r--r--  2.0 unx     2604 b- defN 24-Feb-24 10:27 tools/cust_measures.py
--rw-r--r--  2.0 unx    15114 b- defN 24-Feb-24 10:27 tools/findings_export.py
--rw-r--r--  2.0 unx     9385 b- defN 24-Feb-24 10:27 tools/findings_sync.py
--rw-r--r--  2.0 unx     9142 b- defN 24-Feb-24 10:27 tools/housekeeper.py
--rw-r--r--  2.0 unx     5996 b- defN 24-Feb-24 10:27 tools/loc.py
--rw-r--r--  2.0 unx     9057 b- defN 24-Feb-24 10:27 tools/measures_export.py
--rw-r--r--  2.0 unx     2575 b- defN 24-Feb-24 10:27 tools/projects_export.py
--rw-r--r--  2.0 unx     3855 b- defN 24-Feb-24 10:27 tools/projects_import.py
--rw-r--r--  2.0 unx     7008 b- defN 24-Feb-24 10:27 tools/support.py
--rw-r--r--  2.0 unx     7817 b- defN 24-Feb-24 14:39 sonar_tools-2.8.2.dist-info/LICENSE
--rw-r--r--  2.0 unx    20121 b- defN 24-Feb-24 14:39 sonar_tools-2.8.2.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Feb-24 14:39 sonar_tools-2.8.2.dist-info/WHEEL
--rw-r--r--  2.0 unx      582 b- defN 24-Feb-24 14:39 sonar_tools-2.8.2.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       12 b- defN 24-Feb-24 14:39 sonar_tools-2.8.2.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     6421 b- defN 24-Feb-24 14:39 sonar_tools-2.8.2.dist-info/RECORD
-79 files, 705621 bytes uncompressed, 198410 bytes compressed:  71.9%
+Zip file size: 209027 bytes, number of entries: 79
+-rw-r--r--  2.0 unx      828 b- defN 24-Mar-20 14:07 sonar/__init__.py
+-rw-r--r--  2.0 unx     4052 b- defN 24-Mar-20 14:36 sonar/aggregations.py
+-rw-r--r--  2.0 unx    20257 b- defN 24-Mar-24 18:19 sonar/applications.py
+-rw-r--r--  2.0 unx     7001 b- defN 24-Mar-20 14:07 sonar/components.py
+-rw-r--r--  2.0 unx     2921 b- defN 24-Mar-20 14:07 sonar/custom_measures.py
+-rw-r--r--  2.0 unx    10862 b- defN 24-Mar-20 14:07 sonar/devops.py
+-rw-r--r--  2.0 unx     1493 b- defN 24-Mar-20 14:07 sonar/exceptions.py
+-rw-r--r--  2.0 unx    12296 b- defN 24-Mar-22 08:49 sonar/groups.py
+-rw-r--r--  2.0 unx     3750 b- defN 24-Mar-20 14:07 sonar/languages.py
+-rw-r--r--  2.0 unx     9455 b- defN 24-Mar-20 14:07 sonar/measures.py
+-rw-r--r--  2.0 unx     6281 b- defN 24-Mar-20 14:07 sonar/metrics.py
+-rw-r--r--  2.0 unx     2207 b- defN 24-Mar-20 14:36 sonar/options.py
+-rw-r--r--  2.0 unx    33955 b- defN 24-Mar-24 18:19 sonar/platform.py
+-rw-r--r--  2.0 unx    29924 b- defN 24-Mar-25 12:59 sonar/portfolios.py
+-rw-r--r--  2.0 unx    17660 b- defN 24-Mar-20 14:07 sonar/qualitygates.py
+-rw-r--r--  2.0 unx    29108 b- defN 24-Mar-22 08:49 sonar/qualityprofiles.py
+-rw-r--r--  2.0 unx    11288 b- defN 24-Mar-21 13:49 sonar/rules.py
+-rw-r--r--  2.0 unx    16190 b- defN 24-Mar-20 14:07 sonar/settings.py
+-rw-r--r--  2.0 unx    19125 b- defN 24-Mar-24 18:19 sonar/sif.py
+-rw-r--r--  2.0 unx     6010 b- defN 24-Mar-20 14:36 sonar/sqobject.py
+-rw-r--r--  2.0 unx     8443 b- defN 24-Mar-20 14:07 sonar/syncer.py
+-rw-r--r--  2.0 unx    21269 b- defN 24-Mar-24 18:19 sonar/tasks.py
+-rw-r--r--  2.0 unx     3486 b- defN 24-Mar-20 14:07 sonar/tokens.py
+-rw-r--r--  2.0 unx    17339 b- defN 24-Mar-22 08:49 sonar/users.py
+-rw-r--r--  2.0 unx    16352 b- defN 24-Mar-22 08:31 sonar/utilities.py
+-rw-r--r--  2.0 unx      896 b- defN 24-Mar-20 14:36 sonar/version.py
+-rw-r--r--  2.0 unx     5315 b- defN 24-Mar-20 14:07 sonar/webhooks.py
+-rw-r--r--  2.0 unx     1034 b- defN 24-Mar-20 14:07 sonar/audit/__init__.py
+-rw-r--r--  2.0 unx     2851 b- defN 24-Mar-20 14:07 sonar/audit/config.py
+-rw-r--r--  2.0 unx     2526 b- defN 24-Mar-20 14:07 sonar/audit/problem.py
+-rw-r--r--  2.0 unx    20583 b- defN 24-Mar-24 18:19 sonar/audit/rules.json
+-rw-r--r--  2.0 unx     6153 b- defN 24-Mar-24 18:19 sonar/audit/rules.py
+-rw-r--r--  2.0 unx     1130 b- defN 24-Mar-20 14:07 sonar/audit/severities.py
+-rw-r--r--  2.0 unx    13229 b- defN 24-Mar-20 14:36 sonar/audit/sonar-audit.properties
+-rw-r--r--  2.0 unx     1179 b- defN 24-Mar-20 14:07 sonar/audit/types.py
+-rw-r--r--  2.0 unx      828 b- defN 24-Mar-20 14:07 sonar/dce/__init__.py
+-rw-r--r--  2.0 unx    12510 b- defN 24-Mar-22 08:31 sonar/dce/app_nodes.py
+-rw-r--r--  2.0 unx     1086 b- defN 24-Mar-20 14:07 sonar/dce/nodes.py
+-rw-r--r--  2.0 unx     4586 b- defN 24-Mar-22 08:31 sonar/dce/search_nodes.py
+-rw-r--r--  2.0 unx      828 b- defN 24-Mar-20 14:07 sonar/findings/__init__.py
+-rw-r--r--  2.0 unx     6900 b- defN 24-Mar-20 14:07 sonar/findings/changelog.py
+-rw-r--r--  2.0 unx    12542 b- defN 24-Mar-20 14:07 sonar/findings/findings.py
+-rw-r--r--  2.0 unx    15687 b- defN 24-Mar-20 14:07 sonar/findings/hotspots.py
+-rw-r--r--  2.0 unx    31311 b- defN 24-Mar-20 14:07 sonar/findings/issues.py
+-rw-r--r--  2.0 unx      828 b- defN 24-Mar-20 14:07 sonar/permissions/__init__.py
+-rw-r--r--  2.0 unx     1820 b- defN 24-Mar-20 14:07 sonar/permissions/aggregation_permissions.py
+-rw-r--r--  2.0 unx     1021 b- defN 24-Mar-20 14:07 sonar/permissions/application_permissions.py
+-rw-r--r--  2.0 unx     3341 b- defN 24-Mar-20 14:07 sonar/permissions/global_permissions.py
+-rw-r--r--  2.0 unx     9576 b- defN 24-Mar-20 14:07 sonar/permissions/permission_templates.py
+-rw-r--r--  2.0 unx    11397 b- defN 24-Mar-24 18:19 sonar/permissions/permissions.py
+-rw-r--r--  2.0 unx     1018 b- defN 24-Mar-20 14:07 sonar/permissions/portfolio_permissions.py
+-rw-r--r--  2.0 unx     8252 b- defN 24-Mar-22 08:49 sonar/permissions/project_permissions.py
+-rw-r--r--  2.0 unx     4406 b- defN 24-Mar-20 14:07 sonar/permissions/quality_permissions.py
+-rw-r--r--  2.0 unx     2196 b- defN 24-Mar-20 14:07 sonar/permissions/qualitygate_permissions.py
+-rw-r--r--  2.0 unx     2349 b- defN 24-Mar-20 14:07 sonar/permissions/qualityprofile_permissions.py
+-rw-r--r--  2.0 unx     2955 b- defN 24-Mar-20 14:07 sonar/permissions/template_permissions.py
+-rw-r--r--  2.0 unx      828 b- defN 24-Mar-20 14:07 sonar/projects/__init__.py
+-rw-r--r--  2.0 unx    17542 b- defN 24-Mar-22 08:49 sonar/projects/branches.py
+-rw-r--r--  2.0 unx    56961 b- defN 24-Mar-25 12:59 sonar/projects/projects.py
+-rw-r--r--  2.0 unx     4484 b- defN 24-Mar-22 08:49 sonar/projects/pull_requests.py
+-rwxr-xr-x  2.0 unx     2223 b- defN 24-Mar-25 13:52 sonar_tools-2.9.data/scripts/sonar-tools
+-rw-r--r--  2.0 unx      828 b- defN 24-Mar-20 14:07 tools/__init__.py
+-rw-r--r--  2.0 unx     6700 b- defN 24-Mar-20 14:07 tools/audit.py
+-rw-r--r--  2.0 unx     7932 b- defN 24-Mar-20 14:07 tools/config.py
+-rw-r--r--  2.0 unx     2539 b- defN 24-Mar-20 14:07 tools/cust_measures.py
+-rw-r--r--  2.0 unx    14736 b- defN 24-Mar-25 12:59 tools/findings_export.py
+-rw-r--r--  2.0 unx     9195 b- defN 24-Mar-20 14:07 tools/findings_sync.py
+-rw-r--r--  2.0 unx     8908 b- defN 24-Mar-20 14:07 tools/housekeeper.py
+-rw-r--r--  2.0 unx     5818 b- defN 24-Mar-20 14:07 tools/loc.py
+-rw-r--r--  2.0 unx     8803 b- defN 24-Mar-25 12:59 tools/measures_export.py
+-rw-r--r--  2.0 unx     2508 b- defN 24-Mar-20 14:07 tools/projects_export.py
+-rw-r--r--  2.0 unx     3750 b- defN 24-Mar-20 14:07 tools/projects_import.py
+-rw-r--r--  2.0 unx     6829 b- defN 24-Mar-20 14:07 tools/support.py
+-rw-r--r--  2.0 unx     7652 b- defN 24-Mar-25 13:52 sonar_tools-2.9.dist-info/LICENSE
+-rw-r--r--  2.0 unx    20176 b- defN 24-Mar-25 13:52 sonar_tools-2.9.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Mar-25 13:52 sonar_tools-2.9.dist-info/WHEEL
+-rw-r--r--  2.0 unx      582 b- defN 24-Mar-25 13:52 sonar_tools-2.9.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       12 b- defN 24-Mar-25 13:52 sonar_tools-2.9.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     6408 b- defN 24-Mar-25 13:52 sonar_tools-2.9.dist-info/RECORD
+79 files, 697391 bytes uncompressed, 199107 bytes compressed:  71.4%
```

## zipnote {}

```diff
@@ -174,15 +174,15 @@
 
 Filename: sonar/projects/projects.py
 Comment: 
 
 Filename: sonar/projects/pull_requests.py
 Comment: 
 
-Filename: sonar_tools-2.8.2.data/scripts/sonar-tools
+Filename: sonar_tools-2.9.data/scripts/sonar-tools
 Comment: 
 
 Filename: tools/__init__.py
 Comment: 
 
 Filename: tools/audit.py
 Comment: 
@@ -213,26 +213,26 @@
 
 Filename: tools/projects_import.py
 Comment: 
 
 Filename: tools/support.py
 Comment: 
 
-Filename: sonar_tools-2.8.2.dist-info/LICENSE
+Filename: sonar_tools-2.9.dist-info/LICENSE
 Comment: 
 
-Filename: sonar_tools-2.8.2.dist-info/METADATA
+Filename: sonar_tools-2.9.dist-info/METADATA
 Comment: 
 
-Filename: sonar_tools-2.8.2.dist-info/WHEEL
+Filename: sonar_tools-2.9.dist-info/WHEEL
 Comment: 
 
-Filename: sonar_tools-2.8.2.dist-info/entry_points.txt
+Filename: sonar_tools-2.9.dist-info/entry_points.txt
 Comment: 
 
-Filename: sonar_tools-2.8.2.dist-info/top_level.txt
+Filename: sonar_tools-2.9.dist-info/top_level.txt
 Comment: 
 
-Filename: sonar_tools-2.8.2.dist-info/RECORD
+Filename: sonar_tools-2.9.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## sonar/__init__.py

 * *Ordering differences only*

```diff
@@ -1,19 +1,19 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
```

## sonar/aggregations.py

```diff
@@ -1,95 +1,109 @@
-#
-# sonar-tools
-# Copyright (C) 2022-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-"""
-
-    Parent class of applications and portfolios
-
-"""
-import json
-import sonar.components as comp
-from sonar import utilities, measures
-from sonar.audit import rules, problem
-
-
-class Aggregation(comp.Component):
-    def __init__(self, key, endpoint, data=None):
-        self._nbr_projects = None
-        self._permissions = None
-        super().__init__(key, endpoint)
-
-    def reload(self, data):
-        """Reloads an Aggregatin (Application or Portfolio) from the result of a search or get
-
-        :return: self
-        :rtype: Application or Portfolio
-        """
-        super().reload(data)
-        for d in ("description", "desc"):
-            if d in data:
-                self._description = self._json[d]
-
-    def nbr_projects(self):
-        if self._nbr_projects is None:
-            self._nbr_projects = 0
-            data = json.loads(
-                self.get(
-                    "measures/component",
-                    params={"component": self.key, "metricKeys": "projects,ncloc"},
-                ).text
-            )[
-                "component"
-            ]["measures"]
-            for m in data:
-                if m["metric"] == "projects":
-                    self._nbr_projects = int(m["value"])
-                elif m["metric"] == "ncloc":
-                    self.ncloc = int(m["value"])
-        return self._nbr_projects
-
-    def get_measures(self, metrics_list):
-        m = measures.get(self, metrics_list)
-        if "ncloc" in m:
-            self.ncloc = 0 if not m["ncloc"].value else int(m["ncloc"].value)
-        return m
-
-    def _audit_aggregation_cardinality(self, sizes, broken_rule):
-        problems = []
-        n = self.nbr_projects()
-        if n in sizes:
-            rule = rules.get_rule(broken_rule)
-            msg = rule.msg.format(str(self))
-            problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
-        else:
-            utilities.logger.debug("%s has %d projects", str(self), n)
-        return problems
-
-    def _audit_empty_aggregation(self, broken_rule):
-        return self._audit_aggregation_cardinality((0, None), broken_rule)
-
-    def _audit_singleton_aggregation(self, broken_rule):
-        return self._audit_aggregation_cardinality((1, 1), broken_rule)
-
-
-def count(api, endpoint, params=None):
-    if params is None:
-        params = {}
-    params["ps"] = 1
-    data = json.loads(endpoint.get(api, params=params).text)
-    return data["paging"]["total"]
+#
+# sonar-tools
+# Copyright (C) 2022-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+"""
+
+    Parent module of applications and portfolios
+
+"""
+import json
+import sonar.components as comp
+from sonar import utilities, measures
+from sonar.audit import rules, problem
+
+
+class Aggregation(comp.Component):
+    """Parent class of applications and portfolios"""
+
+    def __init__(self, key: str, endpoint: object, data: dict[str, any] = None) -> None:
+        self._nbr_projects = None
+        self._permissions = None
+        super().__init__(key, endpoint)
+
+    def reload(self, data: dict[str, any]) -> None:
+        """Reloads an Aggregation (Application or Portfolio) from the result of a search or get
+
+        :return: self
+        :rtype: Application or Portfolio
+        """
+        super().reload(data)
+        for d in ("description", "desc"):
+            if d in data:
+                self._description = self._json[d]
+
+    def nbr_projects(self) -> int:
+        """Returns the number of projects of an Aggregation (Application or Portfolio)
+        :return: The number of projects
+        :rtype: int
+        """
+        if self._nbr_projects is None:
+            self._nbr_projects = 0
+            data = json.loads(
+                self.get(
+                    "measures/component",
+                    params={"component": self.key, "metricKeys": "projects,ncloc"},
+                ).text
+            )[
+                "component"
+            ]["measures"]
+            for m in data:
+                if m["metric"] == "projects":
+                    self._nbr_projects = int(m["value"])
+                elif m["metric"] == "ncloc":
+                    self.ncloc = int(m["value"])
+        return self._nbr_projects
+
+    def get_measures(self, metrics_list: list[str]) -> dict[str, any]:
+        """Returns measures of an Aggregation (Application or Portfolio)
+        :return: dict of metric: value
+        :rtype: dict
+        """
+        m = measures.get(self, metrics_list)
+        if "ncloc" in m:
+            self.ncloc = 0 if not m["ncloc"].value else int(m["ncloc"].value)
+        return m
+
+    def _audit_aggregation_cardinality(self, sizes: tuple[int], broken_rule: object) -> list[problem.Problem]:
+        problems = []
+        n = self.nbr_projects()
+        if n in sizes:
+            rule = rules.get_rule(broken_rule)
+            msg = rule.msg.format(str(self))
+            problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
+        else:
+            utilities.logger.debug("%s has %d projects", str(self), n)
+        return problems
+
+    def _audit_empty_aggregation(self, broken_rule: object) -> list[problem.Problem]:
+        return self._audit_aggregation_cardinality((0, None), broken_rule)
+
+    def _audit_singleton_aggregation(self, broken_rule: object) -> list[problem.Problem]:
+        return self._audit_aggregation_cardinality((1, 1), broken_rule)
+
+
+def count(api: str, endpoint: object, params: dict[str, str] = None) -> int:
+    """Returns number of aggregations of a given type (Application OR Portfolio)
+    :return: number of Apps or Portfolios
+    :rtype: int
+    """
+    if params is None:
+        params = {}
+    params["ps"] = 1
+    data = json.loads(endpoint.get(api, params=params).text)
+    return data["paging"]["total"]
```

## sonar/applications.py

```diff
@@ -1,501 +1,501 @@
-#
-# sonar-tools
-# Copyright (C) 2022-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-
-import json
-from http import HTTPStatus
-from threading import Lock
-from requests.exceptions import HTTPError
-from sonar import components, exceptions, settings
-from sonar.projects import projects, branches
-from sonar.permissions import permissions, application_permissions
-import sonar.sqobject as sq
-import sonar.aggregations as aggr
-import sonar.utilities as util
-from sonar.audit import rules
-
-_OBJECTS = {}
-_MAP = {}
-_CLASS_LOCK = Lock()
-
-APIS = {
-    "search": "api/components/search_projects",
-    "get": "api/applications/show",
-    "create": "api/applications/create",
-    "delete": "api/applications/delete",
-    "create_branch": "api/applications/create_branch",
-    "update_branch": "api/applications/update_branch",
-}
-
-_IMPORTABLE_PROPERTIES = ("key", "name", "description", "visibility", "branches", "permissions", "tags")
-_NOT_SUPPORTED = "Applications not supported in community edition"
-
-
-class Application(aggr.Aggregation):
-    """
-    Abstraction of the SonarQube "application" concept
-    """
-
-    @classmethod
-    def get_object(cls, endpoint, key):
-        """Gets an Application object from SonarQube
-
-        :param Platform endpoint: Reference to the SonarQube platform
-        :param str key: Application key, must not already exist on SonarQube
-        :raises UnsupportedOperation: If on a Community Edition
-        :raises ObjectNotFound: If Application key not found in SonarQube
-        :return: The found Application object
-        :rtype: Application
-        """
-        if endpoint.edition() == "community":
-            raise exceptions.UnsupportedOperation(_NOT_SUPPORTED)
-        if key in _OBJECTS:
-            return _OBJECTS[key]
-        try:
-            data = json.loads(endpoint.get(APIS["get"], params={"application": key}).text)
-        except HTTPError as e:
-            if e.response.status_code == HTTPStatus.NOT_FOUND:
-                raise exceptions.ObjectNotFound(key, f"Application '{key}' not found")
-        return cls.load(endpoint, data)
-
-    @classmethod
-    def load(cls, endpoint, data):
-        """Loads an Application object with data retrieved from SonarQube
-
-        :param Platform endpoint: Reference to the SonarQube platform
-        :param str key: Application key, must not already exist on SonarQube
-        :param dict data: Data coming from api/components/search_projects or api/applications/show
-        :raises UnsupportedOperation: If on a Community Edition
-        :raises ObjectNotFound: If Application key not found in SonarQube
-        :return: The found Application object
-        :rtype: Application
-        """
-        if endpoint.edition() == "community":
-            raise exceptions.UnsupportedOperation(_NOT_SUPPORTED)
-        o = _OBJECTS.get(data["key"], cls(endpoint, data["key"], data["name"]))
-        o.reload(data)
-        return o
-
-    @classmethod
-    def create(cls, endpoint, key, name):
-        """Creates an Application object in SonarQube
-
-        :param Platform endpoint: Reference to the SonarQube platform
-        :param str key: Application key, must not already exist on SonarQube
-        :param str name: Application name
-        :raises UnsupportedOperation: If on a Community Edition
-        :raises ObjectAlreadyExists: If key already exist for another Application
-        :return: The created Application object
-        :rtype: Application
-        """
-        if endpoint.edition() == "community":
-            raise exceptions.UnsupportedOperation(_NOT_SUPPORTED)
-        try:
-            endpoint.post(APIS["create"], params={"key": key, "name": name})
-        except HTTPError as e:
-            if e.response.status_code == HTTPStatus.BAD_REQUEST:
-                raise exceptions.ObjectAlreadyExists(key, e.response.text)
-        return Application(endpoint, key, name)
-
-    def __init__(self, endpoint, key, name):
-        """Don't use this directly, go through the class methods to create Objects"""
-        super().__init__(key, endpoint)
-        self._branches = None
-        self._projects = None
-        self._description = None
-        self.name = name
-        util.logger.debug("Created object %s", str(self))
-        _OBJECTS[self.key] = self
-        _MAP[self.name] = self.key
-
-    def refresh(self):
-        """Refreshes the by re-reading SonarQube
-
-        :raises ObjectNotFound: If the Application does not exists anymore
-        :return: self:
-        :rtype: Appplication
-        """
-        try:
-            return self.reload(json.loads(self.get(APIS["get"], params={"application": self.key}).text)["application"])
-        except HTTPError as e:
-            if e.response.status_code == HTTPStatus.NOT_FOUND:
-                _OBJECTS.pop(self.key, None)
-                raise exceptions.ObjectNotFound(self.key, f"{str(self)} not found")
-            raise
-
-    def __str__(self):
-        return f"application key '{self.key}'"
-
-    def permissions(self):
-        """
-        :return: The application permissions
-        :rtype: ApplicationPermissions
-        """
-        if self._permissions is None:
-            self._permissions = application_permissions.ApplicationPermissions(self)
-        return self._permissions
-
-    def projects(self):
-        """
-        :return: The project branches included in the application
-        :rtype: dict{<projectKey>: <branch>}
-        """
-        if self._projects is not None:
-            return self._projects
-        self._projects = {}
-        if self._json is None or "projects" not in self._json:
-            self.refresh()
-        for p in self._json["projects"]:
-            # TODO: Support several branches of same project in the Application
-            # TODO: Return projects in an application branch
-            self._projects[p["key"]] = p["branch"]
-        return self._projects
-
-    def branch_exists(self, branch):
-        """
-        :return: Whether the Application branch exists
-        :rtype: bool
-        """
-        return branch in self.branches()
-
-    def branch_is_main(self, branch):
-        """
-        :return: Whether the Application branch is the main branch
-        :rtype: bool
-        """
-        return branch in self.branches() and self._branches[branch]["isMain"]
-
-    def set_branch(self, branch_name, branch_data):
-        """Creates or updates an Application branch with a set of project branches
-
-        :param str branch_name: The Application branch to set
-        :param dict branch_data: in format returned by api/applications/show or {"projects": {<projectKey>: <branch>, ...}}
-        :raises ObjectNotFound: if a project key does not exist or project branch does not exists
-        :return: self:
-        :rtype: Application
-        """
-        project_list, branch_list = [], []
-        ok = True
-        for p in branch_data.get("projects", []):
-            (pkey, bname) = (p["projectKey"], p["branch"]) if isinstance(p, dict) else (p, branch_data["projects"][p])
-            try:
-                o_proj = projects.Project.get_object(self.endpoint, pkey)
-                if bname == settings.DEFAULT_SETTING:
-                    bname = o_proj.main_branch().name
-                if not branches.exists(self.endpoint, bname, pkey):
-                    ok = False
-                    util.logger.warning("Branch '%s' of %s not found while setting application branch", bname, str(o_proj))
-                else:
-                    project_list.append(pkey)
-                    branch_list.append(bname)
-            except exceptions.ObjectNotFound:
-                ok = False
-
-        if len(project_list) > 0:
-            params = {"application": self.key, "branch": branch_name, "project": project_list, "projectBranch": branch_list}
-            api = APIS["create_branch"]
-            if self.branch_exists(branch_name):
-                api = APIS["update_branch"]
-                params["name"] = params["branch"]
-            ok = ok and self.post(api, params=params).ok
-        return self
-
-    def branches(self):
-        """
-        :return: the list of branches of the application and their definition
-        :rtype: dict {<appBranch: {"projects": {<projectKey>: <projectBranch>, ...}}}
-        """
-        if self._branches is not None:
-            return self._branches
-        if "branches" not in self._json:
-            self.refresh()
-        params = {"application": self.key}
-        self._branches = {}
-
-        for br in self._json["branches"]:
-            if not br["isMain"]:
-                br.pop("isMain")
-            b_name = br.pop("name")
-            params["branch"] = b_name
-            data = json.loads(self.get(APIS["get"], params=params).text)
-            br["projects"] = {}
-            for proj in data["application"]["projects"]:
-                br["projects"][proj["key"]] = proj["branch"]
-            self._branches[b_name] = br
-        return self._branches
-
-    def delete(self):
-        """Deletes an Application
-
-        :param params: Params for delete, typically None
-        :type params: dict, optional
-        :param exit_on_error: When to fail fast and exit if the HTTP status code is not 2XX, defaults to True
-        :type exit_on_error: bool, optional
-        :param mute: Tuple of HTTP Error codes to mute (ie not write an error log for), defaults to None.
-        Typically, Error 404 Not found may be expected sometimes so this can avoid logging an error for 404
-        :type mute: tuple, optional
-        :return: Whether the delete succeeded
-        :rtype: bool
-        """
-        return sq.delete_object(self, "applications/delete", {"application": self.key}, _OBJECTS)
-
-    def _audit_empty(self, audit_settings):
-        """Audits if an application contains 0 projects"""
-        if not audit_settings["audit.applications.empty"]:
-            util.logger.debug("Auditing empty applications is disabled, skipping...")
-            return []
-        return super()._audit_empty_aggregation(broken_rule=rules.RuleId.APPLICATION_EMPTY)
-
-    def _audit_singleton(self, audit_settings):
-        """Audits if an application contains a single project (makes littel sense)"""
-        if not audit_settings["audit.applications.singleton"]:
-            util.logger.debug("Auditing singleton applications is disabled, skipping...")
-            return []
-        return super()._audit_singleton_aggregation(broken_rule=rules.RuleId.APPLICATION_SINGLETON)
-
-    def audit(self, audit_settings):
-        """Audits an application and returns list of problems found
-
-        :param dict audit_settings: Audit configuration settings from sonar-audit properties config file
-        :return: list of problems found
-        :rtype: list [Problem]
-        """
-        util.logger.info("Auditing %s", str(self))
-        return self._audit_empty(audit_settings) + self._audit_singleton(audit_settings) + self._audit_bg_task(audit_settings)
-
-    def export(self, full=False):
-        """Exports an application
-
-        :param full: Whether to do a full export including settings that can't be set, defaults to False
-        :type full: bool, optional
-        """
-        util.logger.info("Exporting %s", str(self))
-        self.refresh()
-        json_data = self._json.copy()
-        json_data.update(
-            {
-                "key": self.key,
-                "name": self.name,
-                "description": None if self._description == "" else self._description,
-                "visibility": self.visibility(),
-                # 'projects': self.projects(),
-                "branches": self.branches(),
-                "permissions": self.permissions().export(),
-                "tags": util.list_to_csv(self.tags(), separator=", "),
-            }
-        )
-        return util.remove_nones(util.filter_export(json_data, _IMPORTABLE_PROPERTIES, full))
-
-    def set_permissions(self, data):
-        """Sets an application permissions
-
-        :param dict data: dict of permission {"users": [<user1>, <user2>, ...], "groups": [<group1>, <group2>, ...]}
-        :raises: ObjectNotFound if a user or a group does not exists
-        :return: self
-        """
-        return self.permissions().set(data)
-
-    def set_tags(self, tags):
-        if tags is None or len(tags) == 0:
-            return
-        if isinstance(tags, list):
-            my_tags = util.list_to_csv(tags)
-        else:
-            my_tags = util.csv_normalize(tags)
-        self.post("applications/set_tags", params={"application": self.key, "tags": my_tags})
-        self._tags = util.csv_to_list(my_tags)
-
-    def add_projects(self, project_list):
-        current_projects = self.projects().keys()
-        ok = True
-        for proj in project_list:
-            if proj in current_projects:
-                util.logger.debug("Won't add project '%s' to %s, it's already added", proj, str(self))
-                continue
-            util.logger.debug("Adding project '%s' to %s", proj, str(self))
-            try:
-                r = self.post("applications/add_project", params={"application": self.key, "project": proj})
-                ok = ok and r.ok
-            except HTTPError as e:
-                if e.response.status_code == HTTPStatus.NOT_FOUND:
-                    util.logger.warning("Project '%s' not found, can't be added to %s", proj, self)
-                    ok = False
-                else:
-                    raise
-        return ok
-
-    def update(self, data):
-        """Updates an Application with data coming from a JSON (export)
-
-        :param dict data:
-        """
-        if "permissions" in data:
-            decoded_perms = {}
-            for ptype in permissions.PERMISSION_TYPES:
-                if ptype not in data["permissions"]:
-                    continue
-                decoded_perms[ptype] = {u: permissions.decode(v) for u, v in data["permissions"][ptype].items()}
-            self.set_permissions(decoded_perms)
-            # perms = {k: permissions.decode(v) for k, v in data.get("permissions", {}).items()}
-            # self.set_permissions(util.csv_to_list(perms))
-        self.add_projects(_project_list(data))
-        self.set_tags(data.get("tags", None))
-        for name, branch_data in data.get("branches", {}).items():
-            self.set_branch(name, branch_data)
-
-    def search_params(self):
-        """Return params used to search for that object
-
-        :meta private:
-        """
-        return {"application": self.key}
-
-
-def _project_list(data):
-    plist = {}
-    for b in data.get("branches", {}).values():
-        if isinstance(b["projects"], dict):
-            plist.update(b["projects"])
-        else:
-            for p in b["projects"]:
-                plist[p["projectKey"]] = ""
-    return plist.keys()
-
-
-def count(endpoint):
-    """returns count of applications
-
-    :param Platform endpoint: Reference to the SonarQube platform
-    :return: Count of applications
-    :rtype: int
-    """
-    data = json.loads(endpoint.get(components.SEARCH_API, params={"ps": 1, "filter": "qualifier = APP"}).text)
-    return data["paging"]["total"]
-
-
-def search(endpoint, params=None):
-    """Searches applications
-
-    :param Platform endpoint: Reference to the SonarQube platform
-    :param params: Search filters (see api/components/search parameters)
-    :raises UnsupportedOperation: If on a community edition
-    :return: dict of applications
-    :rtype: dict {<appKey>: Application, ...}
-    """
-    if endpoint.edition() == "community":
-        raise exceptions.UnsupportedOperation(_NOT_SUPPORTED)
-    new_params = {"filter": "qualifier = APP"}
-    if params is not None:
-        new_params.update(params)
-    return sq.search_objects(
-        api=APIS["search"], params=new_params, returned_field="components", key_field="key", object_class=Application, endpoint=endpoint
-    )
-
-
-def get_list(endpoint, key_list=None, use_cache=True):
-    """
-    :return: List of Applications (all of them if key_list is None or empty)
-    :param key_list: List of app keys to get, if None or empty all portfolios are returned
-    :param use_cache: Whether to use local cache or query SonarQube, default True (use cache)
-    :type use_cache: bool
-    :rtype: dict{<branchName>: <Branch>}
-    """
-    with _CLASS_LOCK:
-        if key_list is None or len(key_list) == 0 or not use_cache:
-            util.logger.info("Listing applications")
-            return search(endpoint=endpoint)
-        object_list = {}
-        for key in util.csv_to_list(key_list):
-            object_list[key] = Application.get_object(endpoint, key)
-    return object_list
-
-
-def export(endpoint, key_list=None, full=False):
-    """Exports applications as JSON
-
-    :param Platform endpoint: Reference to the SonarQube platform
-    :param key_list: list of Application keys to export, defaults to all if None
-    :type key_list: list, optional
-    :param full: Whether to export all attributes, including those that can't be set, defaults to False
-    :type full: bool
-    :return: Dict of applications settings
-    :rtype: dict
-    """
-    apps_settings = {k: app.export(full) for k, app in get_list(endpoint, key_list).items()}
-    for k in apps_settings:
-        # remove key from JSON value, it's already the dict key
-        apps_settings[k].pop("key")
-    return apps_settings
-
-
-def audit(audit_settings, endpoint=None, key_list=None):
-    """Audits applications and return list of problems found
-
-    :param Platform endpoint: Reference to the SonarQube platform
-    :param dict audit_settings: dict of audit config settings
-    :param key_list: list of Application keys to audit, defaults to all if None
-    :type key_list: list, optional
-    :return: List of problems found
-    :rtype: list [Problem]
-    """
-    if endpoint.edition() == "community":
-        return []
-    if not audit_settings["audit.applications"]:
-        util.logger.debug("Auditing applications is disabled, skipping...")
-        return []
-    util.logger.info("--- Auditing applications ---")
-    problems = []
-    for obj in get_list(endpoint, key_list=key_list).values():
-        problems += obj.audit(audit_settings)
-    return problems
-
-
-def import_config(endpoint, config_data, key_list=None):
-    """Imports a list of application configuration in a SonarQube platform
-
-    :param Platform endpoint: Reference to the SonarQube platform
-    :param dict config_data: JSON representation of applications configuration
-    :param key_list: list of Application keys to import, defaults to all if None
-    :type key_list: list, optional
-    :return: Whether import succeeded
-    :rtype: bool
-    """
-    if "applications" not in config_data:
-        util.logger.info("No applications to import")
-        return True
-    if endpoint.edition() == "community":
-        util.logger.warning("Can't import applications in a community edition")
-        return False
-    util.logger.info("Importing applications")
-    search(endpoint=endpoint)
-    new_key_list = util.csv_to_list(key_list)
-    for key, data in config_data["applications"].items():
-        if new_key_list and key not in new_key_list:
-            continue
-        util.logger.info("Importing application key '%s'", key)
-        try:
-            o = Application.get_object(endpoint, key)
-        except exceptions.ObjectNotFound:
-            o = Application.create(endpoint, key, data["name"])
-        o.update(data)
-    return True
-
-
-def search_by_name(endpoint, name):
-    return util.search_by_name(endpoint, name, components.SEARCH_API, "components", extra_params={"qualifiers": "APP"})
+#
+# sonar-tools
+# Copyright (C) 2022-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+
+import json
+from http import HTTPStatus
+from threading import Lock
+from requests.exceptions import HTTPError
+from sonar import components, exceptions, settings
+from sonar.projects import projects, branches
+from sonar.permissions import permissions, application_permissions
+import sonar.sqobject as sq
+import sonar.aggregations as aggr
+import sonar.utilities as util
+from sonar.audit import rules
+
+_OBJECTS = {}
+_MAP = {}
+_CLASS_LOCK = Lock()
+
+APIS = {
+    "search": "api/components/search_projects",
+    "get": "api/applications/show",
+    "create": "api/applications/create",
+    "delete": "api/applications/delete",
+    "create_branch": "api/applications/create_branch",
+    "update_branch": "api/applications/update_branch",
+}
+
+_IMPORTABLE_PROPERTIES = ("key", "name", "description", "visibility", "branches", "permissions", "tags")
+_NOT_SUPPORTED = "Applications not supported in community edition"
+
+
+class Application(aggr.Aggregation):
+    """
+    Abstraction of the SonarQube "application" concept
+    """
+
+    @classmethod
+    def get_object(cls, endpoint, key):
+        """Gets an Application object from SonarQube
+
+        :param Platform endpoint: Reference to the SonarQube platform
+        :param str key: Application key, must not already exist on SonarQube
+        :raises UnsupportedOperation: If on a Community Edition
+        :raises ObjectNotFound: If Application key not found in SonarQube
+        :return: The found Application object
+        :rtype: Application
+        """
+        if endpoint.edition() == "community":
+            raise exceptions.UnsupportedOperation(_NOT_SUPPORTED)
+        if key in _OBJECTS:
+            return _OBJECTS[key]
+        try:
+            data = json.loads(endpoint.get(APIS["get"], params={"application": key}).text)
+        except HTTPError as e:
+            if e.response.status_code == HTTPStatus.NOT_FOUND:
+                raise exceptions.ObjectNotFound(key, f"Application '{key}' not found")
+        return cls.load(endpoint, data)
+
+    @classmethod
+    def load(cls, endpoint, data):
+        """Loads an Application object with data retrieved from SonarQube
+
+        :param Platform endpoint: Reference to the SonarQube platform
+        :param str key: Application key, must not already exist on SonarQube
+        :param dict data: Data coming from api/components/search_projects or api/applications/show
+        :raises UnsupportedOperation: If on a Community Edition
+        :raises ObjectNotFound: If Application key not found in SonarQube
+        :return: The found Application object
+        :rtype: Application
+        """
+        if endpoint.edition() == "community":
+            raise exceptions.UnsupportedOperation(_NOT_SUPPORTED)
+        o = _OBJECTS.get(data["key"], cls(endpoint, data["key"], data["name"]))
+        o.reload(data)
+        return o
+
+    @classmethod
+    def create(cls, endpoint, key, name):
+        """Creates an Application object in SonarQube
+
+        :param Platform endpoint: Reference to the SonarQube platform
+        :param str key: Application key, must not already exist on SonarQube
+        :param str name: Application name
+        :raises UnsupportedOperation: If on a Community Edition
+        :raises ObjectAlreadyExists: If key already exist for another Application
+        :return: The created Application object
+        :rtype: Application
+        """
+        if endpoint.edition() == "community":
+            raise exceptions.UnsupportedOperation(_NOT_SUPPORTED)
+        try:
+            endpoint.post(APIS["create"], params={"key": key, "name": name})
+        except HTTPError as e:
+            if e.response.status_code == HTTPStatus.BAD_REQUEST:
+                raise exceptions.ObjectAlreadyExists(key, e.response.text)
+        return Application(endpoint, key, name)
+
+    def __init__(self, endpoint, key, name):
+        """Don't use this directly, go through the class methods to create Objects"""
+        super().__init__(key, endpoint)
+        self._branches = None
+        self._projects = None
+        self._description = None
+        self.name = name
+        util.logger.debug("Created object %s", str(self))
+        _OBJECTS[self.key] = self
+        _MAP[self.name] = self.key
+
+    def refresh(self):
+        """Refreshes the by re-reading SonarQube
+
+        :raises ObjectNotFound: If the Application does not exists anymore
+        :return: self:
+        :rtype: Appplication
+        """
+        try:
+            return self.reload(json.loads(self.get(APIS["get"], params={"application": self.key}).text)["application"])
+        except HTTPError as e:
+            if e.response.status_code == HTTPStatus.NOT_FOUND:
+                _OBJECTS.pop(self.key, None)
+                raise exceptions.ObjectNotFound(self.key, f"{str(self)} not found")
+            raise
+
+    def __str__(self):
+        return f"application key '{self.key}'"
+
+    def permissions(self):
+        """
+        :return: The application permissions
+        :rtype: ApplicationPermissions
+        """
+        if self._permissions is None:
+            self._permissions = application_permissions.ApplicationPermissions(self)
+        return self._permissions
+
+    def projects(self):
+        """
+        :return: The project branches included in the application
+        :rtype: dict{<projectKey>: <branch>}
+        """
+        if self._projects is not None:
+            return self._projects
+        self._projects = {}
+        if self._json is None or "projects" not in self._json:
+            self.refresh()
+        for p in self._json["projects"]:
+            # TODO: Support several branches of same project in the Application
+            # TODO: Return projects in an application branch
+            self._projects[p["key"]] = p["branch"]
+        return self._projects
+
+    def branch_exists(self, branch):
+        """
+        :return: Whether the Application branch exists
+        :rtype: bool
+        """
+        return branch in self.branches()
+
+    def branch_is_main(self, branch):
+        """
+        :return: Whether the Application branch is the main branch
+        :rtype: bool
+        """
+        return branch in self.branches() and self._branches[branch]["isMain"]
+
+    def set_branch(self, branch_name, branch_data):
+        """Creates or updates an Application branch with a set of project branches
+
+        :param str branch_name: The Application branch to set
+        :param dict branch_data: in format returned by api/applications/show or {"projects": {<projectKey>: <branch>, ...}}
+        :raises ObjectNotFound: if a project key does not exist or project branch does not exists
+        :return: self:
+        :rtype: Application
+        """
+        project_list, branch_list = [], []
+        ok = True
+        for p in branch_data.get("projects", []):
+            (pkey, bname) = (p["projectKey"], p["branch"]) if isinstance(p, dict) else (p, branch_data["projects"][p])
+            try:
+                o_proj = projects.Project.get_object(self.endpoint, pkey)
+                if bname == settings.DEFAULT_SETTING:
+                    bname = o_proj.main_branch().name
+                if not branches.exists(self.endpoint, bname, pkey):
+                    ok = False
+                    util.logger.warning("Branch '%s' of %s not found while setting application branch", bname, str(o_proj))
+                else:
+                    project_list.append(pkey)
+                    branch_list.append(bname)
+            except exceptions.ObjectNotFound:
+                ok = False
+
+        if len(project_list) > 0:
+            params = {"application": self.key, "branch": branch_name, "project": project_list, "projectBranch": branch_list}
+            api = APIS["create_branch"]
+            if self.branch_exists(branch_name):
+                api = APIS["update_branch"]
+                params["name"] = params["branch"]
+            ok = ok and self.post(api, params=params).ok
+        return self
+
+    def branches(self):
+        """
+        :return: the list of branches of the application and their definition
+        :rtype: dict {<appBranch: {"projects": {<projectKey>: <projectBranch>, ...}}}
+        """
+        if self._branches is not None:
+            return self._branches
+        if "branches" not in self._json:
+            self.refresh()
+        params = {"application": self.key}
+        self._branches = {}
+
+        for br in self._json["branches"]:
+            if not br["isMain"]:
+                br.pop("isMain")
+            b_name = br.pop("name")
+            params["branch"] = b_name
+            data = json.loads(self.get(APIS["get"], params=params).text)
+            br["projects"] = {}
+            for proj in data["application"]["projects"]:
+                br["projects"][proj["key"]] = proj["branch"]
+            self._branches[b_name] = br
+        return self._branches
+
+    def delete(self):
+        """Deletes an Application
+
+        :param params: Params for delete, typically None
+        :type params: dict, optional
+        :param exit_on_error: When to fail fast and exit if the HTTP status code is not 2XX, defaults to True
+        :type exit_on_error: bool, optional
+        :param mute: Tuple of HTTP Error codes to mute (ie not write an error log for), defaults to None.
+                     Typically, Error 404 Not found may be expected sometimes so this can avoid logging an error for 404
+        :type mute: tuple, optional
+        :return: Whether the delete succeeded
+        :rtype: bool
+        """
+        return sq.delete_object(self, "applications/delete", {"application": self.key}, _OBJECTS)
+
+    def _audit_empty(self, audit_settings):
+        """Audits if an application contains 0 projects"""
+        if not audit_settings.get("audit.applications.empty", True):
+            util.logger.debug("Auditing empty applications is disabled, skipping...")
+            return []
+        return super()._audit_empty_aggregation(broken_rule=rules.RuleId.APPLICATION_EMPTY)
+
+    def _audit_singleton(self, audit_settings):
+        """Audits if an application contains a single project (makes littel sense)"""
+        if not audit_settings.get("audit.applications.singleton", True):
+            util.logger.debug("Auditing singleton applications is disabled, skipping...")
+            return []
+        return super()._audit_singleton_aggregation(broken_rule=rules.RuleId.APPLICATION_SINGLETON)
+
+    def audit(self, audit_settings):
+        """Audits an application and returns list of problems found
+
+        :param dict audit_settings: Audit configuration settings from sonar-audit properties config file
+        :return: list of problems found
+        :rtype: list [Problem]
+        """
+        util.logger.info("Auditing %s", str(self))
+        return self._audit_empty(audit_settings) + self._audit_singleton(audit_settings) + self._audit_bg_task(audit_settings)
+
+    def export(self, full=False):
+        """Exports an application
+
+        :param full: Whether to do a full export including settings that can't be set, defaults to False
+        :type full: bool, optional
+        """
+        util.logger.info("Exporting %s", str(self))
+        self.refresh()
+        json_data = self._json.copy()
+        json_data.update(
+            {
+                "key": self.key,
+                "name": self.name,
+                "description": None if self._description == "" else self._description,
+                "visibility": self.visibility(),
+                # 'projects': self.projects(),
+                "branches": self.branches(),
+                "permissions": self.permissions().export(),
+                "tags": util.list_to_csv(self.tags(), separator=", "),
+            }
+        )
+        return util.remove_nones(util.filter_export(json_data, _IMPORTABLE_PROPERTIES, full))
+
+    def set_permissions(self, data):
+        """Sets an application permissions
+
+        :param dict data: dict of permission {"users": [<user1>, <user2>, ...], "groups": [<group1>, <group2>, ...]}
+        :raises: ObjectNotFound if a user or a group does not exists
+        :return: self
+        """
+        return self.permissions().set(data)
+
+    def set_tags(self, tags):
+        if tags is None or len(tags) == 0:
+            return
+        if isinstance(tags, list):
+            my_tags = util.list_to_csv(tags)
+        else:
+            my_tags = util.csv_normalize(tags)
+        self.post("applications/set_tags", params={"application": self.key, "tags": my_tags})
+        self._tags = util.csv_to_list(my_tags)
+
+    def add_projects(self, project_list):
+        current_projects = self.projects().keys()
+        ok = True
+        for proj in project_list:
+            if proj in current_projects:
+                util.logger.debug("Won't add project '%s' to %s, it's already added", proj, str(self))
+                continue
+            util.logger.debug("Adding project '%s' to %s", proj, str(self))
+            try:
+                r = self.post("applications/add_project", params={"application": self.key, "project": proj})
+                ok = ok and r.ok
+            except HTTPError as e:
+                if e.response.status_code == HTTPStatus.NOT_FOUND:
+                    util.logger.warning("Project '%s' not found, can't be added to %s", proj, self)
+                    ok = False
+                else:
+                    raise
+        return ok
+
+    def update(self, data):
+        """Updates an Application with data coming from a JSON (export)
+
+        :param dict data:
+        """
+        if "permissions" in data:
+            decoded_perms = {}
+            for ptype in permissions.PERMISSION_TYPES:
+                if ptype not in data["permissions"]:
+                    continue
+                decoded_perms[ptype] = {u: permissions.decode(v) for u, v in data["permissions"][ptype].items()}
+            self.set_permissions(decoded_perms)
+            # perms = {k: permissions.decode(v) for k, v in data.get("permissions", {}).items()}
+            # self.set_permissions(util.csv_to_list(perms))
+        self.add_projects(_project_list(data))
+        self.set_tags(data.get("tags", None))
+        for name, branch_data in data.get("branches", {}).items():
+            self.set_branch(name, branch_data)
+
+    def search_params(self):
+        """Return params used to search for that object
+
+        :meta private:
+        """
+        return {"application": self.key}
+
+
+def _project_list(data):
+    plist = {}
+    for b in data.get("branches", {}).values():
+        if isinstance(b["projects"], dict):
+            plist.update(b["projects"])
+        else:
+            for p in b["projects"]:
+                plist[p["projectKey"]] = ""
+    return plist.keys()
+
+
+def count(endpoint):
+    """returns count of applications
+
+    :param Platform endpoint: Reference to the SonarQube platform
+    :return: Count of applications
+    :rtype: int
+    """
+    data = json.loads(endpoint.get(components.SEARCH_API, params={"ps": 1, "filter": "qualifier = APP"}).text)
+    return data["paging"]["total"]
+
+
+def search(endpoint, params=None):
+    """Searches applications
+
+    :param Platform endpoint: Reference to the SonarQube platform
+    :param params: Search filters (see api/components/search parameters)
+    :raises UnsupportedOperation: If on a community edition
+    :return: dict of applications
+    :rtype: dict {<appKey>: Application, ...}
+    """
+    if endpoint.edition() == "community":
+        raise exceptions.UnsupportedOperation(_NOT_SUPPORTED)
+    new_params = {"filter": "qualifier = APP"}
+    if params is not None:
+        new_params.update(params)
+    return sq.search_objects(
+        api=APIS["search"], params=new_params, returned_field="components", key_field="key", object_class=Application, endpoint=endpoint
+    )
+
+
+def get_list(endpoint, key_list=None, use_cache=True):
+    """
+    :return: List of Applications (all of them if key_list is None or empty)
+    :param key_list: List of app keys to get, if None or empty all portfolios are returned
+    :param use_cache: Whether to use local cache or query SonarQube, default True (use cache)
+    :type use_cache: bool
+    :rtype: dict{<branchName>: <Branch>}
+    """
+    with _CLASS_LOCK:
+        if key_list is None or len(key_list) == 0 or not use_cache:
+            util.logger.info("Listing applications")
+            return search(endpoint=endpoint)
+        object_list = {}
+        for key in util.csv_to_list(key_list):
+            object_list[key] = Application.get_object(endpoint, key)
+    return object_list
+
+
+def export(endpoint, key_list=None, full=False):
+    """Exports applications as JSON
+
+    :param Platform endpoint: Reference to the SonarQube platform
+    :param key_list: list of Application keys to export, defaults to all if None
+    :type key_list: list, optional
+    :param full: Whether to export all attributes, including those that can't be set, defaults to False
+    :type full: bool
+    :return: Dict of applications settings
+    :rtype: dict
+    """
+    apps_settings = {k: app.export(full) for k, app in get_list(endpoint, key_list).items()}
+    for k in apps_settings:
+        # remove key from JSON value, it's already the dict key
+        apps_settings[k].pop("key")
+    return apps_settings
+
+
+def audit(audit_settings, endpoint=None, key_list=None):
+    """Audits applications and return list of problems found
+
+    :param Platform endpoint: Reference to the SonarQube platform
+    :param dict audit_settings: dict of audit config settings
+    :param key_list: list of Application keys to audit, defaults to all if None
+    :type key_list: list, optional
+    :return: List of problems found
+    :rtype: list [Problem]
+    """
+    if endpoint.edition() == "community":
+        return []
+    if not audit_settings.get("audit.applications", True):
+        util.logger.debug("Auditing applications is disabled, skipping...")
+        return []
+    util.logger.info("--- Auditing applications ---")
+    problems = []
+    for obj in get_list(endpoint, key_list=key_list).values():
+        problems += obj.audit(audit_settings)
+    return problems
+
+
+def import_config(endpoint, config_data, key_list=None):
+    """Imports a list of application configuration in a SonarQube platform
+
+    :param Platform endpoint: Reference to the SonarQube platform
+    :param dict config_data: JSON representation of applications configuration
+    :param key_list: list of Application keys to import, defaults to all if None
+    :type key_list: list, optional
+    :return: Whether import succeeded
+    :rtype: bool
+    """
+    if "applications" not in config_data:
+        util.logger.info("No applications to import")
+        return True
+    if endpoint.edition() == "community":
+        util.logger.warning("Can't import applications in a community edition")
+        return False
+    util.logger.info("Importing applications")
+    search(endpoint=endpoint)
+    new_key_list = util.csv_to_list(key_list)
+    for key, data in config_data["applications"].items():
+        if new_key_list and key not in new_key_list:
+            continue
+        util.logger.info("Importing application key '%s'", key)
+        try:
+            o = Application.get_object(endpoint, key)
+        except exceptions.ObjectNotFound:
+            o = Application.create(endpoint, key, data["name"])
+        o.update(data)
+    return True
+
+
+def search_by_name(endpoint, name):
+    return util.search_by_name(endpoint, name, components.SEARCH_API, "components", extra_params={"qualifiers": "APP"})
```

## sonar/components.py

 * *Ordering differences only*

```diff
@@ -1,189 +1,189 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-"""
-
-    Abstraction of the SonarQube "component" concept
-
-"""
-
-import json
-import sonar.sqobject as sq
-from sonar import settings, tasks
-import sonar.utilities as util
-
-SEARCH_API = "components/search"
-_DETAILS_API = "components/show"
-
-
-class Component(sq.SqObject):
-    def __init__(self, key, endpoint=None, data=None):
-        super().__init__(key, endpoint)
-        self.name = None
-        self.nbr_issues = None
-        self.ncloc = None
-        self._description = None
-        self._last_analysis = None
-        self._tags = None
-        self._visibility = None
-        if data is not None:
-            self.reload(data)
-
-    def reload(self, data):
-        if self._json:
-            self._json.update(data)
-        else:
-            self._json = data
-        if "name" in data:
-            self.name = data["name"]
-        if "visibility" in data:
-            self._visibility = data["visibility"]
-        if "analysisDate" in data:
-            self._last_analysis = data["analysisDate"]
-        return self
-
-    def __str__(self):
-        return self.key
-
-    def tags(self):
-        if self._tags is not None:
-            pass
-        elif self._json is not None and "tags" in self._json:
-            self._tags = self._json["tags"]
-        else:
-            data = json.loads(self.get(_DETAILS_API, params={"component": self.key}).text)
-            if self._json is None:
-                self._json = data["component"]
-            else:
-                self._json.update(data["component"])
-            self._tags = self._json["tags"]
-            settings.Setting.load(key=settings.COMPONENT_VISIBILITY, endpoint=self.endpoint, component=self, data=data["component"])
-        return self._tags if len(self._tags) > 0 else None
-
-    def get_subcomponents(self, strategy="children", with_issues=False):
-        parms = {
-            "component": self.key,
-            "strategy": strategy,
-            "ps": 1,
-            "metricKeys": "bugs,vulnerabilities,code_smells,security_hotspots",
-        }
-        data = json.loads(self.get("measures/component_tree", params=parms).text)
-        nb_comp = data["paging"]["total"]
-        util.logger.debug("Found %d subcomponents to %s", nb_comp, str(self))
-        nb_pages = (nb_comp + 500 - 1) // 500
-        comp_list = {}
-        parms["ps"] = 500
-        for page in range(nb_pages):
-            parms["p"] = page + 1
-            data = json.loads(self.get("measures/component_tree", params=parms).text)
-            for d in data["components"]:
-                nbr_issues = 0
-                for m in d["measures"]:
-                    nbr_issues += int(m["value"])
-                if with_issues and nbr_issues == 0:
-                    util.logger.debug("Subcomponent %s has 0 issues, skipping", d["key"])
-                    continue
-                comp_list[d["key"]] = Component(d["key"], self.endpoint, data=d)
-                comp_list[d["key"]].nbr_issues = nbr_issues
-                util.logger.debug("Component %s has %d issues", d["key"], nbr_issues)
-        return comp_list
-
-    def get_number_of_filtered_issues(self, params):
-        from sonar.findings import issues
-
-        params["componentKey"] = self.key
-        params["ps"] = 1
-        returned_data = issues.search(endpoint=self.endpoint, params=params)
-        return returned_data["total"]
-
-    def get_number_of_issues(self):
-        """Returns number of issues of a component"""
-        if self.nbr_issues is None:
-            self.nbr_issues = self.get_number_of_filtered_issues({"componentKey": self.key})
-        return self.nbr_issues
-
-    def get_oldest_issue_date(self):
-        """Returns the oldest date of all issues found"""
-        from sonar.findings import issues
-
-        return issues.get_oldest_issue(endpoint=self.endpoint, params={"componentKeys": self.key})
-
-    def get_newest_issue_date(self):
-        """Returns the newest date of all issues found"""
-        from sonar.findings import issues
-
-        return issues.get_newest_issue(endpoint=self.endpoint, params={"componentKeys": self.key})
-
-    def get_issues(self):
-        from sonar.findings import issues
-
-        issue_list = issues.search(endpoint=self.endpoint, params={"componentKeys": self.key})
-        self.nbr_issues = len(issue_list)
-        return issue_list
-
-    def get_measures(self, metrics_list):
-        # Must be implemented in sub classes
-        return {}
-
-    def get_measure(self, metric, fallback=None):
-        meas = self.get_measures(metric)
-        return meas[metric].value if metric in meas and meas[metric].value is not None else fallback
-
-    def loc(self):
-        if self.ncloc is None:
-            self.ncloc = int(self.get_measure("ncloc", fallback=0))
-        return self.ncloc
-
-    def refresh(self):
-        return self.reload(json.loads(self.endpoint.get("navigation/component", params={"component": self.key}).text))
-
-    def last_analysis(self):
-        if not self._last_analysis:
-            self.refresh()
-        return self._last_analysis
-
-    def url(self):
-        # Must be implemented in sub classes
-        pass
-
-    def visibility(self):
-        if not self._visibility:
-            self._visibility = settings.get_visibility(self.endpoint, component=self).value
-        return self._visibility
-
-    def set_visibility(self, visibility):
-        settings.set_visibility(self.endpoint, visibility=visibility, component=self)
-        self._visibility = visibility
-
-    def _audit_bg_task(self, audit_settings):
-        util.logger.debug("Auditing last background task of %s", str(self))
-        last_task = tasks.search_last(component_key=self.key, endpoint=self.endpoint)
-        if last_task:
-            last_task.concerned_object = self
-            return last_task.audit(audit_settings)
-        return []
-
-
-def get_components(component_types, endpoint):
-    data = json.loads(endpoint.get("projects/search", params={"ps": 500, "qualifiers": component_types}).text)
-    return data["components"]
-
-
-def get_subcomponents(component_key, strategy="children", with_issues=False, endpoint=None):
-    return Component(component_key, endpoint).get_subcomponents(strategy=strategy, with_issues=with_issues)
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+"""
+
+    Abstraction of the SonarQube "component" concept
+
+"""
+
+import json
+import sonar.sqobject as sq
+from sonar import settings, tasks
+import sonar.utilities as util
+
+SEARCH_API = "components/search"
+_DETAILS_API = "components/show"
+
+
+class Component(sq.SqObject):
+    def __init__(self, key, endpoint=None, data=None):
+        super().__init__(key, endpoint)
+        self.name = None
+        self.nbr_issues = None
+        self.ncloc = None
+        self._description = None
+        self._last_analysis = None
+        self._tags = None
+        self._visibility = None
+        if data is not None:
+            self.reload(data)
+
+    def reload(self, data):
+        if self._json:
+            self._json.update(data)
+        else:
+            self._json = data
+        if "name" in data:
+            self.name = data["name"]
+        if "visibility" in data:
+            self._visibility = data["visibility"]
+        if "analysisDate" in data:
+            self._last_analysis = data["analysisDate"]
+        return self
+
+    def __str__(self):
+        return self.key
+
+    def tags(self):
+        if self._tags is not None:
+            pass
+        elif self._json is not None and "tags" in self._json:
+            self._tags = self._json["tags"]
+        else:
+            data = json.loads(self.get(_DETAILS_API, params={"component": self.key}).text)
+            if self._json is None:
+                self._json = data["component"]
+            else:
+                self._json.update(data["component"])
+            self._tags = self._json["tags"]
+            settings.Setting.load(key=settings.COMPONENT_VISIBILITY, endpoint=self.endpoint, component=self, data=data["component"])
+        return self._tags if len(self._tags) > 0 else None
+
+    def get_subcomponents(self, strategy="children", with_issues=False):
+        parms = {
+            "component": self.key,
+            "strategy": strategy,
+            "ps": 1,
+            "metricKeys": "bugs,vulnerabilities,code_smells,security_hotspots",
+        }
+        data = json.loads(self.get("measures/component_tree", params=parms).text)
+        nb_comp = data["paging"]["total"]
+        util.logger.debug("Found %d subcomponents to %s", nb_comp, str(self))
+        nb_pages = (nb_comp + 500 - 1) // 500
+        comp_list = {}
+        parms["ps"] = 500
+        for page in range(nb_pages):
+            parms["p"] = page + 1
+            data = json.loads(self.get("measures/component_tree", params=parms).text)
+            for d in data["components"]:
+                nbr_issues = 0
+                for m in d["measures"]:
+                    nbr_issues += int(m["value"])
+                if with_issues and nbr_issues == 0:
+                    util.logger.debug("Subcomponent %s has 0 issues, skipping", d["key"])
+                    continue
+                comp_list[d["key"]] = Component(d["key"], self.endpoint, data=d)
+                comp_list[d["key"]].nbr_issues = nbr_issues
+                util.logger.debug("Component %s has %d issues", d["key"], nbr_issues)
+        return comp_list
+
+    def get_number_of_filtered_issues(self, params):
+        from sonar.findings import issues
+
+        params["componentKey"] = self.key
+        params["ps"] = 1
+        returned_data = issues.search(endpoint=self.endpoint, params=params)
+        return returned_data["total"]
+
+    def get_number_of_issues(self):
+        """Returns number of issues of a component"""
+        if self.nbr_issues is None:
+            self.nbr_issues = self.get_number_of_filtered_issues({"componentKey": self.key})
+        return self.nbr_issues
+
+    def get_oldest_issue_date(self):
+        """Returns the oldest date of all issues found"""
+        from sonar.findings import issues
+
+        return issues.get_oldest_issue(endpoint=self.endpoint, params={"componentKeys": self.key})
+
+    def get_newest_issue_date(self):
+        """Returns the newest date of all issues found"""
+        from sonar.findings import issues
+
+        return issues.get_newest_issue(endpoint=self.endpoint, params={"componentKeys": self.key})
+
+    def get_issues(self):
+        from sonar.findings import issues
+
+        issue_list = issues.search(endpoint=self.endpoint, params={"componentKeys": self.key})
+        self.nbr_issues = len(issue_list)
+        return issue_list
+
+    def get_measures(self, metrics_list):
+        # Must be implemented in sub classes
+        return {}
+
+    def get_measure(self, metric, fallback=None):
+        meas = self.get_measures(metric)
+        return meas[metric].value if metric in meas and meas[metric].value is not None else fallback
+
+    def loc(self):
+        if self.ncloc is None:
+            self.ncloc = int(self.get_measure("ncloc", fallback=0))
+        return self.ncloc
+
+    def refresh(self):
+        return self.reload(json.loads(self.endpoint.get("navigation/component", params={"component": self.key}).text))
+
+    def last_analysis(self):
+        if not self._last_analysis:
+            self.refresh()
+        return self._last_analysis
+
+    def url(self):
+        # Must be implemented in sub classes
+        pass
+
+    def visibility(self):
+        if not self._visibility:
+            self._visibility = settings.get_visibility(self.endpoint, component=self).value
+        return self._visibility
+
+    def set_visibility(self, visibility):
+        settings.set_visibility(self.endpoint, visibility=visibility, component=self)
+        self._visibility = visibility
+
+    def _audit_bg_task(self, audit_settings):
+        util.logger.debug("Auditing last background task of %s", str(self))
+        last_task = tasks.search_last(component_key=self.key, endpoint=self.endpoint)
+        if last_task:
+            last_task.concerned_object = self
+            return last_task.audit(audit_settings)
+        return []
+
+
+def get_components(component_types, endpoint):
+    data = json.loads(endpoint.get("projects/search", params={"ps": 500, "qualifiers": component_types}).text)
+    return data["components"]
+
+
+def get_subcomponents(component_key, strategy="children", with_issues=False, endpoint=None):
+    return Component(component_key, endpoint).get_subcomponents(strategy=strategy, with_issues=with_issues)
```

## sonar/custom_measures.py

 * *Ordering differences only*

```diff
@@ -1,94 +1,94 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-"""
-
-    Abstraction of the SonarQube "custom measure" concept
-
-"""
-import json
-import sonar.sqobject as sq
-
-
-class CustomMeasure(sq.SqObject):
-    API_ROOT = "api/custom_measures/"
-
-    def __init__(
-        self,
-        key=None,
-        endpoint=None,
-        uuid=None,
-        project_key=None,
-        value=None,
-        description=None,
-    ):
-        super().__init__(key, endpoint)
-        self.uuid = uuid
-        self.projectKey = project_key
-        self.value = value
-        self.description = description
-
-    def create(self, project_key, metric_key, value, description=None):
-        return self.post(
-            CustomMeasure.API_ROOT + "create",
-            {
-                "component": project_key,
-                "metricKeys": metric_key,
-                "value": value,
-                "description": description,
-            },
-        )
-
-    def update(self, value, description=None):
-        return self.post(
-            CustomMeasure.API_ROOT + "update",
-            {"id": self.uuid, "value": value, "description": description},
-        )
-
-    def delete(self):
-        return self.post(CustomMeasure.API_ROOT + "delete", {"id": self.uuid})
-
-
-def search(endpoint, project_key):
-    data = json.loads(endpoint.get(CustomMeasure.API_ROOT + "search", params={"projectKey": project_key, "ps": 500}).text)
-    # nbr_measures = data['total'] if > 500, we're screwed...
-    measures = []
-    for m in data["customMeasures"]:
-        measures.append(
-            CustomMeasure(
-                uuid=m["id"],
-                key=m["metric"]["key"],
-                project_key=m["projectKey"],
-                value=m["value"],
-                description=m["description"],
-                endpoint=endpoint,
-            )
-        )
-    return measures
-
-
-def update(project_key, metric_key, value, description=None, endpoint=None):
-    for m in search(endpoint, project_key):
-        if m.key == metric_key:
-            m.update(value, description)
-            break
-
-
-def delete(id, endpoint):
-    return endpoint.post(CustomMeasure.API_ROOT + "delete", {"id": id})
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+"""
+
+    Abstraction of the SonarQube "custom measure" concept
+
+"""
+import json
+import sonar.sqobject as sq
+
+
+class CustomMeasure(sq.SqObject):
+    API_ROOT = "api/custom_measures/"
+
+    def __init__(
+        self,
+        key=None,
+        endpoint=None,
+        uuid=None,
+        project_key=None,
+        value=None,
+        description=None,
+    ):
+        super().__init__(key, endpoint)
+        self.uuid = uuid
+        self.projectKey = project_key
+        self.value = value
+        self.description = description
+
+    def create(self, project_key, metric_key, value, description=None):
+        return self.post(
+            CustomMeasure.API_ROOT + "create",
+            {
+                "component": project_key,
+                "metricKeys": metric_key,
+                "value": value,
+                "description": description,
+            },
+        )
+
+    def update(self, value, description=None):
+        return self.post(
+            CustomMeasure.API_ROOT + "update",
+            {"id": self.uuid, "value": value, "description": description},
+        )
+
+    def delete(self):
+        return self.post(CustomMeasure.API_ROOT + "delete", {"id": self.uuid})
+
+
+def search(endpoint, project_key):
+    data = json.loads(endpoint.get(CustomMeasure.API_ROOT + "search", params={"projectKey": project_key, "ps": 500}).text)
+    # nbr_measures = data['total'] if > 500, we're screwed...
+    measures = []
+    for m in data["customMeasures"]:
+        measures.append(
+            CustomMeasure(
+                uuid=m["id"],
+                key=m["metric"]["key"],
+                project_key=m["projectKey"],
+                value=m["value"],
+                description=m["description"],
+                endpoint=endpoint,
+            )
+        )
+    return measures
+
+
+def update(project_key, metric_key, value, description=None, endpoint=None):
+    for m in search(endpoint, project_key):
+        if m.key == metric_key:
+            m.update(value, description)
+            break
+
+
+def delete(id, endpoint):
+    return endpoint.post(CustomMeasure.API_ROOT + "delete", {"id": id})
```

## sonar/devops.py

 * *Ordering differences only*

```diff
@@ -1,286 +1,286 @@
-#
-# sonar-tools
-# Copyright (C) 2022-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-
-
-from http import HTTPStatus
-import json
-
-from requests.exceptions import HTTPError
-
-from sonar import sqobject, exceptions
-import sonar.utilities as util
-
-#: DevOps platform types in SonarQube
-DEVOPS_PLATFORM_TYPES = ("github", "azure", "bitbucket", "bitbucketcloud", "gitlab")
-
-
-_OBJECTS = {}
-
-_CREATE_API_GITHUB = "alm_settings/create_github"
-_CREATE_API_GITLAB = "alm_settings/create_gitlab"
-_CREATE_API_AZURE = "alm_settings/create_azure"
-_CREATE_API_BITBUCKET = "alm_settings/create_bitbucket"
-_CREATE_API_BBCLOUD = "alm_settings/create_bitbucketcloud"
-APIS = {"list": "alm_settings/list_definitions"}
-
-_TO_BE_SET = "TO_BE_SET"
-_IMPORTABLE_PROPERTIES = ("key", "type", "url", "workspace", "clientId", "appId")
-
-
-class DevopsPlatform(sqobject.SqObject):
-    """
-    Abstraction of the SonarQube ALM/DevOps Platform concept
-    """
-
-    @classmethod
-    def read(cls, endpoint, key):
-        if key in _OBJECTS:
-            return _OBJECTS[key]
-        data = json.loads(endpoint.get(APIS["list"]).text)
-        for plt_type, platforms in data.items():
-            for p in platforms:
-                if p["key"] == key:
-                    return cls.load(endpoint, plt_type, data)
-        raise exceptions.ObjectNotFound(key, f"DevOps platform key '{key}' not found")
-
-    @classmethod
-    def load(cls, endpoint, plt_type, data):
-        key = data["key"]
-        if key in _OBJECTS:
-            return _OBJECTS[key]
-        o = DevopsPlatform(key, endpoint, plt_type)
-        return o._load(data)
-
-    @classmethod
-    def create(cls, endpoint, key, plt_type, url_or_workspace):
-        params = {"key": key}
-        try:
-            if plt_type == "github":
-                params.update(
-                    {"appId": _TO_BE_SET, "clientId": _TO_BE_SET, "clientSecret": _TO_BE_SET, "privateKey": _TO_BE_SET, "url": url_or_workspace}
-                )
-                endpoint.post(_CREATE_API_GITHUB, params=params)
-            elif plt_type == "azure":
-                # TODO: pass secrets on the cmd line
-                params.update({"personalAccessToken": _TO_BE_SET, "url": url_or_workspace})
-                endpoint.post(_CREATE_API_AZURE, params=params)
-            elif plt_type == "gitlab":
-                params.update({"personalAccessToken": _TO_BE_SET, "url": url_or_workspace})
-                endpoint.post(_CREATE_API_GITLAB, params=params)
-            elif plt_type == "bitbucket":
-                params.update({"personalAccessToken": _TO_BE_SET, "url": url_or_workspace})
-                endpoint.post(_CREATE_API_BITBUCKET, params=params)
-            elif plt_type == "bitbucketcloud":
-                params.update({"clientSecret": _TO_BE_SET, "clientId": _TO_BE_SET, "workspace": url_or_workspace})
-                endpoint.post(_CREATE_API_BBCLOUD, params=params)
-        except HTTPError as e:
-            if e.response.status_code == HTTPStatus.BAD_REQUEST and endpoint.edition() == "developer":
-                util.logger.warning("Can't set DevOps platform '%s', don't you have more that 1 of that type?", key)
-                raise exceptions.UnsupportedOperation(f"Can't set DevOps platform '{key}', don't you have more that 1 of that type?")
-            raise
-        o = DevopsPlatform(key, endpoint, plt_type)
-        o.refresh()
-        return o
-
-    def __init__(self, key, endpoint, platform_type):
-        super().__init__(key, endpoint)
-        self.type = platform_type  #: DevOps platform type
-        self.url = None  #: DevOps platform URL
-        self._specific = None  #: DevOps platform specific settings
-        _OBJECTS[key] = self
-        util.logger.debug("Created object %s", str(self))
-
-    def _load(self, data):
-        self._json = data
-        self.url = "https://bitbucket.org" if self.type == "bitbucketcloud" else data["url"]
-        self._specific = data.copy()
-        for k in ("key", "url"):
-            self._specific.pop(k, None)
-        return self
-
-    def __str__(self):
-        string = f"devops platform '{self.key}'"
-        if self.type == "bitbucketcloud" and self._specific:
-            string += f" workspace '{self._specific['workspace']}'"
-        return string
-
-    def refresh(self):
-        """Reads / Refresh a DevOps platform information
-
-        :return: Whether the operation succeeded
-        :rtype: bool
-        """
-        data = json.loads(self.get(APIS["list"]).text)
-        for alm_data in data.get(self.type, {}):
-            if alm_data["key"] != self.key:
-                self._json = alm_data
-                return True
-        return False
-
-    def to_json(self, full=False):
-        """Exports a DevOps platform configuration in JSON format
-
-        :param full: Whether to export all properties, including those that can't be set, or not, defaults to False
-        :type full: bool, optional
-        :return: The configuration of the DevOps platform (except secrets)
-        :rtype: dict
-        """
-        json_data = self._json.copy()
-        json_data.update({"key": self.key, "type": self.type, "url": self.url})
-        return util.filter_export(json_data, _IMPORTABLE_PROPERTIES, full)
-
-    def set_pat(self, pat, user_name=None):
-        if self.type == "github":
-            util.logger.warning("Can't set PAT for GitHub devops platform")
-            return False
-        return self.post("alm_integrations/set_pat", params={"almSettings": self.key, "pat": pat, "username": user_name}).ok
-
-    def update(self, **kwargs):
-        """Updates a DevOps platform with information from data
-
-        :param dict data: data to update the DevOps platform configuration
-                          (url, clientId, workspace, appId depending on the type of platform)
-        :return: Whether the operation succeeded
-        :rtype: bool
-        """
-        alm_type = kwargs["type"]
-        if alm_type != self.type:
-            util.logger.error("DevOps platform type '%s' for update of %s is incompatible", alm_type, str(self))
-            return False
-
-        params = {"key": self.key, "url": kwargs["url"]}
-        if alm_type == "bitbucketcloud":
-            params.update({"clientId": kwargs["clientId"], "workspace": kwargs["workspace"]})
-        elif alm_type == "github":
-            params.update({"clientId": kwargs["clientId"], "appId": kwargs["appId"]})
-
-        self.post(f"alm_settings/update_{alm_type}", params=params)
-        self.url = kwargs["url"]
-        for k in ("key", "url"):
-            params.pop(k)
-        self._specific = params
-        return self
-
-
-def count(platf_type=None):
-    """
-    :param platf_type: Filter for a specific type, defaults to None (see DEVOPS_PLATFORM_TYPES set)
-    :type platf_type: str or None
-    :return: Count of DevOps platforms
-    :rtype: int
-    """
-    if platf_type is None:
-        return len(_OBJECTS)
-    # Hack: check first 5 chars to that bitbucket cloud and bitbucket server match
-    return sum(1 for o in _OBJECTS.values() if o.type[0:4] == platf_type[0:4])
-
-
-def get_list(endpoint):
-    """Reads all DevOps platforms from SonarQube
-
-    :param Platform endpoint: Reference to the SonarQube platform
-    :return: List of DevOps platforms
-    :rtype: dict{<platformKey>: <DevopsPlatform>}
-    """
-    if endpoint.edition() == "community":
-        return _OBJECTS
-    data = json.loads(endpoint.get(APIS["list"]).text)
-    for alm_type in DEVOPS_PLATFORM_TYPES:
-        for alm_data in data.get(alm_type, {}):
-            DevopsPlatform.load(endpoint, alm_type, alm_data)
-    return _OBJECTS
-
-
-def get_object(devops_platform_key, endpoint):
-    """
-    :param Platform endpoint: Reference to the SonarQube platform
-    :param str devops_platform_key: Key of the platform (its name)
-    :return: The DevOps platforms corresponding to key, or None if not found
-    :rtype: DevopsPlatform
-    """
-    if len(_OBJECTS) == 0:
-        get_list(endpoint)
-    return DevopsPlatform.read(endpoint, devops_platform_key)
-
-
-def exists(devops_platform_key, endpoint):
-    """
-    :param Platform endpoint: Reference to the SonarQube platform
-    :param str devops_platform_key: Key of the platform (its name)
-    :return: Whether the platform exists
-    :rtype: bool
-    """
-    return get_object(devops_platform_key, endpoint) is not None
-
-
-def export(endpoint, full=False):
-    """
-    :meta private:
-    """
-    util.logger.info("Exporting DevOps integration settings")
-    json_data = {}
-    for s in get_list(endpoint).values():
-        json_data[s.uuid()] = s.to_json(full)
-        json_data[s.uuid()].pop("key")
-    return json_data
-
-
-def import_config(endpoint, config_data):
-    """
-    :meta private:
-    """
-    devops_settings = config_data.get("devopsIntegration", {})
-    if len(devops_settings) == 0:
-        util.logger.info("No devops integration settings in config, skipping import...")
-        return
-    if endpoint.edition() == "community":
-        util.logger.warning("Can't import devops integration settings on a community edition")
-        return
-    util.logger.info("Importing devops integration settings")
-    if len(_OBJECTS) == 0:
-        get_list(endpoint)
-    for name, data in devops_settings.items():
-        try:
-            o = DevopsPlatform.read(endpoint, name)
-        except exceptions.ObjectNotFound:
-            info = data["workspace"] if data["type"] == "bitbucketcloud" else data["url"]
-            o = DevopsPlatform.create(key=name, endpoint=endpoint, plt_type=data["type"], url_or_workspace=info)
-        o.update(**data)
-
-
-def devops_type(platform_key, endpoint):
-    """
-    :return: The type of a DevOps platform (see DEVOPS_PLATFORM_TYPES), or None if not found
-    :rtype: str or None
-    """
-    o = get_object(platform_key, endpoint)
-    if o is None:
-        return None
-    return o.type
-
-
-def platform_exists(platform_key, endpoint):
-    """
-    :param Platform endpoint: Reference to the SonarQube platform
-    :param str platform_key: Key of the platform (its name)
-    :return: Whether the platform exists in SonarQube
-    :rtype: bool
-    """
-    return get_object(platform_key, endpoint) is not None
+#
+# sonar-tools
+# Copyright (C) 2022-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+
+
+from http import HTTPStatus
+import json
+
+from requests.exceptions import HTTPError
+
+from sonar import sqobject, exceptions
+import sonar.utilities as util
+
+#: DevOps platform types in SonarQube
+DEVOPS_PLATFORM_TYPES = ("github", "azure", "bitbucket", "bitbucketcloud", "gitlab")
+
+
+_OBJECTS = {}
+
+_CREATE_API_GITHUB = "alm_settings/create_github"
+_CREATE_API_GITLAB = "alm_settings/create_gitlab"
+_CREATE_API_AZURE = "alm_settings/create_azure"
+_CREATE_API_BITBUCKET = "alm_settings/create_bitbucket"
+_CREATE_API_BBCLOUD = "alm_settings/create_bitbucketcloud"
+APIS = {"list": "alm_settings/list_definitions"}
+
+_TO_BE_SET = "TO_BE_SET"
+_IMPORTABLE_PROPERTIES = ("key", "type", "url", "workspace", "clientId", "appId")
+
+
+class DevopsPlatform(sqobject.SqObject):
+    """
+    Abstraction of the SonarQube ALM/DevOps Platform concept
+    """
+
+    @classmethod
+    def read(cls, endpoint, key):
+        if key in _OBJECTS:
+            return _OBJECTS[key]
+        data = json.loads(endpoint.get(APIS["list"]).text)
+        for plt_type, platforms in data.items():
+            for p in platforms:
+                if p["key"] == key:
+                    return cls.load(endpoint, plt_type, data)
+        raise exceptions.ObjectNotFound(key, f"DevOps platform key '{key}' not found")
+
+    @classmethod
+    def load(cls, endpoint, plt_type, data):
+        key = data["key"]
+        if key in _OBJECTS:
+            return _OBJECTS[key]
+        o = DevopsPlatform(key, endpoint, plt_type)
+        return o._load(data)
+
+    @classmethod
+    def create(cls, endpoint, key, plt_type, url_or_workspace):
+        params = {"key": key}
+        try:
+            if plt_type == "github":
+                params.update(
+                    {"appId": _TO_BE_SET, "clientId": _TO_BE_SET, "clientSecret": _TO_BE_SET, "privateKey": _TO_BE_SET, "url": url_or_workspace}
+                )
+                endpoint.post(_CREATE_API_GITHUB, params=params)
+            elif plt_type == "azure":
+                # TODO: pass secrets on the cmd line
+                params.update({"personalAccessToken": _TO_BE_SET, "url": url_or_workspace})
+                endpoint.post(_CREATE_API_AZURE, params=params)
+            elif plt_type == "gitlab":
+                params.update({"personalAccessToken": _TO_BE_SET, "url": url_or_workspace})
+                endpoint.post(_CREATE_API_GITLAB, params=params)
+            elif plt_type == "bitbucket":
+                params.update({"personalAccessToken": _TO_BE_SET, "url": url_or_workspace})
+                endpoint.post(_CREATE_API_BITBUCKET, params=params)
+            elif plt_type == "bitbucketcloud":
+                params.update({"clientSecret": _TO_BE_SET, "clientId": _TO_BE_SET, "workspace": url_or_workspace})
+                endpoint.post(_CREATE_API_BBCLOUD, params=params)
+        except HTTPError as e:
+            if e.response.status_code == HTTPStatus.BAD_REQUEST and endpoint.edition() == "developer":
+                util.logger.warning("Can't set DevOps platform '%s', don't you have more that 1 of that type?", key)
+                raise exceptions.UnsupportedOperation(f"Can't set DevOps platform '{key}', don't you have more that 1 of that type?")
+            raise
+        o = DevopsPlatform(key, endpoint, plt_type)
+        o.refresh()
+        return o
+
+    def __init__(self, key, endpoint, platform_type):
+        super().__init__(key, endpoint)
+        self.type = platform_type  #: DevOps platform type
+        self.url = None  #: DevOps platform URL
+        self._specific = None  #: DevOps platform specific settings
+        _OBJECTS[key] = self
+        util.logger.debug("Created object %s", str(self))
+
+    def _load(self, data):
+        self._json = data
+        self.url = "https://bitbucket.org" if self.type == "bitbucketcloud" else data["url"]
+        self._specific = data.copy()
+        for k in ("key", "url"):
+            self._specific.pop(k, None)
+        return self
+
+    def __str__(self):
+        string = f"devops platform '{self.key}'"
+        if self.type == "bitbucketcloud" and self._specific:
+            string += f" workspace '{self._specific['workspace']}'"
+        return string
+
+    def refresh(self):
+        """Reads / Refresh a DevOps platform information
+
+        :return: Whether the operation succeeded
+        :rtype: bool
+        """
+        data = json.loads(self.get(APIS["list"]).text)
+        for alm_data in data.get(self.type, {}):
+            if alm_data["key"] != self.key:
+                self._json = alm_data
+                return True
+        return False
+
+    def to_json(self, full=False):
+        """Exports a DevOps platform configuration in JSON format
+
+        :param full: Whether to export all properties, including those that can't be set, or not, defaults to False
+        :type full: bool, optional
+        :return: The configuration of the DevOps platform (except secrets)
+        :rtype: dict
+        """
+        json_data = self._json.copy()
+        json_data.update({"key": self.key, "type": self.type, "url": self.url})
+        return util.filter_export(json_data, _IMPORTABLE_PROPERTIES, full)
+
+    def set_pat(self, pat, user_name=None):
+        if self.type == "github":
+            util.logger.warning("Can't set PAT for GitHub devops platform")
+            return False
+        return self.post("alm_integrations/set_pat", params={"almSettings": self.key, "pat": pat, "username": user_name}).ok
+
+    def update(self, **kwargs):
+        """Updates a DevOps platform with information from data
+
+        :param dict data: data to update the DevOps platform configuration
+                          (url, clientId, workspace, appId depending on the type of platform)
+        :return: Whether the operation succeeded
+        :rtype: bool
+        """
+        alm_type = kwargs["type"]
+        if alm_type != self.type:
+            util.logger.error("DevOps platform type '%s' for update of %s is incompatible", alm_type, str(self))
+            return False
+
+        params = {"key": self.key, "url": kwargs["url"]}
+        if alm_type == "bitbucketcloud":
+            params.update({"clientId": kwargs["clientId"], "workspace": kwargs["workspace"]})
+        elif alm_type == "github":
+            params.update({"clientId": kwargs["clientId"], "appId": kwargs["appId"]})
+
+        self.post(f"alm_settings/update_{alm_type}", params=params)
+        self.url = kwargs["url"]
+        for k in ("key", "url"):
+            params.pop(k)
+        self._specific = params
+        return self
+
+
+def count(platf_type=None):
+    """
+    :param platf_type: Filter for a specific type, defaults to None (see DEVOPS_PLATFORM_TYPES set)
+    :type platf_type: str or None
+    :return: Count of DevOps platforms
+    :rtype: int
+    """
+    if platf_type is None:
+        return len(_OBJECTS)
+    # Hack: check first 5 chars to that bitbucket cloud and bitbucket server match
+    return sum(1 for o in _OBJECTS.values() if o.type[0:4] == platf_type[0:4])
+
+
+def get_list(endpoint):
+    """Reads all DevOps platforms from SonarQube
+
+    :param Platform endpoint: Reference to the SonarQube platform
+    :return: List of DevOps platforms
+    :rtype: dict{<platformKey>: <DevopsPlatform>}
+    """
+    if endpoint.edition() == "community":
+        return _OBJECTS
+    data = json.loads(endpoint.get(APIS["list"]).text)
+    for alm_type in DEVOPS_PLATFORM_TYPES:
+        for alm_data in data.get(alm_type, {}):
+            DevopsPlatform.load(endpoint, alm_type, alm_data)
+    return _OBJECTS
+
+
+def get_object(devops_platform_key, endpoint):
+    """
+    :param Platform endpoint: Reference to the SonarQube platform
+    :param str devops_platform_key: Key of the platform (its name)
+    :return: The DevOps platforms corresponding to key, or None if not found
+    :rtype: DevopsPlatform
+    """
+    if len(_OBJECTS) == 0:
+        get_list(endpoint)
+    return DevopsPlatform.read(endpoint, devops_platform_key)
+
+
+def exists(devops_platform_key, endpoint):
+    """
+    :param Platform endpoint: Reference to the SonarQube platform
+    :param str devops_platform_key: Key of the platform (its name)
+    :return: Whether the platform exists
+    :rtype: bool
+    """
+    return get_object(devops_platform_key, endpoint) is not None
+
+
+def export(endpoint, full=False):
+    """
+    :meta private:
+    """
+    util.logger.info("Exporting DevOps integration settings")
+    json_data = {}
+    for s in get_list(endpoint).values():
+        json_data[s.uuid()] = s.to_json(full)
+        json_data[s.uuid()].pop("key")
+    return json_data
+
+
+def import_config(endpoint, config_data):
+    """
+    :meta private:
+    """
+    devops_settings = config_data.get("devopsIntegration", {})
+    if len(devops_settings) == 0:
+        util.logger.info("No devops integration settings in config, skipping import...")
+        return
+    if endpoint.edition() == "community":
+        util.logger.warning("Can't import devops integration settings on a community edition")
+        return
+    util.logger.info("Importing devops integration settings")
+    if len(_OBJECTS) == 0:
+        get_list(endpoint)
+    for name, data in devops_settings.items():
+        try:
+            o = DevopsPlatform.read(endpoint, name)
+        except exceptions.ObjectNotFound:
+            info = data["workspace"] if data["type"] == "bitbucketcloud" else data["url"]
+            o = DevopsPlatform.create(key=name, endpoint=endpoint, plt_type=data["type"], url_or_workspace=info)
+        o.update(**data)
+
+
+def devops_type(platform_key, endpoint):
+    """
+    :return: The type of a DevOps platform (see DEVOPS_PLATFORM_TYPES), or None if not found
+    :rtype: str or None
+    """
+    o = get_object(platform_key, endpoint)
+    if o is None:
+        return None
+    return o.type
+
+
+def platform_exists(platform_key, endpoint):
+    """
+    :param Platform endpoint: Reference to the SonarQube platform
+    :param str platform_key: Key of the platform (its name)
+    :return: Whether the platform exists in SonarQube
+    :rtype: bool
+    """
+    return get_object(platform_key, endpoint) is not None
```

## sonar/exceptions.py

 * *Ordering differences only*

```diff
@@ -1,51 +1,51 @@
-#
-# sonar-tools
-# Copyright (C) 2022-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-
-
-class SonarException(Exception):
-    def __init__(self, message):
-        super().__init__()
-        self.message = message
-
-
-class ObjectNotFound(SonarException):
-    """
-    Object not found during a SonarQube search
-    """
-
-    def __init__(self, key, message):
-        super().__init__(message)
-        self.key = key
-
-
-class ObjectAlreadyExists(SonarException):
-    """
-    Object already exists when trying to create it
-    """
-
-    def __init__(self, key, message):
-        super().__init__(message)
-        self.key = key
-
-
-class UnsupportedOperation(SonarException):
-    """
-    Unsupported operation (most often due to edition not allowing it)
-    """
+#
+# sonar-tools
+# Copyright (C) 2022-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+
+
+class SonarException(Exception):
+    def __init__(self, message):
+        super().__init__()
+        self.message = message
+
+
+class ObjectNotFound(SonarException):
+    """
+    Object not found during a SonarQube search
+    """
+
+    def __init__(self, key, message):
+        super().__init__(message)
+        self.key = key
+
+
+class ObjectAlreadyExists(SonarException):
+    """
+    Object already exists when trying to create it
+    """
+
+    def __init__(self, key, message):
+        super().__init__(message)
+        self.key = key
+
+
+class UnsupportedOperation(SonarException):
+    """
+    Unsupported operation (most often due to edition not allowing it)
+    """
```

## sonar/groups.py

```diff
@@ -1,360 +1,360 @@
-#
-# sonar-tools
-# Copyright (C) 2022-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-
-import sonar.sqobject as sq
-import sonar.utilities as util
-from sonar import exceptions
-
-from sonar.audit import rules, problem
-
-SONAR_USERS = "sonar-users"
-
-_SEARCH_API = "user_groups/search"
-_CREATE_API = "user_groups/create"
-_UPDATE_API = "user_groups/update"
-ADD_USER_API = "user_groups/add_user"
-REMOVE_USER_API = "user_groups/remove_user"
-
-_OBJECTS = {}
-_MAP = {}
-
-
-class Group(sq.SqObject):
-    """
-    Abstraction of the SonarQube "group" concept.
-    Objects of this class must be created with one of the 3 available class methods. Don't use __init__
-    """
-
-    def __init__(self, endpoint, name, data):
-        """Do not use, use class methods to create objects"""
-        super().__init__(data.get("id", name), endpoint)
-        self.name = name  #: Group name
-        self.description = data.get("description", "")  #: Group description
-        self.__members_count = data.get("membersCount", None)
-        self.__is_default = data.get("default", None)
-        self._json = data
-        _OBJECTS[self.key] = self
-        _MAP[self.name] = self.key
-        util.logger.debug("Created %s object", str(self))
-
-    @classmethod
-    def read(cls, endpoint, name):
-        """Creates a Group object corresponding to the group with same name in SonarQube
-        :param Platform endpoint: Reference to the SonarQube platform
-        :type endpoint:
-        :param str name: Group name
-        :raises ObjectNotFound: if group name not found
-        :return: The group object
-        :rtype: Group or None if not found
-        """
-        util.logger.debug("Reading group '%s'", name)
-        if name in _MAP:
-            return _OBJECTS[_MAP[name]]
-        data = util.search_by_name(endpoint, name, _SEARCH_API, "groups")
-        if data is None:
-            raise exceptions.UnsupportedOperation(f"Group '{name}' not found.")
-        # SonarQube 10 compatibility: "id" field is dropped, use "name" instead
-        key = data.get("id", data["name"])
-        if key in _OBJECTS:
-            return _OBJECTS[key]
-        return cls(endpoint, name, data=data)
-
-    @classmethod
-    def create(cls, endpoint, name, description=None):
-        """Creates a new group in SonarQube and returns the corresponding Group object
-
-        :param endpoint: Reference to the SonarQube platform
-        :type endpoint: Platform
-        :param name: Group name
-        :type name: str
-        :param description: Group description
-        :type description: str, optional
-        :return: The group object
-        :rtype: Group or None
-        """
-        util.logger.debug("Creating group '%s'", name)
-        endpoint.post(_CREATE_API, params={"name": name, "description": description})
-        return cls.read(endpoint=endpoint, name=name)
-
-    @classmethod
-    def load(cls, endpoint, data):
-        """Creates a Group object from the result of a SonarQube API group search data
-
-        :param endpoint: Reference to the SonarQube platform
-        :type endpoint: Platform
-        :param data: The JSON data corresponding to the group
-        :type data: dict
-        :return: The group object
-        :rtype: Group or None
-        """
-        return cls(name=data["name"], endpoint=endpoint, data=data)
-
-    def __str__(self):
-        """
-        :return: String formatting of the object
-        :rtype: str
-        """
-        return f"group '{self.name}'"
-
-    def is_default(self):
-        """
-        :return: whether the group is a default group (sonar-users only for now) or not
-        :rtype: bool
-        """
-        return self.__is_default
-
-    def size(self):
-        """
-        :return: Number of users members of the group
-        :rtype: int
-        """
-        return self.__members_count
-
-    def url(self):
-        """
-        :return: the SonarQube permalink URL to the group, actually the global groups page only
-                 since this is as close as we can get to the precise group definition
-        :rtype: str
-        """
-        return f"{self.endpoint.url}/admin/groups"
-
-    def add_user(self, user_login):
-        """Adds a user in the group
-
-        :param user_login: User login
-        :type user_login: str
-        :return: Whether the operation succeeded
-        :rtype: bool
-        """
-        return self.post(ADD_USER_API, params={"login": user_login, "name": self.name}).ok
-
-    def remove_user(self, user_login):
-        """Removes a user from the group
-
-        :param user_login: User login
-        :type user_login: str
-        :return: Whether the operation succeeded
-        :rtype: bool
-        """
-        return self.post(REMOVE_USER_API, params={"login": user_login, "name": self.name}).ok
-
-    def audit(self, audit_settings=None):
-        """Audits a group and return list of problems found
-        Current audit is limited to verifying that the group is not empty
-
-        :param audit_settings: Options of what to audit and thresholds to raise problems, default to None
-        :type audit_settings: dict, optional
-        :return: List of problems found, or empty list
-        :rtype: list[Problem]
-        """
-        util.logger.debug("Auditing %s", str(self))
-        problems = []
-        if audit_settings["audit.groups.empty"] and self.__members_count == 0:
-            rule = rules.get_rule(rules.RuleId.GROUP_EMPTY)
-            problems = [problem.Problem(rule.type, rule.severity, rule.msg.format(str(self)), concerned_object=self)]
-        return problems
-
-    def to_json(self, full_specs=False):
-        """Returns the group properties (name, description, default) as dict
-
-        :param full_specs: Also include properties that are not modifiable, default to False
-        :type full_specs: bool, optional
-        :return: Dict of the group
-        :rtype: dict
-        """
-        if full_specs:
-            json_data = {self.name: self._json}
-        else:
-            json_data = {"name": self.name}
-            json_data["description"] = self.description if self.description and self.description != "" else None
-            if self.__is_default:
-                json_data["default"] = True
-        return util.remove_nones(json_data)
-
-    def set_description(self, description):
-        """Set a group description
-
-        :param description: The new group description
-        :type description: str
-        :return: Whether the new description was successfully set
-        :rtype: bool
-        """
-        if description is None or description == self.description:
-            util.logger.debug("No description to update for %s", str(self))
-            return True
-        util.logger.debug("Updating %s with description = %s", str(self), description)
-        r = self.post(_UPDATE_API, params={"id": self.key, "description": description})
-        if r.ok:
-            self.description = description
-        return r.ok
-
-    def set_name(self, name):
-        """Set a group name
-
-        :param name: The new group name
-        :type name: str
-        :return: Whether the new description was successfully set
-        :rtype: bool
-        """
-        if name is None or name == self.name:
-            util.logger.debug("No name to update for %s", str(self))
-            return True
-        util.logger.debug("Updating %s with name = %s", str(self), name)
-        r = self.post(_UPDATE_API, params={"id": self.key, "name": name})
-        if r.ok:
-            _MAP.pop(self.name, None)
-            self.name = name
-            _MAP[self.name] = self.key
-        return r.ok
-
-
-def search(endpoint, params=None):
-    """Search groups
-
-    :params endpoint: Reference to the SonarQube platform
-    :type endpoint: Platform
-    :param params: List of parameters to narrow down the search, defaults to None
-    :type params: dict, optional
-    :return: dict of groups with group name as key
-    :rtype: dict{name: Group}
-    """
-    return sq.search_objects(api=_SEARCH_API, params=params, key_field="name", returned_field="groups", endpoint=endpoint, object_class=Group)
-
-
-def get_list(endpoint, params=None):
-    """Returns the list of groups
-
-    :params endpoint: Reference to the SonarQube platform
-    :type endpoint: Platform
-    :param params: The group name
-    :type name: str
-    :return: The group data as dict
-    :rtype: dict
-    """
-    util.logger.info("Listing groups")
-    return search(params=params, endpoint=endpoint)
-
-
-def export(endpoint):
-    """Exports all groups configuration as dict
-    Default groups (sonar-users) are not exported
-
-    :param endpoint: reference to the SonarQube platform
-    :type endpoint: Platform
-    :return: list of groups
-    :rtype: dict{name: description}
-    """
-    util.logger.info("Exporting groups")
-    g_list = {}
-    for g_name, g_obj in search(endpoint=endpoint).items():
-        if g_obj.is_default():
-            continue
-        g_list[g_name] = "" if g_obj.description is None else g_obj.description
-    return g_list
-
-
-def audit(audit_settings, endpoint=None):
-    """Audits all groups
-
-    :param audit_settings: Configuration of audit
-    :type audit_settings: dict
-    :param endpoint: reference to the SonarQube platform
-    :type endpoint: Platform
-    :return: list of problems found
-    :rtype: list[Problem]
-    """
-    if not audit_settings["audit.groups"]:
-        util.logger.info("Auditing groups is disabled, skipping...")
-        return []
-    util.logger.info("--- Auditing groups ---")
-    problems = []
-    for _, g in search(endpoint=endpoint).items():
-        problems += g.audit(audit_settings)
-    return problems
-
-
-def get_object(name, endpoint=None):
-    """Returns a group object
-
-    :param name: group name
-    :type name: str
-    :param endpoint: reference to the SonarQube platform
-    :type endpoint: Platform
-    :return: The group
-    :rtype: Group
-    """
-    if len(_OBJECTS) == 0 or name not in _MAP:
-        get_list(endpoint)
-    if name not in _MAP:
-        return None
-    return _OBJECTS[_MAP[name]]
-
-
-def create_or_update(endpoint, name, description):
-    """Creates or updates a group
-
-    :param endpoint: reference to the SonarQube platform
-    :type endpoint: Platform
-    :param name: group name
-    :type name: str
-    :param description: group description
-    :type description: str
-    :return: The group
-    :rtype: Group
-    """
-    o = get_object(endpoint=endpoint, name=name)
-    if o is None:
-        util.logger.debug("Group '%s' does not exist, creating...", name)
-        return Group.create(endpoint, name, description)
-    else:
-        return o.set_description(description)
-
-
-def import_config(endpoint, config_data):
-    """Imports a group configuration in SonarQube
-
-    :param endpoint: reference to the SonarQube platform
-    :type endpoint: Platform
-    :param config_data: the configuration to import
-    :type config_data: dict
-    :return: Nothing
-    """
-    if "groups" not in config_data:
-        util.logger.info("No groups to import")
-        return
-    util.logger.info("Importing groups")
-    for name, data in config_data["groups"].items():
-        if isinstance(data, dict):
-            desc = data["description"]
-        else:
-            desc = data
-        create_or_update(endpoint, name, desc)
-
-
-def exists(group_name, endpoint):
-    """
-    :param group_name: group name to check
-    :type group_name: str
-    :param endpoint: reference to the SonarQube platform
-    :type endpoint: Platform
-    :return: whether the project exists
-    :rtype: bool
-    """
-    return get_object(name=group_name, endpoint=endpoint) is not None
+#
+# sonar-tools
+# Copyright (C) 2022-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+
+import sonar.sqobject as sq
+import sonar.utilities as util
+from sonar import exceptions
+
+from sonar.audit import rules, problem
+
+SONAR_USERS = "sonar-users"
+
+_SEARCH_API = "user_groups/search"
+_CREATE_API = "user_groups/create"
+_UPDATE_API = "user_groups/update"
+ADD_USER_API = "user_groups/add_user"
+REMOVE_USER_API = "user_groups/remove_user"
+
+_OBJECTS = {}
+_MAP = {}
+
+
+class Group(sq.SqObject):
+    """
+    Abstraction of the SonarQube "group" concept.
+    Objects of this class must be created with one of the 3 available class methods. Don't use __init__
+    """
+
+    def __init__(self, endpoint, name, data):
+        """Do not use, use class methods to create objects"""
+        super().__init__(data.get("id", name), endpoint)
+        self.name = name  #: Group name
+        self.description = data.get("description", "")  #: Group description
+        self.__members_count = data.get("membersCount", None)
+        self.__is_default = data.get("default", None)
+        self._json = data
+        _OBJECTS[self.key] = self
+        _MAP[self.name] = self.key
+        util.logger.debug("Created %s object", str(self))
+
+    @classmethod
+    def read(cls, endpoint, name):
+        """Creates a Group object corresponding to the group with same name in SonarQube
+        :param Platform endpoint: Reference to the SonarQube platform
+        :type endpoint:
+        :param str name: Group name
+        :raises ObjectNotFound: if group name not found
+        :return: The group object
+        :rtype: Group or None if not found
+        """
+        util.logger.debug("Reading group '%s'", name)
+        if name in _MAP:
+            return _OBJECTS[_MAP[name]]
+        data = util.search_by_name(endpoint, name, _SEARCH_API, "groups")
+        if data is None:
+            raise exceptions.UnsupportedOperation(f"Group '{name}' not found.")
+        # SonarQube 10 compatibility: "id" field is dropped, use "name" instead
+        key = data.get("id", data["name"])
+        if key in _OBJECTS:
+            return _OBJECTS[key]
+        return cls(endpoint, name, data=data)
+
+    @classmethod
+    def create(cls, endpoint, name, description=None):
+        """Creates a new group in SonarQube and returns the corresponding Group object
+
+        :param endpoint: Reference to the SonarQube platform
+        :type endpoint: Platform
+        :param name: Group name
+        :type name: str
+        :param description: Group description
+        :type description: str, optional
+        :return: The group object
+        :rtype: Group or None
+        """
+        util.logger.debug("Creating group '%s'", name)
+        endpoint.post(_CREATE_API, params={"name": name, "description": description})
+        return cls.read(endpoint=endpoint, name=name)
+
+    @classmethod
+    def load(cls, endpoint, data):
+        """Creates a Group object from the result of a SonarQube API group search data
+
+        :param endpoint: Reference to the SonarQube platform
+        :type endpoint: Platform
+        :param data: The JSON data corresponding to the group
+        :type data: dict
+        :return: The group object
+        :rtype: Group or None
+        """
+        return cls(name=data["name"], endpoint=endpoint, data=data)
+
+    def __str__(self):
+        """
+        :return: String formatting of the object
+        :rtype: str
+        """
+        return f"group '{self.name}'"
+
+    def is_default(self):
+        """
+        :return: whether the group is a default group (sonar-users only for now) or not
+        :rtype: bool
+        """
+        return self.__is_default
+
+    def size(self):
+        """
+        :return: Number of users members of the group
+        :rtype: int
+        """
+        return self.__members_count
+
+    def url(self):
+        """
+        :return: the SonarQube permalink URL to the group, actually the global groups page only
+                 since this is as close as we can get to the precise group definition
+        :rtype: str
+        """
+        return f"{self.endpoint.url}/admin/groups"
+
+    def add_user(self, user_login):
+        """Adds a user in the group
+
+        :param user_login: User login
+        :type user_login: str
+        :return: Whether the operation succeeded
+        :rtype: bool
+        """
+        return self.post(ADD_USER_API, params={"login": user_login, "name": self.name}).ok
+
+    def remove_user(self, user_login):
+        """Removes a user from the group
+
+        :param user_login: User login
+        :type user_login: str
+        :return: Whether the operation succeeded
+        :rtype: bool
+        """
+        return self.post(REMOVE_USER_API, params={"login": user_login, "name": self.name}).ok
+
+    def audit(self, audit_settings=None):
+        """Audits a group and return list of problems found
+        Current audit is limited to verifying that the group is not empty
+
+        :param audit_settings: Options of what to audit and thresholds to raise problems, default to None
+        :type audit_settings: dict, optional
+        :return: List of problems found, or empty list
+        :rtype: list[Problem]
+        """
+        util.logger.debug("Auditing %s", str(self))
+        problems = []
+        if audit_settings.get("audit.groups.empty", True) and self.__members_count == 0:
+            rule = rules.get_rule(rules.RuleId.GROUP_EMPTY)
+            problems = [problem.Problem(rule.type, rule.severity, rule.msg.format(str(self)), concerned_object=self)]
+        return problems
+
+    def to_json(self, full_specs=False):
+        """Returns the group properties (name, description, default) as dict
+
+        :param full_specs: Also include properties that are not modifiable, default to False
+        :type full_specs: bool, optional
+        :return: Dict of the group
+        :rtype: dict
+        """
+        if full_specs:
+            json_data = {self.name: self._json}
+        else:
+            json_data = {"name": self.name}
+            json_data["description"] = self.description if self.description and self.description != "" else None
+            if self.__is_default:
+                json_data["default"] = True
+        return util.remove_nones(json_data)
+
+    def set_description(self, description):
+        """Set a group description
+
+        :param description: The new group description
+        :type description: str
+        :return: Whether the new description was successfully set
+        :rtype: bool
+        """
+        if description is None or description == self.description:
+            util.logger.debug("No description to update for %s", str(self))
+            return True
+        util.logger.debug("Updating %s with description = %s", str(self), description)
+        r = self.post(_UPDATE_API, params={"id": self.key, "description": description})
+        if r.ok:
+            self.description = description
+        return r.ok
+
+    def set_name(self, name):
+        """Set a group name
+
+        :param name: The new group name
+        :type name: str
+        :return: Whether the new description was successfully set
+        :rtype: bool
+        """
+        if name is None or name == self.name:
+            util.logger.debug("No name to update for %s", str(self))
+            return True
+        util.logger.debug("Updating %s with name = %s", str(self), name)
+        r = self.post(_UPDATE_API, params={"id": self.key, "name": name})
+        if r.ok:
+            _MAP.pop(self.name, None)
+            self.name = name
+            _MAP[self.name] = self.key
+        return r.ok
+
+
+def search(endpoint, params=None):
+    """Search groups
+
+    :params endpoint: Reference to the SonarQube platform
+    :type endpoint: Platform
+    :param params: List of parameters to narrow down the search, defaults to None
+    :type params: dict, optional
+    :return: dict of groups with group name as key
+    :rtype: dict{name: Group}
+    """
+    return sq.search_objects(api=_SEARCH_API, params=params, key_field="name", returned_field="groups", endpoint=endpoint, object_class=Group)
+
+
+def get_list(endpoint, params=None):
+    """Returns the list of groups
+
+    :params endpoint: Reference to the SonarQube platform
+    :type endpoint: Platform
+    :param params: The group name
+    :type name: str
+    :return: The group data as dict
+    :rtype: dict
+    """
+    util.logger.info("Listing groups")
+    return search(params=params, endpoint=endpoint)
+
+
+def export(endpoint):
+    """Exports all groups configuration as dict
+    Default groups (sonar-users) are not exported
+
+    :param endpoint: reference to the SonarQube platform
+    :type endpoint: Platform
+    :return: list of groups
+    :rtype: dict{name: description}
+    """
+    util.logger.info("Exporting groups")
+    g_list = {}
+    for g_name, g_obj in search(endpoint=endpoint).items():
+        if g_obj.is_default():
+            continue
+        g_list[g_name] = "" if g_obj.description is None else g_obj.description
+    return g_list
+
+
+def audit(audit_settings, endpoint=None):
+    """Audits all groups
+
+    :param audit_settings: Configuration of audit
+    :type audit_settings: dict
+    :param endpoint: reference to the SonarQube platform
+    :type endpoint: Platform
+    :return: list of problems found
+    :rtype: list[Problem]
+    """
+    if not audit_settings.get("audit.groups", True):
+        util.logger.info("Auditing groups is disabled, skipping...")
+        return []
+    util.logger.info("--- Auditing groups ---")
+    problems = []
+    for _, g in search(endpoint=endpoint).items():
+        problems += g.audit(audit_settings)
+    return problems
+
+
+def get_object(name, endpoint=None):
+    """Returns a group object
+
+    :param name: group name
+    :type name: str
+    :param endpoint: reference to the SonarQube platform
+    :type endpoint: Platform
+    :return: The group
+    :rtype: Group
+    """
+    if len(_OBJECTS) == 0 or name not in _MAP:
+        get_list(endpoint)
+    if name not in _MAP:
+        return None
+    return _OBJECTS[_MAP[name]]
+
+
+def create_or_update(endpoint, name, description):
+    """Creates or updates a group
+
+    :param endpoint: reference to the SonarQube platform
+    :type endpoint: Platform
+    :param name: group name
+    :type name: str
+    :param description: group description
+    :type description: str
+    :return: The group
+    :rtype: Group
+    """
+    o = get_object(endpoint=endpoint, name=name)
+    if o is None:
+        util.logger.debug("Group '%s' does not exist, creating...", name)
+        return Group.create(endpoint, name, description)
+    else:
+        return o.set_description(description)
+
+
+def import_config(endpoint, config_data):
+    """Imports a group configuration in SonarQube
+
+    :param endpoint: reference to the SonarQube platform
+    :type endpoint: Platform
+    :param config_data: the configuration to import
+    :type config_data: dict
+    :return: Nothing
+    """
+    if "groups" not in config_data:
+        util.logger.info("No groups to import")
+        return
+    util.logger.info("Importing groups")
+    for name, data in config_data["groups"].items():
+        if isinstance(data, dict):
+            desc = data["description"]
+        else:
+            desc = data
+        create_or_update(endpoint, name, desc)
+
+
+def exists(group_name, endpoint):
+    """
+    :param group_name: group name to check
+    :type group_name: str
+    :param endpoint: reference to the SonarQube platform
+    :type endpoint: Platform
+    :return: whether the project exists
+    :rtype: bool
+    """
+    return get_object(name=group_name, endpoint=endpoint) is not None
```

## sonar/languages.py

 * *Ordering differences only*

```diff
@@ -1,106 +1,106 @@
-#!/usr/local/bin/python3
-#
-# sonar-tools
-# Copyright (C) 2022-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-
-import json
-from threading import Lock
-from sonar import sqobject, rules
-
-#: List of language APIs
-APIS = {"list": "languages/list"}
-
-_OBJECTS = {}
-_CLASS_LOCK = Lock()
-
-
-class Language(sqobject.SqObject):
-    def __init__(self, endpoint, key, name):
-        super().__init__(key, endpoint)
-        self.name = name  #: Language name
-        self._nb_rules = {"ALL": None, "BUG": None, "VULNERABILITY": None, "COD_SMELL": None, "SECURITY_HOTSPOT": None}
-        _OBJECTS[key] = self
-
-    @classmethod
-    def load(cls, endpoint, data):
-        return _OBJECTS.get(data["key"], cls(endpoint=endpoint, key=data["key"], name=data["name"]))
-
-    @classmethod
-    def read(cls, endpoint, key):
-        """Reads a language and return the corresponding object
-        :return: Language object
-        :rtype: Language or None if not found
-        """
-        get_list(endpoint)
-        return _OBJECTS.get(key, None)
-
-    def number_of_rules(self, rule_type=None):
-        """Count rules in the language, optionally filtering on rule type
-
-        :param rule_type: Rule type to filter on, defaults to None
-        :type rule_type: str
-        :returns: Nbr of rules for that language (and optional type)
-        :rtype: int
-        """
-        if not rule_type or rule_type not in rules.TYPES:
-            r_ndx = "_all"
-        if not self._nb_rules[r_ndx]:
-            self._nb_rules[r_ndx] = rules.search(self.endpoint, languages=self.key, types=rule_type)
-        return self._nb_rules[r_ndx]
-
-
-def read_list(endpoint):
-    """Reads the list of languages existing on the SonarQube platform
-    :param endpoint: Reference of the SonarQube platform
-    :type endpoint: Platform
-    :return: List of languages
-    :rtype: dict{<language_key>: <language_name>}
-    """
-    data = json.loads(endpoint.get(APIS["list"]).text)
-    for lang in data["languages"]:
-        _OBJECTS[lang["key"]] = lang["name"]
-    return _OBJECTS
-
-
-def get_list(endpoint, use_cache=True):
-    """Gets the list of languages existing on the SonarQube platform
-    Unlike read_list, get_list() is using a local cache if available (so no API call)
-    :param endpoint: Reference of the SonarQube platform
-    :type endpoint: Platform
-    :param use_cache: Whether to use local cache or query SonarQube, default True (use cache)
-    :type use_cache: bool
-    :return: List of languages
-    :rtype: dict{<language_key>: <language_name>}
-    """
-    with _CLASS_LOCK:
-        if len(_OBJECTS) == 0 or not use_cache:
-            read_list(endpoint)
-    return _OBJECTS
-
-
-def exists(endpoint, language):
-    """Returns whether a language exists
-    :param endpoint: Reference of the SonarQube platform
-    :type endpoint: Platform
-    :param language: The language key
-    :type language: str
-    :return: Whether the language exists
-    :rtype: dict{<language_key>: <language_name>}
-    """
-    return language in get_list(endpoint)
+#!/usr/local/bin/python3
+#
+# sonar-tools
+# Copyright (C) 2022-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+
+import json
+from threading import Lock
+from sonar import sqobject, rules
+
+#: List of language APIs
+APIS = {"list": "languages/list"}
+
+_OBJECTS = {}
+_CLASS_LOCK = Lock()
+
+
+class Language(sqobject.SqObject):
+    def __init__(self, endpoint, key, name):
+        super().__init__(key, endpoint)
+        self.name = name  #: Language name
+        self._nb_rules = {"ALL": None, "BUG": None, "VULNERABILITY": None, "COD_SMELL": None, "SECURITY_HOTSPOT": None}
+        _OBJECTS[key] = self
+
+    @classmethod
+    def load(cls, endpoint, data):
+        return _OBJECTS.get(data["key"], cls(endpoint=endpoint, key=data["key"], name=data["name"]))
+
+    @classmethod
+    def read(cls, endpoint, key):
+        """Reads a language and return the corresponding object
+        :return: Language object
+        :rtype: Language or None if not found
+        """
+        get_list(endpoint)
+        return _OBJECTS.get(key, None)
+
+    def number_of_rules(self, rule_type=None):
+        """Count rules in the language, optionally filtering on rule type
+
+        :param rule_type: Rule type to filter on, defaults to None
+        :type rule_type: str
+        :returns: Nbr of rules for that language (and optional type)
+        :rtype: int
+        """
+        if not rule_type or rule_type not in rules.TYPES:
+            r_ndx = "_all"
+        if not self._nb_rules[r_ndx]:
+            self._nb_rules[r_ndx] = rules.search(self.endpoint, languages=self.key, types=rule_type)
+        return self._nb_rules[r_ndx]
+
+
+def read_list(endpoint):
+    """Reads the list of languages existing on the SonarQube platform
+    :param endpoint: Reference of the SonarQube platform
+    :type endpoint: Platform
+    :return: List of languages
+    :rtype: dict{<language_key>: <language_name>}
+    """
+    data = json.loads(endpoint.get(APIS["list"]).text)
+    for lang in data["languages"]:
+        _OBJECTS[lang["key"]] = lang["name"]
+    return _OBJECTS
+
+
+def get_list(endpoint, use_cache=True):
+    """Gets the list of languages existing on the SonarQube platform
+    Unlike read_list, get_list() is using a local cache if available (so no API call)
+    :param endpoint: Reference of the SonarQube platform
+    :type endpoint: Platform
+    :param use_cache: Whether to use local cache or query SonarQube, default True (use cache)
+    :type use_cache: bool
+    :return: List of languages
+    :rtype: dict{<language_key>: <language_name>}
+    """
+    with _CLASS_LOCK:
+        if len(_OBJECTS) == 0 or not use_cache:
+            read_list(endpoint)
+    return _OBJECTS
+
+
+def exists(endpoint, language):
+    """Returns whether a language exists
+    :param endpoint: Reference of the SonarQube platform
+    :type endpoint: Platform
+    :param language: The language key
+    :type language: str
+    :return: Whether the language exists
+    :rtype: dict{<language_key>: <language_name>}
+    """
+    return language in get_list(endpoint)
```

## sonar/measures.py

 * *Ordering differences only*

```diff
@@ -1,256 +1,256 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-
-import json
-import re
-from sonar import metrics
-import sonar.utilities as util
-import sonar.sqobject as sq
-
-DATETIME_METRICS = ("last_analysis", "createdAt", "updatedAt", "creation_date", "modification_date")
-
-
-class Measure(sq.SqObject):
-    """
-    Abstraction of the SonarQube "measure" concept
-    """
-
-    API_READ = "measures/component"
-    API_HISTORY = "measures/search_history"
-
-    @classmethod
-    def load(cls, concerned_object, data):
-        """Loads a measure from data
-
-        :param endpoint: Reference to SonarQube platform
-        :type endpoint: Platform
-        :paramm data: Data retrieved from a measure search
-        :type data: dict
-        :return: The created measure
-        :rtype: Measure
-        """
-        metrics.search(concerned_object.endpoint)
-        return cls(key=data["metric"], value=_search_value(data), concerned_object=concerned_object)
-
-    def __init__(self, key, value, concerned_object):
-        super().__init__(key, concerned_object.endpoint)
-        self.value = None  #: Measure value
-        self.metric = key  #: Measure metric
-        self.concerned_object = concerned_object  #: Object concerned by the measure
-        self.value = util.string_to_date(value) if self.metric in DATETIME_METRICS else util.convert_to_type(value)
-
-    def refresh(self):
-        """Refreshes a measure by re-reading it in SonarQube
-
-        :return: The new measure value
-        :rtype: int or float or str
-        """
-        params = util.replace_keys(("project", "application", "portfolio"), "component", self.concerned_object.search_params())
-        data = json.loads(self.get(Measure.API_READ, params=params).text)["component"]["measures"]
-        self.value = _search_value(data)
-        return self.value
-
-    def count_history(self, project_key, params=None):
-        if params is None:
-            params = {}
-        params.update({"component": project_key, "metrics": self.metric, "ps": 1})
-        data = json.loads(self.get(Measure.API_HISTORY, params=params).text)
-        return data["paging"]["total"]
-
-    def search_history(self, project_key, params=None):
-        """Searches the history of the metric, for a given project
-
-        :param project_key: Project key
-        :type project_key: str
-        :param params: List of search parameters to narrow down the search, defaults to None
-        :type params: dict
-        :return: The history of the metric, for a given project
-        :rtype dict: {<date_str>: <value>}
-        """
-        __MAX_PAGE_SIZE = 1000
-        measures = {}
-        new_params = {} if params is None else params.copy()
-        new_params.update({"metrics": self.key, "component": project_key})
-        if "ps" not in new_params:
-            new_params["ps"] = __MAX_PAGE_SIZE
-        page, nbr_pages = 1, 1
-        while page <= nbr_pages:
-            data = json.loads(self.get(Measure.API_HISTORY, params=new_params).text)
-            for m in data["measures"][0]["history"]:
-                measures[m["date"]] = m["value"]
-            nbr_pages = util.nbr_pages(data)
-            page += 1
-        return measures
-
-
-def get(concerned_object, metrics_list, **kwargs):
-    """Reads a list of measures of a component (project, branch, pull request, application or portfolio)
-
-    :param concerned_object: Concerned object (project, branch, pull request, application or portfolio)
-    :type concerned_object: Project, Branch, PullRequest, Application or Portfolio
-    :param metrics_list: List of metrics to read
-    :type metrics_list: list
-    :param endpoint: Reference to the SonarQube platform
-    :type endpoint: Platform
-    :param branch: Branch of the component, defaults to None
-    :type branch: str, optional
-    :param pr_key: Pull request key of the component, defaults to None
-    :type pr_key: str, optional
-    :param kwargs: List of filters to search for the measures, defaults to None
-    :type kwargs: dict, optional
-    :return: Dict of found measures
-    :rtype: dict{<metric>: <value>}
-    """
-    params = util.replace_keys(("project", "application", "portfolio"), "component", concerned_object.search_params())
-    params["metricKeys"] = util.list_to_csv(metrics_list)
-    util.logger.debug("Getting measures with %s", str(params))
-
-    data = json.loads(concerned_object.endpoint.get(Measure.API_READ, params={**kwargs, **params}).text)
-    m_dict = {m: None for m in metrics_list}
-    for m in data["component"]["measures"]:
-        m_dict[m["metric"]] = Measure.load(data=m, concerned_object=concerned_object)
-    return m_dict
-
-
-def get_rating_letter(rating):
-    """
-    :params rating:
-    :type rating: int
-    :return: The rating converted from number to letter, if number between 1 and 5, else the unchanged rating
-    :rtype: str
-    """
-    try:
-        n_int = int(float(rating))
-    except ValueError:
-        return rating
-    return chr(n_int + 64) if 1 <= n_int <= 5 else rating
-
-
-def get_rating_number(rating_letter):
-    """
-    :return: The measure converted from letter to number, if letter in [a-eA-E]
-    :rtype: int
-    """
-    if not isinstance(rating_letter, str):
-        return int(rating_letter)
-    l = rating_letter.upper()
-    if l in ("A", "B", "C", "D", "E"):
-        return ord(l) - 64
-    return rating_letter
-
-
-def as_rating_letter(metric, value):
-    """
-    :param metric: Metric key
-    :type metric: str
-    :param value: Measure value to convert
-    :type value: float or int
-    :return: The measure converted from number to letter, if metric is a rating
-    :rtype: str
-    """
-    if metric in metrics.METRICS_BY_TYPE["RATING"] and value not in ("A", "B", "C", "D", "E"):
-        return get_rating_letter(value)
-    return value
-
-
-def as_rating_number(metric, value):
-    """
-    :param metric: Metric key
-    :type metric: str
-    :param value: Measure value to convert
-    :type value: str (A to E)
-    :return: The measure converted from letter to number, if metric is a rating
-    :rtype: int
-    """
-    if metric in metrics.METRICS_BY_TYPE["RATING"]:
-        return get_rating_number(value)
-    return value
-
-
-def as_ratio(metric, value):
-    """Converts a density or ratio metric to float percentage
-    :param metric: Metric key
-    :type metric: str
-    :param value: Measure value to convert
-    :type value: int or float
-    :return: The converted ratio or density
-    :rtype: float between 0 and 1 (0% and 100%), rounded to first decimal
-    """
-    if metric in metrics.METRICS_BY_TYPE["PERCENT"]:
-        try:
-            # Return pct with 3 significant digits
-            value = int(float(value) * 10) / 1000.0
-        except ValueError:
-            pass
-    return value
-
-
-def as_percent(metric, value):
-    """Converts a density or ratio metric to string percentage
-    :param metric: Metric key
-    :type metric: str
-    :param value: Measure value to convert
-    :type value: int or float
-    :return: The converted ratio or density in "x.y%" format
-    :rtype: str
-    """
-    try:
-        if re.match(r".*(ratio|density|coverage)", metric):
-            # Return pct with one digit after decimals
-            value = str(int(float(value) * 10) / 10.0) + "%"
-    except ValueError:
-        pass
-    return value
-
-
-def format(metric, value, ratings="letters", percents="float", dates="datetime"):
-    """Formats any metric in the the preferred format for display
-
-    :param metric: Metric key
-    :type metric: str
-    :param value: Measure value to convert
-    :type value: str, int or float
-    :param ratings: How to convert ratings
-    :type ratings: str, "letters" or "numbers"
-    :param percents: How to convert percentages
-    :type percents: str, "float" or "percents"
-    :param dates: How to convert dates
-    :type dates: str, "datetime" or "dateonly"
-    :return: The formatted measure
-    :rtype: str
-    """
-    if metrics.is_a_rating(metric):
-        value = as_rating_letter(metric, value) if ratings == "letters" else as_rating_number(metric, value)
-    elif metrics.is_a_percent(metric):
-        value = as_percent(metric, value) if percents == "percents" else as_ratio(metric, value)
-    elif dates == "dateonly" and metric in ("last_analysis", "createdAt", "updatedAt", "creation_date", "modification_date"):
-        value = util.date_to_string(util.string_to_date(value), False)
-    return value
-
-
-def _search_value(data):
-    value = data.get("value", None)
-    if not value and "periods" in data:
-        value = data["periods"][0]["value"]
-    elif not value and "period" in data:
-        value = data["period"]["value"]
-    if metrics.is_a_rating(data["metric"]):
-        value = get_rating_letter(value)
-    return value
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+
+import json
+import re
+from sonar import metrics
+import sonar.utilities as util
+import sonar.sqobject as sq
+
+DATETIME_METRICS = ("last_analysis", "createdAt", "updatedAt", "creation_date", "modification_date")
+
+
+class Measure(sq.SqObject):
+    """
+    Abstraction of the SonarQube "measure" concept
+    """
+
+    API_READ = "measures/component"
+    API_HISTORY = "measures/search_history"
+
+    @classmethod
+    def load(cls, concerned_object, data):
+        """Loads a measure from data
+
+        :param endpoint: Reference to SonarQube platform
+        :type endpoint: Platform
+        :paramm data: Data retrieved from a measure search
+        :type data: dict
+        :return: The created measure
+        :rtype: Measure
+        """
+        metrics.search(concerned_object.endpoint)
+        return cls(key=data["metric"], value=_search_value(data), concerned_object=concerned_object)
+
+    def __init__(self, key, value, concerned_object):
+        super().__init__(key, concerned_object.endpoint)
+        self.value = None  #: Measure value
+        self.metric = key  #: Measure metric
+        self.concerned_object = concerned_object  #: Object concerned by the measure
+        self.value = util.string_to_date(value) if self.metric in DATETIME_METRICS else util.convert_to_type(value)
+
+    def refresh(self):
+        """Refreshes a measure by re-reading it in SonarQube
+
+        :return: The new measure value
+        :rtype: int or float or str
+        """
+        params = util.replace_keys(("project", "application", "portfolio"), "component", self.concerned_object.search_params())
+        data = json.loads(self.get(Measure.API_READ, params=params).text)["component"]["measures"]
+        self.value = _search_value(data)
+        return self.value
+
+    def count_history(self, project_key, params=None):
+        if params is None:
+            params = {}
+        params.update({"component": project_key, "metrics": self.metric, "ps": 1})
+        data = json.loads(self.get(Measure.API_HISTORY, params=params).text)
+        return data["paging"]["total"]
+
+    def search_history(self, project_key, params=None):
+        """Searches the history of the metric, for a given project
+
+        :param project_key: Project key
+        :type project_key: str
+        :param params: List of search parameters to narrow down the search, defaults to None
+        :type params: dict
+        :return: The history of the metric, for a given project
+        :rtype dict: {<date_str>: <value>}
+        """
+        __MAX_PAGE_SIZE = 1000
+        measures = {}
+        new_params = {} if params is None else params.copy()
+        new_params.update({"metrics": self.key, "component": project_key})
+        if "ps" not in new_params:
+            new_params["ps"] = __MAX_PAGE_SIZE
+        page, nbr_pages = 1, 1
+        while page <= nbr_pages:
+            data = json.loads(self.get(Measure.API_HISTORY, params=new_params).text)
+            for m in data["measures"][0]["history"]:
+                measures[m["date"]] = m["value"]
+            nbr_pages = util.nbr_pages(data)
+            page += 1
+        return measures
+
+
+def get(concerned_object, metrics_list, **kwargs):
+    """Reads a list of measures of a component (project, branch, pull request, application or portfolio)
+
+    :param concerned_object: Concerned object (project, branch, pull request, application or portfolio)
+    :type concerned_object: Project, Branch, PullRequest, Application or Portfolio
+    :param metrics_list: List of metrics to read
+    :type metrics_list: list
+    :param endpoint: Reference to the SonarQube platform
+    :type endpoint: Platform
+    :param branch: Branch of the component, defaults to None
+    :type branch: str, optional
+    :param pr_key: Pull request key of the component, defaults to None
+    :type pr_key: str, optional
+    :param kwargs: List of filters to search for the measures, defaults to None
+    :type kwargs: dict, optional
+    :return: Dict of found measures
+    :rtype: dict{<metric>: <value>}
+    """
+    params = util.replace_keys(("project", "application", "portfolio"), "component", concerned_object.search_params())
+    params["metricKeys"] = util.list_to_csv(metrics_list)
+    util.logger.debug("Getting measures with %s", str(params))
+
+    data = json.loads(concerned_object.endpoint.get(Measure.API_READ, params={**kwargs, **params}).text)
+    m_dict = {m: None for m in metrics_list}
+    for m in data["component"]["measures"]:
+        m_dict[m["metric"]] = Measure.load(data=m, concerned_object=concerned_object)
+    return m_dict
+
+
+def get_rating_letter(rating):
+    """
+    :params rating:
+    :type rating: int
+    :return: The rating converted from number to letter, if number between 1 and 5, else the unchanged rating
+    :rtype: str
+    """
+    try:
+        n_int = int(float(rating))
+    except ValueError:
+        return rating
+    return chr(n_int + 64) if 1 <= n_int <= 5 else rating
+
+
+def get_rating_number(rating_letter):
+    """
+    :return: The measure converted from letter to number, if letter in [a-eA-E]
+    :rtype: int
+    """
+    if not isinstance(rating_letter, str):
+        return int(rating_letter)
+    l = rating_letter.upper()
+    if l in ("A", "B", "C", "D", "E"):
+        return ord(l) - 64
+    return rating_letter
+
+
+def as_rating_letter(metric, value):
+    """
+    :param metric: Metric key
+    :type metric: str
+    :param value: Measure value to convert
+    :type value: float or int
+    :return: The measure converted from number to letter, if metric is a rating
+    :rtype: str
+    """
+    if metric in metrics.METRICS_BY_TYPE["RATING"] and value not in ("A", "B", "C", "D", "E"):
+        return get_rating_letter(value)
+    return value
+
+
+def as_rating_number(metric, value):
+    """
+    :param metric: Metric key
+    :type metric: str
+    :param value: Measure value to convert
+    :type value: str (A to E)
+    :return: The measure converted from letter to number, if metric is a rating
+    :rtype: int
+    """
+    if metric in metrics.METRICS_BY_TYPE["RATING"]:
+        return get_rating_number(value)
+    return value
+
+
+def as_ratio(metric, value):
+    """Converts a density or ratio metric to float percentage
+    :param metric: Metric key
+    :type metric: str
+    :param value: Measure value to convert
+    :type value: int or float
+    :return: The converted ratio or density
+    :rtype: float between 0 and 1 (0% and 100%), rounded to first decimal
+    """
+    if metric in metrics.METRICS_BY_TYPE["PERCENT"]:
+        try:
+            # Return pct with 3 significant digits
+            value = int(float(value) * 10) / 1000.0
+        except ValueError:
+            pass
+    return value
+
+
+def as_percent(metric, value):
+    """Converts a density or ratio metric to string percentage
+    :param metric: Metric key
+    :type metric: str
+    :param value: Measure value to convert
+    :type value: int or float
+    :return: The converted ratio or density in "x.y%" format
+    :rtype: str
+    """
+    try:
+        if re.match(r".*(ratio|density|coverage)", metric):
+            # Return pct with one digit after decimals
+            value = str(int(float(value) * 10) / 10.0) + "%"
+    except ValueError:
+        pass
+    return value
+
+
+def format(metric, value, ratings="letters", percents="float", dates="datetime"):
+    """Formats any metric in the the preferred format for display
+
+    :param metric: Metric key
+    :type metric: str
+    :param value: Measure value to convert
+    :type value: str, int or float
+    :param ratings: How to convert ratings
+    :type ratings: str, "letters" or "numbers"
+    :param percents: How to convert percentages
+    :type percents: str, "float" or "percents"
+    :param dates: How to convert dates
+    :type dates: str, "datetime" or "dateonly"
+    :return: The formatted measure
+    :rtype: str
+    """
+    if metrics.is_a_rating(metric):
+        value = as_rating_letter(metric, value) if ratings == "letters" else as_rating_number(metric, value)
+    elif metrics.is_a_percent(metric):
+        value = as_percent(metric, value) if percents == "percents" else as_ratio(metric, value)
+    elif dates == "dateonly" and metric in ("last_analysis", "createdAt", "updatedAt", "creation_date", "modification_date"):
+        value = util.date_to_string(util.string_to_date(value), False)
+    return value
+
+
+def _search_value(data):
+    value = data.get("value", None)
+    if not value and "periods" in data:
+        value = data["periods"][0]["value"]
+    elif not value and "period" in data:
+        value = data["period"]["value"]
+    if metrics.is_a_rating(data["metric"]):
+        value = get_rating_letter(value)
+    return value
```

## sonar/metrics.py

 * *Ordering differences only*

```diff
@@ -1,212 +1,212 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-
-import json
-from threading import Lock
-from sonar import sqobject, utilities
-
-#: List of what can be considered the main metrics
-MAIN_METRICS = (
-    "bugs",
-    "vulnerabilities",
-    "code_smells",
-    "security_hotspots",
-    "reliability_rating",
-    "security_rating",
-    "sqale_rating",
-    "security_review_rating",
-    "sqale_debt_ratio",
-    "sqale_index",
-    "coverage",
-    "duplicated_lines_density",
-    "security_hotspots_reviewed",
-    "new_bugs",
-    "new_vulnerabilities",
-    "new_code_smells",
-    "new_security_hotspots",
-    "new_reliability_rating",
-    "new_security_rating",
-    "new_maintainability_rating",
-    "new_security_review_rating",
-    "new_sqale_debt_ratio",
-    "new_coverage",
-    "new_duplicated_lines_density",
-    "new_security_hotspots_reviewed",
-    "ncloc",
-)
-
-#: Dict of metric grouped by type (INT, FLOAT, WORK_DUR etc...)
-METRICS_BY_TYPE = {}
-
-#: Metrics API
-APIS = {
-    "search": "metrics/search",
-}
-
-__MAX_PAGE_SIZE = 500
-
-_OBJECTS = {}
-_CLASS_LOCK = Lock()
-_VISIBLE_OBJECTS = {}
-
-
-class Metric(sqobject.SqObject):
-    """
-    Abstraction of the SonarQube "metric" concept
-    """
-
-    def __init__(self, key=None, endpoint=None, data=None):
-        super().__init__(key, endpoint)
-        self.type = None  #: Type (FLOAT, INT, STRING, WORK_DUR...)
-        self.name = None  #: Name
-        self.description = None  #: Description
-        self.domain = None  #: Domain
-        self.direction = None  #: Directory
-        self.qualitative = None  #: Qualitative
-        self.hidden = None  #: Hidden
-        self.custom = None  #: Custom
-        self.__load(data)
-        _OBJECTS[self.key] = self
-
-    def __load(self, data):
-        utilities.logger.debug("Loading metric %s", str(data))
-        self.type = data["type"]
-        self.name = data["name"]
-        self.description = data.get("description", "")
-        self.domain = data.get("domain", "")
-        self.qualitative = data["qualitative"]
-        self.hidden = data["hidden"]
-        self.custom = data.get("custom", None)
-        if not self.hidden:
-            _VISIBLE_OBJECTS[self.key] = self
-        if self.type not in METRICS_BY_TYPE:
-            METRICS_BY_TYPE[self.type] = set()
-        METRICS_BY_TYPE[self.type].add(self.key)
-        return True
-
-    def is_a_rating(self):
-        """
-        :returns: Whether a metric is a rating
-        :rtype: bool
-        """
-        return self.type == "RATING"
-
-    def is_a_percent(self):
-        """
-        :returns: Whether a metric is a percentage (or ratio or density)
-        :rtype: bool
-        """
-        return self.type == "PERCENT"
-
-    def is_an_effort(self):
-        """
-        :returns: Whether a metric is an effort
-        :rtype: bool
-        """
-        return self.type == "WORK_DUR"
-
-    def is_of_type(self, metric_type):
-        """
-        :param metric_type:
-        :type metric_type: str
-        :returns: Whether a metric is of a given type (INT, BOOL, FLOAT, WORK_DUR, etc...)
-        :rtype: bool
-        """
-        return self.type in METRICS_BY_TYPE[metric_type]
-
-
-def is_a_rating(metric_key):
-    """
-    :param metric_key: The concerned metric key
-    :type metric_key: str
-    :returns: Whether a metric is a rating
-    :rtype: bool
-    """
-    return is_of_type(metric_key, "RATING")
-
-
-def search(endpoint, show_hidden_metrics=False, use_cache=True):
-    """
-    :param endpoint: Reference to the SonarQube platform object
-    :type endpoint: Platform
-    :param show_hidden_metrics: Whether to also include hidden (private) metrics
-    :type show_hidden_metrics: bool
-    :param use_cache: Whether to use local cache or query SonarQube, default True (use cache)
-    :type use_cache: bool
-    :return: List of metrics
-    :rtype: dict of Metric
-    """
-    with _CLASS_LOCK:
-        if len(_OBJECTS) == 0 or not use_cache:
-            m_list = {}
-            page, nb_pages = 1, 1
-            while page <= nb_pages:
-                data = json.loads(endpoint.get(APIS["search"], params={"ps": __MAX_PAGE_SIZE, "p": page}).text)
-                for m in data["metrics"]:
-                    m_list[m["key"]] = Metric(key=m["key"], endpoint=endpoint, data=m)
-                nb_pages = utilities.nbr_pages(data)
-                page += 1
-
-    return _OBJECTS if show_hidden_metrics else _VISIBLE_OBJECTS
-
-
-def is_a_percent(metric_key):
-    """
-    :param metric_key: The concerned metric key
-    :type metric_key: str
-    :returns: Whether a metric is a percent
-    :rtype: bool
-    """
-    return is_of_type(metric_key, "PERCENT")
-
-
-def is_an_effort(metric_key):
-    """
-    :param metric_key: The concerned metric key
-    :type metric_key: str
-    :returns: Whether a metric is an effort
-    :rtype: bool
-    """
-    return is_of_type(metric_key, "WORK_DUR")
-
-
-def is_of_type(metric_key, metric_type):
-    """
-    :param metric_key: The concerned metric key
-    :type metric_key: str
-    :param metric_type:
-    :type metric_type: str
-    :returns: Whether a metric is of a given type (INT, BOOL, FLOAT, WORK_DUR, etc...)
-    :rtype: bool
-    """
-    return metric_key in METRICS_BY_TYPE[metric_type]
-
-
-def count(endpoint, use_cache=True):
-    """
-    :param endpoint: Reference to the SonarQube platform object
-    :type endpoint: Platform
-    :returns: Count of public metrics
-    :rtype: int
-    """
-    with _CLASS_LOCK:
-        if len(_OBJECTS) == 0 or not use_cache:
-            search(endpoint, True)
-    return len(_VISIBLE_OBJECTS)
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+
+import json
+from threading import Lock
+from sonar import sqobject, utilities
+
+#: List of what can be considered the main metrics
+MAIN_METRICS = (
+    "bugs",
+    "vulnerabilities",
+    "code_smells",
+    "security_hotspots",
+    "reliability_rating",
+    "security_rating",
+    "sqale_rating",
+    "security_review_rating",
+    "sqale_debt_ratio",
+    "sqale_index",
+    "coverage",
+    "duplicated_lines_density",
+    "security_hotspots_reviewed",
+    "new_bugs",
+    "new_vulnerabilities",
+    "new_code_smells",
+    "new_security_hotspots",
+    "new_reliability_rating",
+    "new_security_rating",
+    "new_maintainability_rating",
+    "new_security_review_rating",
+    "new_sqale_debt_ratio",
+    "new_coverage",
+    "new_duplicated_lines_density",
+    "new_security_hotspots_reviewed",
+    "ncloc",
+)
+
+#: Dict of metric grouped by type (INT, FLOAT, WORK_DUR etc...)
+METRICS_BY_TYPE = {}
+
+#: Metrics API
+APIS = {
+    "search": "metrics/search",
+}
+
+__MAX_PAGE_SIZE = 500
+
+_OBJECTS = {}
+_CLASS_LOCK = Lock()
+_VISIBLE_OBJECTS = {}
+
+
+class Metric(sqobject.SqObject):
+    """
+    Abstraction of the SonarQube "metric" concept
+    """
+
+    def __init__(self, key=None, endpoint=None, data=None):
+        super().__init__(key, endpoint)
+        self.type = None  #: Type (FLOAT, INT, STRING, WORK_DUR...)
+        self.name = None  #: Name
+        self.description = None  #: Description
+        self.domain = None  #: Domain
+        self.direction = None  #: Directory
+        self.qualitative = None  #: Qualitative
+        self.hidden = None  #: Hidden
+        self.custom = None  #: Custom
+        self.__load(data)
+        _OBJECTS[self.key] = self
+
+    def __load(self, data):
+        utilities.logger.debug("Loading metric %s", str(data))
+        self.type = data["type"]
+        self.name = data["name"]
+        self.description = data.get("description", "")
+        self.domain = data.get("domain", "")
+        self.qualitative = data["qualitative"]
+        self.hidden = data["hidden"]
+        self.custom = data.get("custom", None)
+        if not self.hidden:
+            _VISIBLE_OBJECTS[self.key] = self
+        if self.type not in METRICS_BY_TYPE:
+            METRICS_BY_TYPE[self.type] = set()
+        METRICS_BY_TYPE[self.type].add(self.key)
+        return True
+
+    def is_a_rating(self):
+        """
+        :returns: Whether a metric is a rating
+        :rtype: bool
+        """
+        return self.type == "RATING"
+
+    def is_a_percent(self):
+        """
+        :returns: Whether a metric is a percentage (or ratio or density)
+        :rtype: bool
+        """
+        return self.type == "PERCENT"
+
+    def is_an_effort(self):
+        """
+        :returns: Whether a metric is an effort
+        :rtype: bool
+        """
+        return self.type == "WORK_DUR"
+
+    def is_of_type(self, metric_type):
+        """
+        :param metric_type:
+        :type metric_type: str
+        :returns: Whether a metric is of a given type (INT, BOOL, FLOAT, WORK_DUR, etc...)
+        :rtype: bool
+        """
+        return self.type in METRICS_BY_TYPE[metric_type]
+
+
+def is_a_rating(metric_key):
+    """
+    :param metric_key: The concerned metric key
+    :type metric_key: str
+    :returns: Whether a metric is a rating
+    :rtype: bool
+    """
+    return is_of_type(metric_key, "RATING")
+
+
+def search(endpoint, show_hidden_metrics=False, use_cache=True):
+    """
+    :param endpoint: Reference to the SonarQube platform object
+    :type endpoint: Platform
+    :param show_hidden_metrics: Whether to also include hidden (private) metrics
+    :type show_hidden_metrics: bool
+    :param use_cache: Whether to use local cache or query SonarQube, default True (use cache)
+    :type use_cache: bool
+    :return: List of metrics
+    :rtype: dict of Metric
+    """
+    with _CLASS_LOCK:
+        if len(_OBJECTS) == 0 or not use_cache:
+            m_list = {}
+            page, nb_pages = 1, 1
+            while page <= nb_pages:
+                data = json.loads(endpoint.get(APIS["search"], params={"ps": __MAX_PAGE_SIZE, "p": page}).text)
+                for m in data["metrics"]:
+                    m_list[m["key"]] = Metric(key=m["key"], endpoint=endpoint, data=m)
+                nb_pages = utilities.nbr_pages(data)
+                page += 1
+
+    return _OBJECTS if show_hidden_metrics else _VISIBLE_OBJECTS
+
+
+def is_a_percent(metric_key):
+    """
+    :param metric_key: The concerned metric key
+    :type metric_key: str
+    :returns: Whether a metric is a percent
+    :rtype: bool
+    """
+    return is_of_type(metric_key, "PERCENT")
+
+
+def is_an_effort(metric_key):
+    """
+    :param metric_key: The concerned metric key
+    :type metric_key: str
+    :returns: Whether a metric is an effort
+    :rtype: bool
+    """
+    return is_of_type(metric_key, "WORK_DUR")
+
+
+def is_of_type(metric_key, metric_type):
+    """
+    :param metric_key: The concerned metric key
+    :type metric_key: str
+    :param metric_type:
+    :type metric_type: str
+    :returns: Whether a metric is of a given type (INT, BOOL, FLOAT, WORK_DUR, etc...)
+    :rtype: bool
+    """
+    return metric_key in METRICS_BY_TYPE[metric_type]
+
+
+def count(endpoint, use_cache=True):
+    """
+    :param endpoint: Reference to the SonarQube platform object
+    :type endpoint: Platform
+    :returns: Count of public metrics
+    :rtype: int
+    """
+    with _CLASS_LOCK:
+        if len(_OBJECTS) == 0 or not use_cache:
+            search(endpoint, True)
+    return len(_VISIBLE_OBJECTS)
```

## sonar/options.py

```diff
@@ -1,71 +1,74 @@
-#
-# sonar-tools
-# Copyright (C) 2022-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-"""
-
-    Cmd line options
-
-"""
-
-WITH_URL = "withURL"
-WITH_NAME = "withName"
-WITH_LAST_ANALYSIS = "withLastAnalysis"
-WITH_BRANCHES = "withBranches"
-NBR_THREADS = "threads"
-
-WHAT_SETTINGS = "settings"
-WHAT_USERS = "users"
-WHAT_GROUPS = "groups"
-WHAT_GATES = "qualitygates"
-WHAT_RULES = "rules"
-WHAT_PROFILES = "qualityprofiles"
-WHAT_PROJECTS = "projects"
-WHAT_APPS = "applications"
-WHAT_PORTFOLIOS = "portfolios"
-WHAT_AUDITABLE = [WHAT_SETTINGS, WHAT_USERS, WHAT_GROUPS, WHAT_GATES, WHAT_PROFILES, WHAT_PROJECTS, WHAT_APPS, WHAT_PORTFOLIOS]
-
-CSV_SEPARATOR = "csvSeparator"
-FORMAT = "format"
-
-DEFAULT = "__default__"
-
-ERR_SONAR_API_AUTHENTICATION = 1
-ERR_SONAR_API_AUTHORIZATION = 2
-ERR_SONAR_API = 3
-ERR_TOKEN_MISSING = 4
-
-ERR_NO_SUCH_KEY = 5
-ERR_WRONG_SEARCH_CRITERIA = 6
-ERR_UNSUPPORTED_OPERATION = 7
-ERR_RULES_LOADING_FAILED = 8
-ERR_SIF_AUDIT_ERROR = 9
-
-ERR_ARGS_ERROR = 10
-# if a global analysis or project analysis token is provided
-ERR_TOKEN_NOT_SUITED = 11
-
-
-def set_url_arg(parser):
-    parser.add_argument(f"--{WITH_URL}", action="store_true", default=False, required=False, help="Add objects URLs in report")
-    return parser
-
-
-def add_thread_arg(parser, action):
-    parser.add_argument(f"--{NBR_THREADS}", required=False, type=int, default=8, help=f"Define number of threads for {action}, default 8")
-    return parser
+#
+# sonar-tools
+# Copyright (C) 2022-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+"""
+
+    Cmd line options
+
+"""
+
+WITH_URL = "withURL"
+WITH_NAME = "withName"
+WITH_LAST_ANALYSIS = "withLastAnalysis"
+WITH_BRANCHES = "withBranches"
+NBR_THREADS = "threads"
+
+WHAT_SETTINGS = "settings"
+WHAT_USERS = "users"
+WHAT_GROUPS = "groups"
+WHAT_GATES = "qualitygates"
+WHAT_RULES = "rules"
+WHAT_PROFILES = "qualityprofiles"
+WHAT_PROJECTS = "projects"
+WHAT_APPS = "applications"
+WHAT_PORTFOLIOS = "portfolios"
+WHAT_AUDITABLE = [WHAT_SETTINGS, WHAT_USERS, WHAT_GROUPS, WHAT_GATES, WHAT_PROFILES, WHAT_PROJECTS, WHAT_APPS, WHAT_PORTFOLIOS]
+
+CSV_SEPARATOR = "csvSeparator"
+FORMAT = "format"
+
+DEFAULT = "__default__"
+
+ERR_SONAR_API_AUTHENTICATION = 1
+ERR_SONAR_API_AUTHORIZATION = 2
+ERR_SONAR_API = 3
+ERR_TOKEN_MISSING = 4
+
+ERR_NO_SUCH_KEY = 5
+ERR_WRONG_SEARCH_CRITERIA = 6
+ERR_UNSUPPORTED_OPERATION = 7
+ERR_RULES_LOADING_FAILED = 8
+ERR_SIF_AUDIT_ERROR = 9
+
+ERR_ARGS_ERROR = 10
+# if a global analysis or project analysis token is provided
+ERR_TOKEN_NOT_SUITED = 11
+
+# HTTP request timeout
+ERR_REQUEST_TIMEOUT = 12
+
+
+def set_url_arg(parser):
+    parser.add_argument(f"--{WITH_URL}", action="store_true", default=False, required=False, help="Add objects URLs in report")
+    return parser
+
+
+def add_thread_arg(parser, action):
+    parser.add_argument(f"--{NBR_THREADS}", required=False, type=int, default=8, help=f"Define number of threads for {action}, default 8")
+    return parser
```

## sonar/platform.py

```diff
@@ -1,789 +1,799 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-
-from http import HTTPStatus
-import sys
-import os
-import time
-import datetime
-import json
-import tempfile
-import requests
-import jprops
-from requests.exceptions import HTTPError
-
-import sonar.utilities as util
-
-from sonar import options, settings, devops, webhooks, version
-from sonar.permissions import permissions, global_permissions, permission_templates
-from sonar.audit import rules, config
-import sonar.audit.severities as sev
-import sonar.audit.types as typ
-import sonar.audit.problem as pb
-
-from sonar import sif
-
-WRONG_CONFIG_MSG = "Audit config property %s has wrong value %s, skipping audit"
-
-_NON_EXISTING_SETTING_SKIPPED = "Setting %s does not exist, skipping..."
-_HTTP_ERROR = "%s Error: %s HTTP status code %d"
-
-_SONAR_TOOLS_AGENT = {"user-agent": f"sonar-tools {version.PACKAGE_VERSION}"}
-_UPDATE_CENTER = "https://raw.githubusercontent.com/SonarSource/sonar-update-center-properties/master/update-center-source.properties"
-
-LTS = None
-LATEST = None
-_HARDCODED_LTS = (9, 9, 3)
-_HARDCODED_LATEST = (10, 3, 0)
-
-
-class Platform:
-    """Abstraction of the SonarQube "platform" concept"""
-
-    def __init__(self, some_url, some_token, cert_file=None):
-        """Creates a SonarQube platform object
-
-        :param some_url: base URL of the SonarQube platform
-        :type some_url: str
-        :param some_token: token to connect to the platform
-        :type some_token: str
-        :param cert_file: Client certificate, if any needed, defaults to None
-        :type cert_file: str, optional
-        :return: the SonarQube object
-        :rtype: Platform
-        """
-        self.url = some_url.rstrip("/")  #: SonarQube URL
-        self.__token = some_token
-        self.__cert_file = cert_file
-        self._version = None
-        self.__sys_info = None
-        self.__global_nav = None
-        self._server_id = None
-        self._permissions = None
-
-    def __str__(self):
-        """
-        :return: string representation of the SonarQube connection, with the token recognizable but largely redacted
-        :rtype: str
-        """
-        return f"{util.redacted_token(self.__token)}@{self.url}"
-
-    def __credentials(self):
-        return (self.__token, "")
-
-    def version(self, digits=3, as_string=False):
-        """Returns the SonarQube platform version
-
-        :param digits: Number of digits to include in the version, defaults to 3
-        :type digits: int, optional
-        :param as_string: Whether to return the version as string or tuple, default to False (ie returns a tuple)
-        :type as_string: bool, optional
-        :return: the SonarQube platform version
-        :rtype: tuple or str
-        """
-        if digits < 1 or digits > 3:
-            digits = 3
-        if self._version is None:
-            self._version = self.get("/api/server/version").text.split(".")
-        if as_string:
-            return ".".join(self._version[0:digits])
-        else:
-            util.logger.debug("Version = %s", self._version)
-            return tuple(int(n) for n in self._version[0:digits])
-
-    def edition(self):
-        """
-        :return: the SonarQube platform edition
-        :rtype: str ("community", "developer", "enterprise" or "datacenter")
-        """
-        if self.version() < (9, 7, 0):
-            return self.sys_info()["Statistics"]["edition"]
-        else:
-            return self.global_nav()["edition"]
-
-    def server_id(self):
-        """
-        :return: the SonarQube platform server id
-        :rtype: str
-        """
-        if self._server_id is not None:
-            return self._server_id
-        if self.__sys_info is not None and "Server ID" in self.__sys_info["System"]:
-            self._server_id = self.__sys_info["System"]["Server ID"]
-        else:
-            self._server_id = json.loads(self.get("system/status").text)["id"]
-        return self._server_id
-
-    def basics(self):
-        """
-        :return: the 3 basic information of the platform: ServerId, Edition and Version
-        :rtype: dict{"serverId": <id>, "edition": <edition>, "version": <version>}
-        """
-        return {
-            "version": self.version(as_string=True),
-            "edition": self.edition(),
-            "serverId": self.server_id(),
-        }
-
-    def get(self, api, params=None, exit_on_error=False, mute=()):
-        """Makes an HTTP GET request to SonarQube
-
-        :param api: API to invoke (without the platform base URL)
-        :type api: str
-        :param params: params to pass in the HTTP request, defaults to None
-        :type params: dict, optional
-        :param exit_on_error: When to fail fast and exit if the HTTP status code is not 2XX, defaults to True
-        :type exit_on_error: bool, optional
-        :param mute: Tuple of HTTP Error codes to mute (ie not write an error log for), defaults to None.
-        Typically, Error 404 Not found may be expected sometimes so this can avoid logging an error for 404
-        :type mute: tuple, optional
-        :return: the result of the HTTP request
-        :rtype: request.Response
-        """
-        api = _normalize_api(api)
-        util.logger.debug("GET: %s", self.__urlstring(api, params))
-        try:
-            r = requests.get(url=self.url + api, auth=self.__credentials(), verify=self.__cert_file, headers=_SONAR_TOOLS_AGENT, params=params)
-            r.raise_for_status()
-        except requests.exceptions.HTTPError as e:
-            if exit_on_error or (r.status_code not in mute and r.status_code in (HTTPStatus.UNAUTHORIZED, HTTPStatus.FORBIDDEN)):
-                util.log_and_exit(r)
-            else:
-                if r.status_code in mute:
-                    util.logger.debug(_HTTP_ERROR, "GET", self.__urlstring(api, params), r.status_code)
-                else:
-                    util.logger.error(_HTTP_ERROR, "GET", self.__urlstring(api, params), r.status_code)
-                raise e
-        except requests.RequestException as e:
-            util.exit_fatal(str(e), options.ERR_SONAR_API)
-        return r
-
-    def post(self, api, params=None, exit_on_error=False, mute=()):
-        """Makes an HTTP POST request to SonarQube
-
-        :param api: API to invoke (without the platform base URL)
-        :type api: str
-        :param params: params to pass in the HTTP request, defaults to None
-        :type params: dict, optional
-        :param exit_on_error: When to fail fast and exit if the HTTP status code is not 2XX, defaults to True
-        :type exit_on_error: bool, optional
-        :param mute: HTTP Error codes to mute (ie not write an error log for), defaults to None
-        Typically, Error 404 Not found may be expected sometimes so this can avoid logging an error for 404
-        :type mute: tuple, optional
-        :return: the result of the HTTP request
-        :rtype: request.Response
-        """
-        api = _normalize_api(api)
-        util.logger.debug("POST: %s", self.__urlstring(api, params))
-        try:
-            r = requests.post(url=self.url + api, auth=self.__credentials(), verify=self.__cert_file, headers=_SONAR_TOOLS_AGENT, data=params)
-            r.raise_for_status()
-        except requests.exceptions.HTTPError:
-            if exit_on_error or r.status_code in (HTTPStatus.UNAUTHORIZED, HTTPStatus.FORBIDDEN):
-                util.log_and_exit(r)
-            else:
-                if r.status_code in mute:
-                    util.logger.debug(_HTTP_ERROR, "POST", self.__urlstring(api, params), r.status_code)
-                else:
-                    util.logger.error(_HTTP_ERROR, "POST", self.__urlstring(api, params), r.status_code)
-                raise
-        except requests.RequestException as e:
-            util.exit_fatal(str(e), options.ERR_SONAR_API)
-        return r
-
-    def delete(self, api, params=None, exit_on_error=False, mute=()):
-        """Makes an HTTP DELETE request to SonarQube
-
-        :param api: API to invoke (without the platform base URL)
-        :type api: str
-        :param params: params to pass in the HTTP request, defaults to None
-        :type params: dict, optional
-        :param exit_on_error: When to fail fast and exit if the HTTP status code is not 2XX, defaults to True
-        :type exit_on_error: bool, optional
-        :param mute: HTTP Error codes to mute (ie not write an error log for), defaults to None
-        Typically, Error 404 Not found may be expected sometimes so this can avoid logging an error for 404
-        :type mute: tuple, optional
-        :return: the result of the HTTP request
-        :rtype: request.Response
-        """
-        api = _normalize_api(api)
-        util.logger.debug("DELETE: %s", self.__urlstring(api, params))
-        try:
-            r = requests.delete(url=self.url + api, auth=self.__credentials(), verify=self.__cert_file, params=params, headers=_SONAR_TOOLS_AGENT)
-            r.raise_for_status()
-        except requests.exceptions.HTTPError:
-            if exit_on_error:
-                util.log_and_exit(r)
-            else:
-                if r.status_code in mute:
-                    util.logger.debug(_HTTP_ERROR, "DELETE", self.__urlstring(api, params), r.status_code)
-                else:
-                    util.logger.error(_HTTP_ERROR, "DELETE", self.__urlstring(api, params), r.status_code)
-                raise
-        except requests.RequestException as e:
-            util.exit_fatal(str(e), options.ERR_SONAR_API)
-
-    def global_permissions(self):
-        """Returns the SonarQube platform global permissions
-
-        :return: dict{"users": {<login>: <permissions comma separated>, ...}, "groups"; {<name>: <permissions comma separated>, ...}}}
-        :rtype: dict
-        """
-        if self._permissions is None:
-            self._permissions = global_permissions.GlobalPermissions(self)
-        return self._permissions
-
-    def sys_info(self):
-        """
-        :return: the SonarQube platform system info file
-        :rtype: dict
-        """
-        if self.__sys_info is None:
-            success, counter = False, 0
-            while not success:
-                try:
-                    resp = self.get("system/info", mute=(HTTPStatus.INTERNAL_SERVER_ERROR,))
-                    success = True
-                except HTTPError as e:
-                    # Hack: SonarQube randomly returns Error 500 on this API, retry up to 10 times
-                    if e.response.status_code == HTTPStatus.INTERNAL_SERVER_ERROR and counter < 10:
-                        util.logger.error("HTTP Error 500 for api/system/info, retrying...")
-                        time.sleep(0.5)
-                        counter += 1
-                    else:
-                        raise e
-            self.__sys_info = json.loads(resp.text)
-            success = True
-        return self.__sys_info
-
-    def global_nav(self):
-        """
-        :return: the SonarQube platform global navigation data
-        :rtype: dict
-        """
-        if self.__global_nav is None:
-            resp = self.get("navigation/global", mute=(HTTPStatus.INTERNAL_SERVER_ERROR,))
-            self.__global_nav = json.loads(resp.text)
-        return self.__global_nav
-
-    def database(self):
-        """
-        :return: the SonarQube platform backend database
-        :rtype: str
-        """
-        if self.version() < (9, 7, 0):
-            return self.sys_info()["Statistics"]["database"]["name"]
-        else:
-            return self.sys_info()["Database"]["Database"]
-
-    def plugins(self):
-        """
-        :return: the SonarQube platform plugins
-        :rtype: dict
-        """
-        if self.version() < (9, 7, 0):
-            return self.sys_info()["Statistics"]["plugins"]
-        else:
-            return self.sys_info()["Plugins"]
-
-    def get_settings(self, settings_list=None):
-        """Returns a list of (or all) platform global settings value from their key
-
-        :param key: settings_list
-        :type key: list or str (comma separated)
-        :return: the list of settings values
-        :rtype: dict{<key>: <value>, ...}
-        """
-        params = util.remove_nones({"keys": util.list_to_csv(settings_list)})
-        resp = self.get("settings/values", params=params)
-        json_s = json.loads(resp.text)
-        platform_settings = {}
-        for s in json_s["settings"]:
-            if "value" in s:
-                platform_settings[s["key"]] = s["value"]
-            elif "values" in s:
-                platform_settings[s["key"]] = ",".join(s["values"])
-            elif "fieldValues" in s:
-                platform_settings[s["key"]] = s["fieldValues"]
-        return platform_settings
-
-    def __settings(self, settings_list=None, include_not_set=False):
-        util.logger.info("getting global settings")
-        return settings.get_bulk(endpoint=self, settings_list=settings_list, include_not_set=include_not_set)
-
-    def get_setting(self, key):
-        """Returns a platform global setting value from its key
-
-        :param key: Setting key
-        :type key: str
-        :return: the setting value
-        :rtype: str or dict
-        """
-        return self.get_settings(key).get(key, None)
-
-    def reset_setting(self, key):
-        """Resets a platform global setting to the SonarQube internal default value
-
-        :param key: Setting key
-        :type key: str
-        :return: Whether the reset was successful or not
-        :rtype: bool
-        """
-        return settings.reset_setting(self, key).ok
-
-    def set_setting(self, key, value):
-        """Sets a platform global setting
-
-        :param key: Setting key
-        :type key: str
-        :param key: value
-        :type key: str
-        :return: Whether setting the value was successful or not
-        :rtype: bool
-        """
-        return settings.set_setting(self, key, value)
-
-    def __urlstring(self, api, params):
-        """Returns a string corresponding to the URL and parameters"""
-        first = True
-        url_prefix = f"{str(self)}{api}"
-        if params is None:
-            return url_prefix
-        for p in params:
-            if params[p] is None:
-                continue
-            sep = "?" if first else "&"
-            first = False
-            if isinstance(params[p], datetime.date):
-                params[p] = util.format_date(params[p])
-            url_prefix += f"{sep}{p}={requests.utils.quote(str(params[p]))}"
-        return url_prefix
-
-    def webhooks(self):
-        """
-        :return: the list of global webhooks
-        :rtype: dict{<webhook_name>: <webhook_data>, ...}
-        """
-        return webhooks.get_list(self)
-
-    def export(self, full=False):
-        """Exports the global platform properties as JSON
-
-        :param full: Whether to also export properties thatc annot be set, defaults to False
-        :type full: bool, optional
-        :return: dict of all properties with their values
-        :rtype: dict
-        """
-        util.logger.info("Exporting platform global settings")
-        json_data = {}
-        for s in self.__settings(include_not_set=True).values():
-            (categ, subcateg) = s.category()
-            util.update_json(json_data, categ, subcateg, s.to_json())
-
-        json_data[settings.GENERAL_SETTINGS].update({"webhooks": webhooks.export(self, full=full)})
-        json_data["permissions"] = self.global_permissions().export()
-        json_data["permissionTemplates"] = permission_templates.export(self, full=full)
-        json_data[settings.DEVOPS_INTEGRATION] = devops.export(self, full=full)
-        return json_data
-
-    def set_webhooks(self, webhooks_data):
-        """Sets global webhooks with a list of webhooks represented as JSON
-
-        :param webhooks_data: the webhooks representation
-        :type webhooks_data: dict
-        :return: Nothing
-        """
-        if webhooks_data is None:
-            return
-        current_wh = self.webhooks()
-        # FIXME: Handle several webhooks with same name
-        current_wh_names = [wh.name for wh in current_wh.values()]
-        wh_map = {wh.name: k for k, wh in current_wh.items()}
-        util.logger.debug("Current WH %s", str(current_wh_names))
-        for wh_name, wh in webhooks_data.items():
-            util.logger.debug("Updating wh with name %s", wh_name)
-            if wh_name in current_wh_names:
-                current_wh[wh_map[wh_name]].update(name=wh_name, **wh)
-            else:
-                webhooks.update(name=wh_name, endpoint=self, project=None, **wh)
-
-    def import_config(self, config_data):
-        """Imports a whole SonarQube platform global configuration represented as JSON
-
-        :param config_data: the configuration representation
-        :type config_data: dict
-        :return: Nothing
-        """
-        if "globalSettings" not in config_data:
-            util.logger.info("No global settings to import")
-            return
-        config_data = config_data["globalSettings"]
-        for section in ("analysisScope", "authentication", "generalSettings", "linters", "sastConfig", "tests", "thirdParty"):
-            if section not in config_data:
-                continue
-            for setting_key, setting_value in config_data[section].items():
-                if setting_key == "webhooks":
-                    self.set_webhooks(setting_value)
-                else:
-                    self.set_setting(setting_key, setting_value)
-
-        if "languages" in config_data:
-            for setting_value in config_data["languages"].values():
-                for s, v in setting_value.items():
-                    self.set_setting(s, v)
-
-        if settings.NEW_CODE_PERIOD in config_data["generalSettings"]:
-            (nc_type, nc_val) = settings.decode(settings.NEW_CODE_PERIOD, config_data["generalSettings"][settings.NEW_CODE_PERIOD])
-            settings.set_new_code_period(self, nc_type, nc_val)
-        permission_templates.import_config(self, config_data)
-        global_permissions.import_config(self, config_data)
-        devops.import_config(self, config_data)
-
-    def audit(self, audit_settings=None):
-        """Audits a global platform configuration and returns the list of problems found
-
-        :param audit_settings: Options of what to audit and thresholds to raise problems
-        :type audit_settings: dict
-        :return: List of problems found, or empty list
-        :rtype: list[Problem]
-        """
-        util.logger.info("--- Auditing global settings ---")
-        problems = []
-        platform_settings = self.get_settings()
-        settings_url = f"{self.url}/admin/settings"
-        for key in audit_settings:
-            if key.startswith("audit.globalSettings.range"):
-                problems += _audit_setting_in_range(key, platform_settings, audit_settings, self.version(), settings_url)
-            elif key.startswith("audit.globalSettings.value"):
-                problems += _audit_setting_value(key, platform_settings, audit_settings, settings_url)
-            elif key.startswith("audit.globalSettings.isSet"):
-                problems += _audit_setting_set(key, True, platform_settings, audit_settings, settings_url)
-            elif key.startswith("audit.globalSettings.isNotSet"):
-                problems += _audit_setting_set(key, False, platform_settings, audit_settings, settings_url)
-
-        pf_sif = self.sys_info()
-        if self.version() >= (9, 7, 0):
-            # Hack: Manually add edition in SIF (it's removed starting from 9.7 :-()
-            pf_sif["edition"] = self.edition()
-        problems += (
-            _audit_maintainability_rating_grid(platform_settings, audit_settings, settings_url)
-            + self._audit_project_default_visibility()
-            + self._audit_admin_password()
-            + self._audit_global_permissions()
-            + self._audit_lts_latest()
-            + sif.Sif(pf_sif, self).audit(audit_settings)
-            + webhooks.audit(self)
-            + permission_templates.audit(self, audit_settings)
-        )
-        return problems
-
-    def _audit_project_default_visibility(self):
-        util.logger.info("Auditing project default visibility")
-        problems = []
-        if self.version() < (8, 7, 0):
-            resp = self.get(
-                "navigation/organization",
-                params={"organization": "default-organization"},
-            )
-            visi = json.loads(resp.text)["organization"]["projectVisibility"]
-        else:
-            resp = self.get("settings/values", params={"keys": "projects.default.visibility"})
-            visi = json.loads(resp.text)["settings"][0]["value"]
-        util.logger.info("Project default visibility is '%s'", visi)
-        if config.get_property("checkDefaultProjectVisibility") and visi != "private":
-            rule = rules.get_rule(rules.RuleId.SETTING_PROJ_DEFAULT_VISIBILITY)
-            problems.append(pb.Problem(rule.type, rule.severity, rule.msq.format(visi), concerned_object=f"{self.url}/admin/projects_management"))
-        return problems
-
-    def _audit_admin_password(self):
-        util.logger.info("Auditing admin password")
-        problems = []
-        try:
-            r = requests.get(url=self.url + "/api/authentication/validate", auth=("admin", "admin"))
-            data = json.loads(r.text)
-            if data.get("valid", False):
-                rule = rules.get_rule(rules.RuleId.DEFAULT_ADMIN_PASSWORD)
-                problems.append(pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=self.url))
-            else:
-                util.logger.info("User 'admin' default password has been changed")
-        except requests.RequestException as e:
-            util.exit_fatal(str(e), options.ERR_SONAR_API)
-        return problems
-
-    def __audit_group_permissions(self):
-        util.logger.info("Auditing group global permissions")
-        problems = []
-        perms_url = f"{self.url}/admin/permissions"
-        groups = self.global_permissions().groups()
-        if len(groups) > 10:
-            msg = f"Too many ({len(groups)}) groups with global permissions"
-            problems.append(pb.Problem(typ.Type.BAD_PRACTICE, sev.Severity.MEDIUM, msg, concerned_object=perms_url))
-
-        for gr_name, gr_perms in groups.items():
-            if gr_name == "Anyone":
-                rule = rules.get_rule(rules.RuleId.ANYONE_WITH_GLOBAL_PERMS)
-                problems.append(pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=perms_url))
-            if gr_name == "sonar-users" and (
-                "admin" in gr_perms or "gateadmin" in gr_perms or "profileadmin" in gr_perms or "provisioning" in gr_perms
-            ):
-                rule = rules.get_rule(rules.RuleId.SONAR_USERS_WITH_ELEVATED_PERMS)
-                problems.append(pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=perms_url))
-
-        maxis = {"admin": 2, "gateadmin": 2, "profileadmin": 2, "scan": 2, "provisioning": 3}
-        for key, name in permissions.ENTERPRISE_GLOBAL_PERMISSIONS.items():
-            counter = self.global_permissions().count(perm_type="groups", perm_filter=(key))
-            if key in maxis and counter > maxis[key]:
-                msg = f"Too many ({counter}) groups with permission '{name}', {maxis[key]} max recommended"
-                problems.append(pb.Problem(typ.Type.BAD_PRACTICE, sev.Severity.MEDIUM, msg, concerned_object=perms_url))
-        return problems
-
-    def __audit_user_permissions(self):
-        util.logger.info("Auditing users global permissions")
-        problems = []
-        perms_url = f"{self.url}/admin/permissions"
-        users = self.global_permissions().users()
-        if len(users) > 10:
-            msg = f"Too many ({len(users)}) users with direct global permissions, use groups instead"
-            problems.append(pb.Problem(typ.Type.BAD_PRACTICE, sev.Severity.MEDIUM, msg, concerned_object=perms_url))
-
-        maxis = {"admin": 3, "gateadmin": 3, "profileadmin": 3, "scan": 3, "provisioning": 3}
-        for key, name in permissions.ENTERPRISE_GLOBAL_PERMISSIONS.items():
-            counter = self.global_permissions().count(perm_type="users", perm_filter=(key))
-            if key in maxis and counter > maxis[key]:
-                msg = f"Too many ({counter}) users with permission '{name}', use groups instead"
-                problems.append(pb.Problem(typ.Type.BAD_PRACTICE, sev.Severity.MEDIUM, msg, concerned_object=perms_url))
-        return problems
-
-    def _audit_global_permissions(self):
-        util.logger.info("--- Auditing global permissions ---")
-        return self.__audit_user_permissions() + self.__audit_group_permissions()
-
-    def _audit_lts_latest(self):
-        sq_vers, v = self.version(3), None
-        if sq_vers < lts(2):
-            rule = rules.get_rule(rules.RuleId.BELOW_LTS)
-            v = lts()
-        elif sq_vers < lts(3):
-            rule = rules.get_rule(rules.RuleId.LTS_PATCH_MISSING)
-            v = lts()
-        elif sq_vers < latest(2):
-            rule = rules.get_rule(rules.RuleId.BELOW_LATEST)
-            v = latest()
-        if not v:
-            return []
-        msg = rule.msg.format(_version_as_string(sq_vers), _version_as_string(v))
-        return [pb.Problem(rule.type, rule.severity, msg, concerned_object=self.url)]
-
-
-# --------------------- Static methods -----------------
-# this is a pointer to the module object instance itself.
-this = sys.modules[__name__]
-this.context = Platform(os.getenv("SONAR_HOST_URL", "http://localhost:9000"), os.getenv("SONAR_TOKEN", ""))
-
-
-def _normalize_api(api):
-    api = api.lower()
-    if api.startswith("/api"):
-        pass
-    elif api.startswith("api"):
-        api = "/" + api
-    elif api.startswith("/"):
-        api = "/api" + api
-    else:
-        api = "/api/" + api
-    return api
-
-
-def _audit_setting_value(key, platform_settings, audit_settings, url):
-    v = _get_multiple_values(4, audit_settings[key], "MEDIUM", "CONFIGURATION")
-    if v is None:
-        util.logger.error(WRONG_CONFIG_MSG, key, audit_settings[key])
-        return []
-    if v[0] not in platform_settings:
-        util.logger.warning(_NON_EXISTING_SETTING_SKIPPED, v[0])
-        return []
-    util.logger.info("Auditing that setting %s has common/recommended value '%s'", v[0], v[1])
-    s = platform_settings.get(v[0], "")
-    if s == v[1]:
-        return []
-    return [pb.Problem(v[2], v[3], f"Setting {v[0]} has potentially incorrect or unsafe value '{s}'", concerned_object=url)]
-
-
-def _audit_setting_in_range(key, platform_settings, audit_settings, sq_version, url):
-    v = _get_multiple_values(5, audit_settings[key], "MEDIUM", "CONFIGURATION")
-    if v is None:
-        util.logger.error(WRONG_CONFIG_MSG, key, audit_settings[key])
-        return []
-    if v[0] not in platform_settings:
-        util.logger.warning(_NON_EXISTING_SETTING_SKIPPED, v[0])
-        return []
-    if v[0] == "sonar.dbcleaner.daysBeforeDeletingInactiveShortLivingBranches" and sq_version >= (8, 0, 0):
-        util.logger.error("Setting %s is ineffective on SonaQube 8.0+, skipping audit", v[0])
-        return []
-    value, min_v, max_v = float(platform_settings[v[0]]), float(v[1]), float(v[2])
-    util.logger.info(
-        "Auditing that setting %s is within recommended range [%f-%f]",
-        v[0],
-        min_v,
-        max_v,
-    )
-    if min_v <= value <= max_v:
-        return []
-    return [
-        pb.Problem(v[4], v[3], f"Setting '{v[0]}' value {platform_settings[v[0]]} is outside recommended range [{v[1]}-{v[2]}]", concerned_object=url)
-    ]
-
-
-def _audit_setting_set(key, check_is_set, platform_settings, audit_settings, url):
-    v = _get_multiple_values(3, audit_settings[key], "MEDIUM", "CONFIGURATION")
-    if v is None:
-        util.logger.error(WRONG_CONFIG_MSG, key, audit_settings[key])
-        return []
-    if key not in platform_settings:
-        util.logger.warning(_NON_EXISTING_SETTING_SKIPPED, key)
-        return []
-    util.logger.info("Auditing whether setting %s is set or not", key)
-    problems = []
-    if platform_settings[key] == "":
-        if check_is_set:
-            rule = rules.get_rule(rules.RuleId.SETTING_NOT_SET)
-            problems = [pb.Problem(rule.type, rule.severity, rule.msg.format(key), concerned_object=url)]
-        else:
-            util.logger.info("Setting %s is not set", key)
-    else:
-        if not check_is_set:
-            util.logger.info("Setting %s is set with value %s", key, platform_settings[key])
-        else:
-            problems = [pb.Problem(v[1], v[2], f"Setting {key} is set, although it should probably not", concerned_object=url)]
-
-    return problems
-
-
-def _audit_maintainability_rating_range(value, range, rating_letter, severity, domain, url):
-    util.logger.debug(
-        "Checking that maintainability rating threshold %3.0f%% for '%s' is within recommended range [%3.0f%%-%3.0f%%]",
-        value * 100,
-        rating_letter,
-        range[0] * 100,
-        range[1] * 100,
-    )
-    if range[0] <= value <= range[1]:
-        return []
-    return [
-        pb.Problem(
-            domain,
-            severity,
-            f"Maintainability rating threshold {value * 100}% for {rating_letter} "
-            f"is NOT within recommended range [{range[0] * 100}%-{range[1] * 100}%]",
-            concerned_object=url,
-        )
-    ]
-
-
-def _audit_maintainability_rating_grid(platform_settings, audit_settings, url):
-    thresholds = util.csv_to_list(platform_settings["sonar.technicalDebt.ratingGrid"])
-    problems = []
-    util.logger.debug("Auditing maintainabillity rating grid")
-    for key in audit_settings:
-        if not key.startswith("audit.globalSettings.maintainabilityRating"):
-            continue
-        (_, _, _, letter, _, _) = key.split(".")
-        if letter not in ["A", "B", "C", "D"]:
-            util.logger.error("Incorrect audit configuration setting %s, skipping audit", key)
-            continue
-        value = float(thresholds[ord(letter.upper()) - 65])
-        v = _get_multiple_values(4, audit_settings[key], sev.Severity.MEDIUM, typ.Type.CONFIGURATION)
-        if v is None:
-            continue
-        problems += _audit_maintainability_rating_range(value, (float(v[0]), float(v[1])), letter, v[2], v[3], url)
-    return problems
-
-
-def _get_multiple_values(n, setting, severity, domain):
-    values = util.csv_to_list(setting)
-    if len(values) < (n - 2):
-        return None
-    if len(values) == (n - 2):
-        values.append(severity)
-    if len(values) == (n - 1):
-        values.append(domain)
-    values[n - 2] = sev.to_severity(values[n - 2])
-    values[n - 1] = typ.to_type(values[n - 1])
-    # TODO Handle case of too many values
-    return values
-
-
-def _version_as_string(a_version):
-    return ".".join([str(n) for n in a_version])
-
-
-def __lts_and_latest():
-    global LTS
-    global LATEST
-    if LTS is None:
-        util.logger.debug("Attempting to reach Sonar update center")
-        _, tmpfile = tempfile.mkstemp(prefix="sonar-tools", suffix=".txt", text=True)
-        try:
-            with open(tmpfile, "w", encoding="utf-8") as fp:
-                print(requests.get(_UPDATE_CENTER, headers=_SONAR_TOOLS_AGENT).text, file=fp)
-            with open(tmpfile, "r", encoding="utf-8") as fp:
-                upd_center_props = jprops.load_properties(fp)
-            v = upd_center_props.get("ltsVersion", "8.9.9").split(".")
-            if len(v) == 2:
-                v.append("0")
-            LTS = tuple(int(n) for n in v)
-            v = upd_center_props.get("publicVersions", "9.5").split(",")[-1].split(".")
-            if len(v) == 2:
-                v.append("0")
-            LATEST = tuple(int(n) for n in v)
-            util.logger.debug("Sonar update center says LTS = %s, LATEST = %s", str(LTS), str(LATEST))
-        except (EnvironmentError, requests.exceptions.HTTPError):
-            LTS = _HARDCODED_LTS
-            LATEST = _HARDCODED_LATEST
-            util.logger.debug("Sonar update center read failed, hardcoding LTS = %s, LATEST = %s", str(LTS), str(LATEST))
-        try:
-            os.remove(tmpfile)
-        except EnvironmentError:
-            pass
-    return (LTS, LATEST)
-
-
-def lts(digits=3):
-    """
-    :return: the current SonarQube LTS version
-    :params digits: number of digits to consider in the version (min 1, max 3), defaults to 3
-    :type digits: int, optional
-    :rtype: tuple (x, y, z)
-    """
-    if digits < 1 or digits > 3:
-        digits = 3
-    return __lts_and_latest()[0][0:digits]
-
-
-def latest(digits=3):
-    """
-    :return: the current SonarQube LATEST version
-    :params digits: number of digits to consider in the version (min 1, max 3), defaults to 3
-    :type digits: int, optional
-    :rtype: tuple (x, y, z)
-    """
-    if digits < 1 or digits > 3:
-        digits = 3
-    return __lts_and_latest()[1][0:digits]
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+"""
+
+    Abstraction of the SonarQube platform or instance concept
+
+"""
+
+from http import HTTPStatus
+import sys
+import os
+import time
+import datetime
+import json
+import tempfile
+import requests
+import jprops
+from requests.exceptions import HTTPError
+
+import sonar.utilities as util
+
+from sonar import options, settings, devops, webhooks, version
+from sonar.permissions import permissions, global_permissions, permission_templates
+from sonar.audit import rules, config
+import sonar.audit.severities as sev
+import sonar.audit.types as typ
+import sonar.audit.problem as pb
+
+from sonar import sif
+
+WRONG_CONFIG_MSG = "Audit config property %s has wrong value %s, skipping audit"
+
+_NON_EXISTING_SETTING_SKIPPED = "Setting %s does not exist, skipping..."
+_HTTP_ERROR = "%s Error: %s HTTP status code %d"
+
+_SONAR_TOOLS_AGENT = {"user-agent": f"sonar-tools {version.PACKAGE_VERSION}"}
+_UPDATE_CENTER = "https://raw.githubusercontent.com/SonarSource/sonar-update-center-properties/master/update-center-source.properties"
+
+LTS = None
+LATEST = None
+_HARDCODED_LTS = (9, 9, 4)
+_HARDCODED_LATEST = (10, 4, 1)
+
+
+class Platform:
+    """Abstraction of the SonarQube "platform" concept"""
+
+    def __init__(self, some_url, some_token, cert_file=None):
+        """Creates a SonarQube platform object
+
+        :param some_url: base URL of the SonarQube platform
+        :type some_url: str
+        :param some_token: token to connect to the platform
+        :type some_token: str
+        :param cert_file: Client certificate, if any needed, defaults to None
+        :type cert_file: str, optional
+        :return: the SonarQube object
+        :rtype: Platform
+        """
+        self.url = some_url.rstrip("/")  #: SonarQube URL
+        self.__token = some_token
+        self.__cert_file = cert_file
+        self._version = None
+        self.__sys_info = None
+        self.__global_nav = None
+        self._server_id = None
+        self._permissions = None
+
+    def __str__(self):
+        """
+        :return: string representation of the SonarQube connection, with the token recognizable but largely redacted
+        :rtype: str
+        """
+        return f"{util.redacted_token(self.__token)}@{self.url}"
+
+    def __credentials(self):
+        return (self.__token, "")
+
+    def version(self, digits=3, as_string=False):
+        """Returns the SonarQube platform version
+
+        :param digits: Number of digits to include in the version, defaults to 3
+        :type digits: int, optional
+        :param as_string: Whether to return the version as string or tuple, default to False (ie returns a tuple)
+        :type as_string: bool, optional
+        :return: the SonarQube platform version
+        :rtype: tuple or str
+        """
+        if digits < 1 or digits > 3:
+            digits = 3
+        if self._version is None:
+            self._version = self.get("/api/server/version").text.split(".")
+            util.logger.debug("Version = %s", self._version)
+        if as_string:
+            return ".".join(self._version[0:digits])
+        else:
+            return tuple(int(n) for n in self._version[0:digits])
+
+    def edition(self):
+        """
+        :return: the SonarQube platform edition
+        :rtype: str ("community", "developer", "enterprise" or "datacenter")
+        """
+        if "edition" in self.global_nav():
+            return util.edition_normalize(self.global_nav()["edition"])
+        else:
+            return util.edition_normalize(self.sys_info()["Statistics"]["edition"])
+
+    def server_id(self):
+        """
+        :return: the SonarQube platform server id
+        :rtype: str
+        """
+        if self._server_id is not None:
+            return self._server_id
+        if self.__sys_info is not None and "Server ID" in self.__sys_info["System"]:
+            self._server_id = self.__sys_info["System"]["Server ID"]
+        else:
+            self._server_id = json.loads(self.get("system/status").text)["id"]
+        return self._server_id
+
+    def basics(self):
+        """
+        :return: the 3 basic information of the platform: ServerId, Edition and Version
+        :rtype: dict{"serverId": <id>, "edition": <edition>, "version": <version>}
+        """
+        return {
+            "version": self.version(as_string=True),
+            "edition": self.edition(),
+            "serverId": self.server_id(),
+        }
+
+    def get(self, api, params=None, exit_on_error=False, mute=()):
+        """Makes an HTTP GET request to SonarQube
+
+        :param api: API to invoke (without the platform base URL)
+        :type api: str
+        :param params: params to pass in the HTTP request, defaults to None
+        :type params: dict, optional
+        :param exit_on_error: When to fail fast and exit if the HTTP status code is not 2XX, defaults to True
+        :type exit_on_error: bool, optional
+        :param mute: Tuple of HTTP Error codes to mute (ie not write an error log for), defaults to None.
+                     Typically, Error 404 Not found may be expected sometimes so this can avoid logging an error for 404
+        :type mute: tuple, optional
+        :return: the result of the HTTP request
+        :rtype: request.Response
+        """
+        api = _normalize_api(api)
+        util.logger.debug("GET: %s", self.__urlstring(api, params))
+        try:
+            r = requests.get(
+                url=self.url + api, auth=self.__credentials(), verify=self.__cert_file, headers=_SONAR_TOOLS_AGENT, params=params, timeout=10
+            )
+            r.raise_for_status()
+        except requests.exceptions.HTTPError as e:
+            if exit_on_error or (r.status_code not in mute and r.status_code in (HTTPStatus.UNAUTHORIZED, HTTPStatus.FORBIDDEN)):
+                util.log_and_exit(r)
+            else:
+                if r.status_code in mute:
+                    util.logger.debug(_HTTP_ERROR, "GET", self.__urlstring(api, params), r.status_code)
+                else:
+                    util.logger.error(_HTTP_ERROR, "GET", self.__urlstring(api, params), r.status_code)
+                raise e
+        except requests.exceptions.Timeout as e:
+            util.exit_fatal(str(e), options.ERR_REQUEST_TIMEOUT)
+        except requests.RequestException as e:
+            util.exit_fatal(str(e), options.ERR_SONAR_API)
+        return r
+
+    def post(self, api, params=None, exit_on_error=False, mute=()):
+        """Makes an HTTP POST request to SonarQube
+
+        :param api: API to invoke (without the platform base URL)
+        :type api: str
+        :param params: params to pass in the HTTP request, defaults to None
+        :type params: dict, optional
+        :param exit_on_error: When to fail fast and exit if the HTTP status code is not 2XX, defaults to True
+        :type exit_on_error: bool, optional
+        :param mute: HTTP Error codes to mute (ie not write an error log for), defaults to None
+                     Typically, Error 404 Not found may be expected sometimes so this can avoid logging an error for 404
+        :type mute: tuple, optional
+        :return: the result of the HTTP request
+        :rtype: request.Response
+        """
+        api = _normalize_api(api)
+        util.logger.debug("POST: %s", self.__urlstring(api, params))
+        try:
+            r = requests.post(
+                url=self.url + api, auth=self.__credentials(), verify=self.__cert_file, headers=_SONAR_TOOLS_AGENT, data=params, timeout=10
+            )
+            r.raise_for_status()
+        except requests.exceptions.HTTPError:
+            if exit_on_error or r.status_code in (HTTPStatus.UNAUTHORIZED, HTTPStatus.FORBIDDEN):
+                util.log_and_exit(r)
+            else:
+                if r.status_code in mute:
+                    util.logger.debug(_HTTP_ERROR, "POST", self.__urlstring(api, params), r.status_code)
+                else:
+                    util.logger.error(_HTTP_ERROR, "POST", self.__urlstring(api, params), r.status_code)
+                raise
+        except requests.exceptions.Timeout as e:
+            util.exit_fatal(str(e), options.ERR_REQUEST_TIMEOUT)
+        except requests.RequestException as e:
+            util.exit_fatal(str(e), options.ERR_SONAR_API)
+        return r
+
+    def delete(self, api, params=None, exit_on_error=False, mute=()):
+        """Makes an HTTP DELETE request to SonarQube
+
+        :param api: API to invoke (without the platform base URL)
+        :type api: str
+        :param params: params to pass in the HTTP request, defaults to None
+        :type params: dict, optional
+        :param exit_on_error: When to fail fast and exit if the HTTP status code is not 2XX, defaults to True
+        :type exit_on_error: bool, optional
+        :param mute: HTTP Error codes to mute (ie not write an error log for), defaults to None
+                     Typically, Error 404 Not found may be expected sometimes so this can avoid logging an error for 404
+        :type mute: tuple, optional
+        :return: the result of the HTTP request
+        :rtype: request.Response
+        """
+        api = _normalize_api(api)
+        util.logger.debug("DELETE: %s", self.__urlstring(api, params))
+        try:
+            r = requests.delete(
+                url=self.url + api, auth=self.__credentials(), verify=self.__cert_file, params=params, headers=_SONAR_TOOLS_AGENT, timeout=10
+            )
+            r.raise_for_status()
+        except requests.exceptions.HTTPError:
+            if exit_on_error:
+                util.log_and_exit(r)
+            else:
+                if r.status_code in mute:
+                    util.logger.debug(_HTTP_ERROR, "DELETE", self.__urlstring(api, params), r.status_code)
+                else:
+                    util.logger.error(_HTTP_ERROR, "DELETE", self.__urlstring(api, params), r.status_code)
+                raise
+        except requests.exceptions.Timeout as e:
+            util.exit_fatal(str(e), options.ERR_REQUEST_TIMEOUT)
+        except requests.RequestException as e:
+            util.exit_fatal(str(e), options.ERR_SONAR_API)
+
+    def global_permissions(self):
+        """Returns the SonarQube platform global permissions
+
+        :return: dict{"users": {<login>: <permissions comma separated>, ...}, "groups"; {<name>: <permissions comma separated>, ...}}}
+        :rtype: dict
+        """
+        if self._permissions is None:
+            self._permissions = global_permissions.GlobalPermissions(self)
+        return self._permissions
+
+    def sys_info(self):
+        """
+        :return: the SonarQube platform system info file
+        :rtype: dict
+        """
+        if self.__sys_info is None:
+            success, counter = False, 0
+            while not success:
+                try:
+                    resp = self.get("system/info", mute=(HTTPStatus.INTERNAL_SERVER_ERROR,))
+                    success = True
+                except HTTPError as e:
+                    # Hack: SonarQube randomly returns Error 500 on this API, retry up to 10 times
+                    if e.response.status_code == HTTPStatus.INTERNAL_SERVER_ERROR and counter < 10:
+                        util.logger.error("HTTP Error 500 for api/system/info, retrying...")
+                        time.sleep(0.5)
+                        counter += 1
+                    else:
+                        raise e
+            self.__sys_info = json.loads(resp.text)
+            success = True
+        return self.__sys_info
+
+    def global_nav(self):
+        """
+        :return: the SonarQube platform global navigation data
+        :rtype: dict
+        """
+        if self.__global_nav is None:
+            resp = self.get("navigation/global", mute=(HTTPStatus.INTERNAL_SERVER_ERROR,))
+            self.__global_nav = json.loads(resp.text)
+        return self.__global_nav
+
+    def database(self):
+        """
+        :return: the SonarQube platform backend database
+        :rtype: str
+        """
+        if self.version() < (9, 7, 0):
+            return self.sys_info()["Statistics"]["database"]["name"]
+        else:
+            return self.sys_info()["Database"]["Database"]
+
+    def plugins(self):
+        """
+        :return: the SonarQube platform plugins
+        :rtype: dict
+        """
+        if self.version() < (9, 7, 0):
+            return self.sys_info()["Statistics"]["plugins"]
+        else:
+            return self.sys_info()["Plugins"]
+
+    def get_settings(self, settings_list=None):
+        """Returns a list of (or all) platform global settings value from their key
+
+        :param key: settings_list
+        :type key: list or str (comma separated)
+        :return: the list of settings values
+        :rtype: dict{<key>: <value>, ...}
+        """
+        params = util.remove_nones({"keys": util.list_to_csv(settings_list)})
+        resp = self.get("settings/values", params=params)
+        json_s = json.loads(resp.text)
+        platform_settings = {}
+        for s in json_s["settings"]:
+            if "value" in s:
+                platform_settings[s["key"]] = s["value"]
+            elif "values" in s:
+                platform_settings[s["key"]] = ",".join(s["values"])
+            elif "fieldValues" in s:
+                platform_settings[s["key"]] = s["fieldValues"]
+        return platform_settings
+
+    def __settings(self, settings_list=None, include_not_set=False):
+        util.logger.info("getting global settings")
+        return settings.get_bulk(endpoint=self, settings_list=settings_list, include_not_set=include_not_set)
+
+    def get_setting(self, key):
+        """Returns a platform global setting value from its key
+
+        :param key: Setting key
+        :type key: str
+        :return: the setting value
+        :rtype: str or dict
+        """
+        return self.get_settings(key).get(key, None)
+
+    def reset_setting(self, key):
+        """Resets a platform global setting to the SonarQube internal default value
+
+        :param key: Setting key
+        :type key: str
+        :return: Whether the reset was successful or not
+        :rtype: bool
+        """
+        return settings.reset_setting(self, key).ok
+
+    def set_setting(self, key, value):
+        """Sets a platform global setting
+
+        :param key: Setting key
+        :type key: str
+        :param key: value
+        :type key: str
+        :return: Whether setting the value was successful or not
+        :rtype: bool
+        """
+        return settings.set_setting(self, key, value)
+
+    def __urlstring(self, api, params):
+        """Returns a string corresponding to the URL and parameters"""
+        first = True
+        url_prefix = f"{str(self)}{api}"
+        if params is None:
+            return url_prefix
+        for p in params:
+            if params[p] is None:
+                continue
+            sep = "?" if first else "&"
+            first = False
+            if isinstance(params[p], datetime.date):
+                params[p] = util.format_date(params[p])
+            url_prefix += f"{sep}{p}={requests.utils.quote(str(params[p]))}"
+        return url_prefix
+
+    def webhooks(self):
+        """
+        :return: the list of global webhooks
+        :rtype: dict{<webhook_name>: <webhook_data>, ...}
+        """
+        return webhooks.get_list(self)
+
+    def export(self, full=False):
+        """Exports the global platform properties as JSON
+
+        :param full: Whether to also export properties thatc annot be set, defaults to False
+        :type full: bool, optional
+        :return: dict of all properties with their values
+        :rtype: dict
+        """
+        util.logger.info("Exporting platform global settings")
+        json_data = {}
+        for s in self.__settings(include_not_set=True).values():
+            (categ, subcateg) = s.category()
+            util.update_json(json_data, categ, subcateg, s.to_json())
+
+        json_data[settings.GENERAL_SETTINGS].update({"webhooks": webhooks.export(self, full=full)})
+        json_data["permissions"] = self.global_permissions().export()
+        json_data["permissionTemplates"] = permission_templates.export(self, full=full)
+        json_data[settings.DEVOPS_INTEGRATION] = devops.export(self, full=full)
+        return json_data
+
+    def set_webhooks(self, webhooks_data):
+        """Sets global webhooks with a list of webhooks represented as JSON
+
+        :param webhooks_data: the webhooks representation
+        :type webhooks_data: dict
+        :return: Nothing
+        """
+        if webhooks_data is None:
+            return
+        current_wh = self.webhooks()
+        # FIXME: Handle several webhooks with same name
+        current_wh_names = [wh.name for wh in current_wh.values()]
+        wh_map = {wh.name: k for k, wh in current_wh.items()}
+        util.logger.debug("Current WH %s", str(current_wh_names))
+        for wh_name, wh in webhooks_data.items():
+            util.logger.debug("Updating wh with name %s", wh_name)
+            if wh_name in current_wh_names:
+                current_wh[wh_map[wh_name]].update(name=wh_name, **wh)
+            else:
+                webhooks.update(name=wh_name, endpoint=self, project=None, **wh)
+
+    def import_config(self, config_data):
+        """Imports a whole SonarQube platform global configuration represented as JSON
+
+        :param config_data: the configuration representation
+        :type config_data: dict
+        :return: Nothing
+        """
+        if "globalSettings" not in config_data:
+            util.logger.info("No global settings to import")
+            return
+        config_data = config_data["globalSettings"]
+        for section in ("analysisScope", "authentication", "generalSettings", "linters", "sastConfig", "tests", "thirdParty"):
+            if section not in config_data:
+                continue
+            for setting_key, setting_value in config_data[section].items():
+                if setting_key == "webhooks":
+                    self.set_webhooks(setting_value)
+                else:
+                    self.set_setting(setting_key, setting_value)
+
+        if "languages" in config_data:
+            for setting_value in config_data["languages"].values():
+                for s, v in setting_value.items():
+                    self.set_setting(s, v)
+
+        if settings.NEW_CODE_PERIOD in config_data["generalSettings"]:
+            (nc_type, nc_val) = settings.decode(settings.NEW_CODE_PERIOD, config_data["generalSettings"][settings.NEW_CODE_PERIOD])
+            settings.set_new_code_period(self, nc_type, nc_val)
+        permission_templates.import_config(self, config_data)
+        global_permissions.import_config(self, config_data)
+        devops.import_config(self, config_data)
+
+    def audit(self, audit_settings=None):
+        """Audits a global platform configuration and returns the list of problems found
+
+        :param audit_settings: Options of what to audit and thresholds to raise problems
+        :type audit_settings: dict
+        :return: List of problems found, or empty list
+        :rtype: list[Problem]
+        """
+        util.logger.info("--- Auditing global settings ---")
+        problems = []
+        platform_settings = self.get_settings()
+        settings_url = f"{self.url}/admin/settings"
+        for key in audit_settings:
+            if key.startswith("audit.globalSettings.range"):
+                problems += _audit_setting_in_range(key, platform_settings, audit_settings, self.version(), settings_url)
+            elif key.startswith("audit.globalSettings.value"):
+                problems += _audit_setting_value(key, platform_settings, audit_settings, settings_url)
+            elif key.startswith("audit.globalSettings.isSet"):
+                problems += _audit_setting_set(key, True, platform_settings, audit_settings, settings_url)
+            elif key.startswith("audit.globalSettings.isNotSet"):
+                problems += _audit_setting_set(key, False, platform_settings, audit_settings, settings_url)
+
+        pf_sif = self.sys_info()
+        if self.version() >= (9, 7, 0):
+            # Hack: Manually add edition in SIF (it's removed starting from 9.7 :-()
+            pf_sif["edition"] = self.edition()
+        problems += (
+            _audit_maintainability_rating_grid(platform_settings, audit_settings, settings_url)
+            + self._audit_project_default_visibility()
+            + self._audit_admin_password()
+            + self._audit_global_permissions()
+            + self._audit_lts_latest()
+            + sif.Sif(pf_sif, self).audit(audit_settings)
+            + webhooks.audit(self)
+            + permission_templates.audit(self, audit_settings)
+        )
+        return problems
+
+    def _audit_project_default_visibility(self):
+        util.logger.info("Auditing project default visibility")
+        problems = []
+        if self.version() < (8, 7, 0):
+            resp = self.get(
+                "navigation/organization",
+                params={"organization": "default-organization"},
+            )
+            visi = json.loads(resp.text)["organization"]["projectVisibility"]
+        else:
+            resp = self.get("settings/values", params={"keys": "projects.default.visibility"})
+            visi = json.loads(resp.text)["settings"][0]["value"]
+        util.logger.info("Project default visibility is '%s'", visi)
+        if config.get_property("checkDefaultProjectVisibility") and visi != "private":
+            rule = rules.get_rule(rules.RuleId.SETTING_PROJ_DEFAULT_VISIBILITY)
+            problems.append(pb.Problem(rule.type, rule.severity, rule.msq.format(visi), concerned_object=f"{self.url}/admin/projects_management"))
+        return problems
+
+    def _audit_admin_password(self):
+        util.logger.info("Auditing admin password")
+        problems = []
+        try:
+            r = requests.get(url=self.url + "/api/authentication/validate", auth=("admin", "admin"), timeout=10)
+            data = json.loads(r.text)
+            if data.get("valid", False):
+                rule = rules.get_rule(rules.RuleId.DEFAULT_ADMIN_PASSWORD)
+                problems.append(pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=self.url))
+            else:
+                util.logger.info("User 'admin' default password has been changed")
+        except requests.RequestException as e:
+            util.exit_fatal(str(e), options.ERR_SONAR_API)
+        return problems
+
+    def __audit_group_permissions(self):
+        util.logger.info("Auditing group global permissions")
+        problems = []
+        perms_url = f"{self.url}/admin/permissions"
+        groups = self.global_permissions().groups()
+        if len(groups) > 10:
+            msg = f"Too many ({len(groups)}) groups with global permissions"
+            problems.append(pb.Problem(typ.Type.BAD_PRACTICE, sev.Severity.MEDIUM, msg, concerned_object=perms_url))
+
+        for gr_name, gr_perms in groups.items():
+            if gr_name == "Anyone":
+                rule = rules.get_rule(rules.RuleId.ANYONE_WITH_GLOBAL_PERMS)
+                problems.append(pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=perms_url))
+            if gr_name == "sonar-users" and (
+                "admin" in gr_perms or "gateadmin" in gr_perms or "profileadmin" in gr_perms or "provisioning" in gr_perms
+            ):
+                rule = rules.get_rule(rules.RuleId.SONAR_USERS_WITH_ELEVATED_PERMS)
+                problems.append(pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=perms_url))
+
+        maxis = {"admin": 2, "gateadmin": 2, "profileadmin": 2, "scan": 2, "provisioning": 3}
+        for key, name in permissions.ENTERPRISE_GLOBAL_PERMISSIONS.items():
+            counter = self.global_permissions().count(perm_type="groups", perm_filter=(key,))
+            if key in maxis and counter > maxis[key]:
+                msg = f"Too many ({counter}) groups with permission '{name}', {maxis[key]} max recommended"
+                problems.append(pb.Problem(typ.Type.BAD_PRACTICE, sev.Severity.MEDIUM, msg, concerned_object=perms_url))
+        return problems
+
+    def __audit_user_permissions(self):
+        util.logger.info("Auditing users global permissions")
+        problems = []
+        perms_url = f"{self.url}/admin/permissions"
+        users = self.global_permissions().users()
+        if len(users) > 10:
+            msg = f"Too many ({len(users)}) users with direct global permissions, use groups instead"
+            problems.append(pb.Problem(typ.Type.BAD_PRACTICE, sev.Severity.MEDIUM, msg, concerned_object=perms_url))
+
+        maxis = {"admin": 3, "gateadmin": 3, "profileadmin": 3, "scan": 3, "provisioning": 3}
+        for key, name in permissions.ENTERPRISE_GLOBAL_PERMISSIONS.items():
+            counter = self.global_permissions().count(perm_type="users", perm_filter=(key,))
+            if key in maxis and counter > maxis[key]:
+                msg = f"Too many ({counter}) users with permission '{name}', use groups instead"
+                problems.append(pb.Problem(typ.Type.BAD_PRACTICE, sev.Severity.MEDIUM, msg, concerned_object=perms_url))
+        return problems
+
+    def _audit_global_permissions(self):
+        util.logger.info("--- Auditing global permissions ---")
+        return self.__audit_user_permissions() + self.__audit_group_permissions()
+
+    def _audit_lts_latest(self):
+        sq_vers, v = self.version(3), None
+        if sq_vers < lts(2):
+            rule = rules.get_rule(rules.RuleId.BELOW_LTS)
+            v = lts()
+        elif sq_vers < lts(3):
+            rule = rules.get_rule(rules.RuleId.LTS_PATCH_MISSING)
+            v = lts()
+        elif sq_vers[:2] > lts(2) and sq_vers < latest(2):
+            rule = rules.get_rule(rules.RuleId.BELOW_LATEST)
+            v = latest()
+        if not v:
+            return []
+        msg = rule.msg.format(_version_as_string(sq_vers), _version_as_string(v))
+        return [pb.Problem(rule.type, rule.severity, msg, concerned_object=self.url)]
+
+
+# --------------------- Static methods -----------------
+# this is a pointer to the module object instance itself.
+this = sys.modules[__name__]
+this.context = Platform(os.getenv("SONAR_HOST_URL", "http://localhost:9000"), os.getenv("SONAR_TOKEN", ""))
+
+
+def _normalize_api(api):
+    api = api.lower()
+    if api.startswith("/api"):
+        pass
+    elif api.startswith("api"):
+        api = "/" + api
+    elif api.startswith("/"):
+        api = "/api" + api
+    else:
+        api = "/api/" + api
+    return api
+
+
+def _audit_setting_value(key, platform_settings, audit_settings, url):
+    v = _get_multiple_values(4, audit_settings[key], "MEDIUM", "CONFIGURATION")
+    if v is None:
+        util.logger.error(WRONG_CONFIG_MSG, key, audit_settings[key])
+        return []
+    if v[0] not in platform_settings:
+        util.logger.warning(_NON_EXISTING_SETTING_SKIPPED, v[0])
+        return []
+    util.logger.info("Auditing that setting %s has common/recommended value '%s'", v[0], v[1])
+    s = platform_settings.get(v[0], "")
+    if s == v[1]:
+        return []
+    return [pb.Problem(v[2], v[3], f"Setting {v[0]} has potentially incorrect or unsafe value '{s}'", concerned_object=url)]
+
+
+def _audit_setting_in_range(key, platform_settings, audit_settings, sq_version, url):
+    v = _get_multiple_values(5, audit_settings[key], "MEDIUM", "CONFIGURATION")
+    if v is None:
+        util.logger.error(WRONG_CONFIG_MSG, key, audit_settings[key])
+        return []
+    if v[0] not in platform_settings:
+        util.logger.warning(_NON_EXISTING_SETTING_SKIPPED, v[0])
+        return []
+    if v[0] == "sonar.dbcleaner.daysBeforeDeletingInactiveShortLivingBranches" and sq_version >= (8, 0, 0):
+        util.logger.error("Setting %s is ineffective on SonaQube 8.0+, skipping audit", v[0])
+        return []
+    value, min_v, max_v = float(platform_settings[v[0]]), float(v[1]), float(v[2])
+    util.logger.info(
+        "Auditing that setting %s is within recommended range [%.2f-%.2f]",
+        v[0],
+        min_v,
+        max_v,
+    )
+    if min_v <= value <= max_v:
+        return []
+    return [
+        pb.Problem(v[4], v[3], f"Setting '{v[0]}' value {platform_settings[v[0]]} is outside recommended range [{v[1]}-{v[2]}]", concerned_object=url)
+    ]
+
+
+def _audit_setting_set(key, check_is_set, platform_settings, audit_settings, url):
+    v = _get_multiple_values(3, audit_settings[key], "MEDIUM", "CONFIGURATION")
+    if v is None:
+        util.logger.error(WRONG_CONFIG_MSG, key, audit_settings[key])
+        return []
+    util.logger.info("Auditing whether setting %s is set or not", v[0])
+    if platform_settings.get(v[0], "") == "":  # Setting is not set
+        if check_is_set:
+            rule = rules.get_rule(rules.RuleId.SETTING_NOT_SET)
+            return [pb.Problem(rule.type, rule.severity, rule.msg.format(v[0]), concerned_object=url)]
+        util.logger.info("Setting %s is not set", v[0])
+    else:
+        if not check_is_set:
+            return [pb.Problem(v[1], v[2], f"Setting {v[0]} is set, although it should probably not", concerned_object=url)]
+        util.logger.info("Setting %s is set with value %s", v[0], platform_settings[v[0]])
+    return []
+
+
+def _audit_maintainability_rating_range(value, range, rating_letter, severity, domain, url):
+    util.logger.debug(
+        "Checking that maintainability rating threshold %.1f%% for '%s' is within recommended range [%.1f%%-%.1f%%]",
+        value * 100,
+        rating_letter,
+        range[0] * 100,
+        range[1] * 100,
+    )
+    if range[0] <= value <= range[1]:
+        return []
+    return [
+        pb.Problem(
+            domain,
+            severity,
+            f"Maintainability rating threshold {value * 100}% for {rating_letter} "
+            f"is NOT within recommended range [{range[0] * 100:.1f}%-{range[1] * 100:.1f}%]",
+            concerned_object=url,
+        )
+    ]
+
+
+def _audit_maintainability_rating_grid(platform_settings, audit_settings, url):
+    thresholds = util.csv_to_list(platform_settings["sonar.technicalDebt.ratingGrid"])
+    problems = []
+    util.logger.debug("Auditing maintainabillity rating grid")
+    for key in audit_settings:
+        if not key.startswith("audit.globalSettings.maintainabilityRating"):
+            continue
+        (_, _, _, letter, _, _) = key.split(".")
+        if letter not in ["A", "B", "C", "D"]:
+            util.logger.error("Incorrect audit configuration setting %s, skipping audit", key)
+            continue
+        value = float(thresholds[ord(letter.upper()) - 65])
+        v = _get_multiple_values(4, audit_settings[key], sev.Severity.MEDIUM, typ.Type.CONFIGURATION)
+        if v is None:
+            continue
+        problems += _audit_maintainability_rating_range(value, (float(v[0]), float(v[1])), letter, v[2], v[3], url)
+    return problems
+
+
+def _get_multiple_values(n, setting, severity, domain):
+    values = util.csv_to_list(setting)
+    if len(values) < (n - 2):
+        return None
+    if len(values) == (n - 2):
+        values.append(severity)
+    if len(values) == (n - 1):
+        values.append(domain)
+    values[n - 2] = sev.to_severity(values[n - 2])
+    values[n - 1] = typ.to_type(values[n - 1])
+    # TODO Handle case of too many values
+    return values
+
+
+def _version_as_string(a_version):
+    return ".".join([str(n) for n in a_version])
+
+
+def __lts_and_latest():
+    global LTS
+    global LATEST
+    if LTS is None:
+        util.logger.debug("Attempting to reach Sonar update center")
+        _, tmpfile = tempfile.mkstemp(prefix="sonar-tools", suffix=".txt", text=True)
+        try:
+            with open(tmpfile, "w", encoding="utf-8") as fp:
+                print(requests.get(_UPDATE_CENTER, headers=_SONAR_TOOLS_AGENT, timeout=10).text, file=fp)
+            with open(tmpfile, "r", encoding="utf-8") as fp:
+                upd_center_props = jprops.load_properties(fp)
+            v = upd_center_props.get("ltsVersion", "9.9.0").split(".")
+            if len(v) == 2:
+                v.append("0")
+            LTS = tuple(int(n) for n in v)
+            v = upd_center_props.get("publicVersions", "10.4").split(",")[-1].split(".")
+            if len(v) == 2:
+                v.append("0")
+            LATEST = tuple(int(n) for n in v)
+            util.logger.debug("Sonar update center says LTS = %s, LATEST = %s", str(LTS), str(LATEST))
+        except (EnvironmentError, requests.exceptions.HTTPError):
+            LTS = _HARDCODED_LTS
+            LATEST = _HARDCODED_LATEST
+            util.logger.debug("Sonar update center read failed, hardcoding LTS = %s, LATEST = %s", str(LTS), str(LATEST))
+        try:
+            os.remove(tmpfile)
+        except EnvironmentError:
+            pass
+    return (LTS, LATEST)
+
+
+def lts(digits=3):
+    """
+    :return: the current SonarQube LTS version
+    :params digits: number of digits to consider in the version (min 1, max 3), defaults to 3
+    :type digits: int, optional
+    :rtype: tuple (x, y, z)
+    """
+    if digits < 1 or digits > 3:
+        digits = 3
+    return __lts_and_latest()[0][0:digits]
+
+
+def latest(digits=3):
+    """
+    :return: the current SonarQube LATEST version
+    :params digits: number of digits to consider in the version (min 1, max 3), defaults to 3
+    :type digits: int, optional
+    :rtype: tuple (x, y, z)
+    """
+    if digits < 1 or digits > 3:
+        digits = 3
+    return __lts_and_latest()[1][0:digits]
```

## sonar/portfolios.py

```diff
@@ -1,712 +1,714 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-"""
-
-    Abstraction of the SonarQube "portfolio" concept
-
-"""
-
-import time
-import json
-from http import HTTPStatus
-from threading import Lock
-from requests.exceptions import HTTPError
-
-from sonar import aggregations, options, exceptions
-import sonar.permissions.permissions as perms
-import sonar.permissions.portfolio_permissions as pperms
-import sonar.sqobject as sq
-import sonar.utilities as util
-from sonar.audit import rules
-
-_OBJECTS = {}
-_CLASS_LOCK = Lock()
-
-_LIST_API = "views/list"
-_SEARCH_API = "views/search"
-_CREATE_API = "views/create"
-_GET_API = "views/show"
-
-MAX_PAGE_SIZE = 500
-_PORTFOLIO_QUALIFIER = "VW"
-_SUBPORTFOLIO_QUALIFIER = "SVW"
-
-SELECTION_MODE_MANUAL = "MANUAL"
-SELECTION_MODE_REGEXP = "REGEXP"
-SELECTION_MODE_TAGS = "TAGS"
-SELECTION_MODE_OTHERS = "REST"
-SELECTION_MODE_NONE = "NONE"
-SELECTION_MODES = (SELECTION_MODE_MANUAL, SELECTION_MODE_REGEXP, SELECTION_MODE_TAGS, SELECTION_MODE_OTHERS, SELECTION_MODE_NONE)
-
-_PROJECT_SELECTION_MODE = "projectSelectionMode"
-_PROJECT_SELECTION_BRANCH = "projectSelectionBranch"
-_PROJECT_SELECTION_REGEXP = "projectSelectionRegexp"
-_PROJECT_SELECTION_TAGS = "projectSelectionTags"
-
-_IMPORTABLE_PROPERTIES = (
-    "key",
-    "name",
-    "description",
-    _PROJECT_SELECTION_MODE,
-    "visibility",
-    _PROJECT_SELECTION_REGEXP,
-    _PROJECT_SELECTION_BRANCH,
-    _PROJECT_SELECTION_TAGS,
-    "permissions",
-    "subPortfolios",
-    "projects",
-)
-
-
-class Portfolio(aggregations.Aggregation):
-    @classmethod
-    def get_object(cls, endpoint, key):
-        util.logger.debug("Getting object '%s'", key)
-        # if root_key is None:
-        # data = search_by_name(endpoint=endpoint, name=name)
-        # else:
-        #    data = _find_sub_portfolio_by_name(name=name, data=_OBJECTS[root_key]._json)
-        # if data is None:
-        #    return None
-        # key = data["key"]
-        if key in _OBJECTS:
-            return _OBJECTS[key]
-        data = search_by_key(endpoint, key)
-        if data is None:
-            raise exceptions.ObjectNotFound(key, f"Portfolios '{key}' not found")
-        return Portfolio.load(endpoint=endpoint, data=data)
-
-    @classmethod
-    def create(cls, endpoint, name, **kwargs):
-        util.logger.debug("Creating portfolio name '%s', key '%s', parent = %s", name, str(kwargs.get("key", None)), str(kwargs.get("parent", None)))
-        params = {"name": name}
-        for p in ("description", "parent", "key", "visibility"):
-            params[p] = kwargs.get(p, None)
-        endpoint.post(_CREATE_API, params=params)
-        o = cls(endpoint=endpoint, name=name, key=kwargs.get("key", None))
-        if "parent" in kwargs:
-            o.set_parent(Portfolio.get_object(endpoint, kwargs["parent"]))
-        # TODO - Allow on the fly selection mode
-        return o
-
-    @classmethod
-    def load(cls, endpoint, data):
-        util.logger.debug("Loading portfolio '%s' with data %s", data["name"], util.json_dump(data))
-        o = cls(endpoint=endpoint, name=data["name"], key=data["key"])
-        o.reload(data)
-        if not o.is_sub_portfolio:
-            o.refresh()
-        return o
-
-    def __init__(self, endpoint, name, key=None):
-        super().__init__(key if key else name, endpoint)
-        self.name = name
-        self._selection_mode = None  #: Portfolio project selection mode
-        self._selection_branch = None  #: project branches on SonarQube 9.2+
-        self._projects = None  #: Portfolio list of projects when selection mode is MANUAL
-        self._regexp = None  #: Project selection regexp is selection mode is REGEXP
-        self._tags = []  #: Portfolio tags when selection mode is TAGS
-        self._description = None  #: Portfolio description
-        self.is_sub_portfolio = False  #: Whether the portfolio is a subportfolio
-        self._visibility = None  #: Portfolio visibility
-        self._sub_portfolios = None  #: Subportfolios
-        self._permissions = None  #: Permissions
-        self.parent = None  #: Ref to parent portfolio object, if any
-        self._root_portfolio = None  #: Ref to root portfolio, if any
-        _OBJECTS[self.uuid()] = self
-        util.logger.debug("Created portfolio object name '%s'", name)
-        util.logger.debug("PORTFOLIOS = %s", str([p.key for p in _OBJECTS.values()]))
-
-    def reload(self, data):
-        util.logger.debug("Reloading %s with %s", str(self), util.json_dump(data))
-        super().reload(data)
-        if "name" in data:
-            self.name = data["name"]
-        if "selectionMode" in self._json:
-            self._selection_mode = self._json["selectionMode"]
-        if "branch" in self._json:
-            self._selection_branch = self._json["branch"]
-        if "regexp" in self._json:
-            self._regexp = self._json["regexp"]
-        self.is_sub_portfolio = self._json.get("qualifier", _PORTFOLIO_QUALIFIER) == _SUBPORTFOLIO_QUALIFIER
-        if "tags" in self._json:
-            self._tags = self._json["tags"]
-        if "visibility" in self._json:
-            self._visibility = self._json["visibility"]
-        parent = data.get("parentKey", data.get("parent", None))
-        if parent:
-            self.set_parent(Portfolio.get_object(self.endpoint, parent))
-
-    def __str__(self):
-        return f"subportfolio '{self.key}'" if self.is_sub_portfolio else f"portfolio '{self.key}'"
-
-    def refresh(self):
-        util.logger.debug("Updating details for %s root key %s", str(self), self._root_portfolio)
-        data = json.loads(self.get(_GET_API, params={"key": self.root_portfolio().key}).text)
-        if not self.is_sub_portfolio:
-            self.reload(data)
-        self.root_portfolio().create_sub_portfolios()
-        self.projects()
-
-    def set_parent(self, parent_portfolio):
-        self.parent = parent_portfolio
-        self._root_portfolio = self.root_portfolio()
-        util.logger.debug("%s: Parent = %s, Root = %s", str(self), str(self.parent), str(self._root_portfolio))
-
-    def url(self):
-        return f"{self.endpoint.url}/portfolio?id={self.key}"
-
-    def selection_mode(self):
-        return self._selection_mode
-
-    def root_portfolio(self):
-        if self.parent is None:
-            util.logger.debug("Found root for %s, parent = %s", self.key, str(self.parent))
-            self._root_portfolio = self
-        else:
-            util.logger.debug("recursing root for %s, parent = %s", self.key, str(self.parent))
-            self._root_portfolio = self.parent.root_portfolio()
-        return self._root_portfolio
-
-    def projects(self):
-        if self._selection_mode != SELECTION_MODE_MANUAL:
-            util.logger.debug("%s: Not manual mode, no projects", str(self))
-            self._projects = {}
-            return self._projects
-        if self._projects is not None:
-            util.logger.debug("%s: Projects already set, returning %s", str(self), str(self._projects))
-            return self._projects
-        if "selectedProjects" not in self._json:
-            self.refresh()
-        self._projects = {}
-        util.logger.debug("%s: Read projects %s", str(self), str(self._projects))
-        if self.endpoint.version() < (9, 3, 0):
-            for p in self._json.get("projects", {}):
-                self._projects[p] = options.DEFAULT
-            return self._projects
-        for p in self._json.get("selectedProjects", {}):
-            if "selectedBranches" in p:
-                self._projects[p["projectKey"]] = util.list_to_csv(p["selectedBranches"], ", ", True)
-            else:
-                self._projects[p["projectKey"]] = options.DEFAULT
-        util.logger.debug("%s: PROJ4 Read projects %s", str(self), str(self._projects))
-        util.logger.debug("%s projects = %s", str(self), util.json_dump(self._projects))
-        return self._projects
-
-    def sub_portfolios(self, full=False):
-        self.refresh()
-        # self._sub_portfolios = _sub_portfolios(self._json, self.endpoint.version(), full=full)
-        self.create_sub_portfolios()
-        return self._sub_portfolios
-
-    def create_sub_portfolios(self):
-        if "subViews" in self._json and len(self._json["subViews"]) > 0:
-            util.logger.debug("Inspecting %s subportfolios data = %s", str(self), util.json_dump(self._json["subViews"]))
-            self._sub_portfolios = {}
-            for oldp in self._json["subViews"]:
-                p = oldp.copy()
-                util.logger.debug("Found subport data = %s", util.json_dump(p))
-                if p["qualifier"] == _PORTFOLIO_QUALIFIER:
-                    key = p.get("originalKey", None)
-                    if key is None:
-                        key = p.pop("key").split(":")[-1]
-                else:
-                    key = p.pop("key")
-                try:
-                    subp = Portfolio.get_object(self.endpoint, key)
-                    subp.set_parent(self)
-                except exceptions.ObjectNotFound:
-                    subp = Portfolio.create(self.endpoint, name=p.pop("name"), key=key, parent=self.key, description=p.pop("desc", None), **p)
-                util.logger.debug("Subp = %s", str(subp))
-                subp.reload(oldp)
-                self._sub_portfolios[subp.key] = subp
-                subp.create_sub_portfolios()
-                subp.projects()
-
-    def regexp(self):
-        if self.selection_mode() != SELECTION_MODE_REGEXP:
-            self._regexp = None
-        elif self._regexp is None:
-            self._regexp = self._json["regexp"]
-        return self._regexp
-
-    def tags(self):
-        if self.selection_mode() != SELECTION_MODE_TAGS:
-            self._tags = None
-        elif self._tags is None:
-            self._tags = self._json.pop("tags", [])
-        return self._tags
-
-    def get_components(self):
-        data = json.loads(
-            self.get(
-                "measures/component_tree",
-                params={
-                    "component": self.key,
-                    "metricKeys": "ncloc",
-                    "strategy": "children",
-                    "ps": 500,
-                },
-            ).text
-        )
-        comp_list = {}
-        for c in data["components"]:
-            comp_list[c["key"]] = c
-        return comp_list
-
-    def delete(self):
-        return sq.delete_object(self, "views/delete", {"key": self.key}, _OBJECTS)
-
-    def _audit_empty(self, audit_settings):
-        if not audit_settings["audit.portfolios.empty"]:
-            util.logger.debug("Auditing empty portfolios is disabled, skipping...")
-            return []
-        return self._audit_empty_aggregation(broken_rule=rules.RuleId.PORTFOLIO_EMPTY)
-
-    def _audit_singleton(self, audit_settings):
-        if not audit_settings["audit.portfolios.singleton"]:
-            util.logger.debug("Auditing singleton portfolios is disabled, skipping...")
-            return []
-        return self._audit_singleton_aggregation(broken_rule=rules.RuleId.PORTFOLIO_SINGLETON)
-
-    def audit(self, audit_settings):
-        util.logger.info("Auditing %s", str(self))
-        return self._audit_empty(audit_settings) + self._audit_singleton(audit_settings) + self._audit_bg_task(audit_settings)
-
-    def export(self, full=False):
-        util.logger.info("Exporting %s", str(self))
-        self.refresh()
-        json_data = self._json
-        json_data.update(self.sub_portfolios(full=full))
-        json_data.update(
-            {
-                "key": self.key,
-                "name": self.name,
-                "description": None if self._description == "" else self._description,
-                _PROJECT_SELECTION_MODE: self.selection_mode(),
-                "visibility": self._visibility,
-                _PROJECT_SELECTION_REGEXP: self.regexp(),
-                _PROJECT_SELECTION_BRANCH: self._selection_branch,
-                _PROJECT_SELECTION_TAGS: util.list_to_csv(self.tags(), separator=", "),
-                "permissions": self.permissions().export(),
-            }
-        )
-        if self.selection_mode() == SELECTION_MODE_MANUAL:
-            json_data["projects"] = self.projects()
-
-        return util.remove_nones(util.filter_export(json_data, _IMPORTABLE_PROPERTIES, full))
-
-    def permissions(self):
-        if self._permissions is None and not self.is_sub_portfolio:
-            # No permissions for sub portfolios
-            self._permissions = pperms.PortfolioPermissions(self)
-        return self._permissions
-
-    def set_permissions(self, portfolio_perms):
-        if not self.is_sub_portfolio:
-            # No permissions for SVW
-            self.permissions().set(portfolio_perms)
-
-    def set_component_tags(self, tags, api):
-        util.logger.warning("Can't set tags on portfolios, operation skipped...")
-
-    def set_projects(self, project_list):
-        self.post("views/set_manual_mode", params={"portfolio": self.key})
-        self._selection_mode = SELECTION_MODE_MANUAL
-        current_projects = self.projects()
-        current_project_keys = list(current_projects.keys())
-        util.logger.debug("Project list = %s", str(project_list))
-        if project_list is None:
-            return False
-        util.logger.debug("Current Project list = %s", str(current_project_keys))
-        ok = True
-        for proj, branches in project_list.items():
-            if proj in current_project_keys:
-                util.logger.debug("Won't add project '%s' branch '%s' to %s, it's already added", proj, project_list[proj], str(self))
-                continue
-            try:
-                r = self.post("views/add_project", params={"key": self.key, "project": proj})
-                ok = ok and r.ok
-                current_projects[proj] = options.DEFAULT
-            except HTTPError as e:
-                if e.response.status_code == HTTPStatus.NOT_FOUND:
-                    raise exceptions.ObjectNotFound(self.key, f"Project '{proj}' not found, can't be added to portfolio '{self.key}'")
-                raise
-
-            for branch in util.csv_to_list(branches):
-                if branch == options.DEFAULT or branch in util.csv_to_list(current_projects[proj]):
-                    util.logger.debug("Won't add project '%s' branch '%s' to %s, it's already added", proj, project_list[proj], str(self))
-                    continue
-                if self.endpoint.version() < (9, 2, 0):
-                    util.logger.warning("Can't add a project branch '%s / %s' in a %s on SonarQube < 9.2", proj, branch, str(self))
-                    ok = False
-                    continue
-                util.logger.debug("Adding project '%s' branch '%s' to %s", proj, str(branch), str(self))
-                try:
-                    ok = ok and self.post("views/add_project_branch", params={"key": self.key, "project": proj, "branch": branch}).ok
-                except HTTPError as e:
-                    if e.response.status_code == HTTPStatus.NOT_FOUND:
-                        util.logger.warning("Branch '%s' of project '%s' not found, can't be added to portfolio '%s'", branch, proj, self.key)
-                        ok = False
-                    else:
-                        raise
-        return ok
-
-    def set_tag_mode(self, tags, branch):
-        self.post("views/set_tags_mode", params={"portfolio": self.key, "tags": util.list_to_csv(tags), "branch": branch})
-
-    def set_regexp_mode(self, regexp, branch):
-        self.post("views/set_regexp_mode", params={"portfolio": self.key, "regexp": regexp, "branch": branch})
-
-    def set_remaining_projects_mode(self, branch):
-        self.post("views/set_remaining_projects_mode", params={"portfolio": self.key, "branch": branch})
-
-    def none(self):
-        # Hack: API change between 9.0 and 9.1
-        if self.endpoint.version() < (9, 1, 0):
-            self.post("views/mode", params={"key": self.key, "selectionMode": "NONE"})
-        else:
-            self.post("views/set_none_mode", params={"portfolio": self.key})
-
-    def set_selection_mode(self, selection_mode, projects=None, regexp=None, tags=None, branch=None):
-        util.logger.debug("Setting selection mode %s for %s", str(selection_mode), str(self))
-        if selection_mode == SELECTION_MODE_MANUAL:
-            self.set_projects(projects)
-        elif selection_mode == SELECTION_MODE_TAGS:
-            self.set_tag_mode(tags=tags, branch=branch)
-        elif selection_mode == SELECTION_MODE_REGEXP:
-            self.set_regexp_mode(regexp=regexp, branch=branch)
-        elif selection_mode == SELECTION_MODE_OTHERS:
-            self.set_remaining_projects_mode(branch)
-        elif selection_mode == SELECTION_MODE_NONE:
-            self.none()
-        else:
-            util.logger.error("Invalid portfolio project selection mode %s during import, skipped...", selection_mode)
-            return self
-        self._selection_mode = selection_mode
-        return self
-
-    def add_subportfolio(self, key, name=None, by_ref=False):
-        # if not exists(key, self.endpoint):
-        #    util.logger.warning("Can't add in %s the subportfolio key '%s' by reference, it does not exists", str(self), key)
-        #    return False
-
-        util.logger.debug("Adding sub-portfolios to %s", str(self))
-        if self.endpoint.version() >= (9, 3, 0):
-            if not by_ref:
-                try:
-                    Portfolio.get_object(self.endpoint, key)
-                except exceptions.ObjectNotFound:
-                    Portfolio.create(self.endpoint, name, key=key, parent=self.key)
-            r = self.post("views/add_portfolio", params={"portfolio": self.key, "reference": key})
-        elif by_ref:
-            r = self.post("views/add_local_view", params={"key": self.key, "ref_key": key})
-        else:
-            r = self.post("views/add_sub_view", params={"key": self.key, "name": name, "subKey": key})
-        if not by_ref:
-            self.recompute()
-            time.sleep(0.5)
-        return r.ok
-
-    def recompute(self):
-        util.logger.debug("Recomputing %s", str(self))
-        key = self._root_portfolio.key if self._root_portfolio else self.key
-        self.post("views/refresh", params={"key": key})
-
-    def update(self, data):
-        util.logger.debug("Updating %s with %s", str(self), util.json_dump(data))
-        if "byReference" not in data or not data["byReference"]:
-            if "permissions" in data:
-                decoded_perms = {}
-                for ptype in perms.PERMISSION_TYPES:
-                    if ptype not in data["permissions"]:
-                        continue
-                    decoded_perms[ptype] = {u: perms.decode(v) for u, v in data["permissions"][ptype].items()}
-                self.set_permissions(decoded_perms)
-                # self.set_permissions(data.get("permissions", {}))
-            selection_mode = data.get(_PROJECT_SELECTION_MODE, "NONE")
-            branch = data.get(_PROJECT_SELECTION_BRANCH, None)
-            regexp = data.get(_PROJECT_SELECTION_REGEXP, None)
-            tags = data.get(_PROJECT_SELECTION_TAGS, None)
-            projects = data.get("projects", None)
-            self._root_portfolio = self.root_portfolio()
-            util.logger.debug("1.Setting root of %s is %s", str(self), str(self._root_portfolio))
-            self.set_selection_mode(selection_mode=selection_mode, projects=projects, branch=branch, regexp=regexp, tags=tags)
-        else:
-            util.logger.debug("Skipping setting portfolio details, it's a reference")
-
-        for key, subp in data.get("subPortfolios", {}).items():
-            key_list = list(self.sub_portfolios(full=True).keys())
-            if subp.get("byReference", False):
-                o_subp = Portfolio.get_object(self.endpoint, key)
-                if o_subp.key not in key_list:
-                    self.add_subportfolio(o_subp.key, name=o_subp.name, by_ref=True)
-                o_subp.update(subp)
-            else:
-                # get_list(endpoint=self.endpoint)
-                try:
-                    o = Portfolio.get_object(self.endpoint, key)
-                except exceptions.ObjectNotFound:
-                    util.logger.info("%s: Creating subportfolio from %s", str(self), util.json_dump(subp))
-                    # o = Portfolio.create(endpoint=self.endpoint, name=name, parent=self.key, **subp)
-                    self.add_subportfolio(key=key, name=subp["name"], by_ref=False)
-                o.set_parent(self)
-                o.update(subp)
-
-    def search_params(self):
-        """Return params used to search for that object
-
-        :meta private:
-        """
-        return {"portfolio": self.key}
-
-
-def count(endpoint=None):
-    return aggregations.count(api=_SEARCH_API, endpoint=endpoint)
-
-
-def get_list(endpoint, key_list=None, use_cache=True):
-    """
-    :return: List of Portfolios (all of them if key_list is None or empty)
-    :param key_list: List of portfolios keys to get, if None or empty all portfolios are returned
-    :param use_cache: Whether to use local cache or query SonarQube, default True (use cache)
-    :type use_cache: bool
-    :rtype: dict{<branchName>: <Branch>}
-    """
-    with _CLASS_LOCK:
-        if key_list is None or len(key_list) == 0 or not use_cache:
-            util.logger.info("Listing portfolios")
-            return search(endpoint=endpoint)
-        object_list = {}
-        for key in util.csv_to_list(key_list):
-            object_list[key] = Portfolio.get_object(endpoint, key)
-    return object_list
-
-
-def search(endpoint, params=None):
-    portfolio_list = {}
-    if endpoint.edition() not in ("enterprise", "datacenter"):
-        util.logger.info("No portfolios in %s edition", endpoint.edition())
-    else:
-        portfolio_list = sq.search_objects(
-            api=_SEARCH_API,
-            params=params,
-            returned_field="components",
-            key_field="key",
-            object_class=Portfolio,
-            endpoint=endpoint,
-        )
-    return portfolio_list
-
-
-def audit(audit_settings, endpoint=None, key_list=None):
-    if not audit_settings["audit.portfolios"]:
-        util.logger.debug("Auditing portfolios is disabled, skipping...")
-        return []
-    util.logger.info("--- Auditing portfolios ---")
-    problems = []
-    for p in get_list(endpoint=endpoint, key_list=key_list).values():
-        problems += p.audit(audit_settings)
-    return problems
-
-
-def loc_csv_header(**kwargs):
-    arr = ["# Portfolio Key"]
-    if kwargs[options.WITH_NAME]:
-        arr.append("Portfolio name")
-    arr.append("LoC")
-    if kwargs[options.WITH_LAST_ANALYSIS]:
-        arr.append("Last Recomputation")
-    if kwargs[options.WITH_URL]:
-        arr.append("URL")
-    return arr
-
-
-def __cleanup_portfolio_json(p):
-    for k in ("visibility", "qualifier", "branch", "referencedBy", "subViews", "selectedProjects"):
-        p.pop(k, None)
-    if "branch" in p:
-        p["projectBranch"] = p.pop("branch")
-    if "selectionMode" in p:
-        if p["selectionMode"] == SELECTION_MODE_REGEXP:
-            p[_PROJECT_SELECTION_REGEXP] = p.pop("regexp")
-        elif p["selectionMode"] == SELECTION_MODE_TAGS:
-            p[_PROJECT_SELECTION_TAGS] = util.list_to_csv(p.pop("tags"), ", ")
-        p[_PROJECT_SELECTION_MODE] = p.pop("selectionMode")
-
-
-"""
-def _sub_portfolios(json_data, version, full=False):
-    subport = {}
-    if "subViews" in json_data and len(json_data["subViews"]) > 0:
-        for p in json_data["subViews"]:
-            qual = p.pop("qualifier", _SUBPORTFOLIO_QUALIFIER)
-            p["byReference"] = qual == _PORTFOLIO_QUALIFIER
-            if qual == _PORTFOLIO_QUALIFIER:
-                p["key"] = p["originalKey"] if full else p.pop("originalKey")
-                if not full:
-                    for k in ("name", "desc"):
-                        p.pop(k, None)
-            p.update(_sub_portfolios(p, version, full))
-            __cleanup_portfolio_json(p)
-            if full:
-                subport[p["key"]] = p
-            else:
-                subport[p.pop("key")] = p
-    projects = _projects(json_data, version)
-    ret = {}
-    if projects is not None and len(projects) > 0:
-        ret["projects"] = projects
-    if len(subport) > 0:
-        ret["subPortfolios"] = subport
-    return ret
-
-
-def _projects(json_data, version):
-    if "selectionMode" not in json_data or json_data["selectionMode"] != SELECTION_MODE_MANUAL:
-        return None
-    projects = {}
-    if version >= (9, 3, 0):
-        for p in json_data["selectedProjects"]:
-            if "selectedBranches" in p:
-                projects[p["projectKey"]] = util.list_to_csv(p["selectedBranches"], ", ", True)
-            else:
-                projects[p["projectKey"]] = options.DEFAULT
-    else:
-        for p in json_data["projects"]:
-            projects[p] = options.DEFAULT
-    return projects
-"""
-
-
-def exists(key, endpoint):
-    try:
-        Portfolio.get_object(endpoint, key)
-        return True
-    except exceptions.ObjectNotFound:
-        return False
-
-
-def import_config(endpoint, config_data, key_list=None):
-    if "portfolios" not in config_data:
-        util.logger.info("No portfolios to import")
-        return
-    if endpoint.edition() in ("community", "developer"):
-        util.logger.warning("Can't import portfolios on a %s edition", endpoint.edition())
-        return
-
-    util.logger.info("Importing portfolios - pass 1: Create all top level portfolios")
-    search(endpoint=endpoint)
-    # First pass to create all top level porfolios that may be referenced
-    new_key_list = util.csv_to_list(key_list)
-    for key, data in config_data["portfolios"].items():
-        if new_key_list and key not in new_key_list:
-            continue
-        util.logger.info("Importing portfolio key '%s'", key)
-        try:
-            o = Portfolio.get_object(endpoint, key)
-        except exceptions.ObjectNotFound:
-            util.logger.debug("Portfolio not found, creating it")
-            newdata = data.copy()
-            name = newdata.pop("name")
-            o = Portfolio.create(endpoint=endpoint, name=name, key=key, **newdata)
-            o.reload(data)
-        nbr_creations = __create_portfolio_hierarchy(endpoint=endpoint, data=data, parent_key=key)
-        # Hack: When subportfolios are created, recompute is needed to get them in the
-        # api/views/search results
-        if nbr_creations > 0:
-            o.recompute()
-            # Sleep 500ms per created portfolio
-            time.sleep(nbr_creations * 500 / 1000)
-    # Second pass to define hierarchies
-    util.logger.info("Importing portfolios - pass 2: Creating sub-portfolios")
-    for key, data in config_data["portfolios"].items():
-        if new_key_list and key not in new_key_list:
-            continue
-        try:
-            o = Portfolio.get_object(endpoint, key)
-            o.update(data)
-        except exceptions.ObjectNotFound:
-            util.logger.error("Can't find portfolio key '%s', name '%s'", key, data["name"])
-
-
-def search_by_name(endpoint, name):
-    return util.search_by_name(endpoint, name, _SEARCH_API, "components")
-
-
-def search_by_key(endpoint, key):
-    return util.search_by_key(endpoint, key, _SEARCH_API, "components")
-
-
-def export(endpoint, key_list=None, full=False):
-    if endpoint.edition() in ("community", "developer"):
-        util.logger.info("No portfolios in community and developer editions")
-        return None
-    util.logger.info("Exporting portfolios")
-    if key_list:
-        nb_portfolios = len(key_list)
-    else:
-        nb_portfolios = count(endpoint=endpoint)
-    i = 0
-    exported_portfolios = {}
-    for k, p in get_list(endpoint=endpoint, key_list=key_list).items():
-        if not p.is_sub_portfolio:
-            exported_portfolios[k] = p.export(full)
-            exported_portfolios[k].pop("key")
-        else:
-            util.logger.debug("Skipping export of %s, it's a standard sub-portfolio", str(p))
-        i += 1
-        if i % 50 == 0 or i == nb_portfolios:
-            util.logger.info("Exported %d/%d portfolios (%d%%)", i, nb_portfolios, (i * 100) // nb_portfolios)
-    return exported_portfolios
-
-
-def recompute(endpoint):
-    endpoint.post("views/refresh")
-
-
-def _find_sub_portfolio(key, data):
-    for subp in data.get("subViews", []):
-        if subp["key"] == key:
-            return subp
-        child = _find_sub_portfolio(key, subp)
-        if child is not None:
-            return child
-    return []
-
-
-def __create_portfolio_hierarchy(endpoint, data, parent_key):
-    nbr_creations = 0
-    o_parent = Portfolio.get_object(endpoint, parent_key)
-    for key, subp in data.get("subPortfolios", {}).items():
-        if subp.get("byReference", False):
-            continue
-        try:
-            o = Portfolio.get_object(endpoint, key)
-        except exceptions.ObjectNotFound:
-            newdata = subp.copy()
-            name = newdata.pop("name")
-            util.logger.debug("Object not found, creating portfolio name '%s'", name)
-            o = Portfolio.create(endpoint, name, key=key, parent=o_parent.key, **newdata)
-            o.reload(subp)
-            nbr_creations += 1
-        o.set_parent(o_parent)
-        nbr_creations += __create_portfolio_hierarchy(endpoint, subp, parent_key=key)
-    return nbr_creations
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+"""
+
+    Abstraction of the SonarQube "portfolio" concept
+
+"""
+
+import time
+import json
+from http import HTTPStatus
+from threading import Lock
+from requests.exceptions import HTTPError
+
+from sonar import aggregations, options, exceptions
+import sonar.permissions.permissions as perms
+import sonar.permissions.portfolio_permissions as pperms
+import sonar.sqobject as sq
+import sonar.utilities as util
+from sonar.audit import rules
+
+_OBJECTS = {}
+_CLASS_LOCK = Lock()
+
+_LIST_API = "views/list"
+_SEARCH_API = "views/search"
+_CREATE_API = "views/create"
+_GET_API = "views/show"
+
+MAX_PAGE_SIZE = 500
+_PORTFOLIO_QUALIFIER = "VW"
+_SUBPORTFOLIO_QUALIFIER = "SVW"
+
+SELECTION_MODE_MANUAL = "MANUAL"
+SELECTION_MODE_REGEXP = "REGEXP"
+SELECTION_MODE_TAGS = "TAGS"
+SELECTION_MODE_OTHERS = "REST"
+SELECTION_MODE_NONE = "NONE"
+SELECTION_MODES = (SELECTION_MODE_MANUAL, SELECTION_MODE_REGEXP, SELECTION_MODE_TAGS, SELECTION_MODE_OTHERS, SELECTION_MODE_NONE)
+
+_PROJECT_SELECTION_MODE = "projectSelectionMode"
+_PROJECT_SELECTION_BRANCH = "projectSelectionBranch"
+_PROJECT_SELECTION_REGEXP = "projectSelectionRegexp"
+_PROJECT_SELECTION_TAGS = "projectSelectionTags"
+
+_IMPORTABLE_PROPERTIES = (
+    "key",
+    "name",
+    "description",
+    _PROJECT_SELECTION_MODE,
+    "visibility",
+    _PROJECT_SELECTION_REGEXP,
+    _PROJECT_SELECTION_BRANCH,
+    _PROJECT_SELECTION_TAGS,
+    "permissions",
+    "subPortfolios",
+    "projects",
+)
+
+
+class Portfolio(aggregations.Aggregation):
+    @classmethod
+    def get_object(cls, endpoint, key):
+        util.logger.debug("Getting object '%s'", key)
+        # if root_key is None:
+        # data = search_by_name(endpoint=endpoint, name=name)
+        # else:
+        #    data = _find_sub_portfolio_by_name(name=name, data=_OBJECTS[root_key]._json)
+        # if data is None:
+        #    return None
+        # key = data["key"]
+        if key in _OBJECTS:
+            return _OBJECTS[key]
+        data = search_by_key(endpoint, key)
+        if data is None:
+            raise exceptions.ObjectNotFound(key, f"Portfolios '{key}' not found")
+        return Portfolio.load(endpoint=endpoint, data=data)
+
+    @classmethod
+    def create(cls, endpoint, name, **kwargs):
+        util.logger.debug("Creating portfolio name '%s', key '%s', parent = %s", name, str(kwargs.get("key", None)), str(kwargs.get("parent", None)))
+        params = {"name": name}
+        for p in ("description", "parent", "key", "visibility"):
+            params[p] = kwargs.get(p, None)
+        endpoint.post(_CREATE_API, params=params)
+        o = cls(endpoint=endpoint, name=name, key=kwargs.get("key", None))
+        if "parent" in kwargs:
+            o.set_parent(Portfolio.get_object(endpoint, kwargs["parent"]))
+        # TODO - Allow on the fly selection mode
+        return o
+
+    @classmethod
+    def load(cls, endpoint, data):
+        util.logger.debug("Loading portfolio '%s' with data %s", data["name"], util.json_dump(data))
+        o = cls(endpoint=endpoint, name=data["name"], key=data["key"])
+        o.reload(data)
+        if not o.is_sub_portfolio:
+            o.refresh()
+        return o
+
+    def __init__(self, endpoint, name, key=None):
+        super().__init__(key if key else name, endpoint)
+        self.name = name
+        self._selection_mode = None  #: Portfolio project selection mode
+        self._selection_branch = None  #: project branches on SonarQube 9.2+
+        self._projects = None  #: Portfolio list of projects when selection mode is MANUAL
+        self._regexp = None  #: Project selection regexp is selection mode is REGEXP
+        self._tags = []  #: Portfolio tags when selection mode is TAGS
+        self._description = None  #: Portfolio description
+        self.is_sub_portfolio = False  #: Whether the portfolio is a subportfolio
+        self._visibility = None  #: Portfolio visibility
+        self._sub_portfolios = None  #: Subportfolios
+        self._permissions = None  #: Permissions
+        self.parent = None  #: Ref to parent portfolio object, if any
+        self._root_portfolio = None  #: Ref to root portfolio, if any
+        _OBJECTS[self.uuid()] = self
+        util.logger.debug("Created portfolio object name '%s'", name)
+        util.logger.debug("PORTFOLIOS = %s", str([p.key for p in _OBJECTS.values()]))
+
+    def reload(self, data):
+        util.logger.debug("Reloading %s with %s", str(self), util.json_dump(data))
+        super().reload(data)
+        if "name" in data:
+            self.name = data["name"]
+        if "selectionMode" in self._json:
+            self._selection_mode = self._json["selectionMode"]
+        if "branch" in self._json:
+            self._selection_branch = self._json["branch"]
+        if "regexp" in self._json:
+            self._regexp = self._json["regexp"]
+        self.is_sub_portfolio = self._json.get("qualifier", _PORTFOLIO_QUALIFIER) == _SUBPORTFOLIO_QUALIFIER
+        if "tags" in self._json:
+            self._tags = self._json["tags"]
+        if "visibility" in self._json:
+            self._visibility = self._json["visibility"]
+        parent = data.get("parentKey", data.get("parent", None))
+        if parent:
+            self.set_parent(Portfolio.get_object(self.endpoint, parent))
+
+    def __str__(self):
+        return f"subportfolio '{self.key}'" if self.is_sub_portfolio else f"portfolio '{self.key}'"
+
+    def refresh(self):
+        util.logger.debug("Updating details for %s root key %s", str(self), self._root_portfolio)
+        data = json.loads(self.get(_GET_API, params={"key": self.root_portfolio().key}).text)
+        if not self.is_sub_portfolio:
+            self.reload(data)
+        self.root_portfolio().create_sub_portfolios()
+        self.projects()
+
+    def set_parent(self, parent_portfolio):
+        self.parent = parent_portfolio
+        self._root_portfolio = self.root_portfolio()
+        util.logger.debug("%s: Parent = %s, Root = %s", str(self), str(self.parent), str(self._root_portfolio))
+
+    def url(self):
+        return f"{self.endpoint.url}/portfolio?id={self.key}"
+
+    def selection_mode(self):
+        return self._selection_mode
+
+    def root_portfolio(self):
+        if self.parent is None:
+            util.logger.debug("Found root for %s, parent = %s", self.key, str(self.parent))
+            self._root_portfolio = self
+        else:
+            util.logger.debug("recursing root for %s, parent = %s", self.key, str(self.parent))
+            self._root_portfolio = self.parent.root_portfolio()
+        return self._root_portfolio
+
+    def projects(self):
+        if self._selection_mode != SELECTION_MODE_MANUAL:
+            util.logger.debug("%s: Not manual mode, no projects", str(self))
+            self._projects = {}
+            return self._projects
+        if self._projects is not None:
+            util.logger.debug("%s: Projects already set, returning %s", str(self), str(self._projects))
+            return self._projects
+        if "selectedProjects" not in self._json:
+            self.refresh()
+        self._projects = {}
+        util.logger.debug("%s: Read projects %s", str(self), str(self._projects))
+        if self.endpoint.version() < (9, 3, 0):
+            for p in self._json.get("projects", {}):
+                self._projects[p] = options.DEFAULT
+            return self._projects
+        for p in self._json.get("selectedProjects", {}):
+            if "selectedBranches" in p:
+                self._projects[p["projectKey"]] = util.list_to_csv(p["selectedBranches"], ", ", True)
+            else:
+                self._projects[p["projectKey"]] = options.DEFAULT
+        util.logger.debug("%s: PROJ4 Read projects %s", str(self), str(self._projects))
+        util.logger.debug("%s projects = %s", str(self), util.json_dump(self._projects))
+        return self._projects
+
+    def sub_portfolios(self, full=False):
+        self.refresh()
+        # self._sub_portfolios = _sub_portfolios(self._json, self.endpoint.version(), full=full)
+        self.create_sub_portfolios()
+        return self._sub_portfolios
+
+    def create_sub_portfolios(self):
+        if "subViews" in self._json and len(self._json["subViews"]) > 0:
+            util.logger.debug("Inspecting %s subportfolios data = %s", str(self), util.json_dump(self._json["subViews"]))
+            self._sub_portfolios = {}
+            for oldp in self._json["subViews"]:
+                p = oldp.copy()
+                util.logger.debug("Found subport data = %s", util.json_dump(p))
+                if p["qualifier"] == _PORTFOLIO_QUALIFIER:
+                    key = p.get("originalKey", None)
+                    if key is None:
+                        key = p.pop("key").split(":")[-1]
+                else:
+                    key = p.pop("key")
+                try:
+                    subp = Portfolio.get_object(self.endpoint, key)
+                    subp.set_parent(self)
+                except exceptions.ObjectNotFound:
+                    subp = Portfolio.create(self.endpoint, name=p.pop("name"), key=key, parent=self.key, description=p.pop("desc", None), **p)
+                util.logger.debug("Subp = %s", str(subp))
+                subp.reload(oldp)
+                self._sub_portfolios[subp.key] = subp
+                subp.create_sub_portfolios()
+                subp.projects()
+
+    def regexp(self):
+        if self.selection_mode() != SELECTION_MODE_REGEXP:
+            self._regexp = None
+        elif self._regexp is None:
+            self._regexp = self._json["regexp"]
+        return self._regexp
+
+    def tags(self):
+        if self.selection_mode() != SELECTION_MODE_TAGS:
+            self._tags = None
+        elif self._tags is None:
+            self._tags = self._json.pop("tags", [])
+        return self._tags
+
+    def get_components(self):
+        data = json.loads(
+            self.get(
+                "measures/component_tree",
+                params={
+                    "component": self.key,
+                    "metricKeys": "ncloc",
+                    "strategy": "children",
+                    "ps": 500,
+                },
+            ).text
+        )
+        comp_list = {}
+        for c in data["components"]:
+            comp_list[c["key"]] = c
+        return comp_list
+
+    def delete(self):
+        return sq.delete_object(self, "views/delete", {"key": self.key}, _OBJECTS)
+
+    def _audit_empty(self, audit_settings):
+        if not audit_settings.get("audit.portfolios.empty", True):
+            util.logger.debug("Auditing empty portfolios is disabled, skipping...")
+            return []
+        return self._audit_empty_aggregation(broken_rule=rules.RuleId.PORTFOLIO_EMPTY)
+
+    def _audit_singleton(self, audit_settings):
+        if not audit_settings.get("audit.portfolios.singleton", True):
+            util.logger.debug("Auditing singleton portfolios is disabled, skipping...")
+            return []
+        return self._audit_singleton_aggregation(broken_rule=rules.RuleId.PORTFOLIO_SINGLETON)
+
+    def audit(self, audit_settings):
+        util.logger.info("Auditing %s", str(self))
+        return self._audit_empty(audit_settings) + self._audit_singleton(audit_settings) + self._audit_bg_task(audit_settings)
+
+    def export(self, full=False):
+        util.logger.info("Exporting %s", str(self))
+        self.refresh()
+        json_data = self._json
+        subp = self.sub_portfolios(full=full)
+        if subp:
+            json_data.update(subp)
+        json_data.update(
+            {
+                "key": self.key,
+                "name": self.name,
+                "description": None if self._description == "" else self._description,
+                _PROJECT_SELECTION_MODE: self.selection_mode(),
+                "visibility": self._visibility,
+                _PROJECT_SELECTION_REGEXP: self.regexp(),
+                _PROJECT_SELECTION_BRANCH: self._selection_branch,
+                _PROJECT_SELECTION_TAGS: util.list_to_csv(self.tags(), separator=", "),
+                "permissions": self.permissions().export(),
+            }
+        )
+        if self.selection_mode() == SELECTION_MODE_MANUAL:
+            json_data["projects"] = self.projects()
+
+        return util.remove_nones(util.filter_export(json_data, _IMPORTABLE_PROPERTIES, full))
+
+    def permissions(self):
+        if self._permissions is None and not self.is_sub_portfolio:
+            # No permissions for sub portfolios
+            self._permissions = pperms.PortfolioPermissions(self)
+        return self._permissions
+
+    def set_permissions(self, portfolio_perms):
+        if not self.is_sub_portfolio:
+            # No permissions for SVW
+            self.permissions().set(portfolio_perms)
+
+    def set_component_tags(self, tags, api):
+        util.logger.warning("Can't set tags on portfolios, operation skipped...")
+
+    def set_projects(self, project_list):
+        self.post("views/set_manual_mode", params={"portfolio": self.key})
+        self._selection_mode = SELECTION_MODE_MANUAL
+        current_projects = self.projects()
+        current_project_keys = list(current_projects.keys())
+        util.logger.debug("Project list = %s", str(project_list))
+        if project_list is None:
+            return False
+        util.logger.debug("Current Project list = %s", str(current_project_keys))
+        ok = True
+        for proj, branches in project_list.items():
+            if proj in current_project_keys:
+                util.logger.debug("Won't add project '%s' branch '%s' to %s, it's already added", proj, project_list[proj], str(self))
+                continue
+            try:
+                r = self.post("views/add_project", params={"key": self.key, "project": proj})
+                ok = ok and r.ok
+                current_projects[proj] = options.DEFAULT
+            except HTTPError as e:
+                if e.response.status_code == HTTPStatus.NOT_FOUND:
+                    raise exceptions.ObjectNotFound(self.key, f"Project '{proj}' not found, can't be added to portfolio '{self.key}'")
+                raise
+
+            for branch in util.csv_to_list(branches):
+                if branch == options.DEFAULT or branch in util.csv_to_list(current_projects[proj]):
+                    util.logger.debug("Won't add project '%s' branch '%s' to %s, it's already added", proj, project_list[proj], str(self))
+                    continue
+                if self.endpoint.version() < (9, 2, 0):
+                    util.logger.warning("Can't add a project branch '%s / %s' in a %s on SonarQube < 9.2", proj, branch, str(self))
+                    ok = False
+                    continue
+                util.logger.debug("Adding project '%s' branch '%s' to %s", proj, str(branch), str(self))
+                try:
+                    ok = ok and self.post("views/add_project_branch", params={"key": self.key, "project": proj, "branch": branch}).ok
+                except HTTPError as e:
+                    if e.response.status_code == HTTPStatus.NOT_FOUND:
+                        util.logger.warning("Branch '%s' of project '%s' not found, can't be added to portfolio '%s'", branch, proj, self.key)
+                        ok = False
+                    else:
+                        raise
+        return ok
+
+    def set_tag_mode(self, tags, branch):
+        self.post("views/set_tags_mode", params={"portfolio": self.key, "tags": util.list_to_csv(tags), "branch": branch})
+
+    def set_regexp_mode(self, regexp, branch):
+        self.post("views/set_regexp_mode", params={"portfolio": self.key, "regexp": regexp, "branch": branch})
+
+    def set_remaining_projects_mode(self, branch):
+        self.post("views/set_remaining_projects_mode", params={"portfolio": self.key, "branch": branch})
+
+    def none(self):
+        # Hack: API change between 9.0 and 9.1
+        if self.endpoint.version() < (9, 1, 0):
+            self.post("views/mode", params={"key": self.key, "selectionMode": "NONE"})
+        else:
+            self.post("views/set_none_mode", params={"portfolio": self.key})
+
+    def set_selection_mode(self, selection_mode, projects=None, regexp=None, tags=None, branch=None):
+        util.logger.debug("Setting selection mode %s for %s", str(selection_mode), str(self))
+        if selection_mode == SELECTION_MODE_MANUAL:
+            self.set_projects(projects)
+        elif selection_mode == SELECTION_MODE_TAGS:
+            self.set_tag_mode(tags=tags, branch=branch)
+        elif selection_mode == SELECTION_MODE_REGEXP:
+            self.set_regexp_mode(regexp=regexp, branch=branch)
+        elif selection_mode == SELECTION_MODE_OTHERS:
+            self.set_remaining_projects_mode(branch)
+        elif selection_mode == SELECTION_MODE_NONE:
+            self.none()
+        else:
+            util.logger.error("Invalid portfolio project selection mode %s during import, skipped...", selection_mode)
+            return self
+        self._selection_mode = selection_mode
+        return self
+
+    def add_subportfolio(self, key, name=None, by_ref=False):
+        # if not exists(key, self.endpoint):
+        #    util.logger.warning("Can't add in %s the subportfolio key '%s' by reference, it does not exists", str(self), key)
+        #    return False
+
+        util.logger.debug("Adding sub-portfolios to %s", str(self))
+        if self.endpoint.version() >= (9, 3, 0):
+            if not by_ref:
+                try:
+                    Portfolio.get_object(self.endpoint, key)
+                except exceptions.ObjectNotFound:
+                    Portfolio.create(self.endpoint, name, key=key, parent=self.key)
+            r = self.post("views/add_portfolio", params={"portfolio": self.key, "reference": key})
+        elif by_ref:
+            r = self.post("views/add_local_view", params={"key": self.key, "ref_key": key})
+        else:
+            r = self.post("views/add_sub_view", params={"key": self.key, "name": name, "subKey": key})
+        if not by_ref:
+            self.recompute()
+            time.sleep(0.5)
+        return r.ok
+
+    def recompute(self):
+        util.logger.debug("Recomputing %s", str(self))
+        key = self._root_portfolio.key if self._root_portfolio else self.key
+        self.post("views/refresh", params={"key": key})
+
+    def update(self, data):
+        util.logger.debug("Updating %s with %s", str(self), util.json_dump(data))
+        if "byReference" not in data or not data["byReference"]:
+            if "permissions" in data:
+                decoded_perms = {}
+                for ptype in perms.PERMISSION_TYPES:
+                    if ptype not in data["permissions"]:
+                        continue
+                    decoded_perms[ptype] = {u: perms.decode(v) for u, v in data["permissions"][ptype].items()}
+                self.set_permissions(decoded_perms)
+                # self.set_permissions(data.get("permissions", {}))
+            selection_mode = data.get(_PROJECT_SELECTION_MODE, "NONE")
+            branch = data.get(_PROJECT_SELECTION_BRANCH, None)
+            regexp = data.get(_PROJECT_SELECTION_REGEXP, None)
+            tags = data.get(_PROJECT_SELECTION_TAGS, None)
+            projects = data.get("projects", None)
+            self._root_portfolio = self.root_portfolio()
+            util.logger.debug("1.Setting root of %s is %s", str(self), str(self._root_portfolio))
+            self.set_selection_mode(selection_mode=selection_mode, projects=projects, branch=branch, regexp=regexp, tags=tags)
+        else:
+            util.logger.debug("Skipping setting portfolio details, it's a reference")
+
+        for key, subp in data.get("subPortfolios", {}).items():
+            key_list = list(self.sub_portfolios(full=True).keys())
+            if subp.get("byReference", False):
+                o_subp = Portfolio.get_object(self.endpoint, key)
+                if o_subp.key not in key_list:
+                    self.add_subportfolio(o_subp.key, name=o_subp.name, by_ref=True)
+                o_subp.update(subp)
+            else:
+                # get_list(endpoint=self.endpoint)
+                try:
+                    o = Portfolio.get_object(self.endpoint, key)
+                except exceptions.ObjectNotFound:
+                    util.logger.info("%s: Creating subportfolio from %s", str(self), util.json_dump(subp))
+                    # o = Portfolio.create(endpoint=self.endpoint, name=name, parent=self.key, **subp)
+                    self.add_subportfolio(key=key, name=subp["name"], by_ref=False)
+                o.set_parent(self)
+                o.update(subp)
+
+    def search_params(self):
+        """Return params used to search for that object
+
+        :meta private:
+        """
+        return {"portfolio": self.key}
+
+
+def count(endpoint=None):
+    return aggregations.count(api=_SEARCH_API, endpoint=endpoint)
+
+
+def get_list(endpoint, key_list=None, use_cache=True):
+    """
+    :return: List of Portfolios (all of them if key_list is None or empty)
+    :param key_list: List of portfolios keys to get, if None or empty all portfolios are returned
+    :param use_cache: Whether to use local cache or query SonarQube, default True (use cache)
+    :type use_cache: bool
+    :rtype: dict{<branchName>: <Branch>}
+    """
+    with _CLASS_LOCK:
+        if key_list is None or len(key_list) == 0 or not use_cache:
+            util.logger.info("Listing portfolios")
+            return search(endpoint=endpoint)
+        object_list = {}
+        for key in util.csv_to_list(key_list):
+            object_list[key] = Portfolio.get_object(endpoint, key)
+    return object_list
+
+
+def search(endpoint, params=None):
+    portfolio_list = {}
+    if endpoint.edition() not in ("enterprise", "datacenter"):
+        util.logger.info("No portfolios in %s edition", endpoint.edition())
+    else:
+        portfolio_list = sq.search_objects(
+            api=_SEARCH_API,
+            params=params,
+            returned_field="components",
+            key_field="key",
+            object_class=Portfolio,
+            endpoint=endpoint,
+        )
+    return portfolio_list
+
+
+def audit(audit_settings, endpoint=None, key_list=None):
+    if not audit_settings.get("audit.portfolios", True):
+        util.logger.debug("Auditing portfolios is disabled, skipping...")
+        return []
+    util.logger.info("--- Auditing portfolios ---")
+    problems = []
+    for p in get_list(endpoint=endpoint, key_list=key_list).values():
+        problems += p.audit(audit_settings)
+    return problems
+
+
+def loc_csv_header(**kwargs):
+    arr = ["# Portfolio Key"]
+    if kwargs[options.WITH_NAME]:
+        arr.append("Portfolio name")
+    arr.append("LoC")
+    if kwargs[options.WITH_LAST_ANALYSIS]:
+        arr.append("Last Recomputation")
+    if kwargs[options.WITH_URL]:
+        arr.append("URL")
+    return arr
+
+
+def __cleanup_portfolio_json(p):
+    for k in ("visibility", "qualifier", "branch", "referencedBy", "subViews", "selectedProjects"):
+        p.pop(k, None)
+    if "branch" in p:
+        p["projectBranch"] = p.pop("branch")
+    if "selectionMode" in p:
+        if p["selectionMode"] == SELECTION_MODE_REGEXP:
+            p[_PROJECT_SELECTION_REGEXP] = p.pop("regexp")
+        elif p["selectionMode"] == SELECTION_MODE_TAGS:
+            p[_PROJECT_SELECTION_TAGS] = util.list_to_csv(p.pop("tags"), ", ")
+        p[_PROJECT_SELECTION_MODE] = p.pop("selectionMode")
+
+
+"""
+def _sub_portfolios(json_data, version, full=False):
+    subport = {}
+    if "subViews" in json_data and len(json_data["subViews"]) > 0:
+        for p in json_data["subViews"]:
+            qual = p.pop("qualifier", _SUBPORTFOLIO_QUALIFIER)
+            p["byReference"] = qual == _PORTFOLIO_QUALIFIER
+            if qual == _PORTFOLIO_QUALIFIER:
+                p["key"] = p["originalKey"] if full else p.pop("originalKey")
+                if not full:
+                    for k in ("name", "desc"):
+                        p.pop(k, None)
+            p.update(_sub_portfolios(p, version, full))
+            __cleanup_portfolio_json(p)
+            if full:
+                subport[p["key"]] = p
+            else:
+                subport[p.pop("key")] = p
+    projects = _projects(json_data, version)
+    ret = {}
+    if projects is not None and len(projects) > 0:
+        ret["projects"] = projects
+    if len(subport) > 0:
+        ret["subPortfolios"] = subport
+    return ret
+
+
+def _projects(json_data, version):
+    if "selectionMode" not in json_data or json_data["selectionMode"] != SELECTION_MODE_MANUAL:
+        return None
+    projects = {}
+    if version >= (9, 3, 0):
+        for p in json_data["selectedProjects"]:
+            if "selectedBranches" in p:
+                projects[p["projectKey"]] = util.list_to_csv(p["selectedBranches"], ", ", True)
+            else:
+                projects[p["projectKey"]] = options.DEFAULT
+    else:
+        for p in json_data["projects"]:
+            projects[p] = options.DEFAULT
+    return projects
+"""
+
+
+def exists(key, endpoint):
+    try:
+        Portfolio.get_object(endpoint, key)
+        return True
+    except exceptions.ObjectNotFound:
+        return False
+
+
+def import_config(endpoint, config_data, key_list=None):
+    if "portfolios" not in config_data:
+        util.logger.info("No portfolios to import")
+        return
+    if endpoint.edition() in ("community", "developer"):
+        util.logger.warning("Can't import portfolios on a %s edition", endpoint.edition())
+        return
+
+    util.logger.info("Importing portfolios - pass 1: Create all top level portfolios")
+    search(endpoint=endpoint)
+    # First pass to create all top level porfolios that may be referenced
+    new_key_list = util.csv_to_list(key_list)
+    for key, data in config_data["portfolios"].items():
+        if new_key_list and key not in new_key_list:
+            continue
+        util.logger.info("Importing portfolio key '%s'", key)
+        try:
+            o = Portfolio.get_object(endpoint, key)
+        except exceptions.ObjectNotFound:
+            util.logger.debug("Portfolio not found, creating it")
+            newdata = data.copy()
+            name = newdata.pop("name")
+            o = Portfolio.create(endpoint=endpoint, name=name, key=key, **newdata)
+            o.reload(data)
+        nbr_creations = __create_portfolio_hierarchy(endpoint=endpoint, data=data, parent_key=key)
+        # Hack: When subportfolios are created, recompute is needed to get them in the
+        # api/views/search results
+        if nbr_creations > 0:
+            o.recompute()
+            # Sleep 500ms per created portfolio
+            time.sleep(nbr_creations * 500 / 1000)
+    # Second pass to define hierarchies
+    util.logger.info("Importing portfolios - pass 2: Creating sub-portfolios")
+    for key, data in config_data["portfolios"].items():
+        if new_key_list and key not in new_key_list:
+            continue
+        try:
+            o = Portfolio.get_object(endpoint, key)
+            o.update(data)
+        except exceptions.ObjectNotFound:
+            util.logger.error("Can't find portfolio key '%s', name '%s'", key, data["name"])
+
+
+def search_by_name(endpoint, name):
+    return util.search_by_name(endpoint, name, _SEARCH_API, "components")
+
+
+def search_by_key(endpoint, key):
+    return util.search_by_key(endpoint, key, _SEARCH_API, "components")
+
+
+def export(endpoint, key_list=None, full=False):
+    if endpoint.edition() in ("community", "developer"):
+        util.logger.info("No portfolios in community and developer editions")
+        return None
+    util.logger.info("Exporting portfolios")
+    if key_list:
+        nb_portfolios = len(key_list)
+    else:
+        nb_portfolios = count(endpoint=endpoint)
+    i = 0
+    exported_portfolios = {}
+    for k, p in get_list(endpoint=endpoint, key_list=key_list).items():
+        if not p.is_sub_portfolio:
+            exported_portfolios[k] = p.export(full)
+            exported_portfolios[k].pop("key")
+        else:
+            util.logger.debug("Skipping export of %s, it's a standard sub-portfolio", str(p))
+        i += 1
+        if i % 50 == 0 or i == nb_portfolios:
+            util.logger.info("Exported %d/%d portfolios (%d%%)", i, nb_portfolios, (i * 100) // nb_portfolios)
+    return exported_portfolios
+
+
+def recompute(endpoint):
+    endpoint.post("views/refresh")
+
+
+def _find_sub_portfolio(key, data):
+    for subp in data.get("subViews", []):
+        if subp["key"] == key:
+            return subp
+        child = _find_sub_portfolio(key, subp)
+        if child is not None:
+            return child
+    return []
+
+
+def __create_portfolio_hierarchy(endpoint, data, parent_key):
+    nbr_creations = 0
+    o_parent = Portfolio.get_object(endpoint, parent_key)
+    for key, subp in data.get("subPortfolios", {}).items():
+        if subp.get("byReference", False):
+            continue
+        try:
+            o = Portfolio.get_object(endpoint, key)
+        except exceptions.ObjectNotFound:
+            newdata = subp.copy()
+            name = newdata.pop("name")
+            util.logger.debug("Object not found, creating portfolio name '%s'", name)
+            o = Portfolio.create(endpoint, name, key=key, parent=o_parent.key, **newdata)
+            o.reload(subp)
+            nbr_creations += 1
+        o.set_parent(o_parent)
+        nbr_creations += __create_portfolio_hierarchy(endpoint, subp, parent_key=key)
+    return nbr_creations
```

## sonar/qualitygates.py

 * *Ordering differences only*

```diff
@@ -1,462 +1,462 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-"""
-
-    Abstraction of the SonarQube "quality gate" concept
-
-"""
-
-from http import HTTPStatus
-import json
-from requests.exceptions import HTTPError
-import sonar.sqobject as sq
-from sonar import measures, exceptions
-import sonar.permissions.qualitygate_permissions as permissions
-from sonar.projects import projects
-import sonar.utilities as util
-
-from sonar.audit import rules, severities, types
-import sonar.audit.problem as pb
-
-
-_OBJECTS = {}
-_MAP = {}
-
-#: Quality gates APIs
-APIS = {
-    "create": "qualitygates/create",
-    "list": "qualitygates/list",
-    "rename": "qualitygates/rename",
-    "details": "qualitygates/show",
-    "get_projects": "qualitygates/search",
-}
-
-__NEW_ISSUES_SHOULD_BE_ZERO = "Any numeric threshold on new issues should be 0 or should be removed from QG conditions"
-
-GOOD_QG_CONDITIONS = {
-    "new_reliability_rating": (1, 1, "Any rating other than A would let bugs slip through in new code"),
-    "new_security_rating": (1, 1, "Any rating other than A would let vulnerabilities slip through in new code"),
-    "new_maintainability_rating": (1, 1, "Expectation is that code smells density on new code is low enough to get A rating"),
-    "new_coverage": (20, 90, "Coverage below 20% is a too low bar, above 90% is overkill"),
-    "new_bugs": (0, 0, __NEW_ISSUES_SHOULD_BE_ZERO),
-    "new_vulnerabilities": (0, 0, __NEW_ISSUES_SHOULD_BE_ZERO),
-    "new_security_hotspots": (0, 0, __NEW_ISSUES_SHOULD_BE_ZERO),
-    "new_blocker_violations": (0, 0, __NEW_ISSUES_SHOULD_BE_ZERO),
-    "new_critical_violations": (0, 0, __NEW_ISSUES_SHOULD_BE_ZERO),
-    "new_major_violations": (0, 0, __NEW_ISSUES_SHOULD_BE_ZERO),
-    "new_duplicated_lines_density": (1, 5, "Duplication on new code of less than 1% is overkill, more than 5% is too relaxed"),
-    "new_security_hotspots_reviewed": (100, 100, "All hotspots on new code must be reviewed, any other condition than 100% make little sense"),
-    "reliability_rating": (4, 4, "Threshold on overall code should not be too strict or passing the QG will be often impossible"),
-    "security_rating": (4, 4, "Threshold on overall code should not be too strict or passing the QG will be often impossible"),
-}
-
-_IMPORTABLE_PROPERTIES = ("isDefault", "isBuiltIn", "conditions", "permissions")
-
-
-class QualityGate(sq.SqObject):
-    """
-    Abstraction of the Sonar Quality Gate concept
-    """
-
-    @classmethod
-    def get_object(cls, endpoint, name):
-        """Reads a quality gate from SonarQube
-
-        :param Platform endpoint: Reference to the SonarQube platform
-        :param str name: Quality gate
-        :return: the QualityGate object or None if not found
-        :rtype: QualityGate or None
-        """
-        if name in _MAP and _MAP[name] in _OBJECTS:
-            return _OBJECTS[_MAP[name]]
-        data = search_by_name(endpoint, name)
-        if not data:
-            raise exceptions.ObjectNotFound(name, f"Quality gate '{name}' not found")
-        return cls.load(endpoint, data)
-
-    @classmethod
-    def load(cls, endpoint, data):
-        """Creates a quality gate from search data
-        :return: the QualityGate object
-        :rtype: QualityGate or None
-        """
-        # SonarQube 10 compatibility: "id" field dropped, replaced by "name"
-        o = _OBJECTS.get(data.get("id", data["name"]))
-        if not o:
-            o = cls(data["name"], endpoint, data=data)
-        o._json = data
-        return o
-
-    @classmethod
-    def create(cls, endpoint, name):
-        r = endpoint.post(APIS["create"], params={"name": name})
-        if not r.ok:
-            return None
-        return cls.get_object(endpoint, name)
-
-    def __init__(self, name, endpoint, data):
-        super().__init__(name, endpoint)
-        self.name = name  #: Object name
-        self.is_built_in = False  #: Whether the quality gate is built in
-        self.is_default = False  #: Whether the quality gate is the default
-        self._conditions = None  #: Quality gate conditions
-        self._permissions = None  #: Quality gate permissions
-        self._projects = None  #: Projects using this quality profile
-        self._json = data
-        self.name = data.pop("name")
-        self.key = data.pop("id", self.name)
-        self.is_default = data.get("isDefault", False)
-        self.is_built_in = data.get("isBuiltIn", False)
-        self.conditions()
-        self.permissions()
-        _OBJECTS[self.key] = self
-        _MAP[self.name] = self.key
-
-    def __str__(self):
-        """
-        :return: String formatting of the object
-        :rtype: str
-        """
-        return f"quality gate '{self.name}'"
-
-    def url(self):
-        """
-        :return: the SonarQube permalink URL to the quality gate
-        :rtype: str
-        """
-        return f"{self.endpoint.url}/quality_gates/show/{self.key}"
-
-    def projects(self):
-        """
-        :raises ObjectNotFound: Quality gate not found
-        :return: The list of projects using this quality gate
-        :rtype: dict {<projectKey>: <projectData>}
-        """
-        if self._projects is not None:
-            return self._projects
-        params = {"gateName": self.name, "ps": 500}
-        page, nb_pages = 1, 1
-        self._projects = {}
-        while page <= nb_pages:
-            params["p"] = page
-            try:
-                resp = self.get(APIS["get_projects"], params=params)
-            except HTTPError as e:
-                if e.response.status_code == HTTPStatus.NOT_FOUND:
-                    raise exceptions.ObjectNotFound(self.name, f"{str(self)} not found")
-            data = json.loads(resp.text)
-            for prj in data["results"]:
-                util.logger.info("Proj = %s", str(prj))
-                key = prj["key"] if "key" in prj else prj["id"]
-                self._projects[key] = projects.Project.get_object(self.endpoint, key)
-            nb_pages = util.nbr_pages(data)
-            page += 1
-        return self._projects
-
-    def count_projects(self):
-        """
-        :return: The number of projects using this quality gate
-        :rtype: int
-        """
-        return len(self.projects())
-
-    def conditions(self, encoded=False):
-        """
-        :param encoded: Whether to encode the conditions or not, defaults to False
-        :type encoded: bool, optional
-        :return: The quality gate conditions, encoded (for simplication) or not
-        :rtype: list
-        """
-        if self._conditions is None:
-            self._conditions = []
-            data = json.loads(self.get(APIS["details"], params={"name": self.name}).text)
-            for c in data.get("conditions", []):
-                self._conditions.append(c)
-        if encoded:
-            return _encode_conditions(self._conditions)
-        return self._conditions
-
-    def clear_conditions(self):
-        """Clears all quality gate conditions, if quality gate is not built-in
-        :return: Nothing
-        """
-        if self.is_built_in:
-            util.logger.debug("Can't clear conditions of built-in %s", str(self))
-        else:
-            util.logger.debug("Clearing conditions of %s", str(self))
-            for c in self.conditions():
-                self.post("qualitygates/delete_condition", params={"id": c["id"]})
-            self._conditions = None
-
-    def set_conditions(self, conditions_list):
-        """Sets quality gate conditions (overriding any previous conditions)
-        :param conditions_list: List of conditions, encoded
-        :type conditions_list: dict
-        :return: Whether the operation succeeded
-        :rtype: bool
-        """
-        if not conditions_list or len(conditions_list) == 0:
-            return True
-        if self.is_built_in:
-            util.logger.debug("Can't set conditions of built-in %s", str(self))
-            return False
-        self.clear_conditions()
-        util.logger.debug("Setting conditions of %s", str(self))
-        params = {"gateName": self.name}
-        ok = True
-        for cond in conditions_list:
-            (params["metric"], params["op"], params["error"]) = _decode_condition(cond)
-            ok = ok and self.post("qualitygates/create_condition", params=params).ok
-        self.conditions()
-        return ok
-
-    def permissions(self):
-        """
-        :return: The quality gate permissions
-        :rtype: QualityGatePermissions
-        """
-        if self._permissions is None:
-            self._permissions = permissions.QualityGatePermissions(self)
-        return self._permissions
-
-    def set_permissions(self, permissions_list):
-        """Sets quality gate permissions
-        :param permissions_list:
-        :type permissions_list: dict {"users": [<userlist>], "groups": [<grouplist>]}
-        :return: Whether the operation succeeded
-        :rtype: bool
-        """
-        return self.permissions().set(permissions_list)
-
-    def set_as_default(self):
-        """Sets the quality gate as the default
-        :return: Whether setting as default quality gate was successful
-        :rtype: bool
-        """
-        r = self.post("qualitygates/set_as_default", params={"name": self.name})
-        if r.ok:
-            self.is_default = True
-            # Turn off default for all other quality gates except the current one
-            for qg in get_list(self.endpoint).values():
-                if qg.name != self.name:
-                    qg.is_default = False
-        return r.ok
-
-    def update(self, **data):
-        """Updates a quality gate
-        :param dict data: Considered keys: "name", "conditions", "permissions"
-        """
-        if "name" in data and data["name"] != self.name:
-            util.logger.info("Renaming %s with %s", str(self), data["name"])
-            self.post(APIS["rename"], params={"id": self.key, "name": data["name"]})
-            _MAP.pop(self.name, None)
-            self.name = data["name"]
-            _MAP[self.name] = self
-        ok = self.set_conditions(data.get("conditions", []))
-        ok = ok and self.set_permissions(data.get("permissions", []))
-        if data.get("isDefault", False):
-            self.set_as_default()
-        return ok
-
-    def __audit_conditions(self):
-        problems = []
-        for c in self.conditions():
-            m = c["metric"]
-            if m not in GOOD_QG_CONDITIONS:
-                rule = rules.get_rule(rules.RuleId.QG_WRONG_METRIC)
-                msg = rule.msg.format(str(self), m)
-                problems.append(pb.Problem(rule.type, rule.severity, msg, concerned_object=self))
-                continue
-            val = int(c["error"])
-            (mini, maxi, msg) = GOOD_QG_CONDITIONS[m]
-            util.logger.debug("Condition on metric '%s': Check that %d in range [%d - %d]", m, val, mini, maxi)
-            if val < mini or val > maxi:
-                msg = f"{str(self)} condition on metric '{m}': {msg}".format(self.name, m, msg)
-                problems.append(pb.Problem(types.Type.BAD_PRACTICE, severities.Severity.HIGH, msg, concerned_object=self))
-        return problems
-
-    def audit(self, audit_settings=None):
-        """
-        :meta private:
-        """
-        my_name = str(self)
-        util.logger.debug("Auditing %s", my_name)
-        problems = []
-        if self.is_built_in:
-            return problems
-        max_cond = int(util.get_setting(audit_settings, "audit.qualitygates.maxConditions", 8))
-        nb_conditions = len(self.conditions())
-        util.logger.debug("Auditing %s number of conditions (%d) is OK", my_name, nb_conditions)
-        if nb_conditions == 0:
-            rule = rules.get_rule(rules.RuleId.QG_NO_COND)
-            msg = rule.msg.format(my_name)
-            problems.append(pb.Problem(rule.type, rule.severity, msg, concerned_object=self))
-        elif nb_conditions > max_cond:
-            rule = rules.get_rule(rules.RuleId.QG_TOO_MANY_COND)
-            msg = rule.msg.format(my_name, nb_conditions, max_cond)
-            problems.append(pb.Problem(rule.type, rule.severity, msg, concerned_object=self))
-        problems += self.__audit_conditions()
-        util.logger.debug("Auditing that %s has some assigned projects", my_name)
-        if not self.is_default and len(self.projects()) == 0:
-            rule = rules.get_rule(rules.RuleId.QG_NOT_USED)
-            msg = rule.msg.format(my_name)
-            problems.append(pb.Problem(rule.type, rule.severity, msg, concerned_object=self))
-        return problems
-
-    def to_json(self, full=False):
-        json_data = self._json
-        if not self.is_default and not full:
-            json_data.pop("isDefault")
-        if self.is_built_in:
-            if full:
-                json_data["_conditions"] = self.conditions(encoded=True)
-        else:
-            if not full:
-                json_data.pop("isBuiltIn")
-            json_data["conditions"] = self.conditions(encoded=True)
-            json_data["permissions"] = self.permissions().export()
-        return util.remove_nones(util.filter_export(json_data, _IMPORTABLE_PROPERTIES, full))
-
-
-def audit(endpoint=None, audit_settings=None):
-    """
-    :meta private:
-    """
-    util.logger.info("--- Auditing quality gates ---")
-    problems = []
-    quality_gates_list = get_list(endpoint)
-    max_qg = util.get_setting(audit_settings, "audit.qualitygates.maxNumber", 5)
-    nb_qg = len(quality_gates_list)
-    util.logger.debug("Auditing that there are no more than %s quality gates", str(max_qg))
-    if nb_qg > max_qg:
-        rule = rules.get_rule(rules.RuleId.QG_TOO_MANY_GATES)
-        problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(nb_qg, 5), concerned_object=f"{endpoint.url}/quality_gates"))
-    for qg in quality_gates_list.values():
-        problems += qg.audit(audit_settings)
-    return problems
-
-
-def get_list(endpoint):
-    """
-    :return: The whole list of quality gates
-    :rtype: dict {<name>: <QualityGate>}
-    """
-    util.logger.info("Getting quality gates")
-    data = json.loads(endpoint.get(APIS["list"]).text)
-    qg_list = {}
-    for qg in data["qualitygates"]:
-        util.logger.debug("Getting QG %s", str(qg))
-        qg_obj = QualityGate(name=qg["name"], endpoint=endpoint, data=qg)
-        if endpoint.version() < (7, 9, 0) and "default" in data and data["default"] == qg["id"]:
-            qg_obj.is_default = True
-        qg_list[qg_obj.name] = qg_obj
-    return qg_list
-
-
-def export(endpoint, full=False):
-    """
-    :return: The list of quality gates in their JSON representation
-    :rtype: dict
-    """
-    util.logger.info("Exporting quality gates")
-    qg_list = {}
-    for k, qg in get_list(endpoint).items():
-        qg_list[k] = qg.to_json(full)
-    return qg_list
-
-
-def import_config(endpoint, config_data):
-    """Imports quality gates in a SonarQube platform
-    Quality gates already existing  are updates with the provided configuration
-
-    :param Platform endpoint: Reference to the SonarQube platform
-    :param dict config_data: JSON representation of quality gates (as per export format)
-    :return: Whether the import succeeded
-    :rtype: bool
-    """
-    if "qualityGates" not in config_data:
-        util.logger.info("No quality gates to import")
-        return True
-    util.logger.info("Importing quality gates")
-    ok = True
-    for name, data in config_data["qualityGates"].items():
-        try:
-            o = QualityGate.get_object(endpoint, name)
-        except exceptions.ObjectNotFound:
-            o = QualityGate.create(endpoint, name)
-        ok = ok and o.update(**data)
-    return ok
-
-
-def count(endpoint):
-    """
-    :param Platform endpoint: Reference to the SonarQube platform
-    :return: Number of quality gates
-    :rtype: int
-    """
-    return len(get_list(endpoint))
-
-
-def exists(endpoint, gate_name):
-    """Returns whether a quality gate exists
-
-    :param Platform endpoint: Reference to the SonarQube platform
-    :param str gate_name: Quality gate name
-    :return: Whether the quality gate exists
-    :rtype: bool
-    """
-    try:
-        _ = QualityGate.get_object(endpoint, gate_name)
-        return True
-    except exceptions.ObjectNotFound:
-        return False
-
-
-def _encode_conditions(conds):
-    simple_conds = []
-    for c in conds:
-        simple_conds.append(_encode_condition(c))
-    return simple_conds
-
-
-def _encode_condition(c):
-    metric, op, val = c["metric"], c["op"], c["error"]
-    if op == "GT":
-        op = ">="
-    elif op == "LT":
-        op = "<="
-    if metric.endswith("rating"):
-        val = measures.get_rating_letter(val)
-    return f"{metric} {op} {val}"
-
-
-def _decode_condition(c):
-    (metric, op, val) = c.strip().split(" ")
-    if op in (">", ">="):
-        op = "GT"
-    elif op in ("<", "<="):
-        op = "LT"
-    if metric.endswith("rating"):
-        val = measures.get_rating_number(val)
-    return (metric, op, val)
-
-
-def search_by_name(endpoint, name):
-    return util.search_by_name(endpoint, name, APIS["list"], "qualitygates")
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+"""
+
+    Abstraction of the SonarQube "quality gate" concept
+
+"""
+
+from http import HTTPStatus
+import json
+from requests.exceptions import HTTPError
+import sonar.sqobject as sq
+from sonar import measures, exceptions
+import sonar.permissions.qualitygate_permissions as permissions
+from sonar.projects import projects
+import sonar.utilities as util
+
+from sonar.audit import rules, severities, types
+import sonar.audit.problem as pb
+
+
+_OBJECTS = {}
+_MAP = {}
+
+#: Quality gates APIs
+APIS = {
+    "create": "qualitygates/create",
+    "list": "qualitygates/list",
+    "rename": "qualitygates/rename",
+    "details": "qualitygates/show",
+    "get_projects": "qualitygates/search",
+}
+
+__NEW_ISSUES_SHOULD_BE_ZERO = "Any numeric threshold on new issues should be 0 or should be removed from QG conditions"
+
+GOOD_QG_CONDITIONS = {
+    "new_reliability_rating": (1, 1, "Any rating other than A would let bugs slip through in new code"),
+    "new_security_rating": (1, 1, "Any rating other than A would let vulnerabilities slip through in new code"),
+    "new_maintainability_rating": (1, 1, "Expectation is that code smells density on new code is low enough to get A rating"),
+    "new_coverage": (20, 90, "Coverage below 20% is a too low bar, above 90% is overkill"),
+    "new_bugs": (0, 0, __NEW_ISSUES_SHOULD_BE_ZERO),
+    "new_vulnerabilities": (0, 0, __NEW_ISSUES_SHOULD_BE_ZERO),
+    "new_security_hotspots": (0, 0, __NEW_ISSUES_SHOULD_BE_ZERO),
+    "new_blocker_violations": (0, 0, __NEW_ISSUES_SHOULD_BE_ZERO),
+    "new_critical_violations": (0, 0, __NEW_ISSUES_SHOULD_BE_ZERO),
+    "new_major_violations": (0, 0, __NEW_ISSUES_SHOULD_BE_ZERO),
+    "new_duplicated_lines_density": (1, 5, "Duplication on new code of less than 1% is overkill, more than 5% is too relaxed"),
+    "new_security_hotspots_reviewed": (100, 100, "All hotspots on new code must be reviewed, any other condition than 100% make little sense"),
+    "reliability_rating": (4, 4, "Threshold on overall code should not be too strict or passing the QG will be often impossible"),
+    "security_rating": (4, 4, "Threshold on overall code should not be too strict or passing the QG will be often impossible"),
+}
+
+_IMPORTABLE_PROPERTIES = ("isDefault", "isBuiltIn", "conditions", "permissions")
+
+
+class QualityGate(sq.SqObject):
+    """
+    Abstraction of the Sonar Quality Gate concept
+    """
+
+    @classmethod
+    def get_object(cls, endpoint, name):
+        """Reads a quality gate from SonarQube
+
+        :param Platform endpoint: Reference to the SonarQube platform
+        :param str name: Quality gate
+        :return: the QualityGate object or None if not found
+        :rtype: QualityGate or None
+        """
+        if name in _MAP and _MAP[name] in _OBJECTS:
+            return _OBJECTS[_MAP[name]]
+        data = search_by_name(endpoint, name)
+        if not data:
+            raise exceptions.ObjectNotFound(name, f"Quality gate '{name}' not found")
+        return cls.load(endpoint, data)
+
+    @classmethod
+    def load(cls, endpoint, data):
+        """Creates a quality gate from search data
+        :return: the QualityGate object
+        :rtype: QualityGate or None
+        """
+        # SonarQube 10 compatibility: "id" field dropped, replaced by "name"
+        o = _OBJECTS.get(data.get("id", data["name"]))
+        if not o:
+            o = cls(data["name"], endpoint, data=data)
+        o._json = data
+        return o
+
+    @classmethod
+    def create(cls, endpoint, name):
+        r = endpoint.post(APIS["create"], params={"name": name})
+        if not r.ok:
+            return None
+        return cls.get_object(endpoint, name)
+
+    def __init__(self, name, endpoint, data):
+        super().__init__(name, endpoint)
+        self.name = name  #: Object name
+        self.is_built_in = False  #: Whether the quality gate is built in
+        self.is_default = False  #: Whether the quality gate is the default
+        self._conditions = None  #: Quality gate conditions
+        self._permissions = None  #: Quality gate permissions
+        self._projects = None  #: Projects using this quality profile
+        self._json = data
+        self.name = data.pop("name")
+        self.key = data.pop("id", self.name)
+        self.is_default = data.get("isDefault", False)
+        self.is_built_in = data.get("isBuiltIn", False)
+        self.conditions()
+        self.permissions()
+        _OBJECTS[self.key] = self
+        _MAP[self.name] = self.key
+
+    def __str__(self):
+        """
+        :return: String formatting of the object
+        :rtype: str
+        """
+        return f"quality gate '{self.name}'"
+
+    def url(self):
+        """
+        :return: the SonarQube permalink URL to the quality gate
+        :rtype: str
+        """
+        return f"{self.endpoint.url}/quality_gates/show/{self.key}"
+
+    def projects(self):
+        """
+        :raises ObjectNotFound: Quality gate not found
+        :return: The list of projects using this quality gate
+        :rtype: dict {<projectKey>: <projectData>}
+        """
+        if self._projects is not None:
+            return self._projects
+        params = {"gateName": self.name, "ps": 500}
+        page, nb_pages = 1, 1
+        self._projects = {}
+        while page <= nb_pages:
+            params["p"] = page
+            try:
+                resp = self.get(APIS["get_projects"], params=params)
+            except HTTPError as e:
+                if e.response.status_code == HTTPStatus.NOT_FOUND:
+                    raise exceptions.ObjectNotFound(self.name, f"{str(self)} not found")
+            data = json.loads(resp.text)
+            for prj in data["results"]:
+                util.logger.info("Proj = %s", str(prj))
+                key = prj["key"] if "key" in prj else prj["id"]
+                self._projects[key] = projects.Project.get_object(self.endpoint, key)
+            nb_pages = util.nbr_pages(data)
+            page += 1
+        return self._projects
+
+    def count_projects(self):
+        """
+        :return: The number of projects using this quality gate
+        :rtype: int
+        """
+        return len(self.projects())
+
+    def conditions(self, encoded=False):
+        """
+        :param encoded: Whether to encode the conditions or not, defaults to False
+        :type encoded: bool, optional
+        :return: The quality gate conditions, encoded (for simplication) or not
+        :rtype: list
+        """
+        if self._conditions is None:
+            self._conditions = []
+            data = json.loads(self.get(APIS["details"], params={"name": self.name}).text)
+            for c in data.get("conditions", []):
+                self._conditions.append(c)
+        if encoded:
+            return _encode_conditions(self._conditions)
+        return self._conditions
+
+    def clear_conditions(self):
+        """Clears all quality gate conditions, if quality gate is not built-in
+        :return: Nothing
+        """
+        if self.is_built_in:
+            util.logger.debug("Can't clear conditions of built-in %s", str(self))
+        else:
+            util.logger.debug("Clearing conditions of %s", str(self))
+            for c in self.conditions():
+                self.post("qualitygates/delete_condition", params={"id": c["id"]})
+            self._conditions = None
+
+    def set_conditions(self, conditions_list):
+        """Sets quality gate conditions (overriding any previous conditions)
+        :param conditions_list: List of conditions, encoded
+        :type conditions_list: dict
+        :return: Whether the operation succeeded
+        :rtype: bool
+        """
+        if not conditions_list or len(conditions_list) == 0:
+            return True
+        if self.is_built_in:
+            util.logger.debug("Can't set conditions of built-in %s", str(self))
+            return False
+        self.clear_conditions()
+        util.logger.debug("Setting conditions of %s", str(self))
+        params = {"gateName": self.name}
+        ok = True
+        for cond in conditions_list:
+            (params["metric"], params["op"], params["error"]) = _decode_condition(cond)
+            ok = ok and self.post("qualitygates/create_condition", params=params).ok
+        self.conditions()
+        return ok
+
+    def permissions(self):
+        """
+        :return: The quality gate permissions
+        :rtype: QualityGatePermissions
+        """
+        if self._permissions is None:
+            self._permissions = permissions.QualityGatePermissions(self)
+        return self._permissions
+
+    def set_permissions(self, permissions_list):
+        """Sets quality gate permissions
+        :param permissions_list:
+        :type permissions_list: dict {"users": [<userlist>], "groups": [<grouplist>]}
+        :return: Whether the operation succeeded
+        :rtype: bool
+        """
+        return self.permissions().set(permissions_list)
+
+    def set_as_default(self):
+        """Sets the quality gate as the default
+        :return: Whether setting as default quality gate was successful
+        :rtype: bool
+        """
+        r = self.post("qualitygates/set_as_default", params={"name": self.name})
+        if r.ok:
+            self.is_default = True
+            # Turn off default for all other quality gates except the current one
+            for qg in get_list(self.endpoint).values():
+                if qg.name != self.name:
+                    qg.is_default = False
+        return r.ok
+
+    def update(self, **data):
+        """Updates a quality gate
+        :param dict data: Considered keys: "name", "conditions", "permissions"
+        """
+        if "name" in data and data["name"] != self.name:
+            util.logger.info("Renaming %s with %s", str(self), data["name"])
+            self.post(APIS["rename"], params={"id": self.key, "name": data["name"]})
+            _MAP.pop(self.name, None)
+            self.name = data["name"]
+            _MAP[self.name] = self
+        ok = self.set_conditions(data.get("conditions", []))
+        ok = ok and self.set_permissions(data.get("permissions", []))
+        if data.get("isDefault", False):
+            self.set_as_default()
+        return ok
+
+    def __audit_conditions(self):
+        problems = []
+        for c in self.conditions():
+            m = c["metric"]
+            if m not in GOOD_QG_CONDITIONS:
+                rule = rules.get_rule(rules.RuleId.QG_WRONG_METRIC)
+                msg = rule.msg.format(str(self), m)
+                problems.append(pb.Problem(rule.type, rule.severity, msg, concerned_object=self))
+                continue
+            val = int(c["error"])
+            (mini, maxi, msg) = GOOD_QG_CONDITIONS[m]
+            util.logger.debug("Condition on metric '%s': Check that %d in range [%d - %d]", m, val, mini, maxi)
+            if val < mini or val > maxi:
+                msg = f"{str(self)} condition on metric '{m}': {msg}".format(self.name, m, msg)
+                problems.append(pb.Problem(types.Type.BAD_PRACTICE, severities.Severity.HIGH, msg, concerned_object=self))
+        return problems
+
+    def audit(self, audit_settings=None):
+        """
+        :meta private:
+        """
+        my_name = str(self)
+        util.logger.debug("Auditing %s", my_name)
+        problems = []
+        if self.is_built_in:
+            return problems
+        max_cond = int(util.get_setting(audit_settings, "audit.qualitygates.maxConditions", 8))
+        nb_conditions = len(self.conditions())
+        util.logger.debug("Auditing %s number of conditions (%d) is OK", my_name, nb_conditions)
+        if nb_conditions == 0:
+            rule = rules.get_rule(rules.RuleId.QG_NO_COND)
+            msg = rule.msg.format(my_name)
+            problems.append(pb.Problem(rule.type, rule.severity, msg, concerned_object=self))
+        elif nb_conditions > max_cond:
+            rule = rules.get_rule(rules.RuleId.QG_TOO_MANY_COND)
+            msg = rule.msg.format(my_name, nb_conditions, max_cond)
+            problems.append(pb.Problem(rule.type, rule.severity, msg, concerned_object=self))
+        problems += self.__audit_conditions()
+        util.logger.debug("Auditing that %s has some assigned projects", my_name)
+        if not self.is_default and len(self.projects()) == 0:
+            rule = rules.get_rule(rules.RuleId.QG_NOT_USED)
+            msg = rule.msg.format(my_name)
+            problems.append(pb.Problem(rule.type, rule.severity, msg, concerned_object=self))
+        return problems
+
+    def to_json(self, full=False):
+        json_data = self._json
+        if not self.is_default and not full:
+            json_data.pop("isDefault")
+        if self.is_built_in:
+            if full:
+                json_data["_conditions"] = self.conditions(encoded=True)
+        else:
+            if not full:
+                json_data.pop("isBuiltIn")
+            json_data["conditions"] = self.conditions(encoded=True)
+            json_data["permissions"] = self.permissions().export()
+        return util.remove_nones(util.filter_export(json_data, _IMPORTABLE_PROPERTIES, full))
+
+
+def audit(endpoint=None, audit_settings=None):
+    """
+    :meta private:
+    """
+    util.logger.info("--- Auditing quality gates ---")
+    problems = []
+    quality_gates_list = get_list(endpoint)
+    max_qg = util.get_setting(audit_settings, "audit.qualitygates.maxNumber", 5)
+    nb_qg = len(quality_gates_list)
+    util.logger.debug("Auditing that there are no more than %s quality gates", str(max_qg))
+    if nb_qg > max_qg:
+        rule = rules.get_rule(rules.RuleId.QG_TOO_MANY_GATES)
+        problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(nb_qg, 5), concerned_object=f"{endpoint.url}/quality_gates"))
+    for qg in quality_gates_list.values():
+        problems += qg.audit(audit_settings)
+    return problems
+
+
+def get_list(endpoint):
+    """
+    :return: The whole list of quality gates
+    :rtype: dict {<name>: <QualityGate>}
+    """
+    util.logger.info("Getting quality gates")
+    data = json.loads(endpoint.get(APIS["list"]).text)
+    qg_list = {}
+    for qg in data["qualitygates"]:
+        util.logger.debug("Getting QG %s", str(qg))
+        qg_obj = QualityGate(name=qg["name"], endpoint=endpoint, data=qg)
+        if endpoint.version() < (7, 9, 0) and "default" in data and data["default"] == qg["id"]:
+            qg_obj.is_default = True
+        qg_list[qg_obj.name] = qg_obj
+    return qg_list
+
+
+def export(endpoint, full=False):
+    """
+    :return: The list of quality gates in their JSON representation
+    :rtype: dict
+    """
+    util.logger.info("Exporting quality gates")
+    qg_list = {}
+    for k, qg in get_list(endpoint).items():
+        qg_list[k] = qg.to_json(full)
+    return qg_list
+
+
+def import_config(endpoint, config_data):
+    """Imports quality gates in a SonarQube platform
+    Quality gates already existing  are updates with the provided configuration
+
+    :param Platform endpoint: Reference to the SonarQube platform
+    :param dict config_data: JSON representation of quality gates (as per export format)
+    :return: Whether the import succeeded
+    :rtype: bool
+    """
+    if "qualityGates" not in config_data:
+        util.logger.info("No quality gates to import")
+        return True
+    util.logger.info("Importing quality gates")
+    ok = True
+    for name, data in config_data["qualityGates"].items():
+        try:
+            o = QualityGate.get_object(endpoint, name)
+        except exceptions.ObjectNotFound:
+            o = QualityGate.create(endpoint, name)
+        ok = ok and o.update(**data)
+    return ok
+
+
+def count(endpoint):
+    """
+    :param Platform endpoint: Reference to the SonarQube platform
+    :return: Number of quality gates
+    :rtype: int
+    """
+    return len(get_list(endpoint))
+
+
+def exists(endpoint, gate_name):
+    """Returns whether a quality gate exists
+
+    :param Platform endpoint: Reference to the SonarQube platform
+    :param str gate_name: Quality gate name
+    :return: Whether the quality gate exists
+    :rtype: bool
+    """
+    try:
+        _ = QualityGate.get_object(endpoint, gate_name)
+        return True
+    except exceptions.ObjectNotFound:
+        return False
+
+
+def _encode_conditions(conds):
+    simple_conds = []
+    for c in conds:
+        simple_conds.append(_encode_condition(c))
+    return simple_conds
+
+
+def _encode_condition(c):
+    metric, op, val = c["metric"], c["op"], c["error"]
+    if op == "GT":
+        op = ">="
+    elif op == "LT":
+        op = "<="
+    if metric.endswith("rating"):
+        val = measures.get_rating_letter(val)
+    return f"{metric} {op} {val}"
+
+
+def _decode_condition(c):
+    (metric, op, val) = c.strip().split(" ")
+    if op in (">", ">="):
+        op = "GT"
+    elif op in ("<", "<="):
+        op = "LT"
+    if metric.endswith("rating"):
+        val = measures.get_rating_number(val)
+    return (metric, op, val)
+
+
+def search_by_name(endpoint, name):
+    return util.search_by_name(endpoint, name, APIS["list"], "qualitygates")
```

## sonar/qualityprofiles.py

```diff
@@ -1,704 +1,704 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-
-import json
-from http import HTTPStatus
-from queue import Queue
-from threading import Thread, Lock
-from requests import HTTPError
-import requests.utils
-from sonar import rules, languages
-import sonar.permissions.qualityprofile_permissions as permissions
-import sonar.sqobject as sq
-import sonar.utilities as util
-
-import sonar.audit.rules as arules
-import sonar.audit.problem as pb
-
-_CREATE_API = "qualityprofiles/create"
-_SEARCH_API = "qualityprofiles/search"
-_DETAILS_API = "qualityprofiles/show"
-_SEARCH_FIELD = "profiles"
-_OBJECTS = {}
-_MAP = {}
-
-_KEY_PARENT = "parent"
-_CHILDREN_KEY = "children"
-
-_IMPORTABLE_PROPERTIES = ("name", "language", "parentName", "isBuiltIn", "isDefault", "rules", "permissions")
-
-_CLASS_LOCK = Lock()
-
-
-class QualityProfile(sq.SqObject):
-    """
-    Abstraction of the SonarQube "quality profile" concept
-    Objects of this class must be created with one of the 3 available class methods. Don't use __init__
-    """
-
-    def __init__(self, key, endpoint, data=None):
-        """Do not use, use class methods to create objects"""
-        super().__init__(key, endpoint)
-
-        self.name = data["name"]  #: Quality profile name
-        self.language = data["language"]  #: Quality profile language
-        self.is_default = data["isDefault"]  #: Quality profile is default
-        self.is_built_in = data["isBuiltIn"]  #: Quality profile is built-in - read-only
-        self._json = data
-        self._permissions = None
-        self._rules = None
-        self.__last_use = None
-        self.__last_update = None
-
-        self._rules = self.rules()
-        self.nbr_rules = int(data["activeRuleCount"])  #: Number of rules in the quality profile
-        self.nbr_deprecated_rules = int(data["activeDeprecatedRuleCount"])  #: Number of deprecated rules in the quality profile
-
-        (self._projects, self._projects_lock) = (None, Lock())
-        self.project_count = data.get("projectCount", None)  #: Number of projects using this quality profile
-        self.parent_name = data.get("parentName", None)  #: Name of parent profile, or None if none
-
-        self.__last_use = util.string_to_date(data.get("lastUsed", None))
-        self.__last_update = util.string_to_date(data.get("rulesUpdatedAt", None))
-
-        util.logger.debug("Created %s", str(self))
-        _MAP[_format(self.name, self.language)] = self.key
-        _OBJECTS[self.key] = self
-
-    @classmethod
-    def read(cls, endpoint, name, language):
-        """Creates a QualityProfile object corresponding to quality profile with same name and language in SonarQube
-
-        :param endpoint: Reference to the SonarQube platform
-        :type endpoint: Platform
-        :param name: Quality profile name
-        :type name: str
-        :param name: Quality profile language
-        :type name: str
-        :return: The quality profile object
-        :rtype: QualityProfile or None if not found
-        """
-        if not languages.exists(endpoint=endpoint, language=language):
-            util.logger.error("Language '%s' does not exist, quality profile creation aborted")
-            return None
-        util.logger.debug("Reading quality profile '%s' of language '%s'", name, language)
-        key = get_id(name, language)
-        if key in _OBJECTS:
-            return _OBJECTS[key]
-        data = util.search_by_name(endpoint, name, _SEARCH_API, _SEARCH_FIELD, extra_params={"language": language})
-        return cls(key=data["key"], endpoint=endpoint, data=data)
-
-    @classmethod
-    def create(cls, endpoint, name, language):
-        """Creates a new quality profile in SonarQube and returns the corresponding QualityProfile object
-
-        :param endpoint: Reference to the SonarQube platform
-        :type endpoint: Platform
-        :param name: Quality profile name
-        :type name: str
-        :param description: Quality profile language
-        :type description: str
-        :return: The quality profile object
-        :rtype: QualityProfile or None if creation failed
-        """
-        if not languages.exists(endpoint=endpoint, language=language):
-            util.logger.error("Language '%s' does not exist, quality profile creation aborted")
-            return None
-        util.logger.debug("Creating quality profile '%s' of language '%s'", name, language)
-        r = endpoint.post(_CREATE_API, params={"name": name, "language": language})
-        if not r.ok:
-            return None
-        return cls.read(endpoint=endpoint, name=name, language=language)
-
-    @classmethod
-    def load(cls, endpoint, data):
-        """Creates a QualityProfile object from the result of a SonarQube API quality profile search data
-
-        :param endpoint: Reference to the SonarQube platform
-        :type endpoint: Platform
-        :param data: The JSON data corresponding to the quality profile
-        :type data: dict
-        :return: The quality profile object
-        :rtype: QualityProfile
-        """
-        util.logger.debug("Loading quality profile '%s' of language '%s'", data["name"], data["language"])
-        return cls(endpoint=endpoint, key=data["key"], data=data)
-
-    def __str__(self):
-        """String formatting of the object
-
-        :rtype: str
-        """
-        return f"quality profile '{self.name}' of language '{self.language}'"
-
-    def url(self):
-        """
-        :return: the SonarQube permalink URL to the quality profile
-        :rtype: str
-        """
-        return f"{self.endpoint.url}/profiles/show?language={self.language}&name={requests.utils.quote(self.name)}"
-
-    def last_use(self):
-        """
-        :return: When the quality profile was last used
-        :rtype: datetime or None if never
-        """
-        return self.__last_use
-
-    def last_update(self):
-        """
-        :return: When the quality profile was last updated
-        :rtype: datetime or None
-        """
-        return self.__last_update
-
-    def set_parent(self, parent_name):
-        """Sets the parent quality profile of the current profile
-
-        :param parent_name: Name of the parent quality profile
-        :type parent_name: str
-        :return: Whether setting the parent was successful or not
-        :rtype: bool
-        """
-        if parent_name is None:
-            return False
-        if get_object(name=parent_name, language=self.language) is None:
-            util.logger.warning("Can't set parent name '%s' to %s, parent not found", str(parent_name), str(self))
-            return False
-        if self.parent_name is None or self.parent_name != parent_name:
-            params = {"qualityProfile": self.name, "language": self.language, "parentQualityProfile": parent_name}
-            r = self.post("qualityprofiles/change_parent", params=params)
-            return r.ok
-        else:
-            util.logger.debug("Won't set parent of %s. It's the same as currently", str(self))
-            return True
-
-    def set_as_default(self):
-        """Sets the quality profile as the default for the language
-        :return: Whether setting as default quality profile was successful
-        :rtype: bool
-        """
-        params = {"qualityProfile": self.name, "language": self.language}
-        r = self.post("qualityprofiles/set_default", params=params)
-        if r.ok:
-            self.is_default = True
-            # Turn off default for all other profiles except the current profile
-            for qp in get_list(self.endpoint).values():
-                if qp.language == self.language and qp.key != self.key:
-                    qp.is_default = False
-        return r.ok
-
-    def is_child(self):
-        """
-        :return: Whether the quality profile has a parent
-        :rtype: bool
-        """
-        return self.parent_name is not None
-
-    def inherits_from_built_in(self):
-        """
-        :return: Whether the quality profile inherits from a built-in profile (following parents of parents)
-        :rtype: bool
-        """
-        return self.built_in_parent() is not None
-
-    def built_in_parent(self):
-        """
-        :return: The built-in parent profile of the profile, or None
-        :rtype: QualityProfile or None if profile does not inherit from a built-in profile
-        """
-        self.is_built_in = self._json.get("isBuiltIn", False)
-        if self.is_built_in:
-            return self
-        if self.parent_name is None:
-            return None
-        return get_object(endpoint=self.endpoint, name=self.parent_name, language=self.language).built_in_parent()
-
-    def rules(self):
-        """
-        :return: The list of rules active in the quality profile
-        :rtype: dict{<rule_key>: <rule_data>}
-        """
-        if self._rules is not None:
-            # Assume nobody changed QP during execution
-            return self._rules
-        self._rules = rules.search(self.endpoint, activation="true", qprofile=self.key, s="key", languages=self.language)
-        return self._rules
-
-    def activate_rule(self, rule_key, severity=None, **params):
-        """Activates a rule in the quality profile
-
-        :param str rule_key: Rule key to activate
-        :param severity: Severity of the rule in the quality profiles, defaults to the rule default severity
-        :type severity: str, optional
-        :param params: List of parameters associated to the rules, defaults to None
-        :type params: dict, optional
-        :return: Whether the activation succeeded
-        :rtype: bool
-        """
-        api_params = {"key": self.key, "rule": rule_key, "severity": severity}
-        if len(params) > 0:
-            api_params["params"] = ";".join([f"{k}={v}" for k, v in params.items()])
-        r = self.post("qualityprofiles/activate_rule", params=api_params)
-        if r.status_code == HTTPStatus.NOT_FOUND:
-            util.logger.error("Rule %s not found, can't activate it in %s", rule_key, str(self))
-        elif r.status_code == HTTPStatus.BAD_REQUEST:
-            util.logger.error("HTTP error %d while trying to activate rule %s in %s", r.status_code, rule_key, str(self))
-        return r.ok
-
-    def activate_rules(self, ruleset):
-        """Activates a list of rules in the quality profile
-        :return: Whether the activation of all rules was successful
-        :rtype: bool
-        """
-        if not ruleset:
-            return False
-        ok = True
-        for r_key, r_data in ruleset.items():
-            util.logger.debug("Activating rule %s in QG %s data %s", r_key, str(self), str(r_data))
-            try:
-                sev = r_data if isinstance(r_data, str) else r_data.get("severity", None)
-                if "params" in r_data:
-                    ok = ok and self.activate_rule(rule_key=r_key, severity=sev, **r_data["params"])
-                else:
-                    ok = ok and self.activate_rule(rule_key=r_key, severity=sev)
-            except HTTPError as e:
-                ok = False
-                util.logger.warning("Activation of rule '%s' in %s failed: HTTP Error %d", r_key, str(self), e.response.status_code)
-        return ok
-
-    def update(self, data, queue):
-        if self.is_built_in:
-            util.logger.debug("Not updating built-in %s", str(self))
-        else:
-            util.logger.debug("Updating %s with %s", str(self), str(data))
-            if "name" in data and data["name"] != self.name:
-                util.logger.info("Renaming %s with %s", str(self), data["name"])
-                self.post("qualitygates/rename", params={"id": self.key, "name": data["name"]})
-                _MAP.pop(_format(self.name, self.language), None)
-                self.name = data["name"]
-                _MAP[_format(self.name, self.language)] = self
-            self.activate_rules(data.get("rules", []))
-            self.set_permissions(data.get("permissions", []))
-            self.set_parent(data.pop(_KEY_PARENT, None))
-            self.is_built_in = data.get("isBuiltIn", False)
-            if data.get("isDefault", False):
-                self.set_as_default()
-
-        _create_or_update_children(name=self.name, language=self.language, endpoint=self.endpoint, children=data.get(_CHILDREN_KEY, {}), queue=queue)
-        return self
-
-    def to_json(self, full=False):
-        """
-        :param full: If True, exports all properties, including those that can't be set
-        :type full: bool
-        :return: the quality profile properties as JSON dict
-        :rtype: dict
-        """
-        json_data = self._json.copy()
-        json_data.update({"name": self.name, "language": self.language, "parentName": self.parent_name})
-        if not self.is_default:
-            json_data.pop("isDefault", None)
-        if not self.is_built_in:
-            json_data.pop("isBuiltIn", None)
-            json_data["rules"] = {k: v.export(full) for k, v in self.rules().items()}
-        json_data["permissions"] = self.permissions().export()
-        return util.remove_nones(util.filter_export(json_data, _IMPORTABLE_PROPERTIES, full))
-
-    def compare(self, another_qp):
-        """Compares 2 quality profiles rulesets
-        :param another_qp: The second quality profile to compare with self
-        :type another_qp: QualityProfile
-        :return: dict result of the compare ("inLeft", "inRight", "same", "modified")
-        :rtype: dict
-        """
-        data = json.loads(self.get("qualityprofiles/compare", params={"leftKey": self.key, "rightKey": another_qp.key}).text)
-        for r in data["inLeft"] + data["same"] + data["inRight"] + data["modified"]:
-            for k in ("name", "pluginKey", "pluginName", "languageKey", "languageName"):
-                r.pop(k, None)
-        return data
-
-    def diff(self, another_qp):
-        """Returns the list of rules added or modified in self compared to another_qp (for inheritance)
-        :param another_qp: The second quality profile to diff
-        :type another_qp: QualityProfile
-        :return: dict result of the diff ("inLeft", "modified")
-        :rtype: dict
-        """
-        util.logger.debug("Comparing %s and %s", str(self), str(another_qp))
-        compare_result = self.compare(another_qp)
-        my_rules = self.rules()
-        diff_rules = _treat_added_rules(my_rules, compare_result["inLeft"])
-        diff_rules.update(_treat_modified_rules(my_rules, compare_result["modified"]))
-        return diff_rules
-
-    def projects(self):
-        """Returns the list of projects keys using this quality profile
-        :return: dict result of the diff ("inLeft", "modified")
-        :rtype: List[project_key]
-        """
-
-        with self._projects_lock:
-            if self._projects is None:
-                self._projects = []
-                params = {"key": self.key, "ps": 500}
-                page = 1
-                more = True
-                while more:
-                    params["p"] = page
-                    data = json.loads(self.get("qualityprofiles/projects", params=params).text)
-                    util.logger.debug("Got QP %s data = %s", self.key, str(data))
-                    self._projects += [p["key"] for p in data["results"]]
-                    page += 1
-                    if self.endpoint.version() >= (10, 0, 0):
-                        nb_pages = (data["paging"]["total"] + 500 - 1) // 500
-                        more = nb_pages >= page
-                    else:
-                        more = data["more"]
-
-                util.logger.debug("Projects for %s = '%s'", str(self), ", ".join(self._projects))
-        return self._projects
-
-    def used_by_project(self, project):
-        """
-        :param project: The project
-        :type project: Project
-        :return: Whether the quality profile is used by the project
-        :rtype: bool
-        """
-        return project.key in self.projects()
-
-    def permissions(self):
-        """
-        :return: The list of users and groups that can edit the quality profile
-        :rtype: dict{"users": <users comma separated>, "groups": <groups comma separated>}
-        """
-        if self._permissions is None:
-            self._permissions = permissions.QualityProfilePermissions(self)
-        return self._permissions
-
-    def set_permissions(self, perms):
-        """Sets the list of users and groups that can can edit the quality profile
-        :params perms:
-        :type perms: dict{"users": <users comma separated>, "groups": <groups comma separated>}
-        :return: Nothing
-        """
-        self.permissions().set(perms)
-
-    def audit(self, audit_settings=None):
-        """Audits a quality profile and return list of problems found
-
-        :param audit_settings: Options of what to audit and thresholds to raise problems
-        :type audit_settings: dict
-        :return: List of problems found, or empty list
-        :rtype: list[Problem]
-        """
-        util.logger.debug("Auditing %s", str(self))
-        if self.is_built_in:
-            util.logger.info("%s is built-in, skipping audit", str(self))
-            return []
-
-        util.logger.debug("Auditing %s (key '%s')", str(self), self.key)
-        problems = []
-        age = util.age(self.last_update(), rounded=True)
-        if age > audit_settings["audit.qualityProfiles.maxLastChangeAge"]:
-            rule = arules.get_rule(arules.RuleId.QP_LAST_CHANGE_DATE)
-            msg = rule.msg.format(str(self), age)
-            problems.append(pb.Problem(rule.type, rule.severity, msg, concerned_object=self))
-
-        total_rules = rules.count(endpoint=self.endpoint, languages=self.language)
-        if self.nbr_rules < int(total_rules * audit_settings["audit.qualityProfiles.minNumberOfRules"]):
-            rule = arules.get_rule(arules.RuleId.QP_TOO_FEW_RULES)
-            msg = rule.msg.format(str(self), self.nbr_rules, total_rules)
-            problems.append(pb.Problem(rule.type, rule.severity, msg, concerned_object=self))
-
-        age = util.age(self.last_use(), rounded=True)
-        if self.project_count == 0 or age is None:
-            rule = arules.get_rule(arules.RuleId.QP_NOT_USED)
-            msg = rule.msg.format(str(self))
-            problems.append(pb.Problem(rule.type, rule.severity, msg, concerned_object=self))
-        elif age > audit_settings["audit.qualityProfiles.maxUnusedAge"]:
-            rule = arules.get_rule(arules.RuleId.QP_LAST_USED_DATE)
-            msg = rule.msg.format(str(self), age)
-            problems.append(pb.Problem(rule.type, rule.severity, msg, concerned_object=self))
-        if audit_settings["audit.qualityProfiles.checkDeprecatedRules"]:
-            max_deprecated_rules = 0
-            parent_qp = self.built_in_parent()
-            if parent_qp is not None:
-                max_deprecated_rules = parent_qp.nbr_deprecated_rules
-            if self.nbr_deprecated_rules > max_deprecated_rules:
-                rule = arules.get_rule(arules.RuleId.QP_USE_DEPRECATED_RULES)
-                msg = rule.msg.format(str(self), self.nbr_deprecated_rules)
-                problems.append(pb.Problem(rule.type, rule.severity, msg, concerned_object=self))
-
-        return problems
-
-
-def search(endpoint, params=None):
-    """Searches projects in SonarQube
-
-    :param params: list of parameters to filter quality profiles to search
-    :type params: dict
-    :return: list of quality profiles
-    :rtype: dict{key: QualityProfile}
-    """
-    return sq.search_objects(
-        endpoint=endpoint, api=_SEARCH_API, params=params, key_field="key", returned_field=_SEARCH_FIELD, object_class=QualityProfile
-    )
-
-
-def get_list(endpoint, use_cache=True):
-    """
-    :param endpoint: Reference to the SonarQube platform
-    :type endpoint: Platform
-    :param use_cache: Whether to use local cache or query SonarQube, default True (use cache)
-    :type use_cache: bool
-    :return: the list of all quality profiles
-    :rtype: dict{key: QualityProfile}
-    """
-
-    with _CLASS_LOCK:
-        if len(_OBJECTS) == 0 or not use_cache:
-            search(endpoint=endpoint)
-    return _OBJECTS
-
-
-def audit(endpoint, audit_settings=None):
-    """Audits all quality profiles and return list of problems found
-
-    :param audit_settings: Configuration of audit
-    :type audit_settings: dict
-    :param endpoint: reference to the SonarQube platform
-    :type endpoint: Platform
-    :return: list of problems found
-    :rtype: list[Problem]
-    """
-    util.logger.info("--- Auditing quality profiles ---")
-    get_list(endpoint=endpoint)
-    problems = []
-    langs = {}
-    for qp in search(endpoint).values():
-        problems += qp.audit(audit_settings)
-        langs[qp.language] = langs.get(qp.language, 0) + 1
-    for lang, nb_qp in langs.items():
-        if nb_qp > 5:
-            rule = arules.get_rule(arules.RuleId.QP_TOO_MANY_QP)
-            problems.append(
-                pb.Problem(rule.type, rule.severity, rule.msg.format(nb_qp, lang, 5), concerned_object=f"{endpoint.url}/profiles?language={lang}")
-            )
-    return problems
-
-
-def hierarchize(qp_list):
-    """Organize a flat list of QP in hierarchical (inheritance) fashion
-
-    :param qp_list: List of quality profiles
-    :type qp_list: {<language>: {<qp_name>: <qd_data>}}
-    :return: Same list with child profiles nested in their parent
-    :rtype: {<language>: {<qp_name>: {"children": <qp_list>; <qp_data>}}}
-    """
-    util.logger.info("Organizing quality profiles in hierarchy")
-    for lang, qpl in qp_list.copy().items():
-        for qp_name, qp_value in qpl.copy().items():
-            util.logger.debug("Treating %s:%s", lang, qp_name)
-            if "parentName" not in qp_value:
-                continue
-            util.logger.debug("QP name '%s:%s' has parent '%s'", lang, qp_name, qp_value["parentName"])
-            if _CHILDREN_KEY not in qp_list[lang][qp_value["parentName"]]:
-                qp_list[lang][qp_value["parentName"]][_CHILDREN_KEY] = {}
-
-            parent_qp = get_object(qp_value["parentName"], lang)
-            this_qp = get_object(name=qp_name, language=lang)
-            qp_value["rules"] = {}
-            for k, v in this_qp.diff(parent_qp).items():
-                qp_value["rules"][k] = v if isinstance(v, str) or "templateKey" not in v else v["severity"]
-
-            qp_list[lang][qp_value["parentName"]][_CHILDREN_KEY][qp_name] = qp_value
-            qp_list[lang].pop(qp_name)
-            qp_value.pop("parentName")
-    return qp_list
-
-
-def export(endpoint, in_hierarchy=True, full=False):
-    """Exports all quality profiles configuration as dict
-
-    :param endpoint: reference to the SonarQube platform
-    :type endpoint: Platform
-    :param in_hierarchy: Whether quality profiles dict should be organized hierarchically (following inheritance)
-    :type in_hierarchy: bool, optional
-    :param full: Whether to export all settings including those that can't be set, defaults to False
-    :type full: bool, optional
-    :return: dict structure of all quality profiles
-    :rtype: dict
-    """
-    util.logger.info("Exporting quality profiles")
-    qp_list = {}
-    for qp in get_list(endpoint=endpoint).values():
-        util.logger.info("Exporting %s", str(qp))
-        json_data = qp.to_json(full=full)
-        lang = json_data.pop("language")
-        name = json_data.pop("name")
-        if lang not in qp_list:
-            qp_list[lang] = {}
-        qp_list[lang][name] = json_data
-    if in_hierarchy:
-        qp_list = hierarchize(qp_list)
-    return qp_list
-
-
-def get_object(name, language, endpoint=None):
-    """Returns a quality profile Object from its name and language
-
-    :param name: Quality profile name
-    :type name: str
-    :param language: Quality profile language
-    :type language: str
-    :param endpoint: Reference to the SonarQube platform
-    :type endpoint: Platform
-    :return: The quality profile object, of None if not found
-    :rtype: QualityProfile or None
-    """
-    get_list(endpoint)
-    fmt = _format(name, language)
-    if fmt not in _MAP:
-        return None
-    return _OBJECTS[_MAP[fmt]]
-
-
-def _create_or_update_children(name, language, endpoint, children, queue):
-    for qp_name, qp_data in children.items():
-        qp_data[_KEY_PARENT] = name
-        util.logger.debug("Adding child profile '%s' to update queue", qp_name)
-        queue.put((qp_name, language, endpoint, qp_data))
-
-
-def __import_thread(queue):
-    while not queue.empty():
-        (name, lang, endpoint, qp_data) = queue.get()
-        o = get_object(name=name, language=lang, endpoint=endpoint)
-        if o is None:
-            o = QualityProfile.create(endpoint=endpoint, name=name, language=lang)
-        util.logger.info("Importing quality profile '%s' of language '%s'", name, lang)
-        o.update(qp_data, queue)
-        util.logger.info("Imported quality profile '%s' of language '%s'", name, lang)
-        queue.task_done()
-
-
-def import_config(endpoint, config_data, threads=8):
-    """Imports a configuration in SonarQube
-
-    :param endpoint: reference to the SonarQube platform
-    :type endpoint: Platform
-    :param config_data: the configuration to import
-    :type config_data: dict
-    :param threads: Number of threads (quality profiles import) to run in parallel
-    :type threads: int
-    :return: Nothing
-    """
-    if "qualityProfiles" not in config_data:
-        util.logger.info("No quality profiles to import")
-        return
-    util.logger.info("Importing quality profiles")
-    q = Queue(maxsize=0)
-    get_list(endpoint=endpoint)
-    for lang, lang_data in config_data["qualityProfiles"].items():
-        if not languages.exists(endpoint=endpoint, language=lang):
-            util.logger.warning("Language '%s' does not exist, quality profile '%s' import skipped", lang, name)
-            continue
-        for name, qp_data in lang_data.items():
-            q.put((name, lang, endpoint, qp_data))
-    for i in range(threads):
-        util.logger.debug("Starting quality profile import thread %d", i)
-        worker = Thread(target=__import_thread, args=[q])
-        worker.setDaemon(True)
-        worker.start()
-    q.join()
-
-
-def _format(name, lang):
-    """
-    :meta private:
-    """
-    return f"{lang}:{name}"
-
-
-def get_id(name, language):
-    """Finds a quality profile (internal) id from its name and language
-
-    The list of quality profile s must have been load by a search before using get_id
-    :param name: Quality profile name
-    :type name: str
-    :param language: Quality profile language
-    :type language: str
-    :return: The quality profile internal key or None
-    :rtype: str or None
-    """
-    return _MAP.get(_format(name, language), None)
-
-
-def exists(endpoint, name, language):
-    """
-    :param endpoint: reference to the SonarQube platform
-    :type endpoint: Platform
-    :param name: Quality profile name
-    :type name: str
-    :param language: Quality profile language
-    :type language: str
-    :return: whether the project exists
-    :rtype: bool
-    """
-    return get_object(name=name, language=language, endpoint=endpoint) is not None
-
-
-def _treat_added_rules(my_rules, added_rules):
-    diff_rules = {}
-    for r in added_rules:
-        r_key = r.pop("key")
-        diff_rules[r_key] = r
-        if r_key in my_rules:
-            diff_rules[r_key] = rules.convert_for_export(my_rules[r_key].to_json(), my_rules[r_key].language)
-        if "severity" in r:
-            if isinstance(diff_rules[r_key], str):
-                diff_rules[r_key] = r["severity"]
-            else:
-                diff_rules[r_key]["severity"] = r["severity"]
-    return diff_rules
-
-
-def _treat_modified_rules(my_rules, modified_rules):
-    diff_rules = {}
-    for r in modified_rules:
-        r_key, r_left, r_right = r["key"], r["left"], r["right"]
-        diff_rules[r_key] = {"modified": True}
-        parms = None
-        if r_left["severity"] != r_right["severity"]:
-            diff_rules[r_key]["severity"] = r_left["severity"]
-        if len(r_left.get("params", {})) > 0:
-            diff_rules[r_key]["params"] = r_left["params"]
-            parms = r_left["params"]
-        if r_key not in my_rules:
-            continue
-        data = rules.convert_for_export(my_rules[r_key].to_json(), my_rules[r_key].language)
-        if "templateKey" in data:
-            diff_rules[r_key]["templateKey"] = data["templateKey"]
-            diff_rules[r_key]["params"] = data["params"]
-            if parms:
-                diff_rules[r_key]["params"].update(parms)
-    return diff_rules
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+
+import json
+from http import HTTPStatus
+from queue import Queue
+from threading import Thread, Lock
+from requests import HTTPError
+import requests.utils
+from sonar import rules, languages
+import sonar.permissions.qualityprofile_permissions as permissions
+import sonar.sqobject as sq
+import sonar.utilities as util
+
+import sonar.audit.rules as arules
+import sonar.audit.problem as pb
+
+_CREATE_API = "qualityprofiles/create"
+_SEARCH_API = "qualityprofiles/search"
+_DETAILS_API = "qualityprofiles/show"
+_SEARCH_FIELD = "profiles"
+_OBJECTS = {}
+_MAP = {}
+
+_KEY_PARENT = "parent"
+_CHILDREN_KEY = "children"
+
+_IMPORTABLE_PROPERTIES = ("name", "language", "parentName", "isBuiltIn", "isDefault", "rules", "permissions")
+
+_CLASS_LOCK = Lock()
+
+
+class QualityProfile(sq.SqObject):
+    """
+    Abstraction of the SonarQube "quality profile" concept
+    Objects of this class must be created with one of the 3 available class methods. Don't use __init__
+    """
+
+    def __init__(self, key, endpoint, data=None):
+        """Do not use, use class methods to create objects"""
+        super().__init__(key, endpoint)
+
+        self.name = data["name"]  #: Quality profile name
+        self.language = data["language"]  #: Quality profile language
+        self.is_default = data["isDefault"]  #: Quality profile is default
+        self.is_built_in = data["isBuiltIn"]  #: Quality profile is built-in - read-only
+        self._json = data
+        self._permissions = None
+        self._rules = None
+        self.__last_use = None
+        self.__last_update = None
+
+        self._rules = self.rules()
+        self.nbr_rules = int(data["activeRuleCount"])  #: Number of rules in the quality profile
+        self.nbr_deprecated_rules = int(data["activeDeprecatedRuleCount"])  #: Number of deprecated rules in the quality profile
+
+        (self._projects, self._projects_lock) = (None, Lock())
+        self.project_count = data.get("projectCount", None)  #: Number of projects using this quality profile
+        self.parent_name = data.get("parentName", None)  #: Name of parent profile, or None if none
+
+        self.__last_use = util.string_to_date(data.get("lastUsed", None))
+        self.__last_update = util.string_to_date(data.get("rulesUpdatedAt", None))
+
+        util.logger.debug("Created %s", str(self))
+        _MAP[_format(self.name, self.language)] = self.key
+        _OBJECTS[self.key] = self
+
+    @classmethod
+    def read(cls, endpoint, name, language):
+        """Creates a QualityProfile object corresponding to quality profile with same name and language in SonarQube
+
+        :param endpoint: Reference to the SonarQube platform
+        :type endpoint: Platform
+        :param name: Quality profile name
+        :type name: str
+        :param name: Quality profile language
+        :type name: str
+        :return: The quality profile object
+        :rtype: QualityProfile or None if not found
+        """
+        if not languages.exists(endpoint=endpoint, language=language):
+            util.logger.error("Language '%s' does not exist, quality profile creation aborted")
+            return None
+        util.logger.debug("Reading quality profile '%s' of language '%s'", name, language)
+        key = get_id(name, language)
+        if key in _OBJECTS:
+            return _OBJECTS[key]
+        data = util.search_by_name(endpoint, name, _SEARCH_API, _SEARCH_FIELD, extra_params={"language": language})
+        return cls(key=data["key"], endpoint=endpoint, data=data)
+
+    @classmethod
+    def create(cls, endpoint, name, language):
+        """Creates a new quality profile in SonarQube and returns the corresponding QualityProfile object
+
+        :param endpoint: Reference to the SonarQube platform
+        :type endpoint: Platform
+        :param name: Quality profile name
+        :type name: str
+        :param description: Quality profile language
+        :type description: str
+        :return: The quality profile object
+        :rtype: QualityProfile or None if creation failed
+        """
+        if not languages.exists(endpoint=endpoint, language=language):
+            util.logger.error("Language '%s' does not exist, quality profile creation aborted")
+            return None
+        util.logger.debug("Creating quality profile '%s' of language '%s'", name, language)
+        r = endpoint.post(_CREATE_API, params={"name": name, "language": language})
+        if not r.ok:
+            return None
+        return cls.read(endpoint=endpoint, name=name, language=language)
+
+    @classmethod
+    def load(cls, endpoint, data):
+        """Creates a QualityProfile object from the result of a SonarQube API quality profile search data
+
+        :param endpoint: Reference to the SonarQube platform
+        :type endpoint: Platform
+        :param data: The JSON data corresponding to the quality profile
+        :type data: dict
+        :return: The quality profile object
+        :rtype: QualityProfile
+        """
+        util.logger.debug("Loading quality profile '%s' of language '%s'", data["name"], data["language"])
+        return cls(endpoint=endpoint, key=data["key"], data=data)
+
+    def __str__(self):
+        """String formatting of the object
+
+        :rtype: str
+        """
+        return f"quality profile '{self.name}' of language '{self.language}'"
+
+    def url(self):
+        """
+        :return: the SonarQube permalink URL to the quality profile
+        :rtype: str
+        """
+        return f"{self.endpoint.url}/profiles/show?language={self.language}&name={requests.utils.quote(self.name)}"
+
+    def last_use(self):
+        """
+        :return: When the quality profile was last used
+        :rtype: datetime or None if never
+        """
+        return self.__last_use
+
+    def last_update(self):
+        """
+        :return: When the quality profile was last updated
+        :rtype: datetime or None
+        """
+        return self.__last_update
+
+    def set_parent(self, parent_name):
+        """Sets the parent quality profile of the current profile
+
+        :param parent_name: Name of the parent quality profile
+        :type parent_name: str
+        :return: Whether setting the parent was successful or not
+        :rtype: bool
+        """
+        if parent_name is None:
+            return False
+        if get_object(name=parent_name, language=self.language) is None:
+            util.logger.warning("Can't set parent name '%s' to %s, parent not found", str(parent_name), str(self))
+            return False
+        if self.parent_name is None or self.parent_name != parent_name:
+            params = {"qualityProfile": self.name, "language": self.language, "parentQualityProfile": parent_name}
+            r = self.post("qualityprofiles/change_parent", params=params)
+            return r.ok
+        else:
+            util.logger.debug("Won't set parent of %s. It's the same as currently", str(self))
+            return True
+
+    def set_as_default(self):
+        """Sets the quality profile as the default for the language
+        :return: Whether setting as default quality profile was successful
+        :rtype: bool
+        """
+        params = {"qualityProfile": self.name, "language": self.language}
+        r = self.post("qualityprofiles/set_default", params=params)
+        if r.ok:
+            self.is_default = True
+            # Turn off default for all other profiles except the current profile
+            for qp in get_list(self.endpoint).values():
+                if qp.language == self.language and qp.key != self.key:
+                    qp.is_default = False
+        return r.ok
+
+    def is_child(self):
+        """
+        :return: Whether the quality profile has a parent
+        :rtype: bool
+        """
+        return self.parent_name is not None
+
+    def inherits_from_built_in(self):
+        """
+        :return: Whether the quality profile inherits from a built-in profile (following parents of parents)
+        :rtype: bool
+        """
+        return self.built_in_parent() is not None
+
+    def built_in_parent(self):
+        """
+        :return: The built-in parent profile of the profile, or None
+        :rtype: QualityProfile or None if profile does not inherit from a built-in profile
+        """
+        self.is_built_in = self._json.get("isBuiltIn", False)
+        if self.is_built_in:
+            return self
+        if self.parent_name is None:
+            return None
+        return get_object(endpoint=self.endpoint, name=self.parent_name, language=self.language).built_in_parent()
+
+    def rules(self):
+        """
+        :return: The list of rules active in the quality profile
+        :rtype: dict{<rule_key>: <rule_data>}
+        """
+        if self._rules is not None:
+            # Assume nobody changed QP during execution
+            return self._rules
+        self._rules = rules.search(self.endpoint, activation="true", qprofile=self.key, s="key", languages=self.language)
+        return self._rules
+
+    def activate_rule(self, rule_key, severity=None, **params):
+        """Activates a rule in the quality profile
+
+        :param str rule_key: Rule key to activate
+        :param severity: Severity of the rule in the quality profiles, defaults to the rule default severity
+        :type severity: str, optional
+        :param params: List of parameters associated to the rules, defaults to None
+        :type params: dict, optional
+        :return: Whether the activation succeeded
+        :rtype: bool
+        """
+        api_params = {"key": self.key, "rule": rule_key, "severity": severity}
+        if len(params) > 0:
+            api_params["params"] = ";".join([f"{k}={v}" for k, v in params.items()])
+        r = self.post("qualityprofiles/activate_rule", params=api_params)
+        if r.status_code == HTTPStatus.NOT_FOUND:
+            util.logger.error("Rule %s not found, can't activate it in %s", rule_key, str(self))
+        elif r.status_code == HTTPStatus.BAD_REQUEST:
+            util.logger.error("HTTP error %d while trying to activate rule %s in %s", r.status_code, rule_key, str(self))
+        return r.ok
+
+    def activate_rules(self, ruleset):
+        """Activates a list of rules in the quality profile
+        :return: Whether the activation of all rules was successful
+        :rtype: bool
+        """
+        if not ruleset:
+            return False
+        ok = True
+        for r_key, r_data in ruleset.items():
+            util.logger.debug("Activating rule %s in QG %s data %s", r_key, str(self), str(r_data))
+            try:
+                sev = r_data if isinstance(r_data, str) else r_data.get("severity", None)
+                if "params" in r_data:
+                    ok = ok and self.activate_rule(rule_key=r_key, severity=sev, **r_data["params"])
+                else:
+                    ok = ok and self.activate_rule(rule_key=r_key, severity=sev)
+            except HTTPError as e:
+                ok = False
+                util.logger.warning("Activation of rule '%s' in %s failed: HTTP Error %d", r_key, str(self), e.response.status_code)
+        return ok
+
+    def update(self, data, queue):
+        if self.is_built_in:
+            util.logger.debug("Not updating built-in %s", str(self))
+        else:
+            util.logger.debug("Updating %s with %s", str(self), str(data))
+            if "name" in data and data["name"] != self.name:
+                util.logger.info("Renaming %s with %s", str(self), data["name"])
+                self.post("qualitygates/rename", params={"id": self.key, "name": data["name"]})
+                _MAP.pop(_format(self.name, self.language), None)
+                self.name = data["name"]
+                _MAP[_format(self.name, self.language)] = self
+            self.activate_rules(data.get("rules", []))
+            self.set_permissions(data.get("permissions", []))
+            self.set_parent(data.pop(_KEY_PARENT, None))
+            self.is_built_in = data.get("isBuiltIn", False)
+            if data.get("isDefault", False):
+                self.set_as_default()
+
+        _create_or_update_children(name=self.name, language=self.language, endpoint=self.endpoint, children=data.get(_CHILDREN_KEY, {}), queue=queue)
+        return self
+
+    def to_json(self, full=False):
+        """
+        :param full: If True, exports all properties, including those that can't be set
+        :type full: bool
+        :return: the quality profile properties as JSON dict
+        :rtype: dict
+        """
+        json_data = self._json.copy()
+        json_data.update({"name": self.name, "language": self.language, "parentName": self.parent_name})
+        if not self.is_default:
+            json_data.pop("isDefault", None)
+        if not self.is_built_in:
+            json_data.pop("isBuiltIn", None)
+            json_data["rules"] = {k: v.export(full) for k, v in self.rules().items()}
+        json_data["permissions"] = self.permissions().export()
+        return util.remove_nones(util.filter_export(json_data, _IMPORTABLE_PROPERTIES, full))
+
+    def compare(self, another_qp):
+        """Compares 2 quality profiles rulesets
+        :param another_qp: The second quality profile to compare with self
+        :type another_qp: QualityProfile
+        :return: dict result of the compare ("inLeft", "inRight", "same", "modified")
+        :rtype: dict
+        """
+        data = json.loads(self.get("qualityprofiles/compare", params={"leftKey": self.key, "rightKey": another_qp.key}).text)
+        for r in data["inLeft"] + data["same"] + data["inRight"] + data["modified"]:
+            for k in ("name", "pluginKey", "pluginName", "languageKey", "languageName"):
+                r.pop(k, None)
+        return data
+
+    def diff(self, another_qp):
+        """Returns the list of rules added or modified in self compared to another_qp (for inheritance)
+        :param another_qp: The second quality profile to diff
+        :type another_qp: QualityProfile
+        :return: dict result of the diff ("inLeft", "modified")
+        :rtype: dict
+        """
+        util.logger.debug("Comparing %s and %s", str(self), str(another_qp))
+        compare_result = self.compare(another_qp)
+        my_rules = self.rules()
+        diff_rules = _treat_added_rules(my_rules, compare_result["inLeft"])
+        diff_rules.update(_treat_modified_rules(my_rules, compare_result["modified"]))
+        return diff_rules
+
+    def projects(self):
+        """Returns the list of projects keys using this quality profile
+        :return: dict result of the diff ("inLeft", "modified")
+        :rtype: List[project_key]
+        """
+
+        with self._projects_lock:
+            if self._projects is None:
+                self._projects = []
+                params = {"key": self.key, "ps": 500}
+                page = 1
+                more = True
+                while more:
+                    params["p"] = page
+                    data = json.loads(self.get("qualityprofiles/projects", params=params).text)
+                    util.logger.debug("Got QP %s data = %s", self.key, str(data))
+                    self._projects += [p["key"] for p in data["results"]]
+                    page += 1
+                    if self.endpoint.version() >= (10, 0, 0):
+                        nb_pages = (data["paging"]["total"] + 500 - 1) // 500
+                        more = nb_pages >= page
+                    else:
+                        more = data["more"]
+
+                util.logger.debug("Projects for %s = '%s'", str(self), ", ".join(self._projects))
+        return self._projects
+
+    def used_by_project(self, project):
+        """
+        :param project: The project
+        :type project: Project
+        :return: Whether the quality profile is used by the project
+        :rtype: bool
+        """
+        return project.key in self.projects()
+
+    def permissions(self):
+        """
+        :return: The list of users and groups that can edit the quality profile
+        :rtype: dict{"users": <users comma separated>, "groups": <groups comma separated>}
+        """
+        if self._permissions is None:
+            self._permissions = permissions.QualityProfilePermissions(self)
+        return self._permissions
+
+    def set_permissions(self, perms):
+        """Sets the list of users and groups that can can edit the quality profile
+        :params perms:
+        :type perms: dict{"users": <users comma separated>, "groups": <groups comma separated>}
+        :return: Nothing
+        """
+        self.permissions().set(perms)
+
+    def audit(self, audit_settings=None):
+        """Audits a quality profile and return list of problems found
+
+        :param audit_settings: Options of what to audit and thresholds to raise problems
+        :type audit_settings: dict
+        :return: List of problems found, or empty list
+        :rtype: list[Problem]
+        """
+        util.logger.debug("Auditing %s", str(self))
+        if self.is_built_in:
+            util.logger.info("%s is built-in, skipping audit", str(self))
+            return []
+
+        util.logger.debug("Auditing %s (key '%s')", str(self), self.key)
+        problems = []
+        age = util.age(self.last_update(), rounded=True)
+        if age > audit_settings.get("audit.qualityProfiles.maxLastChangeAge", 180):
+            rule = arules.get_rule(arules.RuleId.QP_LAST_CHANGE_DATE)
+            msg = rule.msg.format(str(self), age)
+            problems.append(pb.Problem(rule.type, rule.severity, msg, concerned_object=self))
+
+        total_rules = rules.count(endpoint=self.endpoint, languages=self.language)
+        if self.nbr_rules < int(total_rules * audit_settings.get("audit.qualityProfiles.minNumberOfRules", 0.5)):
+            rule = arules.get_rule(arules.RuleId.QP_TOO_FEW_RULES)
+            msg = rule.msg.format(str(self), self.nbr_rules, total_rules)
+            problems.append(pb.Problem(rule.type, rule.severity, msg, concerned_object=self))
+
+        age = util.age(self.last_use(), rounded=True)
+        if self.project_count == 0 or age is None:
+            rule = arules.get_rule(arules.RuleId.QP_NOT_USED)
+            msg = rule.msg.format(str(self))
+            problems.append(pb.Problem(rule.type, rule.severity, msg, concerned_object=self))
+        elif age > audit_settings.get("audit.qualityProfiles.maxUnusedAge", 60):
+            rule = arules.get_rule(arules.RuleId.QP_LAST_USED_DATE)
+            msg = rule.msg.format(str(self), age)
+            problems.append(pb.Problem(rule.type, rule.severity, msg, concerned_object=self))
+        if audit_settings.get("audit.qualityProfiles.checkDeprecatedRules", True):
+            max_deprecated_rules = 0
+            parent_qp = self.built_in_parent()
+            if parent_qp is not None:
+                max_deprecated_rules = parent_qp.nbr_deprecated_rules
+            if self.nbr_deprecated_rules > max_deprecated_rules:
+                rule = arules.get_rule(arules.RuleId.QP_USE_DEPRECATED_RULES)
+                msg = rule.msg.format(str(self), self.nbr_deprecated_rules)
+                problems.append(pb.Problem(rule.type, rule.severity, msg, concerned_object=self))
+
+        return problems
+
+
+def search(endpoint, params=None):
+    """Searches projects in SonarQube
+
+    :param params: list of parameters to filter quality profiles to search
+    :type params: dict
+    :return: list of quality profiles
+    :rtype: dict{key: QualityProfile}
+    """
+    return sq.search_objects(
+        endpoint=endpoint, api=_SEARCH_API, params=params, key_field="key", returned_field=_SEARCH_FIELD, object_class=QualityProfile
+    )
+
+
+def get_list(endpoint, use_cache=True):
+    """
+    :param endpoint: Reference to the SonarQube platform
+    :type endpoint: Platform
+    :param use_cache: Whether to use local cache or query SonarQube, default True (use cache)
+    :type use_cache: bool
+    :return: the list of all quality profiles
+    :rtype: dict{key: QualityProfile}
+    """
+
+    with _CLASS_LOCK:
+        if len(_OBJECTS) == 0 or not use_cache:
+            search(endpoint=endpoint)
+    return _OBJECTS
+
+
+def audit(endpoint, audit_settings=None):
+    """Audits all quality profiles and return list of problems found
+
+    :param audit_settings: Configuration of audit
+    :type audit_settings: dict
+    :param endpoint: reference to the SonarQube platform
+    :type endpoint: Platform
+    :return: list of problems found
+    :rtype: list[Problem]
+    """
+    util.logger.info("--- Auditing quality profiles ---")
+    get_list(endpoint=endpoint)
+    problems = []
+    langs = {}
+    for qp in search(endpoint).values():
+        problems += qp.audit(audit_settings)
+        langs[qp.language] = langs.get(qp.language, 0) + 1
+    for lang, nb_qp in langs.items():
+        if nb_qp > 5:
+            rule = arules.get_rule(arules.RuleId.QP_TOO_MANY_QP)
+            problems.append(
+                pb.Problem(rule.type, rule.severity, rule.msg.format(nb_qp, lang, 5), concerned_object=f"{endpoint.url}/profiles?language={lang}")
+            )
+    return problems
+
+
+def hierarchize(qp_list):
+    """Organize a flat list of QP in hierarchical (inheritance) fashion
+
+    :param qp_list: List of quality profiles
+    :type qp_list: {<language>: {<qp_name>: <qd_data>}}
+    :return: Same list with child profiles nested in their parent
+    :rtype: {<language>: {<qp_name>: {"children": <qp_list>; <qp_data>}}}
+    """
+    util.logger.info("Organizing quality profiles in hierarchy")
+    for lang, qpl in qp_list.copy().items():
+        for qp_name, qp_value in qpl.copy().items():
+            util.logger.debug("Treating %s:%s", lang, qp_name)
+            if "parentName" not in qp_value:
+                continue
+            util.logger.debug("QP name '%s:%s' has parent '%s'", lang, qp_name, qp_value["parentName"])
+            if _CHILDREN_KEY not in qp_list[lang][qp_value["parentName"]]:
+                qp_list[lang][qp_value["parentName"]][_CHILDREN_KEY] = {}
+
+            parent_qp = get_object(qp_value["parentName"], lang)
+            this_qp = get_object(name=qp_name, language=lang)
+            qp_value["rules"] = {}
+            for k, v in this_qp.diff(parent_qp).items():
+                qp_value["rules"][k] = v if isinstance(v, str) or "templateKey" not in v else v["severity"]
+
+            qp_list[lang][qp_value["parentName"]][_CHILDREN_KEY][qp_name] = qp_value
+            qp_list[lang].pop(qp_name)
+            qp_value.pop("parentName")
+    return qp_list
+
+
+def export(endpoint, in_hierarchy=True, full=False):
+    """Exports all quality profiles configuration as dict
+
+    :param endpoint: reference to the SonarQube platform
+    :type endpoint: Platform
+    :param in_hierarchy: Whether quality profiles dict should be organized hierarchically (following inheritance)
+    :type in_hierarchy: bool, optional
+    :param full: Whether to export all settings including those that can't be set, defaults to False
+    :type full: bool, optional
+    :return: dict structure of all quality profiles
+    :rtype: dict
+    """
+    util.logger.info("Exporting quality profiles")
+    qp_list = {}
+    for qp in get_list(endpoint=endpoint).values():
+        util.logger.info("Exporting %s", str(qp))
+        json_data = qp.to_json(full=full)
+        lang = json_data.pop("language")
+        name = json_data.pop("name")
+        if lang not in qp_list:
+            qp_list[lang] = {}
+        qp_list[lang][name] = json_data
+    if in_hierarchy:
+        qp_list = hierarchize(qp_list)
+    return qp_list
+
+
+def get_object(name, language, endpoint=None):
+    """Returns a quality profile Object from its name and language
+
+    :param name: Quality profile name
+    :type name: str
+    :param language: Quality profile language
+    :type language: str
+    :param endpoint: Reference to the SonarQube platform
+    :type endpoint: Platform
+    :return: The quality profile object, of None if not found
+    :rtype: QualityProfile or None
+    """
+    get_list(endpoint)
+    fmt = _format(name, language)
+    if fmt not in _MAP:
+        return None
+    return _OBJECTS[_MAP[fmt]]
+
+
+def _create_or_update_children(name, language, endpoint, children, queue):
+    for qp_name, qp_data in children.items():
+        qp_data[_KEY_PARENT] = name
+        util.logger.debug("Adding child profile '%s' to update queue", qp_name)
+        queue.put((qp_name, language, endpoint, qp_data))
+
+
+def __import_thread(queue):
+    while not queue.empty():
+        (name, lang, endpoint, qp_data) = queue.get()
+        o = get_object(name=name, language=lang, endpoint=endpoint)
+        if o is None:
+            o = QualityProfile.create(endpoint=endpoint, name=name, language=lang)
+        util.logger.info("Importing quality profile '%s' of language '%s'", name, lang)
+        o.update(qp_data, queue)
+        util.logger.info("Imported quality profile '%s' of language '%s'", name, lang)
+        queue.task_done()
+
+
+def import_config(endpoint, config_data, threads=8):
+    """Imports a configuration in SonarQube
+
+    :param endpoint: reference to the SonarQube platform
+    :type endpoint: Platform
+    :param config_data: the configuration to import
+    :type config_data: dict
+    :param threads: Number of threads (quality profiles import) to run in parallel
+    :type threads: int
+    :return: Nothing
+    """
+    if "qualityProfiles" not in config_data:
+        util.logger.info("No quality profiles to import")
+        return
+    util.logger.info("Importing quality profiles")
+    q = Queue(maxsize=0)
+    get_list(endpoint=endpoint)
+    for lang, lang_data in config_data["qualityProfiles"].items():
+        if not languages.exists(endpoint=endpoint, language=lang):
+            util.logger.warning("Language '%s' does not exist, quality profile '%s' import skipped", lang, name)
+            continue
+        for name, qp_data in lang_data.items():
+            q.put((name, lang, endpoint, qp_data))
+    for i in range(threads):
+        util.logger.debug("Starting quality profile import thread %d", i)
+        worker = Thread(target=__import_thread, args=[q])
+        worker.setDaemon(True)
+        worker.start()
+    q.join()
+
+
+def _format(name, lang):
+    """
+    :meta private:
+    """
+    return f"{lang}:{name}"
+
+
+def get_id(name, language):
+    """Finds a quality profile (internal) id from its name and language
+
+    The list of quality profile s must have been load by a search before using get_id
+    :param name: Quality profile name
+    :type name: str
+    :param language: Quality profile language
+    :type language: str
+    :return: The quality profile internal key or None
+    :rtype: str or None
+    """
+    return _MAP.get(_format(name, language), None)
+
+
+def exists(endpoint, name, language):
+    """
+    :param endpoint: reference to the SonarQube platform
+    :type endpoint: Platform
+    :param name: Quality profile name
+    :type name: str
+    :param language: Quality profile language
+    :type language: str
+    :return: whether the project exists
+    :rtype: bool
+    """
+    return get_object(name=name, language=language, endpoint=endpoint) is not None
+
+
+def _treat_added_rules(my_rules, added_rules):
+    diff_rules = {}
+    for r in added_rules:
+        r_key = r.pop("key")
+        diff_rules[r_key] = r
+        if r_key in my_rules:
+            diff_rules[r_key] = rules.convert_for_export(my_rules[r_key].to_json(), my_rules[r_key].language)
+        if "severity" in r:
+            if isinstance(diff_rules[r_key], str):
+                diff_rules[r_key] = r["severity"]
+            else:
+                diff_rules[r_key]["severity"] = r["severity"]
+    return diff_rules
+
+
+def _treat_modified_rules(my_rules, modified_rules):
+    diff_rules = {}
+    for r in modified_rules:
+        r_key, r_left, r_right = r["key"], r["left"], r["right"]
+        diff_rules[r_key] = {"modified": True}
+        parms = None
+        if r_left["severity"] != r_right["severity"]:
+            diff_rules[r_key]["severity"] = r_left["severity"]
+        if len(r_left.get("params", {})) > 0:
+            diff_rules[r_key]["params"] = r_left["params"]
+            parms = r_left["params"]
+        if r_key not in my_rules:
+            continue
+        data = rules.convert_for_export(my_rules[r_key].to_json(), my_rules[r_key].language)
+        if "templateKey" in data:
+            diff_rules[r_key]["templateKey"] = data["templateKey"]
+            diff_rules[r_key]["params"] = data["params"]
+            if parms:
+                diff_rules[r_key]["params"].update(parms)
+    return diff_rules
```

## sonar/rules.py

```diff
@@ -1,286 +1,297 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-"""
-
-    Abstraction of the SonarQube "rule" concept
-
-"""
-import json
-from http import HTTPStatus
-from requests.exceptions import HTTPError
-import sonar.sqobject as sq
-from sonar import utilities, exceptions
-
-_OBJECTS = {}
-SEARCH_API = "rules/search"
-_DETAILS_API = "rules/show"
-_CREATE_API = "rules/create"
-
-TYPES = ("BUG", "VULNERABILITY", "CODE_SMELL", "SECURITY_HOTSPOT")
-
-
-class Rule(sq.SqObject):
-    @classmethod
-    def get_object(cls, endpoint, key):
-        if key in _OBJECTS:
-            return _OBJECTS[key]
-        utilities.logger.debug("Reading rule key '%s'", key)
-        try:
-            r = endpoint.get(_DETAILS_API, params={"key": key})
-        except HTTPError as e:
-            if e.response.status_code == HTTPStatus.NOT_FOUND:
-                raise exceptions.ObjectNotFound(key=key, message=f"Rule key '{key}' does not exist")
-        return Rule(key, endpoint, json.loads(r.text)["rule"])
-
-    @classmethod
-    def create(cls, key, endpoint, **kwargs):
-        params = kwargs.copy()
-        (_, params["custom_key"]) = key.split(":")
-        utilities.logger.debug("Creating rule key '%s'", key)
-        r = endpoint.post(_CREATE_API, params=params)
-        if not r.ok:
-            return None
-        o = cls.get_object(key=key, endpoint=endpoint)
-        return o
-
-    @classmethod
-    def load(cls, key, endpoint, data):
-        if key in _OBJECTS:
-            _OBJECTS[key]._json.update(data)
-            return _OBJECTS[key]
-        return cls(key=key, endpoint=endpoint, data=data)
-
-    @classmethod
-    def instantiate(cls, key, template_key, endpoint, data):
-        try:
-            rule = Rule.get_object(endpoint, key)
-            utilities.logger.info("Rule key '%s' already exists, instantiation skipped...", key)
-            return rule
-        except exceptions.ObjectNotFound:
-            pass
-        utilities.logger.info("Instantiating rule key '%s' from template key '%s'", key, template_key)
-        rule_params = ";".join([f"{k}={v}" for k, v in data["params"].items()])
-        return Rule.create(
-            key=key,
-            endpoint=endpoint,
-            template_key=template_key,
-            name=data.get("name", key),
-            severity=data.get("severity", "MAJOR"),
-            params=rule_params,
-            markdown_description=data.get("description", "NO DESCRIPTION"),
-        )
-
-    def __init__(self, key, endpoint, data):
-        super().__init__(key, endpoint)
-        utilities.logger.debug("Creating rule object '%s'", key)  # utilities.json_dump(data))
-        self._json = data
-        self.severity = data.get("severity", None)
-        self.repo = data.get("repo", None)
-        self.type = data.get("type", None)
-        self.tags = None if len(data.get("tags", [])) == 0 else data["tags"]
-        self.name = data.get("name", None)
-        self.language = data.get("lang", None)
-        self.custom_desc = data.get("mdNote", None)
-        self.created_at = data["createdAt"]
-        self.is_template = data.get("isTemplate", False)
-        self.template_key = data.get("templateKey", None)
-        _OBJECTS[self.key] = self
-
-    def __str__(self):
-        return f"rule key '{self.key}'"
-
-    def to_json(self):
-        return self._json
-
-    def export(self, full=False):
-        return convert_for_export(self.to_json(), self.language, full=full)
-
-    def set_tags(self, tags):
-        if tags is None:
-            return
-        if isinstance(tags, list):
-            tags = utilities.list_to_csv(tags)
-        utilities.logger.debug("Settings custom tags '%s' to %s", tags, str(self))
-        self.post("rules/update", params={"key": self.key, "tags": tags})
-
-    def set_description(self, description):
-        if description is None:
-            return
-        utilities.logger.debug("Settings custom description '%s' to %s", description, str(self))
-        self.post("rules/update", params={"key": self.key, "markdown_note": description})
-
-
-def get_facet(facet, endpoint):
-    data = json.loads(endpoint.get(SEARCH_API, params={"ps": 1, "facets": facet}).text)
-    facet_dict = {}
-    for f in data["facets"][0]["values"]:
-        facet_dict[f["val"]] = f["count"]
-    return facet_dict
-
-
-def search(endpoint, **params):
-    return sq.search_objects(SEARCH_API, endpoint, "key", "rules", Rule, params, threads=4)
-
-
-def count(endpoint, **params):
-    return json.loads(endpoint.get(SEARCH_API, params={**params, "ps": 1}).text)["total"]
-
-
-def get_list(endpoint, **params):
-    return search(endpoint, include_external="false", **params)
-
-
-def get_object(key, endpoint):
-    if key in _OBJECTS:
-        return _OBJECTS[key]
-    try:
-        return Rule.get_object(key, endpoint)
-    except exceptions.ObjectNotFound:
-        return None
-
-
-def export_all(endpoint, full=False):
-    utilities.logger.info("Exporting rules")
-    rule_list, other_rules, instantiated_rules, extended_rules = {}, {}, {}, {}
-    for rule_key, rule in get_list(endpoint=endpoint).items():
-        rule_export = rule.export(full)
-        if rule.template_key is not None:
-            instantiated_rules[rule_key] = rule_export
-        elif rule.tags is not None or rule.custom_desc is not None:
-            if full:
-                extended_rules[rule_key] = rule_export
-                continue
-            extended_rules[rule_key] = {}
-            if rule.tags is not None:
-                extended_rules[rule_key]["tags"] = rule_export["tags"]
-            if rule.custom_desc is not None:
-                extended_rules[rule_key]["description"] = rule_export["description"]
-        else:
-            other_rules[rule_key] = rule_export
-    if len(instantiated_rules) > 0:
-        rule_list["instantiated"] = instantiated_rules
-    if len(extended_rules) > 0:
-        rule_list["extended"] = extended_rules
-    if len(other_rules) > 0:
-        rule_list["standard"] = other_rules
-    return rule_list
-
-
-def export_instantiated(endpoint, full=False):
-    rule_list = {}
-    for template_key in get_list(endpoint=endpoint, is_template="true"):
-        for rule_key, rule in get_list(endpoint=endpoint, template_key=template_key).items():
-            rule_list[rule_key] = rule.export(full)
-    return rule_list if len(rule_list) > 0 else None
-
-
-def export_customized(endpoint, full=False):
-    rule_list = {}
-    for rule_key, rule in get_list(endpoint=endpoint, is_template="false").items():
-        if rule.tags is None and rule.custom_desc is None:
-            continue
-        if full:
-            rule_list[rule_key] = rule.export(full)
-            continue
-        rule_list[rule_key] = {}
-        if rule.tags:
-            rule_list[rule_key]["tags"] = utilities.list_to_csv(rule.tags, ", ")
-        if rule.custom_desc:
-            rule_list[rule_key]["description"] = rule.custom_desc
-    return rule_list if len(rule_list) > 0 else None
-
-
-def export_needed(endpoint, instantiated=True, extended=True, full=False):
-    rule_list = {}
-    if instantiated:
-        rule_list["instantiated"] = export_instantiated(endpoint, full)
-    if extended:
-        rule_list["extended"] = export_customized(endpoint, full)
-    return utilities.remove_nones(rule_list)
-
-
-def export(endpoint, instantiated=True, extended=True, standard=False, full=False):
-    utilities.logger.info("Exporting rules")
-    if standard:
-        return export_all(endpoint, full)
-    else:
-        return export_needed(endpoint, instantiated, extended, full)
-
-
-def import_config(endpoint, config_data):
-    if "rules" not in config_data:
-        utilities.logger.info("No customized rules (custom tags, extended description) to import")
-        return
-    utilities.logger.info("Importing customized (custom tags, extended description) rules")
-    get_list(endpoint=endpoint)
-    for key, custom in config_data["rules"].get("extended", {}).items():
-        try:
-            rule = Rule.get_object(endpoint, key)
-        except exceptions.ObjectNotFound:
-            utilities.logger.warning("Rule key '%s' does not exist, can't import it", key)
-            continue
-        rule.set_description(custom.get("description", None))
-        rule.set_tags(custom.get("tags", None))
-
-    utilities.logger.debug("get_list from import")
-    get_list(endpoint=endpoint, templates=True)
-    utilities.logger.info("Importing custom rules (instantiated from rule templates)")
-    for key, instantiation_data in config_data["rules"].get("instantiated", {}).items():
-        try:
-            rule = Rule.get_object(endpoint, key)
-            utilities.logger.debug("Instantiated rule key '%s' already exists, instantiation skipped", key)
-            continue
-        except exceptions.ObjectNotFound:
-            pass
-        try:
-            template_rule = Rule.get_object(endpoint, instantiation_data["templateKey"])
-        except exceptions.ObjectNotFound:
-            utilities.logger.warning("Rule template key '%s' does not exist, can't instantiate it", key)
-            continue
-        Rule.instantiate(key, template_rule.key, endpoint, instantiation_data)
-
-
-def convert_for_export(rule, qp_lang, with_template_key=True, full=False):
-    d = {"severity": rule.get("severity", "")}
-    if len(rule.get("params", {})) > 0:
-        if not full:
-            d["params"] = {}
-            for p in rule["params"]:
-                d["params"][p["key"]] = p.get("defaultValue", "")
-        else:
-            d["params"] = rule["params"]
-    if rule["isTemplate"]:
-        d["isTemplate"] = True
-    if "tags" in rule and len(rule["tags"]) > 0:
-        d["tags"] = utilities.list_to_csv(rule["tags"])
-    if rule.get("mdNote", None) is not None:
-        d["description"] = rule["mdNote"]
-    if with_template_key and "templateKey" in rule:
-        d["templateKey"] = rule["templateKey"]
-    if "lang" in rule and rule["lang"] != qp_lang:
-        d["language"] = rule["lang"]
-    if full:
-        for k, v in rule.items():
-            if k not in ("severity", "params", "isTemplate", "tags", "mdNote", "templateKey", "lang"):
-                d[f"_{k}"] = v
-        d.pop("_key", None)
-    if len(d) == 1:
-        return d["severity"]
-    return d
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+"""
+
+    Abstraction of the SonarQube "rule" concept
+
+"""
+import json
+from http import HTTPStatus
+from requests.exceptions import HTTPError
+import sonar.sqobject as sq
+from sonar import utilities, exceptions
+
+_OBJECTS = {}
+SEARCH_API = "rules/search"
+_DETAILS_API = "rules/show"
+_CREATE_API = "rules/create"
+
+TYPES = ("BUG", "VULNERABILITY", "CODE_SMELL", "SECURITY_HOTSPOT")
+
+
+class Rule(sq.SqObject):
+    @classmethod
+    def get_object(cls, endpoint, key):
+        if key in _OBJECTS:
+            return _OBJECTS[key]
+        utilities.logger.debug("Reading rule key '%s'", key)
+        try:
+            r = endpoint.get(_DETAILS_API, params={"key": key})
+        except HTTPError as e:
+            if e.response.status_code == HTTPStatus.NOT_FOUND:
+                raise exceptions.ObjectNotFound(key=key, message=f"Rule key '{key}' does not exist")
+        return Rule(key, endpoint, json.loads(r.text)["rule"])
+
+    @classmethod
+    def create(cls, key, endpoint, **kwargs):
+        params = kwargs.copy()
+        (_, params["custom_key"]) = key.split(":")
+        utilities.logger.debug("Creating rule key '%s'", key)
+        r = endpoint.post(_CREATE_API, params=params)
+        if not r.ok:
+            return None
+        o = cls.get_object(key=key, endpoint=endpoint)
+        return o
+
+    @classmethod
+    def load(cls, key, endpoint, data):
+        if key in _OBJECTS:
+            _OBJECTS[key]._json.update(data)
+            return _OBJECTS[key]
+        return cls(key=key, endpoint=endpoint, data=data)
+
+    @classmethod
+    def instantiate(cls, key, template_key, endpoint, data):
+        try:
+            rule = Rule.get_object(endpoint, key)
+            utilities.logger.info("Rule key '%s' already exists, instantiation skipped...", key)
+            return rule
+        except exceptions.ObjectNotFound:
+            pass
+        utilities.logger.info("Instantiating rule key '%s' from template key '%s'", key, template_key)
+        rule_params = ";".join([f"{k}={v}" for k, v in data["params"].items()])
+        return Rule.create(
+            key=key,
+            endpoint=endpoint,
+            template_key=template_key,
+            name=data.get("name", key),
+            severity=data.get("severity", "MAJOR"),
+            params=rule_params,
+            markdown_description=data.get("description", "NO DESCRIPTION"),
+        )
+
+    def __init__(self, key, endpoint, data):
+        super().__init__(key, endpoint)
+        utilities.logger.debug("Creating rule object '%s'", key)  # utilities.json_dump(data))
+        self._json = data
+        self.severity = data.get("severity", None)
+        self.repo = data.get("repo", None)
+        self.type = data.get("type", None)
+        self.tags = None if len(data.get("tags", [])) == 0 else data["tags"]
+        self.name = data.get("name", None)
+        self.language = data.get("lang", None)
+        self.custom_desc = data.get("mdNote", None)
+        self.created_at = data["createdAt"]
+        self.is_template = data.get("isTemplate", False)
+        self.template_key = data.get("templateKey", None)
+        self._impacts = data.get("impacts", None)
+        self._clean_code_attribute = {
+            "attribute": data.get("cleanCodeAttribute", None),
+            "attribute_category": data.get("cleanCodeAttributeCategory", None),
+        }
+        _OBJECTS[self.key] = self
+
+    def __str__(self):
+        return f"rule key '{self.key}'"
+
+    def to_json(self):
+        return self._json
+
+    def export(self, full=False):
+        return convert_for_export(self.to_json(), self.language, full=full)
+
+    def set_tags(self, tags):
+        if tags is None:
+            return
+        if isinstance(tags, list):
+            tags = utilities.list_to_csv(tags)
+        utilities.logger.debug("Settings custom tags '%s' to %s", tags, str(self))
+        self.post("rules/update", params={"key": self.key, "tags": tags})
+
+    def set_description(self, description):
+        if description is None:
+            return
+        utilities.logger.debug("Settings custom description '%s' to %s", description, str(self))
+        self.post("rules/update", params={"key": self.key, "markdown_note": description})
+
+    def clean_code_attribute(self) -> dict:
+        return self._clean_code_attribute
+
+    def impacts(self) -> dict:
+        return self._impacts
+
+
+def get_facet(facet, endpoint):
+    data = json.loads(endpoint.get(SEARCH_API, params={"ps": 1, "facets": facet}).text)
+    facet_dict = {}
+    for f in data["facets"][0]["values"]:
+        facet_dict[f["val"]] = f["count"]
+    return facet_dict
+
+
+def search(endpoint, **params):
+    return sq.search_objects(SEARCH_API, endpoint, "key", "rules", Rule, params, threads=4)
+
+
+def count(endpoint, **params):
+    return json.loads(endpoint.get(SEARCH_API, params={**params, "ps": 1}).text)["total"]
+
+
+def get_list(endpoint, **params):
+    return search(endpoint, include_external="false", **params)
+
+
+def get_object(key, endpoint):
+    if key in _OBJECTS:
+        return _OBJECTS[key]
+    try:
+        return Rule.get_object(key, endpoint)
+    except exceptions.ObjectNotFound:
+        return None
+
+
+def export_all(endpoint, full=False):
+    utilities.logger.info("Exporting rules")
+    rule_list, other_rules, instantiated_rules, extended_rules = {}, {}, {}, {}
+    for rule_key, rule in get_list(endpoint=endpoint).items():
+        rule_export = rule.export(full)
+        if rule.template_key is not None:
+            instantiated_rules[rule_key] = rule_export
+        elif rule.tags is not None or rule.custom_desc is not None:
+            if full:
+                extended_rules[rule_key] = rule_export
+                continue
+            extended_rules[rule_key] = {}
+            if rule.tags is not None:
+                extended_rules[rule_key]["tags"] = rule_export["tags"]
+            if rule.custom_desc is not None:
+                extended_rules[rule_key]["description"] = rule_export["description"]
+        else:
+            other_rules[rule_key] = rule_export
+    if len(instantiated_rules) > 0:
+        rule_list["instantiated"] = instantiated_rules
+    if len(extended_rules) > 0:
+        rule_list["extended"] = extended_rules
+    if len(other_rules) > 0:
+        rule_list["standard"] = other_rules
+    return rule_list
+
+
+def export_instantiated(endpoint, full=False):
+    rule_list = {}
+    for template_key in get_list(endpoint=endpoint, is_template="true"):
+        for rule_key, rule in get_list(endpoint=endpoint, template_key=template_key).items():
+            rule_list[rule_key] = rule.export(full)
+    return rule_list if len(rule_list) > 0 else None
+
+
+def export_customized(endpoint, full=False):
+    rule_list = {}
+    for rule_key, rule in get_list(endpoint=endpoint, is_template="false").items():
+        if rule.tags is None and rule.custom_desc is None:
+            continue
+        if full:
+            rule_list[rule_key] = rule.export(full)
+            continue
+        rule_list[rule_key] = {}
+        if rule.tags:
+            rule_list[rule_key]["tags"] = utilities.list_to_csv(rule.tags, ", ")
+        if rule.custom_desc:
+            rule_list[rule_key]["description"] = rule.custom_desc
+    return rule_list if len(rule_list) > 0 else None
+
+
+def export_needed(endpoint, instantiated=True, extended=True, full=False):
+    rule_list = {}
+    if instantiated:
+        rule_list["instantiated"] = export_instantiated(endpoint, full)
+    if extended:
+        rule_list["extended"] = export_customized(endpoint, full)
+    return utilities.remove_nones(rule_list)
+
+
+def export(endpoint, instantiated=True, extended=True, standard=False, full=False):
+    utilities.logger.info("Exporting rules")
+    if standard:
+        return export_all(endpoint, full)
+    else:
+        return export_needed(endpoint, instantiated, extended, full)
+
+
+def import_config(endpoint, config_data):
+    if "rules" not in config_data:
+        utilities.logger.info("No customized rules (custom tags, extended description) to import")
+        return
+    utilities.logger.info("Importing customized (custom tags, extended description) rules")
+    get_list(endpoint=endpoint)
+    for key, custom in config_data["rules"].get("extended", {}).items():
+        try:
+            rule = Rule.get_object(endpoint, key)
+        except exceptions.ObjectNotFound:
+            utilities.logger.warning("Rule key '%s' does not exist, can't import it", key)
+            continue
+        rule.set_description(custom.get("description", None))
+        rule.set_tags(custom.get("tags", None))
+
+    utilities.logger.debug("get_list from import")
+    get_list(endpoint=endpoint, templates=True)
+    utilities.logger.info("Importing custom rules (instantiated from rule templates)")
+    for key, instantiation_data in config_data["rules"].get("instantiated", {}).items():
+        try:
+            rule = Rule.get_object(endpoint, key)
+            utilities.logger.debug("Instantiated rule key '%s' already exists, instantiation skipped", key)
+            continue
+        except exceptions.ObjectNotFound:
+            pass
+        try:
+            template_rule = Rule.get_object(endpoint, instantiation_data["templateKey"])
+        except exceptions.ObjectNotFound:
+            utilities.logger.warning("Rule template key '%s' does not exist, can't instantiate it", key)
+            continue
+        Rule.instantiate(key, template_rule.key, endpoint, instantiation_data)
+
+
+def convert_for_export(rule, qp_lang, with_template_key=True, full=False):
+    d = {"severity": rule.get("severity", "")}
+    if len(rule.get("params", {})) > 0:
+        if not full:
+            d["params"] = {}
+            for p in rule["params"]:
+                d["params"][p["key"]] = p.get("defaultValue", "")
+        else:
+            d["params"] = rule["params"]
+    if rule["isTemplate"]:
+        d["isTemplate"] = True
+    if "tags" in rule and len(rule["tags"]) > 0:
+        d["tags"] = utilities.list_to_csv(rule["tags"])
+    if rule.get("mdNote", None) is not None:
+        d["description"] = rule["mdNote"]
+    if with_template_key and "templateKey" in rule:
+        d["templateKey"] = rule["templateKey"]
+    if "lang" in rule and rule["lang"] != qp_lang:
+        d["language"] = rule["lang"]
+    if full:
+        for k, v in rule.items():
+            if k not in ("severity", "params", "isTemplate", "tags", "mdNote", "templateKey", "lang"):
+                d[f"_{k}"] = v
+        d.pop("_key", None)
+    if len(d) == 1:
+        return d["severity"]
+    return d
```

## sonar/settings.py

 * *Ordering differences only*

```diff
@@ -1,426 +1,426 @@
-#
-# sonar-tools
-# Copyright (C) 2022-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-"""
-    Abstraction of the SonarQube setting concept
-"""
-
-import re
-import json
-from sonar import sqobject
-import sonar.utilities as util
-
-DEVOPS_INTEGRATION = "devopsIntegration"
-GENERAL_SETTINGS = "generalSettings"
-LANGUAGES_SETTINGS = "languages"
-AUTH_SETTINGS = "authentication"
-LINTER_SETTINGS = "linters"
-THIRD_PARTY_SETTINGS = "thirdParty"
-ANALYSIS_SCOPE_SETTINGS = "analysisScope"
-SAST_CONFIG_SETTINGS = "sastConfig"
-TEST_SETTINGS = "tests"
-UNIVERSAL_SEPARATOR = ":"
-
-CATEGORIES = (
-    GENERAL_SETTINGS,
-    LANGUAGES_SETTINGS,
-    ANALYSIS_SCOPE_SETTINGS,
-    TEST_SETTINGS,
-    LINTER_SETTINGS,
-    AUTH_SETTINGS,
-    SAST_CONFIG_SETTINGS,
-    THIRD_PARTY_SETTINGS,
-)
-
-NEW_CODE_PERIOD = "newCodePeriod"
-COMPONENT_VISIBILITY = "visibility"
-PROJECT_DEFAULT_VISIBILITY = "projects.default.visibility"
-
-DEFAULT_SETTING = "__default__"
-
-_OBJECTS = {}
-
-_PRIVATE_SETTINGS = (
-    "sonaranalyzer",
-    "sonar.updatecenter",
-    "sonar.plugins.risk.consent",
-    "sonar.core.id",
-    "sonar.core.startTime",
-    "sonar.plsql.jdbc.driver.class",
-)
-
-_INLINE_SETTINGS = (
-    r"^.*\.file\.suffixes$",
-    r"^.*\.reportPaths$",
-    r"^sonar(\.[a-z]+)?\.exclusions$",
-    r"^sonar\.javascript\.(globals|environments)$",
-    r"^sonar\.dbcleaner\.branchesToKeepWhenInactive$",
-    r"^sonar\.rpg\.suffixes$",
-    r"^sonar\.cs\.roslyn\.(bug|codeSmell|vulnerability)Categories$",
-    r"^sonar\.governance\.report\.view\.recipients$",
-    r"^sonar\.portfolios\.recompute\.hours$",
-    r"^sonar\.cobol\.copy\.(directories|exclusions)$",
-    r"^sonar\.cobol\.sql\.catalog\.defaultSchema$",
-)
-
-_API_SET = "settings/set"
-_CREATE_API = "settings/set"
-_API_GET = "settings/values"
-_API_LIST = "settings/list_definitions"
-API_NEW_CODE_GET = "new_code_periods/show"
-_API_NEW_CODE_SET = "new_code_periods/set"
-
-VALID_SETTINGS = set()
-
-
-class Setting(sqobject.SqObject):
-    @classmethod
-    def read(cls, key, endpoint, component=None):
-        util.logger.debug("Reading setting '%s' for %s", key, str(component))
-        uu = _uuid_p(key, component)
-        if uu in _OBJECTS:
-            return _OBJECTS[uu]
-        if key == NEW_CODE_PERIOD:
-            params = get_component_params(component, name="project")
-            data = json.loads(endpoint.get(API_NEW_CODE_GET, params=params).text)
-        else:
-            params = get_component_params(component)
-            params.update({"keys": key})
-            data = json.loads(endpoint.get(_API_GET, params=params).text)["settings"][0]
-        return Setting.load(key=key, endpoint=endpoint, data=data, component=component)
-
-    @classmethod
-    def create(cls, key, endpoint, value=None, component=None):
-        util.logger.debug("Creating setting '%s' of component '%s' value '%s'", key, str(component), str(value))
-        r = endpoint.post(_CREATE_API, params={"key": key, "component": component})
-        if not r.ok:
-            return None
-        o = cls.read(key=key, endpoint=endpoint, component=component)
-        return o
-
-    @classmethod
-    def load(cls, key, endpoint, data, component=None):
-        util.logger.debug("Loading setting '%s' of component '%s' with data %s", key, str(component), str(data))
-        uu = _uuid_p(key, component)
-        o = _OBJECTS[uu] if uu in _OBJECTS else cls(key=key, endpoint=endpoint, data=data, component=component)
-        o.reload(data)
-        return o
-
-    def __init__(self, key, endpoint, component=None, data=None):
-        super().__init__(key, endpoint)
-        self.component = component
-        self.value = None
-        self.inherited = None
-        self.reload(data)
-        util.logger.debug("Created %s uuid %s value %s", str(self), self.uuid(), str(self.value))
-        _OBJECTS[self.uuid()] = self
-
-    def reload(self, data):
-        if not data:
-            return
-        if self.key == NEW_CODE_PERIOD:
-            self.value = new_code_to_string(data)
-        elif self.key == COMPONENT_VISIBILITY:
-            self.value = data["visibility"]
-        elif self.key.startswith("sonar.issue."):
-            self.value = data.get("fieldValues", None)
-        else:
-            self.value = util.convert_string(data.get("value", data.get("values", data.get("defaultValue", ""))))
-
-        if "inherited" in data:
-            self.inherited = data["inherited"]
-        elif self.key == NEW_CODE_PERIOD:
-            self.inherited = False
-        elif "parentValues" in data or "parentValue" in data or "parentFieldValues" in data:
-            self.inherited = False
-        elif "category" in data:
-            self.inherited = True
-        elif self.component is not None:
-            self.inherited = False
-        else:
-            self.inherited = True
-        if self.component is None:
-            self.inherited = True
-
-    def uuid(self):
-        return _uuid_p(self.key, self.component)
-
-    def __str__(self):
-        if self.component is None:
-            return f"setting '{self.key}'"
-        else:
-            return f"setting '{self.key}' of {str(self.component)}"
-
-    def set(self, value):
-        util.logger.debug("%s set to '%s'", str(self), str(value))
-        if not is_valid(self.key, self.endpoint):
-            util.logger.error("Setting '%s' does not seem to be a valid setting, trying to set anyway...", str(self))
-        if value is None or value == "":
-            # TODO: return endpoint.reset_setting(key)
-            return True
-        if self.key in (COMPONENT_VISIBILITY, PROJECT_DEFAULT_VISIBILITY):
-            return set_visibility(endpoint=self.endpoint, component=self.component, visibility=value)
-
-        # Hack: Up to 9.4 cobol settings are comma separated mono-valued, in 9.5+ they are multi-valued
-        if self.endpoint.version() > (9, 4, 0) or not __is_cobol_setting(self.key):
-            value = decode(self.key, value)
-
-        util.logger.debug("Setting %s to value '%s'", str(self), str(value))
-        params = {"key": self.key, "component": self.component.key if self.component else None}
-        if isinstance(value, list):
-            if isinstance(value[0], str):
-                params["values"] = value
-            else:
-                params["fieldValues"] = [util.json.dumps(v) for v in value]
-        else:
-            if isinstance(value, bool):
-                value = "true" if value else "false"
-            params["value"] = value
-        return self.post(_API_SET, params=params).ok
-
-    def to_json(self):
-        return {self.key: encode(self.key, self.value)}
-
-    def category(self):
-        m = re.match(
-            r"^sonar\.(cpd\.)?(abap|apex|cloudformation|c|cpp|cfamily|cobol|cs|css|flex|go|html|java|"
-            r"javascript|json|jsp|kotlin|objc|php|pli|plsql|python|rpg|ruby|scala|swift|terraform|tsql|"
-            r"typescript|vb|vbnet|xml|yaml)\.",
-            self.key,
-        )
-        if m:
-            lang = m.group(2)
-            if lang in ("c", "cpp", "objc", "cfamily"):
-                lang = "cfamily"
-            return (LANGUAGES_SETTINGS, lang)
-        if re.match(
-            r"^.*([lL]int|govet|flake8|checkstyle|pmd|spotbugs|phpstan|psalm|detekt|bandit|rubocop|scalastyle|scapegoat).*$",
-            self.key,
-        ):
-            return (LINTER_SETTINGS, None)
-        if re.match(r"^sonar\.security\.config\..+$", self.key):
-            return (SAST_CONFIG_SETTINGS, None)
-        if re.match(r"^.*\.(exclusions$|inclusions$|issue\..+)$", self.key):
-            return (ANALYSIS_SCOPE_SETTINGS, None)
-
-        if re.match(r"^.*(\.reports?Paths?$|unit\..*$|cov.*$)", self.key):
-            return (TEST_SETTINGS, None)
-        m = re.match(r"^sonar\.(auth\.|authenticator\.downcase).*$", self.key)
-        if m:
-            return (AUTH_SETTINGS, None)
-        m = re.match(r"^sonar\.forceAuthentication$", self.key)
-        if m:
-            return (AUTH_SETTINGS, None)
-        if self.key not in (NEW_CODE_PERIOD, PROJECT_DEFAULT_VISIBILITY, COMPONENT_VISIBILITY) and not re.match(
-            r"^(email|sonar\.core|sonar\.allowPermission|sonar\.builtInQualityProfiles|sonar\.core|"
-            r"sonar\.cpd|sonar\.dbcleaner|sonar\.developerAggregatedInfo|sonar\.governance|sonar\.issues|sonar\.lf|sonar\.notifications|"
-            r"sonar\.portfolios|sonar\.qualitygate|sonar\.scm\.disabled|sonar\.scm\.provider|sonar\.technicalDebt|sonar\.validateWebhooks).*$",
-            self.key,
-        ):
-            return ("thirdParty", None)
-        return (GENERAL_SETTINGS, None)
-
-
-def get_object(key, component=None):
-    return _OBJECTS.get(_uuid_p(key, component), None)
-
-
-def get_bulk(endpoint, settings_list=None, component=None, include_not_set=False):
-    """Gets several settings as bulk (returns a dict)"""
-    settings_dict = {}
-    params = get_component_params(component)
-    if include_not_set:
-        data = json.loads(endpoint.get(_API_LIST, params=params).text)
-        for s in data["definitions"]:
-            if s["key"].endswith("coverage.reportPath") or s["key"] == "languageSpecificParameters":
-                continue
-            o = Setting.load(key=s["key"], endpoint=endpoint, data=s, component=component)
-            settings_dict[o.key] = o
-    if settings_list is None:
-        pass
-    elif isinstance(settings_list, list):
-        params["keys"] = util.list_to_csv(settings_list)
-    else:
-        params["keys"] = util.csv_normalize(settings_list)
-    data = json.loads(endpoint.get(_API_GET, params=params).text)
-    settings_type_list = ["settings"]
-    # Hack: Sonar API also return setSecureSettings for projects although it's irrelevant
-    if component is None:
-        settings_type_list = ["setSecuredSettings"]
-    settings_type_list += ["settings"]
-    for setting_type in settings_type_list:
-        util.logger.debug("Looking at %s", setting_type)
-        for s in data.get(setting_type, {}):
-            (key, sdata) = (s, {}) if isinstance(s, str) else (s["key"], s)
-            if is_private(key) > 0:
-                util.logger.debug("Skipping private setting %s", s["key"])
-                continue
-            o = Setting.load(key=key, endpoint=endpoint, component=component, data=sdata)
-            settings_dict[o.key] = o
-
-    # Hack since projects.default.visibility is not returned by settings/list_definitions
-    o = get_visibility(endpoint, component)
-    settings_dict[o.key] = o
-
-    o = get_new_code_period(endpoint, component)
-    settings_dict[o.key] = o
-    VALID_SETTINGS.update(set(settings_dict.keys()))
-    VALID_SETTINGS.update({"sonar.scm.provider"})
-    return settings_dict
-
-
-def get_all(endpoint, project=None):
-    return get_bulk(endpoint, component=project, include_not_set=True)
-
-
-def uuid(key, project_key=None):
-    """Computes uuid for a setting"""
-    if project_key is None:
-        return key
-    else:
-        return f"{key}#{project_key}"
-
-
-def _uuid_p(key, component):
-    """Computes uuid for a setting"""
-    pk = None if component is None else component.key
-    return uuid(key, pk)
-
-
-def new_code_to_string(data):
-    if isinstance(data, (int, str)):
-        return data
-    if data.get("inherited", False):
-        return None
-    if data["type"] == "PREVIOUS_VERSION":
-        return data["type"]
-    elif data["type"] == "SPECIFIC_ANALYSIS":
-        return f"{data['type']} = {data['effectiveValue']}"
-    else:
-        return f"{data['type']} = {data['value']}"
-
-
-def get_new_code_period(endpoint, project_or_branch):
-    return Setting.read(key=NEW_CODE_PERIOD, endpoint=endpoint, component=project_or_branch)
-
-
-def string_to_new_code(value):
-    return re.split(r"\s*=\s*", value)
-
-
-def set_new_code_period(endpoint, nc_type, nc_value, project_key=None, branch=None):
-    util.logger.debug(
-        "Setting new code period for project '%s' branch '%s' to value '%s = %s'", str(project_key), str(branch), str(nc_type), str(nc_value)
-    )
-    return endpoint.post(_API_NEW_CODE_SET, params={"type": nc_type, "value": nc_value, "project": project_key, "branch": branch})
-
-
-def get_visibility(endpoint, component):
-    uu = uuid(COMPONENT_VISIBILITY, component.key) if component else uuid(PROJECT_DEFAULT_VISIBILITY)
-    if uu in _OBJECTS:
-        return _OBJECTS[uu]
-    if component:
-        data = json.loads(endpoint.get("components/show", params={"component": component.key}).text)
-        return Setting.load(key=COMPONENT_VISIBILITY, endpoint=endpoint, component=component, data=data["component"])
-    else:
-        data = json.loads(endpoint.get(_API_GET, params={"keys": PROJECT_DEFAULT_VISIBILITY}).text)
-        return Setting.load(key=PROJECT_DEFAULT_VISIBILITY, endpoint=endpoint, component=None, data=data["settings"][0])
-
-
-def set_visibility(endpoint, visibility, component=None):
-    if component:
-        util.logger.debug("Setting setting '%s' of %s to value '%s'", COMPONENT_VISIBILITY, str(component), visibility)
-        return endpoint.post("projects/update_visibility", params={"project": component.key, "visibility": visibility})
-    else:
-        util.logger.debug("Setting setting '%s' to value '%s'", PROJECT_DEFAULT_VISIBILITY, str(visibility))
-        r = endpoint.post("projects/update_default_visibility", params={"projectVisibility": visibility})
-        return r
-
-
-def __is_cobol_setting(key):
-    return re.match(r"^sonar\.cobol\..*$", key)
-
-
-def set_setting(endpoint, key, value, component=None):
-    return Setting.load(key, endpoint=endpoint, component=component, data=None).set(value)
-
-
-def encode(setting_key, setting_value):
-    if setting_value is None:
-        return ""
-    if setting_key == NEW_CODE_PERIOD:
-        return new_code_to_string(setting_value)
-    if isinstance(setting_value, str):
-        return setting_value
-    if not isinstance(setting_value, list):
-        return setting_value
-    val = setting_value.copy()
-    for reg in _INLINE_SETTINGS:
-        if re.match(reg, setting_key):
-            val = util.list_to_csv(val, ", ", True)
-            break
-    if val is None:
-        val = ""
-    return val
-
-
-def decode(setting_key, setting_value):
-    if setting_key == NEW_CODE_PERIOD:
-        if isinstance(setting_value, int):
-            return ("NUMBER_OF_DAYS", setting_value)
-        elif setting_value == "PREVIOUS_VERSION":
-            return (setting_value, "")
-        return string_to_new_code(setting_value)
-    if not isinstance(setting_value, str):
-        return setting_value
-    # TODO: Handle all comma separated settings
-    for reg in _INLINE_SETTINGS:
-        if re.match(reg, setting_key):
-            setting_value = util.csv_to_list(setting_value)
-            break
-    return setting_value
-
-
-def reset_setting(endpoint, setting_key, project_key=None):
-    util.logger.info("Resetting setting '%s", setting_key)
-    return endpoint.post("settings/reset", params={"key": setting_key, "component": project_key})
-
-
-def get_component_params(component, name="component"):
-    if not component:
-        return {}
-    elif type(component).__name__ == "Branch":
-        return {name: component.project.key, "branch": component.key}
-    else:
-        return {name: component.key}
-
-
-def is_valid(setting_key, endpoint):
-    if len(VALID_SETTINGS) == 0:
-        get_bulk(endpoint=endpoint, include_not_set=True)
-    if setting_key not in VALID_SETTINGS:
-        return False
-    return not is_private(setting_key)
-
-
-def is_private(setting_key):
-    for prefix in _PRIVATE_SETTINGS:
-        if setting_key.startswith(prefix):
-            return True
-    return False
+#
+# sonar-tools
+# Copyright (C) 2022-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+"""
+    Abstraction of the SonarQube setting concept
+"""
+
+import re
+import json
+from sonar import sqobject
+import sonar.utilities as util
+
+DEVOPS_INTEGRATION = "devopsIntegration"
+GENERAL_SETTINGS = "generalSettings"
+LANGUAGES_SETTINGS = "languages"
+AUTH_SETTINGS = "authentication"
+LINTER_SETTINGS = "linters"
+THIRD_PARTY_SETTINGS = "thirdParty"
+ANALYSIS_SCOPE_SETTINGS = "analysisScope"
+SAST_CONFIG_SETTINGS = "sastConfig"
+TEST_SETTINGS = "tests"
+UNIVERSAL_SEPARATOR = ":"
+
+CATEGORIES = (
+    GENERAL_SETTINGS,
+    LANGUAGES_SETTINGS,
+    ANALYSIS_SCOPE_SETTINGS,
+    TEST_SETTINGS,
+    LINTER_SETTINGS,
+    AUTH_SETTINGS,
+    SAST_CONFIG_SETTINGS,
+    THIRD_PARTY_SETTINGS,
+)
+
+NEW_CODE_PERIOD = "newCodePeriod"
+COMPONENT_VISIBILITY = "visibility"
+PROJECT_DEFAULT_VISIBILITY = "projects.default.visibility"
+
+DEFAULT_SETTING = "__default__"
+
+_OBJECTS = {}
+
+_PRIVATE_SETTINGS = (
+    "sonaranalyzer",
+    "sonar.updatecenter",
+    "sonar.plugins.risk.consent",
+    "sonar.core.id",
+    "sonar.core.startTime",
+    "sonar.plsql.jdbc.driver.class",
+)
+
+_INLINE_SETTINGS = (
+    r"^.*\.file\.suffixes$",
+    r"^.*\.reportPaths$",
+    r"^sonar(\.[a-z]+)?\.exclusions$",
+    r"^sonar\.javascript\.(globals|environments)$",
+    r"^sonar\.dbcleaner\.branchesToKeepWhenInactive$",
+    r"^sonar\.rpg\.suffixes$",
+    r"^sonar\.cs\.roslyn\.(bug|codeSmell|vulnerability)Categories$",
+    r"^sonar\.governance\.report\.view\.recipients$",
+    r"^sonar\.portfolios\.recompute\.hours$",
+    r"^sonar\.cobol\.copy\.(directories|exclusions)$",
+    r"^sonar\.cobol\.sql\.catalog\.defaultSchema$",
+)
+
+_API_SET = "settings/set"
+_CREATE_API = "settings/set"
+_API_GET = "settings/values"
+_API_LIST = "settings/list_definitions"
+API_NEW_CODE_GET = "new_code_periods/show"
+_API_NEW_CODE_SET = "new_code_periods/set"
+
+VALID_SETTINGS = set()
+
+
+class Setting(sqobject.SqObject):
+    @classmethod
+    def read(cls, key, endpoint, component=None):
+        util.logger.debug("Reading setting '%s' for %s", key, str(component))
+        uu = _uuid_p(key, component)
+        if uu in _OBJECTS:
+            return _OBJECTS[uu]
+        if key == NEW_CODE_PERIOD:
+            params = get_component_params(component, name="project")
+            data = json.loads(endpoint.get(API_NEW_CODE_GET, params=params).text)
+        else:
+            params = get_component_params(component)
+            params.update({"keys": key})
+            data = json.loads(endpoint.get(_API_GET, params=params).text)["settings"][0]
+        return Setting.load(key=key, endpoint=endpoint, data=data, component=component)
+
+    @classmethod
+    def create(cls, key, endpoint, value=None, component=None):
+        util.logger.debug("Creating setting '%s' of component '%s' value '%s'", key, str(component), str(value))
+        r = endpoint.post(_CREATE_API, params={"key": key, "component": component})
+        if not r.ok:
+            return None
+        o = cls.read(key=key, endpoint=endpoint, component=component)
+        return o
+
+    @classmethod
+    def load(cls, key, endpoint, data, component=None):
+        util.logger.debug("Loading setting '%s' of component '%s' with data %s", key, str(component), str(data))
+        uu = _uuid_p(key, component)
+        o = _OBJECTS[uu] if uu in _OBJECTS else cls(key=key, endpoint=endpoint, data=data, component=component)
+        o.reload(data)
+        return o
+
+    def __init__(self, key, endpoint, component=None, data=None):
+        super().__init__(key, endpoint)
+        self.component = component
+        self.value = None
+        self.inherited = None
+        self.reload(data)
+        util.logger.debug("Created %s uuid %s value %s", str(self), self.uuid(), str(self.value))
+        _OBJECTS[self.uuid()] = self
+
+    def reload(self, data):
+        if not data:
+            return
+        if self.key == NEW_CODE_PERIOD:
+            self.value = new_code_to_string(data)
+        elif self.key == COMPONENT_VISIBILITY:
+            self.value = data["visibility"]
+        elif self.key.startswith("sonar.issue."):
+            self.value = data.get("fieldValues", None)
+        else:
+            self.value = util.convert_string(data.get("value", data.get("values", data.get("defaultValue", ""))))
+
+        if "inherited" in data:
+            self.inherited = data["inherited"]
+        elif self.key == NEW_CODE_PERIOD:
+            self.inherited = False
+        elif "parentValues" in data or "parentValue" in data or "parentFieldValues" in data:
+            self.inherited = False
+        elif "category" in data:
+            self.inherited = True
+        elif self.component is not None:
+            self.inherited = False
+        else:
+            self.inherited = True
+        if self.component is None:
+            self.inherited = True
+
+    def uuid(self):
+        return _uuid_p(self.key, self.component)
+
+    def __str__(self):
+        if self.component is None:
+            return f"setting '{self.key}'"
+        else:
+            return f"setting '{self.key}' of {str(self.component)}"
+
+    def set(self, value):
+        util.logger.debug("%s set to '%s'", str(self), str(value))
+        if not is_valid(self.key, self.endpoint):
+            util.logger.error("Setting '%s' does not seem to be a valid setting, trying to set anyway...", str(self))
+        if value is None or value == "":
+            # TODO: return endpoint.reset_setting(key)
+            return True
+        if self.key in (COMPONENT_VISIBILITY, PROJECT_DEFAULT_VISIBILITY):
+            return set_visibility(endpoint=self.endpoint, component=self.component, visibility=value)
+
+        # Hack: Up to 9.4 cobol settings are comma separated mono-valued, in 9.5+ they are multi-valued
+        if self.endpoint.version() > (9, 4, 0) or not __is_cobol_setting(self.key):
+            value = decode(self.key, value)
+
+        util.logger.debug("Setting %s to value '%s'", str(self), str(value))
+        params = {"key": self.key, "component": self.component.key if self.component else None}
+        if isinstance(value, list):
+            if isinstance(value[0], str):
+                params["values"] = value
+            else:
+                params["fieldValues"] = [util.json.dumps(v) for v in value]
+        else:
+            if isinstance(value, bool):
+                value = "true" if value else "false"
+            params["value"] = value
+        return self.post(_API_SET, params=params).ok
+
+    def to_json(self):
+        return {self.key: encode(self.key, self.value)}
+
+    def category(self):
+        m = re.match(
+            r"^sonar\.(cpd\.)?(abap|apex|cloudformation|c|cpp|cfamily|cobol|cs|css|flex|go|html|java|"
+            r"javascript|json|jsp|kotlin|objc|php|pli|plsql|python|rpg|ruby|scala|swift|terraform|tsql|"
+            r"typescript|vb|vbnet|xml|yaml)\.",
+            self.key,
+        )
+        if m:
+            lang = m.group(2)
+            if lang in ("c", "cpp", "objc", "cfamily"):
+                lang = "cfamily"
+            return (LANGUAGES_SETTINGS, lang)
+        if re.match(
+            r"^.*([lL]int|govet|flake8|checkstyle|pmd|spotbugs|phpstan|psalm|detekt|bandit|rubocop|scalastyle|scapegoat).*$",
+            self.key,
+        ):
+            return (LINTER_SETTINGS, None)
+        if re.match(r"^sonar\.security\.config\..+$", self.key):
+            return (SAST_CONFIG_SETTINGS, None)
+        if re.match(r"^.*\.(exclusions$|inclusions$|issue\..+)$", self.key):
+            return (ANALYSIS_SCOPE_SETTINGS, None)
+
+        if re.match(r"^.*(\.reports?Paths?$|unit\..*$|cov.*$)", self.key):
+            return (TEST_SETTINGS, None)
+        m = re.match(r"^sonar\.(auth\.|authenticator\.downcase).*$", self.key)
+        if m:
+            return (AUTH_SETTINGS, None)
+        m = re.match(r"^sonar\.forceAuthentication$", self.key)
+        if m:
+            return (AUTH_SETTINGS, None)
+        if self.key not in (NEW_CODE_PERIOD, PROJECT_DEFAULT_VISIBILITY, COMPONENT_VISIBILITY) and not re.match(
+            r"^(email|sonar\.core|sonar\.allowPermission|sonar\.builtInQualityProfiles|sonar\.core|"
+            r"sonar\.cpd|sonar\.dbcleaner|sonar\.developerAggregatedInfo|sonar\.governance|sonar\.issues|sonar\.lf|sonar\.notifications|"
+            r"sonar\.portfolios|sonar\.qualitygate|sonar\.scm\.disabled|sonar\.scm\.provider|sonar\.technicalDebt|sonar\.validateWebhooks).*$",
+            self.key,
+        ):
+            return ("thirdParty", None)
+        return (GENERAL_SETTINGS, None)
+
+
+def get_object(key, component=None):
+    return _OBJECTS.get(_uuid_p(key, component), None)
+
+
+def get_bulk(endpoint, settings_list=None, component=None, include_not_set=False):
+    """Gets several settings as bulk (returns a dict)"""
+    settings_dict = {}
+    params = get_component_params(component)
+    if include_not_set:
+        data = json.loads(endpoint.get(_API_LIST, params=params).text)
+        for s in data["definitions"]:
+            if s["key"].endswith("coverage.reportPath") or s["key"] == "languageSpecificParameters":
+                continue
+            o = Setting.load(key=s["key"], endpoint=endpoint, data=s, component=component)
+            settings_dict[o.key] = o
+    if settings_list is None:
+        pass
+    elif isinstance(settings_list, list):
+        params["keys"] = util.list_to_csv(settings_list)
+    else:
+        params["keys"] = util.csv_normalize(settings_list)
+    data = json.loads(endpoint.get(_API_GET, params=params).text)
+    settings_type_list = ["settings"]
+    # Hack: Sonar API also return setSecureSettings for projects although it's irrelevant
+    if component is None:
+        settings_type_list = ["setSecuredSettings"]
+    settings_type_list += ["settings"]
+    for setting_type in settings_type_list:
+        util.logger.debug("Looking at %s", setting_type)
+        for s in data.get(setting_type, {}):
+            (key, sdata) = (s, {}) if isinstance(s, str) else (s["key"], s)
+            if is_private(key) > 0:
+                util.logger.debug("Skipping private setting %s", s["key"])
+                continue
+            o = Setting.load(key=key, endpoint=endpoint, component=component, data=sdata)
+            settings_dict[o.key] = o
+
+    # Hack since projects.default.visibility is not returned by settings/list_definitions
+    o = get_visibility(endpoint, component)
+    settings_dict[o.key] = o
+
+    o = get_new_code_period(endpoint, component)
+    settings_dict[o.key] = o
+    VALID_SETTINGS.update(set(settings_dict.keys()))
+    VALID_SETTINGS.update({"sonar.scm.provider"})
+    return settings_dict
+
+
+def get_all(endpoint, project=None):
+    return get_bulk(endpoint, component=project, include_not_set=True)
+
+
+def uuid(key, project_key=None):
+    """Computes uuid for a setting"""
+    if project_key is None:
+        return key
+    else:
+        return f"{key}#{project_key}"
+
+
+def _uuid_p(key, component):
+    """Computes uuid for a setting"""
+    pk = None if component is None else component.key
+    return uuid(key, pk)
+
+
+def new_code_to_string(data):
+    if isinstance(data, (int, str)):
+        return data
+    if data.get("inherited", False):
+        return None
+    if data["type"] == "PREVIOUS_VERSION":
+        return data["type"]
+    elif data["type"] == "SPECIFIC_ANALYSIS":
+        return f"{data['type']} = {data['effectiveValue']}"
+    else:
+        return f"{data['type']} = {data['value']}"
+
+
+def get_new_code_period(endpoint, project_or_branch):
+    return Setting.read(key=NEW_CODE_PERIOD, endpoint=endpoint, component=project_or_branch)
+
+
+def string_to_new_code(value):
+    return re.split(r"\s*=\s*", value)
+
+
+def set_new_code_period(endpoint, nc_type, nc_value, project_key=None, branch=None):
+    util.logger.debug(
+        "Setting new code period for project '%s' branch '%s' to value '%s = %s'", str(project_key), str(branch), str(nc_type), str(nc_value)
+    )
+    return endpoint.post(_API_NEW_CODE_SET, params={"type": nc_type, "value": nc_value, "project": project_key, "branch": branch})
+
+
+def get_visibility(endpoint, component):
+    uu = uuid(COMPONENT_VISIBILITY, component.key) if component else uuid(PROJECT_DEFAULT_VISIBILITY)
+    if uu in _OBJECTS:
+        return _OBJECTS[uu]
+    if component:
+        data = json.loads(endpoint.get("components/show", params={"component": component.key}).text)
+        return Setting.load(key=COMPONENT_VISIBILITY, endpoint=endpoint, component=component, data=data["component"])
+    else:
+        data = json.loads(endpoint.get(_API_GET, params={"keys": PROJECT_DEFAULT_VISIBILITY}).text)
+        return Setting.load(key=PROJECT_DEFAULT_VISIBILITY, endpoint=endpoint, component=None, data=data["settings"][0])
+
+
+def set_visibility(endpoint, visibility, component=None):
+    if component:
+        util.logger.debug("Setting setting '%s' of %s to value '%s'", COMPONENT_VISIBILITY, str(component), visibility)
+        return endpoint.post("projects/update_visibility", params={"project": component.key, "visibility": visibility})
+    else:
+        util.logger.debug("Setting setting '%s' to value '%s'", PROJECT_DEFAULT_VISIBILITY, str(visibility))
+        r = endpoint.post("projects/update_default_visibility", params={"projectVisibility": visibility})
+        return r
+
+
+def __is_cobol_setting(key):
+    return re.match(r"^sonar\.cobol\..*$", key)
+
+
+def set_setting(endpoint, key, value, component=None):
+    return Setting.load(key, endpoint=endpoint, component=component, data=None).set(value)
+
+
+def encode(setting_key, setting_value):
+    if setting_value is None:
+        return ""
+    if setting_key == NEW_CODE_PERIOD:
+        return new_code_to_string(setting_value)
+    if isinstance(setting_value, str):
+        return setting_value
+    if not isinstance(setting_value, list):
+        return setting_value
+    val = setting_value.copy()
+    for reg in _INLINE_SETTINGS:
+        if re.match(reg, setting_key):
+            val = util.list_to_csv(val, ", ", True)
+            break
+    if val is None:
+        val = ""
+    return val
+
+
+def decode(setting_key, setting_value):
+    if setting_key == NEW_CODE_PERIOD:
+        if isinstance(setting_value, int):
+            return ("NUMBER_OF_DAYS", setting_value)
+        elif setting_value == "PREVIOUS_VERSION":
+            return (setting_value, "")
+        return string_to_new_code(setting_value)
+    if not isinstance(setting_value, str):
+        return setting_value
+    # TODO: Handle all comma separated settings
+    for reg in _INLINE_SETTINGS:
+        if re.match(reg, setting_key):
+            setting_value = util.csv_to_list(setting_value)
+            break
+    return setting_value
+
+
+def reset_setting(endpoint, setting_key, project_key=None):
+    util.logger.info("Resetting setting '%s", setting_key)
+    return endpoint.post("settings/reset", params={"key": setting_key, "component": project_key})
+
+
+def get_component_params(component, name="component"):
+    if not component:
+        return {}
+    elif type(component).__name__ == "Branch":
+        return {name: component.project.key, "branch": component.key}
+    else:
+        return {name: component.key}
+
+
+def is_valid(setting_key, endpoint):
+    if len(VALID_SETTINGS) == 0:
+        get_bulk(endpoint=endpoint, include_not_set=True)
+    if setting_key not in VALID_SETTINGS:
+        return False
+    return not is_private(setting_key)
+
+
+def is_private(setting_key):
+    for prefix in _PRIVATE_SETTINGS:
+        if setting_key.startswith(prefix):
+            return True
+    return False
```

## sonar/sif.py

```diff
@@ -1,471 +1,470 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-"""
-
-    Abstraction of the SonarQube System Info File (or Support Info File) concept
-
-"""
-
-import datetime
-import re
-from dateutil.relativedelta import relativedelta
-import sonar.utilities as util
-
-from sonar.audit import rules, types, severities
-import sonar.audit.problem as pb
-
-import sonar.dce.app_nodes as appnodes
-import sonar.dce.search_nodes as searchnodes
-
-_RELEASE_DATE_6_7 = datetime.datetime(2017, 11, 8) + relativedelta(months=+6)
-_RELEASE_DATE_7_9 = datetime.datetime(2019, 7, 1) + relativedelta(months=+6)
-_RELEASE_DATE_8_9 = datetime.datetime(2021, 5, 4) + relativedelta(months=+6)
-
-_APP_NODES = "Application Nodes"
-_ES_NODES = "Search Nodes"
-_SYSTEM = "System"
-_SETTINGS = "Settings"
-_STATS = "Statistics"
-_STORE_SIZE = "Store Size"
-_SEARCH_STATE = "Search State"
-
-_JVM_OPTS = ("sonar.{}.javaOpts", "sonar.{}.javaAdditionalOpts")
-
-_MIN_DATE_LOG4SHELL = datetime.datetime(2021, 12, 1)
-
-
-class NotSystemInfo(Exception):
-    def __init__(self, message):
-        super().__init__()
-        self.message = message
-
-
-class Sif:
-    def __init__(self, json_sif, concerned_object=None):
-        if not is_sysinfo(json_sif):
-            util.logger.critical("Provided JSON does not seem to be a system info")
-            raise NotSystemInfo("JSON is not a system info nor a support info")
-        self.json = json_sif
-        self.concerned_object = concerned_object
-        self._url = None
-
-    def url(self):
-        if not self._url:
-            if self.concerned_object:
-                self._url = self.concerned_object.url
-            else:
-                self._url = self.json.get("Settings", {}).get("sonar.core.serverBaseURL", "")
-        return self._url
-
-    def edition(self):
-        try:
-            ed = self.json[_STATS]["edition"]
-        except KeyError:
-            try:
-                ed = self.json["License"]["edition"]
-            except KeyError:
-                try:
-                    # FIXME: Can't get edition in SIF of SonarQube 9.7+, this is an unsolvable problem
-                    ed = self.json["edition"]
-                except KeyError:
-                    return ""
-        # Old SIFs could return "Enterprise Edition"
-        return ed.split(" ")[0].lower()
-
-    def database(self):
-        if self.version() < (9, 7, 0):
-            return self.json[_STATS]["database"]["name"]
-        else:
-            return self.json["Database"]["Database"]
-
-    def plugins(self):
-        if self.version() < (9, 7, 0):
-            return self.json[_STATS]["plugins"]
-        else:
-            return self.json["Plugins"]
-
-    def license_type(self):
-        if "License" not in self.json:
-            return None
-        elif "type" in self.json["License"]:
-            return self.json["License"]["type"]
-        return None
-
-    def version(self, digits=3, as_string=False):
-        sif_v = self.__get_field("Version")
-        if sif_v is None:
-            return None
-
-        split_version = sif_v.split(".")
-        if as_string:
-            return ".".join(split_version[0:digits])
-        else:
-            return tuple(int(n) for n in split_version[0:digits])
-
-    def server_id(self):
-        return self.__get_field("Server ID")
-
-    def start_time(self):
-        try:
-            return util.string_to_date(self.json[_SETTINGS]["sonar.core.startTime"]).replace(tzinfo=None)
-        except KeyError:
-            pass
-        try:
-            return util.string_to_date(self.json[_SYSTEM]["Start Time"]).replace(tzinfo=None)
-        except KeyError:
-            return None
-
-    def store_size(self):
-        setting = None
-        try:
-            setting = self.json[_SEARCH_STATE][_STORE_SIZE]
-        except KeyError:
-            try:
-                for v in self.json["Elasticsearch"]["Nodes"].values():
-                    if _STORE_SIZE in v:
-                        setting = v[_STORE_SIZE]
-                        break
-            except KeyError:
-                pass
-        if setting is None:
-            return None
-        return util.int_memory(setting)
-
-    def audit(self, audit_settings):
-        util.logger.info("Auditing System Info")
-        problems = self.__audit_jdbc_url() + self.__audit_web_settings(audit_settings)
-        if self.edition() == "datacenter":
-            problems += self.__audit_dce_settings()
-        else:
-            problems += (
-                self.__audit_ce_settings()
-                + self.__audit_background_tasks()
-                + self.__audit_es_settings()
-                + self.__audit_log_level()
-                + self.__audit_version()
-                + self.__audit_branch_use()
-                + self.__audit_undetected_scm()
-            )
-        return problems
-
-    def __audit_branch_use(self):
-        if self.edition() == "community":
-            return []
-        util.logger.info("Auditing usage of branch analysis")
-        try:
-            use_br = self.json[_STATS]["usingBranches"]
-            if use_br:
-                return []
-            rule = rules.get_rule(rules.RuleId.NOT_USING_BRANCH_ANALYSIS)
-            return [pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=self)]
-        except KeyError:
-            util.logger.info("Branch usage information not in SIF, ignoring audit...")
-            return []
-
-    def __audit_undetected_scm(self):
-        util.logger.info("Auditing SCM integration")
-        try:
-            scm_count, undetected_scm_count = 0, 0
-            for scm in self.json[_STATS]["projectCountByScm"]:
-                scm_count += scm["count"]
-                if scm["scm"] == "undetected":
-                    undetected_scm_count = scm["count"]
-            if undetected_scm_count == 0:
-                return []
-            rule = rules.get_rule(rules.RuleId.UNDETECTED_SCM)
-            return [pb.Problem(rule.type, rule.severity, rule.msg.format(undetected_scm_count), concerned_object=self)]
-        except KeyError:
-            util.logger.info("SCM information not in SIF, ignoring audit...")
-            return []
-
-    def __get_field(self, name, node_type=_APP_NODES):
-        if _SYSTEM in self.json and name in self.json[_SYSTEM]:
-            return self.json[_SYSTEM][name]
-        elif "SonarQube" in self.json and name in self.json["SonarQube"]:
-            return self.json["SonarQube"][name]
-        elif node_type in self.json:
-            for node in self.json[node_type]:
-                try:
-                    return node[_SYSTEM][name]
-                except KeyError:
-                    pass
-        return None
-
-    def __process_cmdline(self, process):
-        opts = [x.format(process) for x in _JVM_OPTS]
-        if _SETTINGS in self.json:
-            return self.json[_SETTINGS][opts[1]] + " " + self.json[_SETTINGS][opts[0]]
-        else:
-            return None
-
-    def web_jvm_cmdline(self):
-        return self.__process_cmdline("web")
-
-    def ce_jvm_cmdline(self):
-        return self.__process_cmdline("ce")
-
-    def search_jvm_cmdline(self):
-        return self.__process_cmdline("search")
-
-    def __eligible_to_log4shell_check(self):
-        st_time = self.start_time()
-        if st_time is None:
-            return False
-        return st_time > _MIN_DATE_LOG4SHELL
-
-    def __audit_log4shell(self, jvm_settings, broken_rule):
-        # If SIF is older than 2022 don't audit for log4shell to avoid noise
-        if not self.__eligible_to_log4shell_check():
-            return []
-
-        util.logger.debug("Auditing log4shell vulnerability fix")
-        sq_version = self.version()
-        if sq_version < (8, 9, 6) or ((9, 0, 0) <= sq_version < (9, 2, 4)):
-            for s in jvm_settings.split(" "):
-                if s == "-Dlog4j2.formatMsgNoLookups=true":
-                    return []
-            rule = rules.get_rule(broken_rule)
-            return [pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=self)]
-        return []
-
-    def __audit_jdbc_url(self):
-        util.logger.info("Auditing JDBC settings")
-        problems = []
-        stats = self.json.get(_SETTINGS)
-        if stats is None:
-            util.logger.error("Can't verify Database settings in System Info File, was it corrupted or redacted ?")
-            return problems
-        jdbc_url = stats.get("sonar.jdbc.url", None)
-        util.logger.debug("JDBC URL = %s", str(jdbc_url))
-        if jdbc_url is None:
-            rule = rules.get_rule(rules.RuleId.SETTING_JDBC_URL_NOT_SET)
-            problems.append(pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=self))
-        elif re.search(
-            r":(postgresql://|sqlserver://|oracle:thin:@)(localhost|127\.0+\.0+\.1)[:;/]",
-            jdbc_url,
-        ):
-            lic = self.license_type()
-            if lic == "PRODUCTION":
-                rule = rules.get_rule(rules.RuleId.SETTING_DB_ON_SAME_HOST)
-                problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(jdbc_url), concerned_object=self))
-        return problems
-
-    def __audit_dce_settings(self):
-        util.logger.info("Auditing DCE settings")
-        problems = []
-        if self.version() < (9, 7, 0):
-            stats = self.json.get(_STATS)
-        else:
-            stats = self.json
-        if stats is None:
-            util.logger.error("Can't verify edition in System Info File, was it corrupted or redacted ?")
-            return problems
-        sq_edition = stats.get("edition", None)
-        if sq_edition is None:
-            util.logger.error("Can't verify edition in System Info File, was it corrupted or redacted ?")
-            return problems
-        if sq_edition != "datacenter":
-            util.logger.info("Not a Data Center Edition, skipping DCE checks")
-            return problems
-        if _APP_NODES in self.json:
-            problems += appnodes.audit(self.json[_APP_NODES], self)
-        else:
-            util.logger.info("Sys Info too old (pre-8.9), can't check plugins")
-
-        if _ES_NODES in self.json:
-            problems += searchnodes.audit(self.json[_ES_NODES], self)
-        else:
-            util.logger.info("Sys Info too old (pre-8.9), can't check plugins")
-        return problems
-
-    def __audit_log_level(self):
-        util.logger.debug("Auditing log levels")
-        log_level = self.__get_field("Web Logging")
-        if log_level is None:
-            return []
-        log_level = log_level["Logs Level"]
-        if log_level not in ("DEBUG", "TRACE"):
-            return []
-        if log_level == "TRACE":
-            return [
-                pb.Problem(
-                    types.Type.PERFORMANCE,
-                    severities.Severity.CRITICAL,
-                    "Log level set to TRACE, this does very negatively affect platform performance, reverting to INFO is required",
-                    concerned_object=self,
-                )
-            ]
-        if log_level == "DEBUG":
-            return [
-                pb.Problem(
-                    types.Type.PERFORMANCE,
-                    severities.Severity.HIGH,
-                    "Log level is set to DEBUG, this may affect platform performance, reverting to INFO is recommended",
-                    concerned_object=self,
-                )
-            ]
-        return []
-
-    def __audit_version(self):
-        st_time = self.start_time()
-        if st_time is None:
-            util.logger.warning("SIF date is not available, skipping audit on SonarQube version (aligned with LTS)...")
-            return []
-        sq_version = self.version()
-        if (
-            (st_time > _RELEASE_DATE_6_7 and sq_version < (6, 7, 0))
-            or (st_time > _RELEASE_DATE_7_9 and sq_version < (7, 9, 0))
-            or (st_time > _RELEASE_DATE_8_9 and sq_version < (8, 9, 0))
-        ):
-            rule = rules.get_rule(rules.RuleId.BELOW_LTS)
-            return [pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=self)]
-        return []
-
-    def __audit_web_settings(self, audit_settings):
-        util.logger.debug("Auditing Web settings")
-        problems = []
-        jvm_cmdline = self.web_jvm_cmdline()
-        if jvm_cmdline is None:
-            util.logger.warning("Can't retrieve web JVM command line, skipping heap and log4shell audits...")
-            return []
-        web_ram = util.jvm_heap(jvm_cmdline)
-        min_heap = audit_settings.get("audit.web.heapMin", 1024)
-        max_heap = audit_settings.get("audit.web.heapMax", 2048)
-        if web_ram is None:
-            rule = rules.get_rule(rules.RuleId.SETTING_WEB_NO_HEAP)
-            problems.append(pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=self))
-        elif web_ram < min_heap or web_ram > max_heap:
-            rule = rules.get_rule(rules.RuleId.SETTING_WEB_HEAP)
-            problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(web_ram, 1024, 2048), concerned_object=self))
-        else:
-            util.logger.debug(
-                "sonar.web.javaOpts -Xmx memory setting value is %d MB, within the recommended range [1024-2048]",
-                web_ram,
-            )
-
-        problems += self.__audit_log4shell(jvm_cmdline, rules.RuleId.LOG4SHELL_WEB)
-        return problems
-
-    def __audit_ce_settings(self):
-        util.logger.info("Auditing CE settings")
-        problems = []
-        jvm_cmdline = self.ce_jvm_cmdline()
-        if jvm_cmdline is None:
-            util.logger.warning("Can't retrieve CE JVM command line, heap and logshell checks skipped")
-            return []
-        ce_ram = util.jvm_heap(jvm_cmdline)
-        ce_tasks = self.__get_field("Compute Engine Tasks")
-        if ce_tasks is None:
-            return []
-        ce_workers = ce_tasks["Worker Count"]
-        MAX_WORKERS = 4
-        if ce_workers > MAX_WORKERS:
-            rule = rules.get_rule(rules.RuleId.SETTING_CE_TOO_MANY_WORKERS)
-            problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(ce_workers, MAX_WORKERS), concerned_object=self))
-        else:
-            util.logger.debug(
-                "%d CE workers configured, correct compared to the max %d recommended",
-                ce_workers,
-                MAX_WORKERS,
-            )
-
-        if ce_ram is None:
-            rule = rules.get_rule(rules.RuleId.SETTING_CE_NO_HEAP)
-            problems.append(pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=self))
-        elif ce_ram < 512 * ce_workers or ce_ram > 2048 * ce_workers:
-            rule = rules.get_rule(rules.RuleId.SETTING_CE_HEAP)
-            problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(ce_ram, 512, 2048, ce_workers), concerned_object=self))
-        else:
-            util.logger.debug(
-                "sonar.ce.javaOpts -Xmx memory setting value is %d MB, within recommended range ([512-2048] x %d workers)",
-                ce_ram,
-                ce_workers,
-            )
-
-        problems += self.__audit_log4shell(jvm_cmdline, rules.RuleId.LOG4SHELL_CE)
-        return problems
-
-    def __audit_background_tasks(self):
-        util.logger.debug("Auditing CE background tasks")
-        problems = []
-        ce_tasks = self.__get_field("Compute Engine Tasks")
-        if ce_tasks is None:
-            return []
-        ce_success = ce_tasks["Processed With Success"]
-        ce_error = ce_tasks["Processed With Error"]
-        if ce_success == 0 and ce_error == 0:
-            failure_rate = 0
-        else:
-            failure_rate = ce_error / (ce_success + ce_error)
-        if ce_error > 10 and failure_rate > 0.01:
-            rule = rules.get_rule(rules.RuleId.BACKGROUND_TASKS_FAILURE_RATE_HIGH)
-            problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(int(failure_rate * 100)), concerned_object=self))
-        else:
-            util.logger.debug(
-                "Number of failed background tasks (%d), and failure rate %d%% is OK",
-                ce_error,
-                int(failure_rate * 100),
-            )
-
-        ce_pending = ce_tasks["Pending"]
-        if ce_pending > 100:
-            rule = rules.get_rule(rules.RuleId.BACKGROUND_TASKS_PENDING_QUEUE_VERY_LONG)
-            problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(ce_pending), concerned_object=self))
-        elif ce_pending > 20 and ce_pending > (10 * ce_tasks["Worker Count"]):
-            rule = rules.get_rule(rules.RuleId.BACKGROUND_TASKS_PENDING_QUEUE_LONG)
-            problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(ce_pending), concerned_object=self))
-        else:
-            util.logger.debug("Number of pending background tasks (%d) is OK", ce_pending)
-        return problems
-
-    def __audit_es_settings(self):
-        util.logger.info("Auditing Search Server settings")
-        problems = []
-        jvm_cmdline = self.search_jvm_cmdline()
-        if jvm_cmdline is None:
-            util.logger.warning("Can't retrieve search JVM command line, heap and logshell checks skipped")
-            return []
-        es_ram = util.jvm_heap(jvm_cmdline)
-        index_size = self.store_size()
-
-        if index_size is None:
-            util.logger.warning("Search server index size is missing. Audit of ES heap vs index size is skipped...")
-        elif es_ram is None:
-            rule = rules.get_rule(rules.RuleId.SETTING_ES_NO_HEAP)
-            problems.append(pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=self))
-        elif es_ram < 2 * index_size and es_ram < index_size + 1000:
-            rule = rules.get_rule(rules.RuleId.SETTING_ES_HEAP)
-            problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(es_ram, index_size), concerned_object=self))
-        else:
-            util.logger.debug(
-                "Search server memory %d MB is correct wrt to index size of %d MB",
-                es_ram,
-                index_size,
-            )
-        problems += self.__audit_log4shell(jvm_cmdline, rules.RuleId.LOG4SHELL_ES)
-        return problems
-
-
-def is_sysinfo(sysinfo):
-    counter = 0
-    for key in (_SETTINGS, _SYSTEM, "Database", "License"):
-        if key in sysinfo:
-            counter += 1
-    return counter >= 2
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+"""
+
+    Abstraction of the SonarQube System Info File (or Support Info File) concept
+
+"""
+
+import datetime
+import re
+from dateutil.relativedelta import relativedelta
+import sonar.utilities as util
+
+from sonar.audit import rules, types, severities
+import sonar.audit.problem as pb
+
+import sonar.dce.app_nodes as appnodes
+import sonar.dce.search_nodes as searchnodes
+
+_RELEASE_DATE_6_7 = datetime.datetime(2017, 11, 8) + relativedelta(months=+6)
+_RELEASE_DATE_7_9 = datetime.datetime(2019, 7, 1) + relativedelta(months=+6)
+_RELEASE_DATE_8_9 = datetime.datetime(2021, 5, 4) + relativedelta(months=+6)
+
+_APP_NODES = "Application Nodes"
+_ES_NODES = "Search Nodes"
+_SYSTEM = "System"
+_SETTINGS = "Settings"
+_STATS = "Statistics"
+_STORE_SIZE = "Store Size"
+_SEARCH_STATE = "Search State"
+
+_JVM_OPTS = ("sonar.{}.javaOpts", "sonar.{}.javaAdditionalOpts")
+
+_MIN_DATE_LOG4SHELL = datetime.datetime(2021, 12, 1)
+
+
+class NotSystemInfo(Exception):
+    def __init__(self, message):
+        super().__init__()
+        self.message = message
+
+
+class Sif:
+    def __init__(self, json_sif, concerned_object=None):
+        if not is_sysinfo(json_sif):
+            util.logger.critical("Provided JSON does not seem to be a system info")
+            raise NotSystemInfo("JSON is not a system info nor a support info")
+        self.json = json_sif
+        self.concerned_object = concerned_object
+        self._url = None
+
+    def url(self):
+        if not self._url:
+            if self.concerned_object:
+                self._url = self.concerned_object.url
+            else:
+                self._url = self.json.get("Settings", {}).get("sonar.core.serverBaseURL", "")
+        return self._url
+
+    def edition(self):
+        try:
+            ed = self.json[_STATS]["edition"]
+        except KeyError:
+            try:
+                ed = self.json["License"]["edition"]
+            except KeyError:
+                try:
+                    # FIXME: Can't get edition in SIF of SonarQube 9.7+, this is an unsolvable problem
+                    ed = self.json["edition"]
+                except KeyError:
+                    return None
+        # Old SIFs could return "Enterprise Edition"
+        return util.edition_normalize(ed)
+
+    def database(self):
+        if self.version() < (9, 7, 0):
+            return self.json[_STATS]["database"]["name"]
+        else:
+            return self.json["Database"]["Database"]
+
+    def plugins(self):
+        if self.version() < (9, 7, 0):
+            return self.json[_STATS]["plugins"]
+        else:
+            return self.json["Plugins"]
+
+    def license_type(self):
+        if "License" not in self.json:
+            return None
+        elif "type" in self.json["License"]:
+            return self.json["License"]["type"]
+        return None
+
+    def version(self, digits=3, as_string=False):
+        return util.string_to_version(self.__get_field("Version"), digits=digits, as_string=as_string)
+
+    def server_id(self):
+        return self.__get_field("Server ID")
+
+    def start_time(self):
+        try:
+            return util.string_to_date(self.json[_SETTINGS]["sonar.core.startTime"]).replace(tzinfo=None)
+        except KeyError:
+            pass
+        try:
+            return util.string_to_date(self.json[_SYSTEM]["Start Time"]).replace(tzinfo=None)
+        except KeyError:
+            return None
+
+    def store_size(self):
+        setting = None
+        try:
+            setting = self.json[_SEARCH_STATE][_STORE_SIZE]
+        except KeyError:
+            try:
+                for v in self.json["Elasticsearch"]["Nodes"].values():
+                    if _STORE_SIZE in v:
+                        setting = v[_STORE_SIZE]
+                        break
+            except KeyError:
+                pass
+        if setting is None:
+            return None
+        return util.int_memory(setting)
+
+    def audit(self, audit_settings):
+        util.logger.info("Auditing System Info")
+        problems = self.__audit_jdbc_url()
+        util.logger.debug("Edition = %s", self.edition())
+        if self.edition() == "datacenter":
+            util.logger.info("DCE SIF audit")
+            problems += self.__audit_dce_settings()
+        else:
+            problems += (
+                self.__audit_ce_settings()
+                + self.__audit_background_tasks()
+                + self.__audit_es_settings()
+                + self.__audit_log_level()
+                + self.__audit_version()
+                + self.__audit_branch_use()
+                + self.__audit_undetected_scm()
+                + self.__audit_web_settings(audit_settings)
+            )
+        return problems
+
+    def __audit_branch_use(self):
+        if self.edition() == "community":
+            return []
+        util.logger.info("Auditing usage of branch analysis")
+        try:
+            use_br = self.json[_STATS]["usingBranches"]
+            if use_br:
+                return []
+            rule = rules.get_rule(rules.RuleId.NOT_USING_BRANCH_ANALYSIS)
+            return [pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=self)]
+        except KeyError:
+            util.logger.info("Branch usage information not in SIF, ignoring audit...")
+            return []
+
+    def __audit_undetected_scm(self):
+        util.logger.info("Auditing SCM integration")
+        try:
+            scm_count, undetected_scm_count = 0, 0
+            for scm in self.json[_STATS]["projectCountByScm"]:
+                scm_count += scm["count"]
+                if scm["scm"] == "undetected":
+                    undetected_scm_count = scm["count"]
+            if undetected_scm_count == 0:
+                return []
+            rule = rules.get_rule(rules.RuleId.SIF_UNDETECTED_SCM)
+            return [pb.Problem(rule.type, rule.severity, rule.msg.format(undetected_scm_count), concerned_object=self)]
+        except KeyError:
+            util.logger.info("SCM information not in SIF, ignoring audit...")
+            return []
+
+    def __get_field(self, name, node_type=_APP_NODES):
+        if _SYSTEM in self.json and name in self.json[_SYSTEM]:
+            return self.json[_SYSTEM][name]
+        elif "SonarQube" in self.json and name in self.json["SonarQube"]:
+            return self.json["SonarQube"][name]
+        elif node_type in self.json:
+            for node in self.json[node_type]:
+                try:
+                    return node[_SYSTEM][name]
+                except KeyError:
+                    pass
+        return None
+
+    def __process_cmdline(self, process):
+        opts = [x.format(process) for x in _JVM_OPTS]
+        if _SETTINGS in self.json:
+            return self.json[_SETTINGS][opts[1]] + " " + self.json[_SETTINGS][opts[0]]
+        else:
+            return None
+
+    def web_jvm_cmdline(self):
+        return self.__process_cmdline("web")
+
+    def ce_jvm_cmdline(self):
+        return self.__process_cmdline("ce")
+
+    def search_jvm_cmdline(self):
+        return self.__process_cmdline("search")
+
+    def __eligible_to_log4shell_check(self):
+        st_time = self.start_time()
+        if st_time is None:
+            return False
+        return st_time > _MIN_DATE_LOG4SHELL
+
+    def __audit_log4shell(self, jvm_settings, broken_rule):
+        # If SIF is older than 2022 don't audit for log4shell to avoid noise
+        if not self.__eligible_to_log4shell_check():
+            return []
+
+        util.logger.debug("Auditing log4shell vulnerability fix")
+        sq_version = self.version()
+        if sq_version < (8, 9, 6) or ((9, 0, 0) <= sq_version < (9, 2, 4)):
+            for s in jvm_settings.split(" "):
+                if s == "-Dlog4j2.formatMsgNoLookups=true":
+                    return []
+            rule = rules.get_rule(broken_rule)
+            return [pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=self)]
+        return []
+
+    def __audit_jdbc_url(self):
+        util.logger.info("Auditing JDBC settings")
+        problems = []
+        stats = self.json.get(_SETTINGS)
+        if stats is None:
+            util.logger.error("Can't verify Database settings in System Info File, was it corrupted or redacted ?")
+            return problems
+        jdbc_url = stats.get("sonar.jdbc.url", None)
+        util.logger.debug("JDBC URL = %s", str(jdbc_url))
+        if jdbc_url is None:
+            rule = rules.get_rule(rules.RuleId.SETTING_JDBC_URL_NOT_SET)
+            problems.append(pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=self))
+        elif re.search(
+            r":(postgresql://|sqlserver://|oracle:thin:@)(localhost|127\.0+\.0+\.1)[:;/]",
+            jdbc_url,
+        ):
+            lic = self.license_type()
+            if lic == "PRODUCTION":
+                rule = rules.get_rule(rules.RuleId.SETTING_DB_ON_SAME_HOST)
+                problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(jdbc_url), concerned_object=self))
+        return problems
+
+    def __audit_dce_settings(self):
+        util.logger.info("Auditing DCE settings for version %s", str(self.version()))
+        problems = []
+        sq_edition = self.edition()
+        if sq_edition is None:
+            util.logger.error("Can't verify edition in System Info File (2_), was it corrupted or redacted ?")
+            return problems
+        if sq_edition != "datacenter":
+            util.logger.info("Not a Data Center Edition, skipping DCE checks")
+            return problems
+        if _APP_NODES in self.json:
+            problems += appnodes.audit(self.json[_APP_NODES], self)
+        else:
+            util.logger.info("Sys Info too old (pre-8.9), can't check plugins")
+
+        if _ES_NODES in self.json:
+            problems += searchnodes.audit(self.json[_ES_NODES], self)
+        else:
+            util.logger.info("Sys Info too old (pre-8.9), can't check plugins")
+        return problems
+
+    def __audit_log_level(self):
+        util.logger.debug("Auditing log levels")
+        log_level = self.__get_field("Web Logging")
+        if log_level is None:
+            return []
+        log_level = log_level["Logs Level"]
+        if log_level not in ("DEBUG", "TRACE"):
+            return []
+        if log_level == "TRACE":
+            return [
+                pb.Problem(
+                    types.Type.PERFORMANCE,
+                    severities.Severity.CRITICAL,
+                    "Log level set to TRACE, this does very negatively affect platform performance, reverting to INFO is required",
+                    concerned_object=self,
+                )
+            ]
+        if log_level == "DEBUG":
+            return [
+                pb.Problem(
+                    types.Type.PERFORMANCE,
+                    severities.Severity.HIGH,
+                    "Log level is set to DEBUG, this may affect platform performance, reverting to INFO is recommended",
+                    concerned_object=self,
+                )
+            ]
+        return []
+
+    def __audit_version(self):
+        st_time = self.start_time()
+        if st_time is None:
+            util.logger.warning("SIF date is not available, skipping audit on SonarQube version (aligned with LTS)...")
+            return []
+        sq_version = self.version()
+        if (
+            (st_time > _RELEASE_DATE_6_7 and sq_version < (6, 7, 0))
+            or (st_time > _RELEASE_DATE_7_9 and sq_version < (7, 9, 0))
+            or (st_time > _RELEASE_DATE_8_9 and sq_version < (8, 9, 0))
+        ):
+            rule = rules.get_rule(rules.RuleId.BELOW_LTS)
+            return [pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=self)]
+        return []
+
+    def __audit_web_settings(self, audit_settings):
+        util.logger.debug("Auditing Web settings")
+        problems = []
+        if self.version() < (9, 0, 0):
+            jvm_cmdline = self.web_jvm_cmdline()
+            if jvm_cmdline is None:
+                util.logger.warning("Can't retrieve web JVM command line, skipping heap and log4shell audits...")
+                return []
+            problems += self.__audit_log4shell(jvm_cmdline, rules.RuleId.LOG4SHELL_WEB)
+            heap_size = util.jvm_heap(jvm_cmdline)
+        else:
+            try:
+                heap_size = self.json["Web JVM State"]["Heap Max (MB)"]
+            except KeyError:
+                util.logger.warning("Can't retrieve web JVM heap")
+                return []
+        min_heap = audit_settings.get("audit.web.heapMin", 1024)
+        max_heap = audit_settings.get("audit.web.heapMax", 2048)
+        if heap_size is None:
+            rule = rules.get_rule(rules.RuleId.SETTING_WEB_NO_HEAP)
+            problems.append(pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=self))
+        elif heap_size < min_heap or heap_size > max_heap:
+            rule = rules.get_rule(rules.RuleId.SETTING_WEB_HEAP)
+            problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(heap_size, min_heap, max_heap), concerned_object=self))
+        else:
+            util.logger.debug("Sonar Web process heap value is %d MB, within the recommended range [%d-%d]", heap_size, min_heap, max_heap)
+
+        return problems
+
+    def __audit_ce_settings(self):
+        util.logger.info("Auditing CE settings")
+        problems = []
+        if self.version() < (9, 0, 0):
+            jvm_cmdline = self.ce_jvm_cmdline()
+            if jvm_cmdline is None:
+                util.logger.warning("Can't retrieve CE JVM command line, heap and logshell checks skipped")
+                return []
+            problems += self.__audit_log4shell(jvm_cmdline, rules.RuleId.LOG4SHELL_CE)
+            ce_ram = util.jvm_heap(jvm_cmdline)
+        else:
+            try:
+                ce_ram = self.json["Compute Engine JVM State"]["Heap Max (MB)"]
+            except KeyError:
+                util.logger.warning("Can't retrieve CE JVM heap")
+                return []
+        ce_tasks = self.__get_field("Compute Engine Tasks")
+        if ce_tasks is None:
+            return []
+        ce_workers = ce_tasks["Worker Count"]
+        MAX_WORKERS = 4
+        if ce_workers > MAX_WORKERS:
+            rule = rules.get_rule(rules.RuleId.SETTING_CE_TOO_MANY_WORKERS)
+            problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(ce_workers, MAX_WORKERS), concerned_object=self))
+        else:
+            util.logger.debug(
+                "%d CE workers configured, correct compared to the max %d recommended",
+                ce_workers,
+                MAX_WORKERS,
+            )
+
+        if ce_ram is None:
+            rule = rules.get_rule(rules.RuleId.SETTING_CE_NO_HEAP)
+            problems.append(pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=self))
+        elif ce_ram < 512 * ce_workers or ce_ram > 2048 * ce_workers:
+            rule = rules.get_rule(rules.RuleId.SETTING_CE_HEAP)
+            problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(ce_ram, 512, 2048, ce_workers), concerned_object=self))
+        else:
+            util.logger.debug(
+                "SonarQube CE memory setting value is %d MB, within recommended range ([512-2048] x %d workers)",
+                ce_ram,
+                ce_workers,
+            )
+
+        return problems
+
+    def __audit_background_tasks(self):
+        util.logger.debug("Auditing CE background tasks")
+        problems = []
+        ce_tasks = self.__get_field("Compute Engine Tasks")
+        if ce_tasks is None:
+            return []
+        ce_success = ce_tasks["Processed With Success"]
+        ce_error = ce_tasks["Processed With Error"]
+        if ce_success == 0 and ce_error == 0:
+            failure_rate = 0
+        else:
+            failure_rate = ce_error / (ce_success + ce_error)
+        if ce_error > 10 and failure_rate > 0.01:
+            rule = rules.get_rule(rules.RuleId.BACKGROUND_TASKS_FAILURE_RATE_HIGH)
+            problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(int(failure_rate * 100)), concerned_object=self))
+        else:
+            util.logger.debug(
+                "Number of failed background tasks (%d), and failure rate %d%% is OK",
+                ce_error,
+                int(failure_rate * 100),
+            )
+
+        ce_pending = ce_tasks["Pending"]
+        if ce_pending > 100:
+            rule = rules.get_rule(rules.RuleId.BACKGROUND_TASKS_PENDING_QUEUE_VERY_LONG)
+            problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(ce_pending), concerned_object=self))
+        elif ce_pending > 20 and ce_pending > (10 * ce_tasks["Worker Count"]):
+            rule = rules.get_rule(rules.RuleId.BACKGROUND_TASKS_PENDING_QUEUE_LONG)
+            problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(ce_pending), concerned_object=self))
+        else:
+            util.logger.debug("Number of pending background tasks (%d) is OK", ce_pending)
+        return problems
+
+    def __audit_es_settings(self):
+        util.logger.info("Auditing Search Server settings")
+        problems = []
+        jvm_cmdline = self.search_jvm_cmdline()
+        if jvm_cmdline is None:
+            util.logger.warning("Can't retrieve search JVM command line, heap and logshell checks skipped")
+            return []
+        es_ram = util.jvm_heap(jvm_cmdline)
+        index_size = self.store_size()
+
+        if index_size is None:
+            util.logger.warning("Search server index size is missing. Audit of ES heap vs index size is skipped...")
+        elif es_ram is None:
+            rule = rules.get_rule(rules.RuleId.SETTING_ES_NO_HEAP)
+            problems.append(pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=self))
+        elif es_ram < 2 * index_size and es_ram < index_size + 1000:
+            rule = rules.get_rule(rules.RuleId.SETTING_ES_HEAP)
+            problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(es_ram, index_size), concerned_object=self))
+        else:
+            util.logger.debug(
+                "Search server memory %d MB is correct wrt to index size of %d MB",
+                es_ram,
+                index_size,
+            )
+        problems += self.__audit_log4shell(jvm_cmdline, rules.RuleId.LOG4SHELL_ES)
+        return problems
+
+
+def is_sysinfo(sysinfo):
+    counter = 0
+    for key in (_SETTINGS, _SYSTEM, "Database", "License"):
+        if key in sysinfo:
+            counter += 1
+    return counter >= 2
```

## sonar/sqobject.py

```diff
@@ -1,139 +1,139 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-"""
-
-    Abstraction of the SonarQube general object concept
-
-"""
-
-import json
-from http import HTTPStatus
-from queue import Queue
-from threading import Thread
-from requests.exceptions import HTTPError
-from sonar import utilities, exceptions
-
-
-class SqObject:
-    def __init__(self, key, endpoint):
-        self.key = key  #: Object unique key (unique in its class)
-        self.endpoint = endpoint  #: Reference to the SonarQube platform
-        self._json = None
-
-    def uuid(self):
-        return self.key
-
-    def reload(self, data):
-        if self._json is None:
-            self._json = data
-        else:
-            self._json.update(data)
-
-    def get(self, api, params=None, exit_on_error=False, mute=()):
-        """Executes and HTTP GET against the SonarQube platform
-
-        :param str api: API to invoke (eg api/issues/search)
-        :param dict params: List of parameters to pass to the API
-        :param exit_on_error: When to fail fast and exit if the HTTP status code is not 2XX, defaults to True
-        :type exit_on_error: bool, optional
-        :param mute: Tuple of HTTP Error codes to mute (ie not write an error log for), defaults to None.
-        Typically, Error 404 Not found may be expected sometimes so this can avoid logging an error for 404
-        :type mute: tuple, optional
-        :return: The request response
-        :rtype: requests.Response
-        """
-        return self.endpoint.get(api=api, params=params, exit_on_error=exit_on_error, mute=mute)
-
-    def post(self, api, params=None, exit_on_error=False, mute=()):
-        """Executes and HTTP POST against the SonarQube platform
-
-        :param str api: API to invoke (eg api/issues/search)
-        :param dict params: List of parameters to pass to the API
-        :param exit_on_error: When to fail fast and exit if the HTTP status code is not 2XX, defaults to True
-        :type exit_on_error: bool, optional
-        :param mute: Tuple of HTTP Error codes to mute (ie not write an error log for), defaults to None.
-        Typically, Error 404 Not found may be expected sometimes so this can avoid logging an error for 404
-        :type mute: tuple, optional
-        :return: The request response
-        :rtype: requests.Response
-        """
-        return self.endpoint.post(api=api, params=params, exit_on_error=exit_on_error, mute=mute)
-
-
-def __search_thread(queue):
-    while not queue.empty():
-        (endpoint, api, objects, key_field, returned_field, object_class, params, page) = queue.get()
-        page_params = params.copy()
-        page_params["p"] = page
-        utilities.logger.debug("Threaded search: API = %s params = %s", api, str(params))
-        data = json.loads(endpoint.get(api, params=page_params).text)
-        for obj in data[returned_field]:
-            if object_class.__name__ in ("QualityProfile", "QualityGate", "Groups", "Portfolio", "Project"):
-                objects[obj[key_field]] = object_class.load(endpoint=endpoint, data=obj)
-            else:
-                objects[obj[key_field]] = object_class(obj[key_field], endpoint, data=obj)
-        queue.task_done()
-
-
-def search_objects(api, endpoint, key_field, returned_field, object_class, params, threads=8):
-    """Runs a multi-threaded SonarQube search on any type of object (api, returned field etc... being passed in the request)
-    :meta private:
-    """
-    __MAX_SEARCH = 500
-    new_params = {} if params is None else params.copy()
-    if "ps" not in new_params:
-        new_params["ps"] = __MAX_SEARCH
-    new_params["p"] = 1
-    objects_list = {}
-    data = json.loads(endpoint.get(api, params=new_params).text)
-    for obj in data[returned_field]:
-        if object_class.__name__ in ("Portfolio", "Group", "QualityProfile", "User", "Application", "Project"):
-            objects_list[obj[key_field]] = object_class.load(endpoint=endpoint, data=obj)
-        else:
-            objects_list[obj[key_field]] = object_class(obj[key_field], endpoint, data=obj)
-    nb_pages = utilities.nbr_pages(data)
-    if nb_pages == 1:
-        # If everything is returned on the 1st page, no multi-threading needed
-        return objects_list
-    q = Queue(maxsize=0)
-    for page in range(2, nb_pages + 1):
-        q.put((endpoint, api, objects_list, key_field, returned_field, object_class, new_params, page))
-    for i in range(threads):
-        utilities.logger.debug("Starting %s search thread %d", object_class.__name__, i)
-        worker = Thread(target=__search_thread, args=[q])
-        worker.setDaemon(True)
-        worker.setName(f"Search{i}")
-        worker.start()
-    q.join()
-    return objects_list
-
-
-def delete_object(object, api, params, map):
-    try:
-        utilities.logger.info("Deleting %s", str(object))
-        r = object.post(api, params=params, mute=(HTTPStatus.NOT_FOUND,))
-        map.pop(object.uuid(), None)
-        utilities.logger.info("Successfully deleted %s", str(object))
-        return r.ok
-    except HTTPError as e:
-        if e.response.status_code == HTTPStatus.NOT_FOUND:
-            map.pop(object.uuid(), None)
-            raise exceptions.ObjectNotFound(object.key, f"{str(object)} not found for delete")
-        raise
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+"""
+
+    Abstraction of the SonarQube general object concept
+
+"""
+
+import json
+from http import HTTPStatus
+from queue import Queue
+from threading import Thread
+from requests.exceptions import HTTPError
+from sonar import utilities, exceptions
+
+
+class SqObject:
+    def __init__(self, key, endpoint):
+        self.key = key  #: Object unique key (unique in its class)
+        self.endpoint = endpoint  #: Reference to the SonarQube platform
+        self._json = None
+
+    def uuid(self):
+        return self.key
+
+    def reload(self, data):
+        if self._json is None:
+            self._json = data
+        else:
+            self._json.update(data)
+
+    def get(self, api, params=None, exit_on_error=False, mute=()):
+        """Executes and HTTP GET against the SonarQube platform
+
+        :param str api: API to invoke (eg api/issues/search)
+        :param dict params: List of parameters to pass to the API
+        :param exit_on_error: When to fail fast and exit if the HTTP status code is not 2XX, defaults to True
+        :type exit_on_error: bool, optional
+        :param mute: Tuple of HTTP Error codes to mute (ie not write an error log for), defaults to None.
+                     Typically, Error 404 Not found may be expected sometimes so this can avoid logging an error for 404
+        :type mute: tuple, optional
+        :return: The request response
+        :rtype: requests.Response
+        """
+        return self.endpoint.get(api=api, params=params, exit_on_error=exit_on_error, mute=mute)
+
+    def post(self, api, params=None, exit_on_error=False, mute=()):
+        """Executes and HTTP POST against the SonarQube platform
+
+        :param str api: API to invoke (eg api/issues/search)
+        :param dict params: List of parameters to pass to the API
+        :param exit_on_error: When to fail fast and exit if the HTTP status code is not 2XX, defaults to True
+        :type exit_on_error: bool, optional
+        :param mute: Tuple of HTTP Error codes to mute (ie not write an error log for), defaults to None.
+                     Typically, Error 404 Not found may be expected sometimes so this can avoid logging an error for 404
+        :type mute: tuple, optional
+        :return: The request response
+        :rtype: requests.Response
+        """
+        return self.endpoint.post(api=api, params=params, exit_on_error=exit_on_error, mute=mute)
+
+
+def __search_thread(queue):
+    while not queue.empty():
+        (endpoint, api, objects, key_field, returned_field, object_class, params, page) = queue.get()
+        page_params = params.copy()
+        page_params["p"] = page
+        utilities.logger.debug("Threaded search: API = %s params = %s", api, str(params))
+        data = json.loads(endpoint.get(api, params=page_params).text)
+        for obj in data[returned_field]:
+            if object_class.__name__ in ("QualityProfile", "QualityGate", "Groups", "Portfolio", "Project"):
+                objects[obj[key_field]] = object_class.load(endpoint=endpoint, data=obj)
+            else:
+                objects[obj[key_field]] = object_class(obj[key_field], endpoint, data=obj)
+        queue.task_done()
+
+
+def search_objects(api, endpoint, key_field, returned_field, object_class, params, threads=8):
+    """Runs a multi-threaded SonarQube search on any type of object (api, returned field etc... being passed in the request)
+    :meta private:
+    """
+    __MAX_SEARCH = 500
+    new_params = {} if params is None else params.copy()
+    if "ps" not in new_params:
+        new_params["ps"] = __MAX_SEARCH
+    new_params["p"] = 1
+    objects_list = {}
+    data = json.loads(endpoint.get(api, params=new_params).text)
+    for obj in data[returned_field]:
+        if object_class.__name__ in ("Portfolio", "Group", "QualityProfile", "User", "Application", "Project"):
+            objects_list[obj[key_field]] = object_class.load(endpoint=endpoint, data=obj)
+        else:
+            objects_list[obj[key_field]] = object_class(obj[key_field], endpoint, data=obj)
+    nb_pages = utilities.nbr_pages(data)
+    if nb_pages == 1:
+        # If everything is returned on the 1st page, no multi-threading needed
+        return objects_list
+    q = Queue(maxsize=0)
+    for page in range(2, nb_pages + 1):
+        q.put((endpoint, api, objects_list, key_field, returned_field, object_class, new_params, page))
+    for i in range(threads):
+        utilities.logger.debug("Starting %s search thread %d", object_class.__name__, i)
+        worker = Thread(target=__search_thread, args=[q])
+        worker.setDaemon(True)
+        worker.setName(f"Search{i}")
+        worker.start()
+    q.join()
+    return objects_list
+
+
+def delete_object(object, api, params, map):
+    try:
+        utilities.logger.info("Deleting %s", str(object))
+        r = object.post(api, params=params, mute=(HTTPStatus.NOT_FOUND,))
+        map.pop(object.uuid(), None)
+        utilities.logger.info("Successfully deleted %s", str(object))
+        return r.ok
+    except HTTPError as e:
+        if e.response.status_code == HTTPStatus.NOT_FOUND:
+            map.pop(object.uuid(), None)
+            raise exceptions.ObjectNotFound(object.key, f"{str(object)} not found for delete")
+        raise
```

## sonar/syncer.py

 * *Ordering differences only*

```diff
@@ -1,242 +1,242 @@
-#
-# sonar-tools
-# Copyright (C) 2022-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-
-"""Findings syncer"""
-
-import sonar.utilities as util
-
-
-SYNC_IGNORE_COMPONENTS = "ignore_components"
-SYNC_ADD_LINK = "add_link"
-SYNC_ADD_COMMENTS = "add_comments"
-SYNC_COMMENTS = "sync_comments"
-SYNC_ASSIGN = "sync_assignments"
-SYNC_SERVICE_ACCOUNTS = "sync_service_accounts"
-
-SRC_KEY = "sourceFindingKey"
-SRC_URL = "sourceFindingUrl"
-SYNC_MSG = "syncMessage"
-SYNC_MATCHES = "matches"
-TGT_KEY = "targetFindingKey"
-TGT_URL = "targetFindingUrl"
-SYNC_STATUS = "syncStatus"
-
-
-def __name(obj):
-    return type(obj).__name__.lower()
-
-
-def __get_findings(findings_list):
-    find_list = []
-    for finding in findings_list:
-        find_list.append({SRC_KEY: finding.key, SRC_URL: finding.url()})
-    return find_list
-
-
-def __process_exact_sibling(finding, sibling, settings):
-    if finding.has_changelog() or finding.has_comments():
-        sibling.apply_changelog(finding, settings)
-        msg = f"Source {__name(finding)} changelog applied successfully"
-    else:
-        msg = f"Source {__name(finding)} has no changelog"
-    return {
-        SRC_KEY: finding.key,
-        SRC_URL: finding.url(),
-        SYNC_STATUS: "synchronized",
-        SYNC_MSG: msg,
-        TGT_KEY: sibling.key,
-        TGT_URL: sibling.url(),
-    }
-
-
-def __process_no_match(finding):
-    return {
-        SRC_KEY: finding.key,
-        SRC_URL: finding.url(),
-        SYNC_STATUS: "no match",
-        SYNC_MSG: f"Source {__name(finding)} has no match in target project",
-    }
-
-
-def __process_multiple_exact_siblings(finding, siblings):
-    util.logger.info("Multiple matches for %s, cannot automatically apply changelog", str(finding))
-    name = __name(finding)
-    for sib in siblings:
-        comment = ""
-        i = 0
-        for sib2 in siblings:
-            if sib.key == sib2.key:
-                continue
-            i += 1
-            comment += f"[{name} {i}]({sib2.url()}), "
-        sib.add_comment(
-            f"Sync did not happen due to multiple matches. [This original {name}]({finding.url()}) "
-            f"corresponds to this {name},\nbut also to these other {name}s: {comment[:-2]}"
-        )
-    return {
-        SRC_KEY: finding.key,
-        SRC_URL: finding.url(),
-        SYNC_STATUS: "unsynchronized",
-        SYNC_MSG: "Multiple matches",
-        SYNC_MATCHES: __get_findings(siblings),
-    }
-
-
-def __process_approx_siblings(finding, siblings):
-    util.logger.info(
-        "Found %d approximate siblings for %s, cannot automatically apply changelog",
-        len(siblings),
-        str(finding),
-    )
-    return {
-        SRC_KEY: finding.key,
-        SRC_URL: finding.url(),
-        SYNC_STATUS: "unsynchronized",
-        SYNC_MSG: "Approximate matches only",
-        SYNC_MATCHES: __get_findings(siblings),
-    }
-
-
-def __process_modified_siblings(finding, siblings):
-    util.logger.info(
-        "Found %d siblings for %s, but they already have a changelog, cannot automatically apply changelog",
-        len(siblings),
-        str(finding),
-    )
-    return {
-        SRC_KEY: finding.key,
-        SRC_URL: finding.url(),
-        TGT_KEY: siblings[0].key,
-        TGT_URL: siblings[0].url(),
-        SYNC_STATUS: "unsynchronized",
-        SYNC_MSG: f"Target {__name(finding)} already has a changelog",
-    }
-
-
-def __sync_findings_list(src_findings, tgt_findings, settings):
-    counters = {
-        "nb_to_sync": len(src_findings),
-        "nb_applies": 0,
-        "nb_approx_match": 0,
-        "nb_tgt_has_changelog": 0,
-        "nb_multiple_matches": 0,
-    }
-    report = []
-    name = __name(list(src_findings.values())[0])
-    util.logger.info(
-        "%d %ss to sync, %d %ss in target",
-        len(src_findings),
-        name,
-        len(tgt_findings),
-        name,
-    )
-    for _, finding in src_findings.items():
-        util.logger.debug("Searching sibling for %s", str(finding))
-        (exact_siblings, approx_siblings, modified_siblings) = finding.search_siblings(
-            tgt_findings,
-            allowed_users=settings[SYNC_SERVICE_ACCOUNTS],
-            ignore_component=settings[SYNC_IGNORE_COMPONENTS],
-        )
-        if len(exact_siblings) == 1:
-            report.append(__process_exact_sibling(finding, exact_siblings[0], settings))
-            counters["nb_applies"] += 1
-        elif len(exact_siblings) > 1:
-            report.append(__process_multiple_exact_siblings(finding, exact_siblings))
-            counters["nb_multiple_matches"] += 1
-        elif approx_siblings:
-            report.append(__process_approx_siblings(finding, approx_siblings))
-            counters["nb_approx_match"] += 1
-        elif modified_siblings:
-            counters["nb_tgt_has_changelog"] += 1
-            report.append(__process_modified_siblings(finding, modified_siblings))
-        else:  # No match
-            report.append(__process_no_match(finding))
-    counters["nb_no_match"] = counters["nb_to_sync"] - (
-        counters["nb_applies"] + counters["nb_tgt_has_changelog"] + counters["nb_multiple_matches"] + counters["nb_approx_match"]
-    )
-    util.json_dump_debug(counters, "COUNTERS")
-    return (report, counters)
-
-
-def sync_lists(src_findings, tgt_findings, src_object, tgt_object, sync_settings=None):
-    interesting_src_findings = {}
-    if len(src_findings) == 0 or len(tgt_findings) == 0:
-        util.logger.info("source or target list of findings to sync empty, skipping...")
-        counters = {
-            "nb_to_sync": 0,
-            "nb_applies": 0,
-            "nb_approx_match": 0,
-            "nb_tgt_has_changelog": 0,
-            "nb_multiple_matches": 0,
-        }
-        return ([], counters)
-    name = __name(list(src_findings.values())[0])
-    util.logger.info(
-        "Syncing %d %ss from %s into %d %ss from %s",
-        len(src_findings),
-        name,
-        str(src_object),
-        len(tgt_findings),
-        name,
-        str(tgt_object),
-    )
-    for key1, finding in src_findings.items():
-        if not (finding.has_changelog() or finding.has_comments()):
-            util.logger.info("%s has no changelog or comments, skipped in sync", str(finding))
-            continue
-        if finding.is_closed():
-            util.logger.info(
-                "%s is closed, so it will not be synchronized despite having a changelog",
-                str(finding),
-            )
-            continue
-        modifiers = finding.modifiers().union(finding.commenters())
-        # TODO - Manage more than 1 sync account - diff the 2 lists
-        syncer = sync_settings[SYNC_SERVICE_ACCOUNTS][0]
-        if sync_settings is None:
-            sync_settings = {}
-        if sync_settings.get(SYNC_SERVICE_ACCOUNTS, None) is None:
-            sync_settings[SYNC_SERVICE_ACCOUNTS] = [syncer]
-
-        if len(modifiers) == 1 and list(modifiers)[0] == syncer:
-            util.logger.info(
-                "%s has only been changed by %s, so it will not be synchronized despite having a changelog",
-                str(finding),
-                syncer,
-            )
-            continue
-        interesting_src_findings[key1] = finding
-    util.logger.info(
-        "Found %d %ss with manual changes in %s",
-        len(interesting_src_findings),
-        name,
-        str(src_object),
-    )
-    if len(interesting_src_findings) <= 0:
-        util.logger.info("No %ss with manual changes in %s, skipping...", name, str(src_object))
-        counters = {
-            "nb_to_sync": 0,
-            "nb_applies": 0,
-            "nb_approx_match": 0,
-            "nb_tgt_has_changelog": 0,
-            "nb_multiple_matches": 0,
-        }
-        return ([], counters)
-    return __sync_findings_list(interesting_src_findings, tgt_findings, sync_settings)
+#
+# sonar-tools
+# Copyright (C) 2022-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+
+"""Findings syncer"""
+
+import sonar.utilities as util
+
+
+SYNC_IGNORE_COMPONENTS = "ignore_components"
+SYNC_ADD_LINK = "add_link"
+SYNC_ADD_COMMENTS = "add_comments"
+SYNC_COMMENTS = "sync_comments"
+SYNC_ASSIGN = "sync_assignments"
+SYNC_SERVICE_ACCOUNTS = "sync_service_accounts"
+
+SRC_KEY = "sourceFindingKey"
+SRC_URL = "sourceFindingUrl"
+SYNC_MSG = "syncMessage"
+SYNC_MATCHES = "matches"
+TGT_KEY = "targetFindingKey"
+TGT_URL = "targetFindingUrl"
+SYNC_STATUS = "syncStatus"
+
+
+def __name(obj):
+    return type(obj).__name__.lower()
+
+
+def __get_findings(findings_list):
+    find_list = []
+    for finding in findings_list:
+        find_list.append({SRC_KEY: finding.key, SRC_URL: finding.url()})
+    return find_list
+
+
+def __process_exact_sibling(finding, sibling, settings):
+    if finding.has_changelog() or finding.has_comments():
+        sibling.apply_changelog(finding, settings)
+        msg = f"Source {__name(finding)} changelog applied successfully"
+    else:
+        msg = f"Source {__name(finding)} has no changelog"
+    return {
+        SRC_KEY: finding.key,
+        SRC_URL: finding.url(),
+        SYNC_STATUS: "synchronized",
+        SYNC_MSG: msg,
+        TGT_KEY: sibling.key,
+        TGT_URL: sibling.url(),
+    }
+
+
+def __process_no_match(finding):
+    return {
+        SRC_KEY: finding.key,
+        SRC_URL: finding.url(),
+        SYNC_STATUS: "no match",
+        SYNC_MSG: f"Source {__name(finding)} has no match in target project",
+    }
+
+
+def __process_multiple_exact_siblings(finding, siblings):
+    util.logger.info("Multiple matches for %s, cannot automatically apply changelog", str(finding))
+    name = __name(finding)
+    for sib in siblings:
+        comment = ""
+        i = 0
+        for sib2 in siblings:
+            if sib.key == sib2.key:
+                continue
+            i += 1
+            comment += f"[{name} {i}]({sib2.url()}), "
+        sib.add_comment(
+            f"Sync did not happen due to multiple matches. [This original {name}]({finding.url()}) "
+            f"corresponds to this {name},\nbut also to these other {name}s: {comment[:-2]}"
+        )
+    return {
+        SRC_KEY: finding.key,
+        SRC_URL: finding.url(),
+        SYNC_STATUS: "unsynchronized",
+        SYNC_MSG: "Multiple matches",
+        SYNC_MATCHES: __get_findings(siblings),
+    }
+
+
+def __process_approx_siblings(finding, siblings):
+    util.logger.info(
+        "Found %d approximate siblings for %s, cannot automatically apply changelog",
+        len(siblings),
+        str(finding),
+    )
+    return {
+        SRC_KEY: finding.key,
+        SRC_URL: finding.url(),
+        SYNC_STATUS: "unsynchronized",
+        SYNC_MSG: "Approximate matches only",
+        SYNC_MATCHES: __get_findings(siblings),
+    }
+
+
+def __process_modified_siblings(finding, siblings):
+    util.logger.info(
+        "Found %d siblings for %s, but they already have a changelog, cannot automatically apply changelog",
+        len(siblings),
+        str(finding),
+    )
+    return {
+        SRC_KEY: finding.key,
+        SRC_URL: finding.url(),
+        TGT_KEY: siblings[0].key,
+        TGT_URL: siblings[0].url(),
+        SYNC_STATUS: "unsynchronized",
+        SYNC_MSG: f"Target {__name(finding)} already has a changelog",
+    }
+
+
+def __sync_findings_list(src_findings, tgt_findings, settings):
+    counters = {
+        "nb_to_sync": len(src_findings),
+        "nb_applies": 0,
+        "nb_approx_match": 0,
+        "nb_tgt_has_changelog": 0,
+        "nb_multiple_matches": 0,
+    }
+    report = []
+    name = __name(list(src_findings.values())[0])
+    util.logger.info(
+        "%d %ss to sync, %d %ss in target",
+        len(src_findings),
+        name,
+        len(tgt_findings),
+        name,
+    )
+    for _, finding in src_findings.items():
+        util.logger.debug("Searching sibling for %s", str(finding))
+        (exact_siblings, approx_siblings, modified_siblings) = finding.search_siblings(
+            tgt_findings,
+            allowed_users=settings[SYNC_SERVICE_ACCOUNTS],
+            ignore_component=settings[SYNC_IGNORE_COMPONENTS],
+        )
+        if len(exact_siblings) == 1:
+            report.append(__process_exact_sibling(finding, exact_siblings[0], settings))
+            counters["nb_applies"] += 1
+        elif len(exact_siblings) > 1:
+            report.append(__process_multiple_exact_siblings(finding, exact_siblings))
+            counters["nb_multiple_matches"] += 1
+        elif approx_siblings:
+            report.append(__process_approx_siblings(finding, approx_siblings))
+            counters["nb_approx_match"] += 1
+        elif modified_siblings:
+            counters["nb_tgt_has_changelog"] += 1
+            report.append(__process_modified_siblings(finding, modified_siblings))
+        else:  # No match
+            report.append(__process_no_match(finding))
+    counters["nb_no_match"] = counters["nb_to_sync"] - (
+        counters["nb_applies"] + counters["nb_tgt_has_changelog"] + counters["nb_multiple_matches"] + counters["nb_approx_match"]
+    )
+    util.json_dump_debug(counters, "COUNTERS")
+    return (report, counters)
+
+
+def sync_lists(src_findings, tgt_findings, src_object, tgt_object, sync_settings=None):
+    interesting_src_findings = {}
+    if len(src_findings) == 0 or len(tgt_findings) == 0:
+        util.logger.info("source or target list of findings to sync empty, skipping...")
+        counters = {
+            "nb_to_sync": 0,
+            "nb_applies": 0,
+            "nb_approx_match": 0,
+            "nb_tgt_has_changelog": 0,
+            "nb_multiple_matches": 0,
+        }
+        return ([], counters)
+    name = __name(list(src_findings.values())[0])
+    util.logger.info(
+        "Syncing %d %ss from %s into %d %ss from %s",
+        len(src_findings),
+        name,
+        str(src_object),
+        len(tgt_findings),
+        name,
+        str(tgt_object),
+    )
+    for key1, finding in src_findings.items():
+        if not (finding.has_changelog() or finding.has_comments()):
+            util.logger.info("%s has no changelog or comments, skipped in sync", str(finding))
+            continue
+        if finding.is_closed():
+            util.logger.info(
+                "%s is closed, so it will not be synchronized despite having a changelog",
+                str(finding),
+            )
+            continue
+        modifiers = finding.modifiers().union(finding.commenters())
+        # TODO - Manage more than 1 sync account - diff the 2 lists
+        syncer = sync_settings[SYNC_SERVICE_ACCOUNTS][0]
+        if sync_settings is None:
+            sync_settings = {}
+        if sync_settings.get(SYNC_SERVICE_ACCOUNTS, None) is None:
+            sync_settings[SYNC_SERVICE_ACCOUNTS] = [syncer]
+
+        if len(modifiers) == 1 and list(modifiers)[0] == syncer:
+            util.logger.info(
+                "%s has only been changed by %s, so it will not be synchronized despite having a changelog",
+                str(finding),
+                syncer,
+            )
+            continue
+        interesting_src_findings[key1] = finding
+    util.logger.info(
+        "Found %d %ss with manual changes in %s",
+        len(interesting_src_findings),
+        name,
+        str(src_object),
+    )
+    if len(interesting_src_findings) <= 0:
+        util.logger.info("No %ss with manual changes in %s, skipping...", name, str(src_object))
+        counters = {
+            "nb_to_sync": 0,
+            "nb_applies": 0,
+            "nb_approx_match": 0,
+            "nb_tgt_has_changelog": 0,
+            "nb_multiple_matches": 0,
+        }
+        return ([], counters)
+    return __sync_findings_list(interesting_src_findings, tgt_findings, sync_settings)
```

## sonar/tasks.py

```diff
@@ -1,522 +1,532 @@
-#
-# sonar-tools
-# Copyright (C) 2022-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-
-import time
-import datetime
-import json
-import re
-from sonar.audit import rules, problem
-import sonar.sqobject as sq
-import sonar.utilities as util
-
-SUCCESS = "SUCCESS"
-PENDING = "PENDING"
-IN_PROGRESS = "IN_PROGRESS"
-FAILED = "FAILED"
-CANCELED = "CANCELED"
-
-TIMEOUT = "TIMEOUT"
-
-STATUSES = (SUCCESS, PENDING, IN_PROGRESS, FAILED, CANCELED)
-
-__SUSPICIOUS_EXCLUSIONS = None
-__SUSPICIOUS_EXCEPTIONS = None
-
-SCANNER_VERSIONS = {
-    "ScannerCLI": {
-        "4.8.0": datetime.datetime(2022, 2, 6),
-        "4.7.0": datetime.datetime(2022, 2, 2),
-        "4.6.2": datetime.datetime(2021, 5, 7),
-        "4.6.1": datetime.datetime(2021, 4, 30),
-        "4.6.0": datetime.datetime(2021, 1, 13),
-        "4.5.0": datetime.datetime(2020, 10, 5),
-        "4.4.0": datetime.datetime(2020, 7, 3),
-        "4.3.0": datetime.datetime(2019, 3, 9),
-        "4.2.0": datetime.datetime(2019, 10, 1),
-        "4.1.0": datetime.datetime(2019, 9, 9),
-    },
-    "ScannerMaven": {
-        "3.9.1": datetime.datetime(2021, 11, 1),
-        "3.9.0": datetime.datetime(2021, 4, 1),
-        "3.8.0": datetime.datetime(2021, 1, 1),
-        "3.7.0": datetime.datetime(2019, 10, 1),
-        "3.6.1": datetime.datetime(2019, 8, 1),
-        "3.6.0": datetime.datetime(2019, 1, 1),
-        "3.5.0": datetime.datetime(2018, 9, 1),
-        "3.4.1": datetime.datetime(2018, 6, 1),
-        "3.4.0": datetime.datetime(2017, 11, 1),
-        "3.3.0": datetime.datetime(2017, 3, 1),
-        "3.2.0": datetime.datetime(2016, 9, 1),
-        "3.1.1": datetime.datetime(2016, 9, 1),
-        "3.1.0": datetime.datetime(2016, 9, 1),
-        "3.0.2": datetime.datetime(2016, 4, 1),
-        "3.0.1": datetime.datetime(2016, 1, 1),
-        "3.0.0": datetime.datetime(2016, 1, 1),
-    },
-    "ScannerGradle": {
-        "4.0.0": datetime.datetime(2023, 2, 17),
-        "3.5.0": datetime.datetime(2022, 10, 27),
-        "3.4.0": datetime.datetime(2022, 6, 8),
-        "3.3.0": datetime.datetime(2021, 6, 10),
-        "3.2.0": datetime.datetime(2021, 4, 30),
-        "3.1.1": datetime.datetime(2021, 1, 25),
-        "3.1.0": datetime.datetime(2021, 1, 13),
-        "3.0.0": datetime.datetime(2020, 6, 20),
-        "2.8.0": datetime.datetime(2019, 10, 1),
-    },
-    "ScannerMSBuild": {
-        "5.13.0": datetime.datetime(2023, 4, 5),
-        "5.12.0": datetime.datetime(2023, 3, 17),
-        "5.11.0": datetime.datetime(2023, 1, 27),
-        "5.10.0": datetime.datetime(2023, 1, 13),
-        "5.9.2": datetime.datetime(2022, 12, 14),
-        "5.9.1": datetime.datetime(2022, 12, 6),
-        "5.9.0": datetime.datetime(2022, 12, 1),
-        "5.8.0": datetime.datetime(2022, 8, 24),
-        "5.7.2": datetime.datetime(2022, 7, 12),
-        "5.7.1": datetime.datetime(2022, 6, 21),
-        "5.7.0": datetime.datetime(2022, 6, 20),
-        "5.6.0": datetime.datetime(2022, 5, 30),
-        "5.5.3": datetime.datetime(2022, 2, 14),
-        "5.5.2": datetime.datetime(2022, 2, 10),
-        "5.5.1": datetime.datetime(2022, 2, 8),
-        "5.5.0": datetime.datetime(2022, 2, 7),
-        "5.4.1": datetime.datetime(2021, 12, 23),
-        "5.4.0": datetime.datetime(2021, 11, 26),
-        "5.3.2": datetime.datetime(2021, 10, 28),
-        "5.3.1": datetime.datetime(2021, 9, 1),
-        "5.2.2": datetime.datetime(2021, 6, 24),
-        "5.2.1": datetime.datetime(2021, 4, 30),
-        "5.2.0": datetime.datetime(2021, 4, 9),
-        "5.1.0": datetime.datetime(2021, 3, 9),
-        "5.0.4": datetime.datetime(2020, 11, 11),
-        "5.0.3": datetime.datetime(2020, 11, 10),
-        "5.0.0": datetime.datetime(2020, 11, 5),
-        "4.10.0": datetime.datetime(2020, 6, 29),
-        "4.9.0": datetime.datetime(2020, 5, 5),
-        "4.8.0": datetime.datetime(2019, 11, 6),
-        "4.7.1": datetime.datetime(2019, 9, 10),
-        "4.7.0": datetime.datetime(2019, 9, 3),
-    },
-    "ScannerNpm": {
-        "3.0.1": datetime.datetime(2023, 2, 10),
-        "3.0.0": datetime.datetime(2023, 1, 4),
-        "2.9.0": datetime.datetime(2022, 12, 4),
-        "2.8.9": datetime.datetime(2022, 12, 4),
-        "2.8.2": datetime.datetime(2022, 9, 25),
-        "2.8.1": datetime.datetime(2021, 6, 10),
-        "2.8.0": datetime.datetime(2020, 11, 10),
-        "2.7.0": datetime.datetime(2020, 7, 6),
-        "2.6.0": datetime.datetime(2020, 3, 24),
-        "2.5.0": datetime.datetime(2019, 6, 26),
-        "2.4.1": datetime.datetime(2019, 5, 28),
-        "2.4.0": datetime.datetime(2019, 2, 23),
-        "2.3.0": datetime.datetime(2019, 2, 19),
-        "2.2.0": datetime.datetime(2019, 2, 12),
-        "2.1.2": datetime.datetime(2018, 10, 8),
-        "2.1.1": datetime.datetime(2018, 8, 28),
-        "2.1.0": datetime.datetime(2018, 7, 10),
-    },
-    "Ant": {
-        "2.7.1": datetime.datetime(2021, 4, 30),
-        "2.7": datetime.datetime(2019, 10, 1),
-    },
-}
-
-
-class Task(sq.SqObject):
-    """
-    Abstraction of the SonarQube "background task" concept
-    """
-
-    def __init__(self, task_id, endpoint, concerned_object=None, data=None):
-        super().__init__(task_id, endpoint)
-        self._json = data
-        self.concerned_object = concerned_object
-        self._context = None
-        self._error = None
-        self._submitted_at = None
-        self._started_at = None
-        self._ended_at = None
-
-    def __str__(self):
-        """
-        :return: String formatting of the object
-        :rtype: str
-        """
-        return f"background task '{self.key}'"
-
-    def url(self):
-        """
-        :return: the SonarQube permalink URL to the background task
-        :rtype: str
-        """
-        return f"{self.endpoint.url}/project/background_tasks?id={self.concerned_object.key}"
-
-    def __load(self):
-        if self._json is not None:
-            return
-        self.__load_context()
-
-    def __load_context(self, force=False):
-        if not force and self._json is not None and ("scannerContext" in self._json or not self.has_scanner_context()):
-            # Context already retrieved or not available
-            return
-        params = {"id": self.key, "additionalFields": "scannerContext,stacktrace"}
-        self._json.update(json.loads(self.get("ce/task", params=params).text)["task"])
-
-    def id(self):
-        """
-        :return: the background task id
-        :rtype: str
-        """
-        return self.key
-
-    def __json_field(self, field):
-        self.__load()
-        if field not in self._json:
-            self.__load_context(force=True)
-        return self._json[field]
-
-    def type(self):
-        """
-        :return: the background task type
-        :rtype: str
-        """
-        return self.__json_field("type")
-
-    def status(self):
-        """
-        :return: the background task status
-        :rtype: str
-        """
-        return self.__json_field("status")
-
-    def component(self):
-        """
-        :return: the background task component key or None
-        :rtype: str or None
-        """
-        return self.__json_field("componentKey")
-
-    def execution_time(self):
-        """
-        :return: the background task execution time in millisec
-        :rtype: int
-        """
-        return int(self.__json_field("executionTimeMs"))
-
-    def submitter(self):
-        """
-        :return: the background task submitter
-        :rtype: str
-        """
-        self.__load()
-        return self._json.get("submitterLogin", "anonymous")
-
-    def has_scanner_context(self):
-        """
-        :return: Whether the background task has a scanner context
-        :rtype: bool
-        """
-        self.__load()
-        return self._json.get("hasScannerContext", False)
-
-    def warnings(self):
-        """
-        :return: the background task warnings, if any
-        :rtype: list
-        """
-        if not self._json.get("warnings", None):
-            data = json.loads(self.get("ce/task", params={"id": self.key, "additionalFields": "warnings"}).text)
-            self._json["warnings"] = []
-            self._json.update(data["task"])
-        return self._json["warnings"]
-
-    def warning_count(self):
-        """
-        :return: the number of warnings in the background
-        :rtype: int
-        """
-        return self.__json_field("warningCount")
-
-    def wait_for_completion(self, timeout=180):
-        """Waits for a background task to complete
-
-        :param timeout: Timeout to wait in seconds, defaults to 180
-        :type timeout: int, optional
-        :return: the background task status
-        :rtype: str
-        """
-        wait_time = 0
-        sleep_time = 0.5
-        params = {"status": ",".join(STATUSES), "type": self.type()}
-        if self.endpoint.version() >= (8, 0, 0):
-            params["component"] = self.component()
-        else:
-            params["q"] = self.component()
-        status = PENDING
-        while status not in (SUCCESS, FAILED, CANCELED, TIMEOUT):
-            time.sleep(sleep_time)
-            wait_time += sleep_time
-            sleep_time *= 2
-            data = json.loads(self.get("ce/activity", params=params).text)
-            for t in data["tasks"]:
-                if t["id"] != self.key:
-                    continue
-                status = t["status"]
-            if wait_time >= timeout and status not in (SUCCESS, FAILED, CANCELED):
-                status = TIMEOUT
-            util.logger.debug("%s is '%s'", str(self), status)
-        return status
-
-    def scanner_context(self):
-        """
-        :return: the background task scanner context
-        :rtype: dict
-        """
-        if not self.has_scanner_context():
-            return None
-        self.__load_context()
-        context_line = self._json.get("scannerContext", None)
-        if context_line is None:
-            return None
-        context = {}
-        for line in context_line.split("\n  - "):
-            if not line.startswith("sonar"):
-                continue
-            (prop, val) = line.split("=", 1)
-            context[prop] = val
-        return context
-
-    def error_details(self):
-        """
-        :return: The background task error details
-        :rtype: tuple (errorMsg (str), stackTrace (str)
-        """
-        self.__load_context()
-        return (self._json.get("errorMessage", None), self._json.get("errorStacktrace", None))
-
-    def error_message(self):
-        """
-        :return: The background task error message
-        :rtype: str
-        """
-        self.__load_context()
-        return self._json.get("errorMessage", None)
-
-    def __audit_exclusions(self, exclusion_pattern, susp_exclusions, susp_exceptions):
-        problems = []
-        for susp in susp_exclusions:
-            if not re.search(rf"{susp}", exclusion_pattern):
-                continue
-            is_exception = False
-            for exception in susp_exceptions:
-                if re.search(rf"{exception}", exclusion_pattern):
-                    util.logger.debug("Exclusion %s matches exception %s, no audit problem will be raised", exclusion_pattern, exception)
-                    is_exception = True
-                    break
-            if not is_exception:
-                rule = rules.get_rule(rules.RuleId.PROJ_SUSPICIOUS_EXCLUSION)
-                msg = rule.msg.format(str(self.concerned_object), exclusion_pattern)
-                problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self.concerned_object))
-                break  # Report only on the 1st suspicious match
-        return problems
-
-    def __audit_disabled_scm(self, audit_settings, scan_context):
-        if not audit_settings.get("audit.project.scm.disabled", True):
-            util.logger.info("Auditing disabled SCM integration is turned off, skipping...")
-            return []
-
-        if scan_context.get("sonar.scm.disabled", "false") == "false":
-            return []
-        rule = rules.get_rule(rules.RuleId.PROJ_SCM_DISABLED)
-        return [problem.Problem(rule.type, rule.severity, rule.msg.format(str(self.concerned_object)), concerned_object=self)]
-
-    def __audit_warnings(self, audit_settings):
-        if not audit_settings.get("audit.projects.analysisWarnings", True):
-            util.logger.info("Project analysis warnings auditing disabled, skipping...")
-            return []
-        warnings = self.warnings()
-        if len(warnings) == 0:
-            return []
-        rule = rules.get_rule(rules.RuleId.PROJ_ANALYSIS_WARNING)
-        msg = rule.msg.format(str(self.concerned_object), " --- ".join(warnings))
-        return [problem.Problem(rule.type, rule.severity, msg, concerned_object=self)]
-
-    def __audit_failed_task(self, audit_settings):
-        if not audit_settings.get("audit.projects.failedTasks", True):
-            util.logger.debug("Project failed background tasks auditing disabled, skipping...")
-            return []
-        if self._json["status"] != "FAILED":
-            util.logger.debug("Last bg task of %s has status %s...", str(self.concerned_object), self._json["status"])
-            return []
-        rule = rules.get_rule(rules.RuleId.BG_TASK_FAILED)
-        msg = rule.msg.format(str(self.concerned_object))
-        return [problem.Problem(rule.type, rule.severity, msg, concerned_object=self)]
-
-    def __audit_scanner_version(self, audit_settings):
-        if not self.has_scanner_context():
-            return []
-        context = self.scanner_context()
-        scanner_type = context.get("sonar.scanner.app", None)
-        scanner_version = context.get("sonar.scanner.appVersion", None)
-        util.logger.debug("Scanner type = %s, Scanner version = %s", scanner_type, scanner_version)
-        if not scanner_version:
-            util.logger.warning(
-                "%s has been scanned with scanner '%s' with no version, skipping check scanner version obsolescence",
-                str(self.concerned_object),
-                scanner_type,
-            )
-            return []
-        if scanner_type not in SCANNER_VERSIONS:
-            util.logger.warning(
-                "%s has been scanned with scanner '%s' which is not inventoried, skipping check on scanner obsolescence",
-                str(self.concerned_object),
-                scanner_type,
-            )
-            return []
-
-        if scanner_type == "Ant":
-            rule = rules.get_rule(rules.RuleId.ANT_SCANNER_DEPRECATED)
-            msg = rule.msg.format(str(self.concerned_object))
-            return [problem.Problem(rule.type, rule.severity, msg, concerned_object=self.concerned_object)]
-
-        if scanner_type in ("ScannerGradle", "ScannerMaven"):
-            (scanner_version, build_tool_version) = scanner_version.split("/")
-            scanner_version = scanner_version.replace("-SNAPSHOT", "")
-        scanner_version = [int(n) for n in scanner_version.split(".")]
-        if len(scanner_version) == 2:
-            scanner_version.append(0)
-        scanner_version = tuple(scanner_version[0:3])
-        str_version = ".".join([str(n) for n in scanner_version])
-        versions_list = SCANNER_VERSIONS[scanner_type].keys()
-        util.logger.debug("versions = %s", str(versions_list))
-        try:
-            release_date = SCANNER_VERSIONS[scanner_type][str_version]
-        except KeyError:
-            util.logger.warning(
-                "Scanner '%s' version '%s' is not referenced in sonar-tools. "
-                "Scanner obsolescence check skipped. "
-                "Please report to author at https://github.com/okorach/sonar-tools/issues",
-                scanner_type,
-                str_version,
-            )
-            return []
-
-        tuple_version_list = []
-        for v in versions_list:
-            tuple_version_list.append(tuple([int(n) for n in v.split(".")]))
-        tuple_version_list.sort(reverse=True)
-
-        delta_days = (datetime.datetime.today() - release_date).days
-        index = tuple_version_list.index(scanner_version)
-        util.logger.debug("Scanner used is %d versions old", index)
-        if delta_days > audit_settings["audit.projects.scannerMaxAge"]:
-            rule = rules.get_rule(rules.RuleId.OBSOLETE_SCANNER) if index >= 3 else rules.get_rule(rules.RuleId.NOT_LATEST_SCANNER)
-            msg = rule.msg.format(str(self.concerned_object), scanner_type, str_version, util.date_to_string(release_date, with_time=False))
-            return [problem.Problem(rule.type, rule.severity, msg, concerned_object=self.concerned_object)]
-        return []
-
-    def audit(self, audit_settings):
-        """
-        :meta private:
-        """
-        if not audit_settings.get("audit.projects.exclusions", True):
-            util.logger.debug("Project exclusions auditing disabled, skipping...")
-            return []
-        util.logger.debug("Auditing %s", str(self))
-        problems = []
-        if self.has_scanner_context():
-            problems = []
-            context = self.scanner_context()
-            susp_exclusions = _get_suspicious_exclusions(audit_settings.get("audit.projects.suspiciousExclusionsPatterns", ""))
-            susp_exceptions = _get_suspicious_exceptions(audit_settings.get("audit.projects.suspiciousExclusionsExceptions", ""))
-            for prop in ("sonar.exclusions", "sonar.global.exclusions"):
-                if context.get(prop, None) is None:
-                    continue
-                for excl in util.csv_to_list(context[prop]):
-                    util.logger.debug("Pattern = '%s'", excl)
-                    problems += self.__audit_exclusions(excl, susp_exclusions, susp_exceptions)
-            problems += self.__audit_disabled_scm(audit_settings, context)
-        elif type(self.concerned_object).__name__ == "Project":
-            util.logger.debug("Last background task of %s has no scanner context, can't audit it", str(self.concerned_object))
-
-        problems += self.__audit_warnings(audit_settings)
-        problems += self.__audit_failed_task(audit_settings)
-        problems += self.__audit_scanner_version(audit_settings)
-
-        return problems
-
-
-def search(endpoint, only_current=False, component_key=None):
-    """Searches background tasks
-
-    :param Platform endpoint: Reference to the SonarQube platform
-    :param only_current: only the most recent background task of each object, defaults to False
-    :param component_key: filter for a given component key only, defaults to None
-    :param component_key: str, optional
-    :return: The list of found background tasks
-    :rtype: list[Task]
-    """
-    params = {"status": ",".join(STATUSES), "additionalFields": "warnings"}
-    if only_current:
-        params["onlyCurrents"] = "true"
-    if component_key is not None:
-        params["component"] = component_key
-    data = json.loads(endpoint.get("ce/activity", params=params).text)
-    task_list = []
-    for t in data["tasks"]:
-        task_list.append(Task(t["id"], endpoint, data=t))
-    return task_list
-
-
-def search_all_last(component_key=None, endpoint=None):
-    return search(only_current=True, component_key=component_key, endpoint=endpoint)
-
-
-def search_last(component_key, endpoint=None):
-    bg_tasks = search(only_current=True, component_key=component_key, endpoint=endpoint)
-    if bg_tasks is None or not bg_tasks:
-        # No bgtask was found
-        return None
-    return bg_tasks[0]
-
-
-def search_all(component_key, endpoint=None):
-    return search(component_key=component_key, endpoint=endpoint)
-
-
-def _get_suspicious_exclusions(patterns):
-    global __SUSPICIOUS_EXCLUSIONS
-    if __SUSPICIOUS_EXCLUSIONS is not None:
-        return __SUSPICIOUS_EXCLUSIONS
-    # __SUSPICIOUS_EXCLUSIONS = [x.strip().replace('*', '\\*').replace('.', '\\.').replace('?', '\\?')
-    __SUSPICIOUS_EXCLUSIONS = util.csv_to_list(patterns)
-    return __SUSPICIOUS_EXCLUSIONS
-
-
-def _get_suspicious_exceptions(patterns):
-    global __SUSPICIOUS_EXCEPTIONS
-    if __SUSPICIOUS_EXCEPTIONS is not None:
-        return __SUSPICIOUS_EXCEPTIONS
-    #    __SUSPICIOUS_EXCEPTIONS = [x.strip().replace('*', '\\*').replace('.', '\\.').replace('?', '\\?')
-    __SUSPICIOUS_EXCEPTIONS = util.csv_to_list(patterns)
-    return __SUSPICIOUS_EXCEPTIONS
+#
+# sonar-tools
+# Copyright (C) 2022-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+
+import time
+import datetime
+import json
+import re
+from sonar.audit import rules, problem
+import sonar.sqobject as sq
+import sonar.utilities as util
+
+SUCCESS = "SUCCESS"
+PENDING = "PENDING"
+IN_PROGRESS = "IN_PROGRESS"
+FAILED = "FAILED"
+CANCELED = "CANCELED"
+
+TIMEOUT = "TIMEOUT"
+
+STATUSES = (SUCCESS, PENDING, IN_PROGRESS, FAILED, CANCELED)
+
+__SUSPICIOUS_EXCLUSIONS = None
+__SUSPICIOUS_EXCEPTIONS = None
+
+SCANNER_VERSIONS = {
+    "ScannerCLI": {
+        "4.8.0": datetime.datetime(2022, 2, 6),
+        "4.7.0": datetime.datetime(2022, 2, 2),
+        "4.6.2": datetime.datetime(2021, 5, 7),
+        "4.6.1": datetime.datetime(2021, 4, 30),
+        "4.6.0": datetime.datetime(2021, 1, 13),
+        "4.5.0": datetime.datetime(2020, 10, 5),
+        "4.4.0": datetime.datetime(2020, 7, 3),
+        "4.3.0": datetime.datetime(2019, 3, 9),
+        "4.2.0": datetime.datetime(2019, 10, 1),
+        "4.1.0": datetime.datetime(2019, 9, 9),
+    },
+    "ScannerMaven": {
+        "3.9.1": datetime.datetime(2021, 11, 1),
+        "3.9.0": datetime.datetime(2021, 4, 1),
+        "3.8.0": datetime.datetime(2021, 1, 1),
+        "3.7.0": datetime.datetime(2019, 10, 1),
+        "3.6.1": datetime.datetime(2019, 8, 1),
+        "3.6.0": datetime.datetime(2019, 1, 1),
+        "3.5.0": datetime.datetime(2018, 9, 1),
+        "3.4.1": datetime.datetime(2018, 6, 1),
+        "3.4.0": datetime.datetime(2017, 11, 1),
+        "3.3.0": datetime.datetime(2017, 3, 1),
+        "3.2.0": datetime.datetime(2016, 9, 1),
+        "3.1.1": datetime.datetime(2016, 9, 1),
+        "3.1.0": datetime.datetime(2016, 9, 1),
+        "3.0.2": datetime.datetime(2016, 4, 1),
+        "3.0.1": datetime.datetime(2016, 1, 1),
+        "3.0.0": datetime.datetime(2016, 1, 1),
+    },
+    "ScannerGradle": {
+        "4.0.0": datetime.datetime(2023, 2, 17),
+        "3.5.0": datetime.datetime(2022, 10, 27),
+        "3.4.0": datetime.datetime(2022, 6, 8),
+        "3.3.0": datetime.datetime(2021, 6, 10),
+        "3.2.0": datetime.datetime(2021, 4, 30),
+        "3.1.1": datetime.datetime(2021, 1, 25),
+        "3.1.0": datetime.datetime(2021, 1, 13),
+        "3.0.0": datetime.datetime(2020, 6, 20),
+        "2.8.0": datetime.datetime(2019, 10, 1),
+    },
+    "ScannerMSBuild": {
+        "5.13.0": datetime.datetime(2023, 4, 5),
+        "5.12.0": datetime.datetime(2023, 3, 17),
+        "5.11.0": datetime.datetime(2023, 1, 27),
+        "5.10.0": datetime.datetime(2023, 1, 13),
+        "5.9.2": datetime.datetime(2022, 12, 14),
+        "5.9.1": datetime.datetime(2022, 12, 6),
+        "5.9.0": datetime.datetime(2022, 12, 1),
+        "5.8.0": datetime.datetime(2022, 8, 24),
+        "5.7.2": datetime.datetime(2022, 7, 12),
+        "5.7.1": datetime.datetime(2022, 6, 21),
+        "5.7.0": datetime.datetime(2022, 6, 20),
+        "5.6.0": datetime.datetime(2022, 5, 30),
+        "5.5.3": datetime.datetime(2022, 2, 14),
+        "5.5.2": datetime.datetime(2022, 2, 10),
+        "5.5.1": datetime.datetime(2022, 2, 8),
+        "5.5.0": datetime.datetime(2022, 2, 7),
+        "5.4.1": datetime.datetime(2021, 12, 23),
+        "5.4.0": datetime.datetime(2021, 11, 26),
+        "5.3.2": datetime.datetime(2021, 10, 28),
+        "5.3.1": datetime.datetime(2021, 9, 1),
+        "5.2.2": datetime.datetime(2021, 6, 24),
+        "5.2.1": datetime.datetime(2021, 4, 30),
+        "5.2.0": datetime.datetime(2021, 4, 9),
+        "5.1.0": datetime.datetime(2021, 3, 9),
+        "5.0.4": datetime.datetime(2020, 11, 11),
+        "5.0.3": datetime.datetime(2020, 11, 10),
+        "5.0.0": datetime.datetime(2020, 11, 5),
+        "4.10.0": datetime.datetime(2020, 6, 29),
+        "4.9.0": datetime.datetime(2020, 5, 5),
+        "4.8.0": datetime.datetime(2019, 11, 6),
+        "4.7.1": datetime.datetime(2019, 9, 10),
+        "4.7.0": datetime.datetime(2019, 9, 3),
+    },
+    "ScannerNpm": {
+        "3.0.1": datetime.datetime(2023, 2, 10),
+        "3.0.0": datetime.datetime(2023, 1, 4),
+        "2.9.0": datetime.datetime(2022, 12, 4),
+        "2.8.9": datetime.datetime(2022, 12, 4),
+        "2.8.2": datetime.datetime(2022, 9, 25),
+        "2.8.1": datetime.datetime(2021, 6, 10),
+        "2.8.0": datetime.datetime(2020, 11, 10),
+        "2.7.0": datetime.datetime(2020, 7, 6),
+        "2.6.0": datetime.datetime(2020, 3, 24),
+        "2.5.0": datetime.datetime(2019, 6, 26),
+        "2.4.1": datetime.datetime(2019, 5, 28),
+        "2.4.0": datetime.datetime(2019, 2, 23),
+        "2.3.0": datetime.datetime(2019, 2, 19),
+        "2.2.0": datetime.datetime(2019, 2, 12),
+        "2.1.2": datetime.datetime(2018, 10, 8),
+        "2.1.1": datetime.datetime(2018, 8, 28),
+        "2.1.0": datetime.datetime(2018, 7, 10),
+    },
+    "Ant": {
+        "2.7.1": datetime.datetime(2021, 4, 30),
+        "2.7": datetime.datetime(2019, 10, 1),
+    },
+}
+
+
+class Task(sq.SqObject):
+    """
+    Abstraction of the SonarQube "background task" concept
+    """
+
+    def __init__(self, task_id, endpoint, concerned_object=None, data=None):
+        super().__init__(task_id, endpoint)
+        self._json = data
+        self.concerned_object = concerned_object
+        self._context = None
+        self._error = None
+        self._submitted_at = None
+        self._started_at = None
+        self._ended_at = None
+
+    def __str__(self):
+        """
+        :return: String formatting of the object
+        :rtype: str
+        """
+        return f"background task '{self.key}'"
+
+    def url(self):
+        """
+        :return: the SonarQube permalink URL to the background task
+        :rtype: str
+        """
+        return f"{self.endpoint.url}/project/background_tasks?id={self.concerned_object.key}"
+
+    def __load(self):
+        if self._json is not None:
+            return
+        self.__load_context()
+
+    def __load_context(self, force=False):
+        if not force and self._json is not None and ("scannerContext" in self._json or not self.has_scanner_context()):
+            # Context already retrieved or not available
+            return
+        params = {"id": self.key, "additionalFields": "scannerContext,stacktrace"}
+        self._json.update(json.loads(self.get("ce/task", params=params).text)["task"])
+
+    def id(self):
+        """
+        :return: the background task id
+        :rtype: str
+        """
+        return self.key
+
+    def __json_field(self, field):
+        self.__load()
+        if field not in self._json:
+            self.__load_context(force=True)
+        return self._json[field]
+
+    def type(self):
+        """
+        :return: the background task type
+        :rtype: str
+        """
+        return self.__json_field("type")
+
+    def status(self):
+        """
+        :return: the background task status
+        :rtype: str
+        """
+        return self.__json_field("status")
+
+    def component(self):
+        """
+        :return: the background task component key or None
+        :rtype: str or None
+        """
+        return self.__json_field("componentKey")
+
+    def execution_time(self):
+        """
+        :return: the background task execution time in millisec
+        :rtype: int
+        """
+        return int(self.__json_field("executionTimeMs"))
+
+    def submitter(self):
+        """
+        :return: the background task submitter
+        :rtype: str
+        """
+        self.__load()
+        return self._json.get("submitterLogin", "anonymous")
+
+    def has_scanner_context(self):
+        """
+        :return: Whether the background task has a scanner context
+        :rtype: bool
+        """
+        self.__load()
+        return self._json.get("hasScannerContext", False)
+
+    def warnings(self):
+        """
+        :return: the background task warnings, if any
+        :rtype: list
+        """
+        if not self._json.get("warnings", None):
+            data = json.loads(self.get("ce/task", params={"id": self.key, "additionalFields": "warnings"}).text)
+            self._json["warnings"] = []
+            self._json.update(data["task"])
+        return self._json["warnings"]
+
+    def warning_count(self):
+        """
+        :return: the number of warnings in the background
+        :rtype: int
+        """
+        return self.__json_field("warningCount")
+
+    def wait_for_completion(self, timeout=180):
+        """Waits for a background task to complete
+
+        :param timeout: Timeout to wait in seconds, defaults to 180
+        :type timeout: int, optional
+        :return: the background task status
+        :rtype: str
+        """
+        wait_time = 0
+        sleep_time = 0.5
+        params = {"status": ",".join(STATUSES), "type": self.type()}
+        if self.endpoint.version() >= (8, 0, 0):
+            params["component"] = self.component()
+        else:
+            params["q"] = self.component()
+        status = PENDING
+        while status not in (SUCCESS, FAILED, CANCELED, TIMEOUT):
+            time.sleep(sleep_time)
+            wait_time += sleep_time
+            sleep_time *= 2
+            data = json.loads(self.get("ce/activity", params=params).text)
+            for t in data["tasks"]:
+                if t["id"] != self.key:
+                    continue
+                status = t["status"]
+            if wait_time >= timeout and status not in (SUCCESS, FAILED, CANCELED):
+                status = TIMEOUT
+            util.logger.debug("%s is '%s'", str(self), status)
+        return status
+
+    def scanner_context(self):
+        """
+        :return: the background task scanner context
+        :rtype: dict
+        """
+        if not self.has_scanner_context():
+            return None
+        self.__load_context()
+        context_line = self._json.get("scannerContext", None)
+        if context_line is None:
+            return None
+        context = {}
+        for line in context_line.split("\n  - "):
+            if not line.startswith("sonar"):
+                continue
+            (prop, val) = line.split("=", 1)
+            context[prop] = val
+        return context
+
+    def error_details(self):
+        """
+        :return: The background task error details
+        :rtype: tuple (errorMsg (str), stackTrace (str)
+        """
+        self.__load_context()
+        return (self._json.get("errorMessage", None), self._json.get("errorStacktrace", None))
+
+    def error_message(self):
+        """
+        :return: The background task error message
+        :rtype: str
+        """
+        self.__load_context()
+        return self._json.get("errorMessage", None)
+
+    def __audit_exclusions(self, exclusion_pattern, susp_exclusions, susp_exceptions):
+        problems = []
+        for susp in susp_exclusions:
+            if not re.search(rf"{susp}", exclusion_pattern):
+                continue
+            is_exception = False
+            for exception in susp_exceptions:
+                if re.search(rf"{exception}", exclusion_pattern):
+                    util.logger.debug("Exclusion %s matches exception %s, no audit problem will be raised", exclusion_pattern, exception)
+                    is_exception = True
+                    break
+            if not is_exception:
+                rule = rules.get_rule(rules.RuleId.PROJ_SUSPICIOUS_EXCLUSION)
+                msg = rule.msg.format(str(self.concerned_object), exclusion_pattern)
+                problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self.concerned_object))
+                break  # Report only on the 1st suspicious match
+        return problems
+
+    def __audit_disabled_scm(self, audit_settings, scan_context):
+        if not audit_settings.get("audit.project.scm.disabled", True):
+            util.logger.info("Auditing disabled SCM integration is turned off, skipping...")
+            return []
+
+        if scan_context.get("sonar.scm.disabled", "false") == "false":
+            return []
+        rule = rules.get_rule(rules.RuleId.PROJ_SCM_DISABLED)
+        return [problem.Problem(rule.type, rule.severity, rule.msg.format(str(self.concerned_object)), concerned_object=self)]
+
+    def __audit_warnings(self, audit_settings):
+        if not audit_settings.get("audit.projects.analysisWarnings", True):
+            util.logger.info("Project analysis warnings auditing disabled, skipping...")
+            return []
+        pbs = []
+        warnings = self.warnings()
+        warnings_left = []
+        for w in warnings:
+            if w.find("SCM provider autodetection failed") >= 0:
+                rule = rules.get_rule(rules.RuleId.PROJ_SCM_UNDETECTED)
+                pbs.append(
+                    problem.Problem(rule.type, rule.severity, rule.msg.format(str(self.concerned_object)), concerned_object=self.concerned_object)
+                )
+            else:
+                warnings_left.append(w)
+        if len(warnings_left) > 0:
+            rule = rules.get_rule(rules.RuleId.PROJ_ANALYSIS_WARNING)
+            msg = rule.msg.format(str(self.concerned_object), " --- ".join(warnings_left))
+            pbs.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
+        return pbs
+
+    def __audit_failed_task(self, audit_settings):
+        if not audit_settings.get("audit.projects.failedTasks", True):
+            util.logger.debug("Project failed background tasks auditing disabled, skipping...")
+            return []
+        if self._json["status"] != "FAILED":
+            util.logger.debug("Last bg task of %s has status %s...", str(self.concerned_object), self._json["status"])
+            return []
+        rule = rules.get_rule(rules.RuleId.BG_TASK_FAILED)
+        msg = rule.msg.format(str(self.concerned_object))
+        return [problem.Problem(rule.type, rule.severity, msg, concerned_object=self)]
+
+    def __audit_scanner_version(self, audit_settings):
+        if not self.has_scanner_context():
+            return []
+        context = self.scanner_context()
+        scanner_type = context.get("sonar.scanner.app", None)
+        scanner_version = context.get("sonar.scanner.appVersion", None)
+        util.logger.debug("Scanner type = %s, Scanner version = %s", scanner_type, scanner_version)
+        if not scanner_version:
+            util.logger.warning(
+                "%s has been scanned with scanner '%s' with no version, skipping check scanner version obsolescence",
+                str(self.concerned_object),
+                scanner_type,
+            )
+            return []
+        if scanner_type not in SCANNER_VERSIONS:
+            util.logger.warning(
+                "%s has been scanned with scanner '%s' which is not inventoried, skipping check on scanner obsolescence",
+                str(self.concerned_object),
+                scanner_type,
+            )
+            return []
+
+        if scanner_type == "Ant":
+            rule = rules.get_rule(rules.RuleId.ANT_SCANNER_DEPRECATED)
+            msg = rule.msg.format(str(self.concerned_object))
+            return [problem.Problem(rule.type, rule.severity, msg, concerned_object=self.concerned_object)]
+
+        if scanner_type in ("ScannerGradle", "ScannerMaven"):
+            (scanner_version, build_tool_version) = scanner_version.split("/")
+            scanner_version = scanner_version.replace("-SNAPSHOT", "")
+        scanner_version = [int(n) for n in scanner_version.split(".")]
+        if len(scanner_version) == 2:
+            scanner_version.append(0)
+        scanner_version = tuple(scanner_version[0:3])
+        str_version = ".".join([str(n) for n in scanner_version])
+        versions_list = SCANNER_VERSIONS[scanner_type].keys()
+        util.logger.debug("versions = %s", str(versions_list))
+        try:
+            release_date = SCANNER_VERSIONS[scanner_type][str_version]
+        except KeyError:
+            util.logger.warning(
+                "Scanner '%s' version '%s' is not referenced in sonar-tools. "
+                "Scanner obsolescence check skipped. "
+                "Please report to author at https://github.com/okorach/sonar-tools/issues",
+                scanner_type,
+                str_version,
+            )
+            return []
+
+        tuple_version_list = []
+        for v in versions_list:
+            tuple_version_list.append(tuple([int(n) for n in v.split(".")]))
+        tuple_version_list.sort(reverse=True)
+
+        delta_days = (datetime.datetime.today() - release_date).days
+        index = tuple_version_list.index(scanner_version)
+        util.logger.debug("Scanner used is %d versions old", index)
+        if delta_days > audit_settings.get("audit.projects.scannerMaxAge", 730):
+            rule = rules.get_rule(rules.RuleId.OBSOLETE_SCANNER) if index >= 3 else rules.get_rule(rules.RuleId.NOT_LATEST_SCANNER)
+            msg = rule.msg.format(str(self.concerned_object), scanner_type, str_version, util.date_to_string(release_date, with_time=False))
+            return [problem.Problem(rule.type, rule.severity, msg, concerned_object=self.concerned_object)]
+        return []
+
+    def audit(self, audit_settings):
+        """
+        :meta private:
+        """
+        if not audit_settings.get("audit.projects.exclusions", True):
+            util.logger.debug("Project exclusions auditing disabled, skipping...")
+            return []
+        util.logger.debug("Auditing %s", str(self))
+        problems = []
+        if self.has_scanner_context():
+            problems = []
+            context = self.scanner_context()
+            susp_exclusions = _get_suspicious_exclusions(audit_settings.get("audit.projects.suspiciousExclusionsPatterns", ""))
+            susp_exceptions = _get_suspicious_exceptions(audit_settings.get("audit.projects.suspiciousExclusionsExceptions", ""))
+            for prop in ("sonar.exclusions", "sonar.global.exclusions"):
+                if context.get(prop, None) is None:
+                    continue
+                for excl in util.csv_to_list(context[prop]):
+                    util.logger.debug("Pattern = '%s'", excl)
+                    problems += self.__audit_exclusions(excl, susp_exclusions, susp_exceptions)
+            problems += self.__audit_disabled_scm(audit_settings, context)
+        elif type(self.concerned_object).__name__ == "Project":
+            util.logger.debug("Last background task of %s has no scanner context, can't audit it", str(self.concerned_object))
+
+        problems += self.__audit_warnings(audit_settings)
+        problems += self.__audit_failed_task(audit_settings)
+        problems += self.__audit_scanner_version(audit_settings)
+
+        return problems
+
+
+def search(endpoint, only_current=False, component_key=None):
+    """Searches background tasks
+
+    :param Platform endpoint: Reference to the SonarQube platform
+    :param only_current: only the most recent background task of each object, defaults to False
+    :param component_key: filter for a given component key only, defaults to None
+    :param component_key: str, optional
+    :return: The list of found background tasks
+    :rtype: list[Task]
+    """
+    params = {"status": ",".join(STATUSES), "additionalFields": "warnings"}
+    if only_current:
+        params["onlyCurrents"] = "true"
+    if component_key is not None:
+        params["component"] = component_key
+    data = json.loads(endpoint.get("ce/activity", params=params).text)
+    task_list = []
+    for t in data["tasks"]:
+        task_list.append(Task(t["id"], endpoint, data=t))
+    return task_list
+
+
+def search_all_last(component_key=None, endpoint=None):
+    return search(only_current=True, component_key=component_key, endpoint=endpoint)
+
+
+def search_last(component_key, endpoint=None):
+    bg_tasks = search(only_current=True, component_key=component_key, endpoint=endpoint)
+    if bg_tasks is None or not bg_tasks:
+        # No bgtask was found
+        return None
+    return bg_tasks[0]
+
+
+def search_all(component_key, endpoint=None):
+    return search(component_key=component_key, endpoint=endpoint)
+
+
+def _get_suspicious_exclusions(patterns):
+    global __SUSPICIOUS_EXCLUSIONS
+    if __SUSPICIOUS_EXCLUSIONS is not None:
+        return __SUSPICIOUS_EXCLUSIONS
+    # __SUSPICIOUS_EXCLUSIONS = [x.strip().replace('*', '\\*').replace('.', '\\.').replace('?', '\\?')
+    __SUSPICIOUS_EXCLUSIONS = util.csv_to_list(patterns)
+    return __SUSPICIOUS_EXCLUSIONS
+
+
+def _get_suspicious_exceptions(patterns):
+    global __SUSPICIOUS_EXCEPTIONS
+    if __SUSPICIOUS_EXCEPTIONS is not None:
+        return __SUSPICIOUS_EXCEPTIONS
+    #    __SUSPICIOUS_EXCEPTIONS = [x.strip().replace('*', '\\*').replace('.', '\\.').replace('?', '\\?')
+    __SUSPICIOUS_EXCEPTIONS = util.csv_to_list(patterns)
+    return __SUSPICIOUS_EXCEPTIONS
```

## sonar/tokens.py

 * *Ordering differences only*

```diff
@@ -1,94 +1,94 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-
-import json
-import sonar.sqobject as sq
-import sonar.utilities as util
-
-
-class UserToken(sq.SqObject):
-    """
-    Abstraction of the SonarQube "user token" concept
-    """
-
-    API_ROOT = "user_tokens"
-    API_REVOKE = API_ROOT + "/revoke"
-    API_SEARCH = API_ROOT + "/search"
-    API_GENERATE = API_ROOT + "/generate"
-
-    def __init__(self, login, name=None, json_data=None, created_at=None, token=None, endpoint=None):
-        super().__init__(login, endpoint)
-        self.login = login  #: User login
-        self.name = name  #: Token name
-        self.created_at = None  #: Token creation date
-        self.last_connection_date = None  #: Token last connection date
-        if isinstance(created_at, str):
-            self.created_at = util.string_to_date(created_at)
-        else:
-            self.created_at = created_at
-        if self.name is None and "name" in json_data:
-            self.name = json_data["name"]
-        if self.created_at is None and "createdAt" in json_data:
-            self.created_at = util.string_to_date(json_data["createdAt"])
-        if "lastConnectionDate" in json_data:
-            self.last_connection_date = util.string_to_date(json_data["lastConnectionDate"])
-        self.token = token
-        util.logger.debug("Created '%s'", str(self))
-
-    def __str__(self):
-        """
-        :return: Token string representation
-        :rtype: str
-        """
-        return f"token '{self.name}' of user '{self.login}'"
-
-    def revoke(self):
-        """Revokes the token
-        :return: Whether the revocation succeeded
-        :rtype: bool
-        """
-        if self.name is None:
-            return False
-        util.logger.info("Revoking token '%s' of user login '%s'", self.name, self.login)
-        return self.post(UserToken.API_REVOKE, {"name": self.name, "login": self.login}).ok
-
-
-def search(endpoint, login):
-    """Searches tokens of a given user
-
-    :param login: login of the user
-    :type login: str
-    :return: list of tokens
-    :rtype: list[UserToken]
-    """
-    data = json.loads(endpoint.get(UserToken.API_SEARCH, {"login": login}).text)
-    token_list = []
-    for tk in data["userTokens"]:
-        token_list.append(UserToken(login=data["login"], json_data=tk, endpoint=endpoint))
-    return token_list
-
-
-def generate(name, endpoint, login=None):
-    """Generates a new token for a given user
-    :return: the generated Token object
-    :rtype: Token
-    """
-    data = json.loads(endpoint.post(UserToken.API_GENERATE, {"name": name, "login": login}).text)
-    return UserToken(endpoint=endpoint, login=data["login"], name=data["name"], created_at=data["createdAt"], token=data["token"])
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+
+import json
+import sonar.sqobject as sq
+import sonar.utilities as util
+
+
+class UserToken(sq.SqObject):
+    """
+    Abstraction of the SonarQube "user token" concept
+    """
+
+    API_ROOT = "user_tokens"
+    API_REVOKE = API_ROOT + "/revoke"
+    API_SEARCH = API_ROOT + "/search"
+    API_GENERATE = API_ROOT + "/generate"
+
+    def __init__(self, login, name=None, json_data=None, created_at=None, token=None, endpoint=None):
+        super().__init__(login, endpoint)
+        self.login = login  #: User login
+        self.name = name  #: Token name
+        self.created_at = None  #: Token creation date
+        self.last_connection_date = None  #: Token last connection date
+        if isinstance(created_at, str):
+            self.created_at = util.string_to_date(created_at)
+        else:
+            self.created_at = created_at
+        if self.name is None and "name" in json_data:
+            self.name = json_data["name"]
+        if self.created_at is None and "createdAt" in json_data:
+            self.created_at = util.string_to_date(json_data["createdAt"])
+        if "lastConnectionDate" in json_data:
+            self.last_connection_date = util.string_to_date(json_data["lastConnectionDate"])
+        self.token = token
+        util.logger.debug("Created '%s'", str(self))
+
+    def __str__(self):
+        """
+        :return: Token string representation
+        :rtype: str
+        """
+        return f"token '{self.name}' of user '{self.login}'"
+
+    def revoke(self):
+        """Revokes the token
+        :return: Whether the revocation succeeded
+        :rtype: bool
+        """
+        if self.name is None:
+            return False
+        util.logger.info("Revoking token '%s' of user login '%s'", self.name, self.login)
+        return self.post(UserToken.API_REVOKE, {"name": self.name, "login": self.login}).ok
+
+
+def search(endpoint, login):
+    """Searches tokens of a given user
+
+    :param login: login of the user
+    :type login: str
+    :return: list of tokens
+    :rtype: list[UserToken]
+    """
+    data = json.loads(endpoint.get(UserToken.API_SEARCH, {"login": login}).text)
+    token_list = []
+    for tk in data["userTokens"]:
+        token_list.append(UserToken(login=data["login"], json_data=tk, endpoint=endpoint))
+    return token_list
+
+
+def generate(name, endpoint, login=None):
+    """Generates a new token for a given user
+    :return: the generated Token object
+    :rtype: Token
+    """
+    data = json.loads(endpoint.post(UserToken.API_GENERATE, {"name": name, "login": login}).text)
+    return UserToken(endpoint=endpoint, login=data["login"], name=data["name"], created_at=data["createdAt"], token=data["token"])
```

## sonar/users.py

```diff
@@ -1,442 +1,441 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-
-import datetime as dt
-import pytz
-from sonar import groups, sqobject, tokens, exceptions
-import sonar.utilities as util
-from sonar.audit import rules, problem
-
-
-_OBJECTS = {}
-
-SEARCH_API = "users/search"
-CREATE_API = "users/create"
-UPDATE_API = "users/update"
-DEACTIVATE_API = "users/deactivate"
-UPDATE_LOGIN_API = "users/update_login"
-
-SETTABLE_PROPERTIES = ("login", "name", "scmAccounts", "email", "groups", "local")
-
-
-class User(sqobject.SqObject):
-    """
-    Abstraction of the SonarQube "user" concept
-    Objects of this class must be created with one of the 3 available class constructor methods. Don't use __init__
-    """
-
-    def __init__(self, login, endpoint, data):
-        """Do not use to create users, use on of the constructor class methods"""
-        super().__init__(login, endpoint)
-        self.login = login  #: User login (str)
-        self.name = None  #: User name (str)
-        self.groups = None  #: User groups (list)
-        self.scm_accounts = None  #: User SCM accounts (list)
-        self.email = None  #: User email (str)
-        self.is_local = None  #: Whether user is local (bool) - read-only
-        self.last_login = None  #: User last login (datetime) - read-only
-        self.nb_tokens = None  #: Nbr of tokens (int) - read-only
-        self.__tokens = None
-        self.__load(data)
-        util.logger.debug("Created %s", str(self))
-        _OBJECTS[self.login] = self
-
-    @classmethod
-    def load(cls, endpoint, data):
-        """Creates a user object from the result of a SonarQube API user search data
-
-        :param endpoint: Reference to the SonarQube platform
-        :type endpoint: Platform
-        :param data: The JSON data corresponding to the group
-        :type data: dict
-        :return: The user object
-        :rtype: User or None
-        """
-        util.logger.debug("Loading user '%s'", data["login"])
-        return cls(login=data["login"], endpoint=endpoint, data=data)
-
-    @classmethod
-    def create(cls, endpoint, login, name=None, is_local=True, password=None):
-        """Creates a new user in SonarQube and returns the corresponding User object
-
-        :param Platform endpoint: Reference to the SonarQube platform
-        :param str login: User login
-        :param name: User name, default to login
-        :type name: str, optional
-        :param is_local: Whether the user is local, defaults to True
-        :type is_local: bool, optional
-        :param password: The password if user is local, defaults to login
-        :type password: str, optional
-        :return: The user object
-        :rtype: User or None
-        """
-        util.logger.debug("Creating user '%s'", login)
-        params = {"login": login, "local": str(is_local).lower(), "name": name}
-        if is_local:
-            params["password"] = password if password else login
-        endpoint.post(CREATE_API, params=params)
-        return cls.get_object(endpoint=endpoint, login=login)
-
-    @classmethod
-    def get_object(cls, endpoint, login):
-        """Creates a User object corresponding to the user with same login in SonarQube
-
-        :param Platform endpoint: Reference to the SonarQube platform
-        :param str login: User login
-        :raise ObjectNotFound: if login not found
-        :return: The user object
-        :rtype: User
-        """
-        if login in _OBJECTS:
-            return _OBJECTS[login]
-        util.logger.debug("Getting user '%s'", login)
-        for k, o in search(endpoint, params={"q": login}).items():
-            if k == login:
-                return o
-        raise exceptions.ObjectNotFound(login, f"User '{login}' not found")
-
-    def __str__(self):
-        """
-        :return: String formatting of the object
-        :rtype: str
-        """
-        return f"user '{self.login}'"
-
-    def __load(self, data):
-        self.name = data["name"]  #: User name
-        self.groups = data.get("groups", [])  #: User groups
-        self.scm_accounts = data.pop("scmAccounts", None)  #: User SCM accounts
-        self.email = data.get("email", None)  #: User email
-        self.is_local = data.get("local", False)  #: User is local - read-only
-        self.last_login = util.string_to_date(data.get("lastConnectionDate", None))  #: User last login - read-only
-        self.nb_tokens = data.get("tokenCount", None)  #: Nbr of tokens - read-only
-        self.__tokens = None
-        self._json = data
-
-    def refresh(self):
-        """Refreshes a User object from SonarQube data
-
-        :return:  Nothing
-        """
-        data = self.get(SEARCH_API, params={"q": self.login})
-        for d in data["users"]:
-            if d["login"] == self.login:
-                self.__load(d)
-                break
-
-    def url(self):
-        """
-        :return: the SonarQube permalink to the user, actually the global users page only
-                 since this is as close as we can get to the precise user definition
-        :rtype: str
-        """
-        return f"{self.endpoint.url}/admin/users"
-
-    def deactivate(self):
-        """Deactivates the user
-
-        :return: Whether the deactivation succeeded
-        :rtype: bool
-        """
-        return self.post(DEACTIVATE_API, {"name": self.name, "login": self.login}).ok
-
-    def tokens(self):
-        """
-        :return: The list of tokens of the user
-        :rtype: list[Token]
-        """
-        if self.__tokens is None:
-            self.__tokens = tokens.search(self.endpoint, self.login)
-        return self.__tokens
-
-    def update(self, **kwargs):
-        """Updates a user with name, email, login, SCM accounts, group memberships
-
-        :param name: New name of the user
-        :type name: str, optional
-        :param email: New email of the user
-        :type email: str, optional
-        :param login: New login of the user
-        :type login: str, optional
-        :param groups: List of groups to add membership
-        :type groups: list[str], optional
-        :param scm_accounts: List of SCM accounts
-        :type scm_accounts: list[str], optional
-        :return: self
-        :rtype: User
-        """
-        util.logger.debug("Updating %s with %s", str(self), str(kwargs))
-        params = {"login": self.login}
-        my_data = vars(self)
-        if self.is_local:
-            for p in ("name", "email"):
-                if p in kwargs and kwargs[p] != my_data[p]:
-                    params[p] = kwargs[p]
-            if len(params) > 1:
-                self.post(UPDATE_API, params=params)
-            self.set_scm_accounts(kwargs.get("scmAccounts", ""))
-            if "login" in kwargs:
-                new_login = kwargs["login"]
-                if new_login not in _OBJECTS:
-                    self.post(UPDATE_LOGIN_API, params={"login": self.login, "newLogin": new_login})
-                    _OBJECTS.pop(self.login, None)
-                    self.login = new_login
-                    _OBJECTS[self.login] = self
-        self.set_groups(kwargs.get("groups", ""))
-        return self
-
-    def add_to_group(self, group_name):
-        """Adds group membership to the user
-
-        :param group_name: Group to add membership
-        :type group_name: str
-        :return: Whether operation succeeded
-        :rtype: bool
-        """
-        group = groups.Group.read(endpoint=self.endpoint, name=group_name)
-        if not group:
-            util.logger.warning("Group '%s' does not exists, can't add membership for %s", group_name, str(self))
-            return False
-        return group.add_user(self.login)
-
-    def remove_from_group(self, group_name):
-        """Removes group membership to the user
-
-        :param str group_name: Group to remove membership
-        :raises UnsupportedOperation: if trying to remove a user from built-in groups ("sonar-users" only for now)
-        :raises ObjectNotFound: if group name not found
-        :return: Whether operation succeeded
-        :rtype: bool
-        """
-        group = groups.Group.read(endpoint=self.endpoint, name=group_name)
-        if group.is_default():
-            raise exceptions.UnsupportedOperation(f"Group '{group_name}' is built-in, can't remove membership for {str(self)}")
-        return group.remove_user(self.login)
-
-    def set_groups(self, group_list):
-        """Set the user group membership (replaces current groups)
-
-        :param group_list: List of groups to set membership
-        :type group_list: list[str]
-        :return: Whether all group membership were OK
-        :rtype: bool
-        """
-        ok = True
-        for g in list(set(group_list) - set(self.groups)):
-            if g != "sonar-users":
-                ok = ok and self.add_to_group(g)
-        for g in list(set(self.groups) - set(group_list)):
-            if g != "sonar-users":
-                ok = ok and self.remove_from_group(g)
-        if ok:
-            self.groups = group_list
-        else:
-            self.refresh()
-        return ok
-
-    def add_scm_accounts(self, accounts_list):
-        """Adds SCM accounts to the user (on top of existing ones)
-
-        :param accounts_list: List of SCM accounts to add
-        :type accounts_list: list[str]
-        :return: Whether SCM accounts were successfully set
-        :rtype: bool
-        """
-        accounts_list = util.csv_to_list(accounts_list)
-        if len(accounts_list) == 0:
-            return False
-        util.logger.info("Adding SCM accounts '%s' to %s", str(accounts_list), str(self))
-        return self.set_scm_accounts(list(set(self.scm_accounts) | set(accounts_list)))
-
-    def set_scm_accounts(self, accounts_list):
-        """Sets SCM accounts to the user (on top of existing ones)
-
-        :param accounts_list: List of SCM accounts to set
-        :type accounts_list: list[str]
-        :return: Whether SCM accounts were successfully set
-        :rtype: bool
-        """
-        accounts_list = util.csv_to_list(accounts_list)
-        util.logger.debug("Setting SCM accounts of %s to '%s'", str(self), str(accounts_list))
-        r = self.post(UPDATE_API, params={"login": self.login, "scmAccount": accounts_list})
-        if not r.ok:
-            self.scm_accounts = []
-            return False
-        self.scm_accounts = accounts_list
-        return True
-
-    def audit(self, settings=None):
-        """Audits a user (user last connection date and tokens) and
-        returns the list of problems found (too old)
-
-        :param settings: Options of what to audit and thresholds to raise problems
-        :type settings: dict
-        :return: List of problems found, or empty list
-        :rtype: list[Problem]
-        """
-        util.logger.debug("Auditing %s", str(self))
-        protected_users = util.csv_to_list(settings["audit.tokens.neverExpire"])
-        if self.login in protected_users:
-            util.logger.info("%s is protected, last connection date is ignored, tokens never expire", str(self))
-            return []
-
-        today = dt.datetime.today().replace(tzinfo=pytz.UTC)
-        problems = []
-        for t in self.tokens():
-            age = abs((today - t.created_at).days)
-            if age > settings["audit.tokens.maxAge"]:
-                rule = rules.get_rule(rules.RuleId.TOKEN_TOO_OLD)
-                msg = rule.msg.format(str(t), age)
-                problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
-            if t.last_connection_date is None and age > settings["audit.tokens.maxUnusedAge"]:
-                rule = rules.get_rule(rules.RuleId.TOKEN_NEVER_USED)
-                msg = rule.msg.format(str(t), age)
-                problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
-            if t.last_connection_date is None:
-                continue
-            last_cnx_age = abs((today - t.last_connection_date).days)
-            if last_cnx_age > settings["audit.tokens.maxUnusedAge"]:
-                rule = rules.get_rule(rules.RuleId.TOKEN_UNUSED)
-                msg = rule.msg.format(str(t), last_cnx_age)
-                problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
-
-        if self.last_login:
-            age = abs((today - self.last_login).days)
-            if age > settings["audit.users.maxLoginAge"]:
-                rule = rules.get_rule(rules.RuleId.USER_UNUSED)
-                msg = rule.msg.format(str(self), age)
-                problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
-        return problems
-
-    def to_json(self, full=False):
-        """Exports the user data (login, email, groups, SCM accounts local or not) as dict
-
-        :return: User data
-        :rtype: dict
-        """
-        json_data = self._json.copy()
-        scm = self.scm_accounts
-        json_data["scmAccounts"] = util.list_to_csv(scm) if scm else None
-        my_groups = self.groups.copy()
-        my_groups.remove("sonar-users")
-        json_data["groups"] = util.list_to_csv(my_groups, ", ", True)
-        if not full and not json_data["local"]:
-            json_data.pop("local")
-        return util.remove_nones(util.filter_export(json_data, SETTABLE_PROPERTIES, full))
-
-
-def search(endpoint, params=None):
-    """Searches users in SonarQube
-
-    :param endpoint: Reference to the SonarQube platform
-    :type endpoint: Platform
-    :param params: list of parameters to narrow down the search
-    :type params: dict
-    :return: list of projects
-    :rtype: dict{login: User}
-    """
-    util.logger.debug("Searching users with params %s", str(params))
-    return sqobject.search_objects(api=SEARCH_API, params=params, returned_field="users", key_field="login", object_class=User, endpoint=endpoint)
-
-
-def export(endpoint, full=False):
-    """Exports all users as dict
-
-    :param endpoint: reference to the SonarQube platform
-    :type endpoint: Platform
-    :param full: Whether to export all settings including those useless for re-import, defaults to False
-    :type full: bool, optional
-    :return: list of projects
-    :rtype: dict{key: Project}
-    """
-    util.logger.info("Exporting users")
-    u_list = {}
-    for u_login, u_obj in search(endpoint=endpoint).items():
-        u_list[u_login] = u_obj.to_json(full)
-        u_list[u_login].pop("login", None)
-    return u_list
-
-
-def audit(endpoint, audit_settings):
-    """Audits all users for last login date and too old tokens
-
-    :param endpoint: reference to the SonarQube platform
-    :type endpoint: Platform
-    :param audit_settings: Configuration of audit
-    :type audit_settings: dict
-    :return: list of problems found
-    :rtype: list[Problem]
-    """
-    if not audit_settings["audit.users"]:
-        util.logger.info("Auditing users is disabled, skipping...")
-        return []
-    util.logger.info("--- Auditing users ---")
-    problems = []
-    for _, u in search(endpoint=endpoint).items():
-        problems += u.audit(audit_settings)
-    return problems
-
-
-def get_login_from_name(name, endpoint):
-    """Returns the login corresponding to name
-    If more than one login matches the name, the first occurence is returned
-
-    :param name: User name
-    :type name: str
-    :param endpoint: reference to the SonarQube platform
-    :type endpoint: Platform
-    :return: User login or None if name not found
-    :rtype: str or None
-    """
-    u_list = search(endpoint=endpoint, params={"q": name})
-    if not u_list:
-        return None
-    if len(u_list) > 1:
-        util.logger.warning("More than 1 user with name '%s', will return the 1st one", name)
-    return list(u_list.keys()).pop(0)
-
-
-def import_config(endpoint, config_data):
-    """Imports in SonarQube a complete users configuration described from a JSON
-
-    :param endpoint: reference to the SonarQube platform
-    :type endpoint: Platform
-    :param config_data: the configuration to import
-    :type config_data: dict
-    :return: Nothing
-    """
-    if "users" not in config_data:
-        util.logger.info("No users to import")
-        return
-    util.logger.info("Importing users")
-    for login, data in config_data["users"].items():
-        data = _decode(data)
-        data.pop("login", None)
-        try:
-            o = User.get_object(endpoint, login)
-        except exceptions.ObjectNotFound:
-            o = User.create(endpoint, login, data.get("name", login), data.get("local", False))
-        o.update(**data)
-
-
-def _decode(data):
-    data["scm_accounts"] = util.csv_to_list(data.pop("scmAccounts", ""))
-    data["groups"] = util.csv_to_list(data.pop("groups", ""))
-    return data
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+
+import datetime as dt
+from sonar import groups, sqobject, tokens, exceptions
+import sonar.utilities as util
+from sonar.audit import rules, problem
+
+
+_OBJECTS = {}
+
+SEARCH_API = "users/search"
+CREATE_API = "users/create"
+UPDATE_API = "users/update"
+DEACTIVATE_API = "users/deactivate"
+UPDATE_LOGIN_API = "users/update_login"
+
+SETTABLE_PROPERTIES = ("login", "name", "scmAccounts", "email", "groups", "local")
+
+
+class User(sqobject.SqObject):
+    """
+    Abstraction of the SonarQube "user" concept
+    Objects of this class must be created with one of the 3 available class constructor methods. Don't use __init__
+    """
+
+    def __init__(self, login, endpoint, data):
+        """Do not use to create users, use on of the constructor class methods"""
+        super().__init__(login, endpoint)
+        self.login = login  #: User login (str)
+        self.name = None  #: User name (str)
+        self.groups = None  #: User groups (list)
+        self.scm_accounts = None  #: User SCM accounts (list)
+        self.email = None  #: User email (str)
+        self.is_local = None  #: Whether user is local (bool) - read-only
+        self.last_login = None  #: User last login (datetime) - read-only
+        self.nb_tokens = None  #: Nbr of tokens (int) - read-only
+        self.__tokens = None
+        self.__load(data)
+        util.logger.debug("Created %s", str(self))
+        _OBJECTS[self.login] = self
+
+    @classmethod
+    def load(cls, endpoint, data):
+        """Creates a user object from the result of a SonarQube API user search data
+
+        :param endpoint: Reference to the SonarQube platform
+        :type endpoint: Platform
+        :param data: The JSON data corresponding to the group
+        :type data: dict
+        :return: The user object
+        :rtype: User or None
+        """
+        util.logger.debug("Loading user '%s'", data["login"])
+        return cls(login=data["login"], endpoint=endpoint, data=data)
+
+    @classmethod
+    def create(cls, endpoint, login, name=None, is_local=True, password=None):
+        """Creates a new user in SonarQube and returns the corresponding User object
+
+        :param Platform endpoint: Reference to the SonarQube platform
+        :param str login: User login
+        :param name: User name, default to login
+        :type name: str, optional
+        :param is_local: Whether the user is local, defaults to True
+        :type is_local: bool, optional
+        :param password: The password if user is local, defaults to login
+        :type password: str, optional
+        :return: The user object
+        :rtype: User or None
+        """
+        util.logger.debug("Creating user '%s'", login)
+        params = {"login": login, "local": str(is_local).lower(), "name": name}
+        if is_local:
+            params["password"] = password if password else login
+        endpoint.post(CREATE_API, params=params)
+        return cls.get_object(endpoint=endpoint, login=login)
+
+    @classmethod
+    def get_object(cls, endpoint, login):
+        """Creates a User object corresponding to the user with same login in SonarQube
+
+        :param Platform endpoint: Reference to the SonarQube platform
+        :param str login: User login
+        :raise ObjectNotFound: if login not found
+        :return: The user object
+        :rtype: User
+        """
+        if login in _OBJECTS:
+            return _OBJECTS[login]
+        util.logger.debug("Getting user '%s'", login)
+        for k, o in search(endpoint, params={"q": login}).items():
+            if k == login:
+                return o
+        raise exceptions.ObjectNotFound(login, f"User '{login}' not found")
+
+    def __str__(self):
+        """
+        :return: String formatting of the object
+        :rtype: str
+        """
+        return f"user '{self.login}'"
+
+    def __load(self, data):
+        self.name = data["name"]  #: User name
+        self.groups = data.get("groups", [])  #: User groups
+        self.scm_accounts = data.pop("scmAccounts", None)  #: User SCM accounts
+        self.email = data.get("email", None)  #: User email
+        self.is_local = data.get("local", False)  #: User is local - read-only
+        self.last_login = util.string_to_date(data.get("lastConnectionDate", None))  #: User last login - read-only
+        self.nb_tokens = data.get("tokenCount", None)  #: Nbr of tokens - read-only
+        self.__tokens = None
+        self._json = data
+
+    def refresh(self):
+        """Refreshes a User object from SonarQube data
+
+        :return:  Nothing
+        """
+        data = self.get(SEARCH_API, params={"q": self.login})
+        for d in data["users"]:
+            if d["login"] == self.login:
+                self.__load(d)
+                break
+
+    def url(self):
+        """
+        :return: the SonarQube permalink to the user, actually the global users page only
+                 since this is as close as we can get to the precise user definition
+        :rtype: str
+        """
+        return f"{self.endpoint.url}/admin/users"
+
+    def deactivate(self):
+        """Deactivates the user
+
+        :return: Whether the deactivation succeeded
+        :rtype: bool
+        """
+        return self.post(DEACTIVATE_API, {"name": self.name, "login": self.login}).ok
+
+    def tokens(self):
+        """
+        :return: The list of tokens of the user
+        :rtype: list[Token]
+        """
+        if self.__tokens is None:
+            self.__tokens = tokens.search(self.endpoint, self.login)
+        return self.__tokens
+
+    def update(self, **kwargs):
+        """Updates a user with name, email, login, SCM accounts, group memberships
+
+        :param name: New name of the user
+        :type name: str, optional
+        :param email: New email of the user
+        :type email: str, optional
+        :param login: New login of the user
+        :type login: str, optional
+        :param groups: List of groups to add membership
+        :type groups: list[str], optional
+        :param scm_accounts: List of SCM accounts
+        :type scm_accounts: list[str], optional
+        :return: self
+        :rtype: User
+        """
+        util.logger.debug("Updating %s with %s", str(self), str(kwargs))
+        params = {"login": self.login}
+        my_data = vars(self)
+        if self.is_local:
+            for p in ("name", "email"):
+                if p in kwargs and kwargs[p] != my_data[p]:
+                    params[p] = kwargs[p]
+            if len(params) > 1:
+                self.post(UPDATE_API, params=params)
+            self.set_scm_accounts(kwargs.get("scmAccounts", ""))
+            if "login" in kwargs:
+                new_login = kwargs["login"]
+                if new_login not in _OBJECTS:
+                    self.post(UPDATE_LOGIN_API, params={"login": self.login, "newLogin": new_login})
+                    _OBJECTS.pop(self.login, None)
+                    self.login = new_login
+                    _OBJECTS[self.login] = self
+        self.set_groups(kwargs.get("groups", ""))
+        return self
+
+    def add_to_group(self, group_name):
+        """Adds group membership to the user
+
+        :param group_name: Group to add membership
+        :type group_name: str
+        :return: Whether operation succeeded
+        :rtype: bool
+        """
+        group = groups.Group.read(endpoint=self.endpoint, name=group_name)
+        if not group:
+            util.logger.warning("Group '%s' does not exists, can't add membership for %s", group_name, str(self))
+            return False
+        return group.add_user(self.login)
+
+    def remove_from_group(self, group_name):
+        """Removes group membership to the user
+
+        :param str group_name: Group to remove membership
+        :raises UnsupportedOperation: if trying to remove a user from built-in groups ("sonar-users" only for now)
+        :raises ObjectNotFound: if group name not found
+        :return: Whether operation succeeded
+        :rtype: bool
+        """
+        group = groups.Group.read(endpoint=self.endpoint, name=group_name)
+        if group.is_default():
+            raise exceptions.UnsupportedOperation(f"Group '{group_name}' is built-in, can't remove membership for {str(self)}")
+        return group.remove_user(self.login)
+
+    def set_groups(self, group_list):
+        """Set the user group membership (replaces current groups)
+
+        :param group_list: List of groups to set membership
+        :type group_list: list[str]
+        :return: Whether all group membership were OK
+        :rtype: bool
+        """
+        ok = True
+        for g in list(set(group_list) - set(self.groups)):
+            if g != "sonar-users":
+                ok = ok and self.add_to_group(g)
+        for g in list(set(self.groups) - set(group_list)):
+            if g != "sonar-users":
+                ok = ok and self.remove_from_group(g)
+        if ok:
+            self.groups = group_list
+        else:
+            self.refresh()
+        return ok
+
+    def add_scm_accounts(self, accounts_list):
+        """Adds SCM accounts to the user (on top of existing ones)
+
+        :param accounts_list: List of SCM accounts to add
+        :type accounts_list: list[str]
+        :return: Whether SCM accounts were successfully set
+        :rtype: bool
+        """
+        accounts_list = util.csv_to_list(accounts_list)
+        if len(accounts_list) == 0:
+            return False
+        util.logger.info("Adding SCM accounts '%s' to %s", str(accounts_list), str(self))
+        return self.set_scm_accounts(list(set(self.scm_accounts) | set(accounts_list)))
+
+    def set_scm_accounts(self, accounts_list):
+        """Sets SCM accounts to the user (on top of existing ones)
+
+        :param accounts_list: List of SCM accounts to set
+        :type accounts_list: list[str]
+        :return: Whether SCM accounts were successfully set
+        :rtype: bool
+        """
+        accounts_list = util.csv_to_list(accounts_list)
+        util.logger.debug("Setting SCM accounts of %s to '%s'", str(self), str(accounts_list))
+        r = self.post(UPDATE_API, params={"login": self.login, "scmAccount": accounts_list})
+        if not r.ok:
+            self.scm_accounts = []
+            return False
+        self.scm_accounts = accounts_list
+        return True
+
+    def audit(self, settings=None):
+        """Audits a user (user last connection date and tokens) and
+        returns the list of problems found (too old)
+
+        :param settings: Options of what to audit and thresholds to raise problems
+        :type settings: dict
+        :return: List of problems found, or empty list
+        :rtype: list[Problem]
+        """
+        util.logger.debug("Auditing %s", str(self))
+        protected_users = util.csv_to_list(settings.get("audit.tokens.neverExpire", ""))
+        if self.login in protected_users:
+            util.logger.info("%s is protected, last connection date is ignored, tokens never expire", str(self))
+            return []
+
+        today = dt.datetime.today().replace(tzinfo=dt.timezone.utc)
+        problems = []
+        for t in self.tokens():
+            age = abs((today - t.created_at).days)
+            if age > settings.get("audit.tokens.maxAge", 90):
+                rule = rules.get_rule(rules.RuleId.TOKEN_TOO_OLD)
+                msg = rule.msg.format(str(t), age)
+                problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
+            if t.last_connection_date is None and age > settings.get("audit.tokens.maxUnusedAge", 30):
+                rule = rules.get_rule(rules.RuleId.TOKEN_NEVER_USED)
+                msg = rule.msg.format(str(t), age)
+                problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
+            if t.last_connection_date is None:
+                continue
+            last_cnx_age = abs((today - t.last_connection_date).days)
+            if last_cnx_age > settings.get("audit.tokens.maxUnusedAge", 30):
+                rule = rules.get_rule(rules.RuleId.TOKEN_UNUSED)
+                msg = rule.msg.format(str(t), last_cnx_age)
+                problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
+
+        if self.last_login:
+            age = abs((today - self.last_login).days)
+            if age > settings.get("audit.users.maxLoginAge", 180):
+                rule = rules.get_rule(rules.RuleId.USER_UNUSED)
+                msg = rule.msg.format(str(self), age)
+                problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
+        return problems
+
+    def to_json(self, full=False):
+        """Exports the user data (login, email, groups, SCM accounts local or not) as dict
+
+        :return: User data
+        :rtype: dict
+        """
+        json_data = self._json.copy()
+        scm = self.scm_accounts
+        json_data["scmAccounts"] = util.list_to_csv(scm) if scm else None
+        my_groups = self.groups.copy()
+        my_groups.remove("sonar-users")
+        json_data["groups"] = util.list_to_csv(my_groups, ", ", True)
+        if not full and not json_data["local"]:
+            json_data.pop("local")
+        return util.remove_nones(util.filter_export(json_data, SETTABLE_PROPERTIES, full))
+
+
+def search(endpoint, params=None):
+    """Searches users in SonarQube
+
+    :param endpoint: Reference to the SonarQube platform
+    :type endpoint: Platform
+    :param params: list of parameters to narrow down the search
+    :type params: dict
+    :return: list of projects
+    :rtype: dict{login: User}
+    """
+    util.logger.debug("Searching users with params %s", str(params))
+    return sqobject.search_objects(api=SEARCH_API, params=params, returned_field="users", key_field="login", object_class=User, endpoint=endpoint)
+
+
+def export(endpoint, full=False):
+    """Exports all users as dict
+
+    :param endpoint: reference to the SonarQube platform
+    :type endpoint: Platform
+    :param full: Whether to export all settings including those useless for re-import, defaults to False
+    :type full: bool, optional
+    :return: list of projects
+    :rtype: dict{key: Project}
+    """
+    util.logger.info("Exporting users")
+    u_list = {}
+    for u_login, u_obj in search(endpoint=endpoint).items():
+        u_list[u_login] = u_obj.to_json(full)
+        u_list[u_login].pop("login", None)
+    return u_list
+
+
+def audit(endpoint, audit_settings):
+    """Audits all users for last login date and too old tokens
+
+    :param endpoint: reference to the SonarQube platform
+    :type endpoint: Platform
+    :param audit_settings: Configuration of audit
+    :type audit_settings: dict
+    :return: list of problems found
+    :rtype: list[Problem]
+    """
+    if not audit_settings.get("audit.users", True):
+        util.logger.info("Auditing users is disabled, skipping...")
+        return []
+    util.logger.info("--- Auditing users ---")
+    problems = []
+    for _, u in search(endpoint=endpoint).items():
+        problems += u.audit(audit_settings)
+    return problems
+
+
+def get_login_from_name(name, endpoint):
+    """Returns the login corresponding to name
+    If more than one login matches the name, the first occurence is returned
+
+    :param name: User name
+    :type name: str
+    :param endpoint: reference to the SonarQube platform
+    :type endpoint: Platform
+    :return: User login or None if name not found
+    :rtype: str or None
+    """
+    u_list = search(endpoint=endpoint, params={"q": name})
+    if not u_list:
+        return None
+    if len(u_list) > 1:
+        util.logger.warning("More than 1 user with name '%s', will return the 1st one", name)
+    return list(u_list.keys()).pop(0)
+
+
+def import_config(endpoint, config_data):
+    """Imports in SonarQube a complete users configuration described from a JSON
+
+    :param endpoint: reference to the SonarQube platform
+    :type endpoint: Platform
+    :param config_data: the configuration to import
+    :type config_data: dict
+    :return: Nothing
+    """
+    if "users" not in config_data:
+        util.logger.info("No users to import")
+        return
+    util.logger.info("Importing users")
+    for login, data in config_data["users"].items():
+        data = _decode(data)
+        data.pop("login", None)
+        try:
+            o = User.get_object(endpoint, login)
+        except exceptions.ObjectNotFound:
+            o = User.create(endpoint, login, data.get("name", login), data.get("local", False))
+        o.update(**data)
+
+
+def _decode(data):
+    data["scm_accounts"] = util.csv_to_list(data.pop("scmAccounts", ""))
+    data["groups"] = util.csv_to_list(data.pop("groups", ""))
+    return data
```

## sonar/utilities.py

```diff
@@ -1,568 +1,598 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-"""
-
-    Utilities for SonarQube API
-
-"""
-from http import HTTPStatus
-import sys
-import os
-import contextlib
-import re
-import logging
-import argparse
-import json
-import datetime
-import pytz
-from sonar import options
-
-OPT_VERBOSE = "verbosity"
-OPT_MODE = "mode"
-DRY_RUN = "dryrun"
-CONFIRM = "confirm"
-BATCH = "batch"
-RUN_MODE = DRY_RUN
-ISO_DATE_FORMAT = "%04d-%02d-%02d"
-SQ_DATETIME_FORMAT = "%Y-%m-%dT%H:%M:%S%z"
-SQ_DATE_FORMAT = "%Y-%m-%d"
-SQ_TIME_FORMAT = "%H:%M:%S"
-
-CSV_SEPARATOR = ","
-
-logger = logging.getLogger("sonar-tools")
-formatter = logging.Formatter("%(asctime)s | %(name)s | %(levelname)-7s | %(threadName)-15s | %(message)s")
-fh = logging.FileHandler("sonar-tools.log")
-ch = logging.StreamHandler()
-logger.addHandler(fh)
-logger.addHandler(ch)
-fh.setFormatter(formatter)
-ch.setFormatter(formatter)
-
-
-def set_logger(name):
-    global logger
-    logger = logging.getLogger(name)
-    new_fh = logging.FileHandler(name + ".log")
-    new_ch = logging.StreamHandler()
-    logger.addHandler(new_fh)
-    logger.addHandler(new_ch)
-    new_fh.setFormatter(formatter)
-    new_ch.setFormatter(formatter)
-
-
-def set_common_args(desc):
-    """Parses options common to all sonar-tools scripts"""
-    parser = argparse.ArgumentParser(description=desc)
-    parser.add_argument(
-        "-t",
-        "--token",
-        required=False,
-        default=os.getenv("SONAR_TOKEN", None),
-        help="""Token to authenticate to the source SonarQube, default is environment variable $SONAR_TOKEN
-        - Unauthenticated usage is not possible""",
-    )
-    parser.add_argument(
-        "-u",
-        "--url",
-        required=False,
-        default=os.getenv("SONAR_HOST_URL", "http://localhost:9000"),
-        help="""Root URL of the source SonarQube server,
-        default is environment variable $SONAR_HOST_URL or http://localhost:9000 if not set""",
-    )
-    parser.add_argument(
-        "-v",
-        "--" + OPT_VERBOSE,
-        required=False,
-        choices=["WARN", "INFO", "DEBUG"],
-        default="INFO",
-        help="Logging verbosity level",
-    )
-    parser.add_argument(
-        "-c",
-        "--clientCert",
-        required=False,
-        default=None,
-        help="Optional client certificate file (as .pem file)",
-    )
-    return parser
-
-
-def set_key_arg(parser):
-    parser.add_argument(
-        "-k",
-        "--projectKeys",
-        "--keys",
-        "--projectKey",
-        required=False,
-        help="Commas separated keys of the objects to select",
-    )
-    return parser
-
-
-def set_target_sonar_args(parser):
-    parser.add_argument(
-        "-U",
-        "--urlTarget",
-        required=False,
-        help="Root URL of the target SonarQube server",
-    )
-    parser.add_argument(
-        "-T",
-        "--tokenTarget",
-        required=False,
-        help="Token to authenticate to target SonarQube - Unauthenticated usage is not possible",
-    )
-    return parser
-
-
-def set_output_file_args(parser, json_fmt=True, csv_fmt=True):
-    parser.add_argument(
-        "-f",
-        "--file",
-        required=False,
-        help="Output file for the report, stdout by default",
-    )
-    if json_fmt and csv_fmt:
-        parser.add_argument(
-            "--" + options.FORMAT,
-            choices=["csv", "json"],
-            required=False,
-            help="Output format for generated report.\nIf not specified, it is the output file extension if json or csv, then csv by default",
-        )
-    if csv_fmt:
-        parser.add_argument(
-            "--" + options.CSV_SEPARATOR,
-            required=False,
-            default=CSV_SEPARATOR,
-            help=f"CSV separator (for CSV output), default {CSV_SEPARATOR}",
-        )
-
-    return parser
-
-
-def set_what(parser, what_list, operation):
-    parser.add_argument(
-        "-w",
-        "--what",
-        required=False,
-        default="",
-        help=f"What to {operation} {','.join(what_list)}",
-    )
-    return parser
-
-
-def get_logging_level(level):
-    if level == "DEBUG":
-        lvl = logging.DEBUG
-    elif level in ("WARN", "WARNING"):
-        lvl = logging.WARNING
-    elif level == "ERROR":
-        lvl = logging.ERROR
-    elif level == "CRITICAL":
-        lvl = logging.CRITICAL
-    else:
-        lvl = logging.INFO
-    return lvl
-
-
-def set_debug_level(level):
-    logger.setLevel(get_logging_level(level))
-    logger.info("Set debug level to %s", level)
-
-
-def check_environment(kwargs):
-    set_debug_level(kwargs.pop(OPT_VERBOSE))
-
-
-def parse_and_check_token(parser):
-    args = parser.parse_args()
-    if args.token is None:
-        exit_fatal(
-            "Token is missing (Argument -t/--token)",
-            options.ERR_SONAR_API_AUTHENTICATION,
-        )
-    return args
-
-
-def token_type(token):
-    if token[0:4] == "sqa_":
-        return "global-analysis"
-    elif token[0:4] == "sqp_":
-        return "project-analysis"
-    else:
-        return "user"
-
-
-def check_token(token):
-    if token_type(token) != "user":
-        exit_fatal(
-            f"The provided token {redacted_token(token)} is a {token_type(token)} token, a user token is required for sonar-tools",
-            options.ERR_TOKEN_NOT_SUITED,
-        )
-
-
-def json_dump_debug(json_data, pre_string=""):
-    logger.debug("%s%s", pre_string, json_dump(json_data))
-
-
-def format_date_ymd(year, month, day):
-    return ISO_DATE_FORMAT % (year, month, day)
-
-
-def format_date(somedate):
-    return ISO_DATE_FORMAT % (somedate.year, somedate.month, somedate.day)
-
-
-def string_to_date(string):
-    try:
-        return datetime.datetime.strptime(string, SQ_DATETIME_FORMAT)
-    except (ValueError, TypeError):
-        return string
-
-
-def date_to_string(date, with_time=True):
-    return "" if date is None else date.strftime(SQ_DATETIME_FORMAT if with_time else SQ_DATE_FORMAT)
-
-
-def age(some_date, rounded=True):
-    """returns the age (in days) of a date
-
-    :param some_date: date
-    :type date: datetime
-    :param rounded: Whether to rounddown to nearest day
-    :type rounded: bool
-    :return: The age in days, or by the second if not rounded
-    :rtype: timedelta or int if rounded
-    """
-    if not some_date:
-        return None
-    delta = datetime.datetime.today().replace(tzinfo=pytz.UTC) - some_date
-    return delta.days if rounded else delta
-
-
-def get_setting(settings, key, default):
-    if settings is None:
-        return default
-    return settings.get(key, default)
-
-
-def redacted_token(token):
-    if token is None:
-        return "-"
-    if token[0:4] in ("squ_", "sqa_", "sqp_"):
-        return re.sub(r"(......).*(..)", r"\1***\2", token)
-    else:
-        return re.sub(r"(..).*(..)", r"\1***\2", token)
-
-
-def convert_to_type(value):
-    try:
-        newval = int(value)
-        return newval
-    except ValueError:
-        pass
-    try:
-        newval = float(value)
-        return newval
-    except ValueError:
-        pass
-    return value
-
-
-def remove_nones(d):
-    if isinstance(d, dict):
-        return {k: v for k, v in d.items() if v is not None}
-    else:
-        return d
-
-
-def dict_subset(d, subset_list):
-    """Returns the subset of dict only with subset_list keys"""
-    return {key: d[key] for key in subset_list if key in d}
-
-
-def allowed_values_string(original_str, allowed_values):
-    return list_to_csv([v for v in csv_to_list(original_str) if v in allowed_values])
-
-
-def json_dump(jsondata, indent=3):
-    return json.dumps(remove_nones(jsondata), indent=indent, sort_keys=True, separators=(",", ": "))
-
-
-def str_none(v):
-    if v is None:
-        return ""
-    else:
-        return str(v)
-
-
-def csv_to_list(string, separator=","):
-    if isinstance(string, list):
-        return string
-    if string is None or re.match(r"^\s*$", string):
-        return []
-    return [s.strip() for s in string.split(separator)]
-
-
-def list_to_csv(array, separator=",", check_for_separator=False):
-    if isinstance(array, str):
-        return csv_normalize(array, separator) if " " in array else array
-    if array is None:
-        return None
-    if check_for_separator:
-        # Don't convert to string if one array item contains the string separator
-        s = separator.strip()
-        for item in array:
-            if s in item:
-                return array
-    return separator.join([v.strip() for v in array])
-
-
-def csv_normalize(string, separator=","):
-    return list_to_csv(csv_to_list(string, separator))
-
-
-def intersection(list1, list2):
-    return [value for value in list1 if value in list2]
-
-
-def union(list1, list2):
-    return list1 + [value for value in list2 if value not in list1]
-
-
-def difference(list1, list2):
-    return [value for value in list1 if value not in list2]
-
-
-def quote(string, sep):
-    if sep in string:
-        string = '"' + string.replace('"', '""') + '"'
-    if "\n" in string:
-        string = string.replace("\n", " ")
-    return string
-
-
-def jvm_heap(cmdline):
-    for s in cmdline.split(" "):
-        if not re.match("-Xmx", s):
-            continue
-        try:
-            val = int(s[4:-1])
-            unit = s[-1].upper()
-            if unit == "M":
-                return val
-            elif unit == "G":
-                return val * 1024
-            elif unit == "K":
-                return val // 1024
-        except ValueError:
-            logger.warning("JVM -Xmx heap specified seems invalid in '%s'", cmdline)
-            return None
-    logger.warning("No JVM heap memory settings specified in '%s'", cmdline)
-    return None
-
-
-def int_memory(string):
-    (val, unit) = string.split(" ")
-    # For decimal separator in some countries
-    val = float(val.replace(",", "."))
-    if unit == "MB":
-        return int(val)
-    elif unit == "GB":
-        return int(val * 1024)
-    elif unit == "KB":
-        return val / 1024
-    elif unit == "bytes":
-        return val / 1024 / 1024
-    return None
-
-
-def dict_add(dict1, dict2):
-    for k in dict2:
-        if k not in dict1:
-            dict1[k] = 0
-        dict1[k] += dict2[k]
-    return dict1
-
-
-def exit_fatal(err_msg, exit_code):
-    logger.fatal(err_msg)
-    print(f"FATAL: {err_msg}", file=sys.stderr)
-    sys.exit(exit_code)
-
-
-def convert_string(value):
-    if not isinstance(value, str):
-        return value
-    if value.lower() in ("yes", "true", "on"):
-        value = True
-    elif value.lower() in ("no", "false", "off"):
-        value = False
-    else:
-        try:
-            value = int(value)
-        except ValueError:
-            try:
-                value = float(value)
-            except ValueError:
-                pass
-    return value
-
-
-def update_json(json_data, categ, subcateg, value):
-    if categ not in json_data:
-        if subcateg is None:
-            json_data[categ] = value
-        else:
-            json_data[categ] = {subcateg: value}
-    elif subcateg is not None:
-        if subcateg in json_data[categ]:
-            json_data[categ][subcateg].update(value)
-        else:
-            json_data[categ][subcateg] = value
-    else:
-        json_data[categ].update(value)
-    return json_data
-
-
-def int_div_ceil(number, divider):
-    return (number + divider - 1) // divider
-
-
-def nbr_pages(sonar_api_json):
-    if "total" in sonar_api_json:
-        return int_div_ceil(sonar_api_json["total"], sonar_api_json["ps"])
-    elif "paging" in sonar_api_json:
-        return int_div_ceil(sonar_api_json["paging"]["total"], sonar_api_json["paging"]["pageSize"])
-    else:
-        return 1
-
-
-@contextlib.contextmanager
-def open_file(file=None, mode="w"):
-    if file and file != "-":
-        logger.debug("Opening file '%s'", file)
-        fd = open(file=file, mode=mode, encoding="utf-8", newline="")
-    else:
-        logger.debug("Writing to stdout")
-        fd = sys.stdout
-    try:
-        yield fd
-    finally:
-        if fd is not sys.stdout:
-            fd.close()
-
-
-def load_json_file(file):
-    with open(file, "r", encoding="utf-8") as fd:
-        return json.loads(fd.read())
-
-
-def search_by_name(endpoint, name, api, returned_field, extra_params=None):
-    params = {"q": name}
-    if extra_params is not None:
-        params.update(extra_params)
-    data = json.loads(endpoint.get(api, params=params).text)
-    for d in data[returned_field]:
-        if d["name"] == name:
-            return d
-    return None
-
-
-def search_by_key(endpoint, key, api, returned_field, extra_params=None):
-    params = {"q": key}
-    if extra_params is not None:
-        params.update(extra_params)
-    data = json.loads(endpoint.get(api, params=params).text)
-    for d in data[returned_field]:
-        if d["key"] == key:
-            return d
-    return None
-
-
-def log_and_exit(response):
-    if response.ok:
-        return
-    tool_msg = f"For request URL {response.request.url}\n"
-    code = response.status_code
-    try:
-        sq_msg = " | ".join([e["msg"] for e in json.loads(response.text)["errors"]])
-    except json.decoder.JSONDecodeError:
-        sq_msg = ""
-
-    if code == HTTPStatus.UNAUTHORIZED:
-        tool_msg += f"HTTP error {code} - Authentication error. Is token valid ?"
-        err_code = options.ERR_SONAR_API_AUTHENTICATION
-    elif code == HTTPStatus.FORBIDDEN:
-        tool_msg += f"HTTP error {code} - Insufficient permissions to perform operation"
-        err_code = options.ERR_SONAR_API_AUTHORIZATION
-    else:
-        tool_msg += f"HTTP error {code} - Exiting"
-        err_code = options.ERR_SONAR_API
-    exit_fatal(f"{tool_msg}: {sq_msg}", err_code)
-
-
-def object_key(key_or_obj):
-    if isinstance(key_or_obj, str):
-        return key_or_obj
-    else:
-        return key_or_obj.key
-
-
-def check_what(what, allowed_values, operation="processed"):
-    if what == "":
-        return allowed_values
-    what = csv_to_list(what)
-    for w in what:
-        if w in allowed_values:
-            continue
-        exit_fatal(
-            f"'{w}' is not something that can be {operation}, chose among {','.join(allowed_values)}",
-            exit_code=options.ERR_ARGS_ERROR,
-        )
-    return what
-
-
-def __prefix(value):
-    if isinstance(value, dict):
-        return {f"_{k}": __prefix(v) for k, v in value.items()}
-    elif isinstance(value, list):
-        return [__prefix(v) for v in value]
-    else:
-        return value
-
-
-def filter_export(json_data, key_properties, full):
-    new_json_data = json_data.copy()
-    for k in json_data:
-        if k not in key_properties:
-            if full and k != "actions":
-                new_json_data[f"_{k}"] = __prefix(new_json_data.pop(k))
-            else:
-                new_json_data.pop(k)
-    return new_json_data
-
-
-def replace_keys(key_list, new_key, data):
-    for k in key_list:
-        if k in data:
-            data[new_key] = data.pop(k)
-    return data
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+"""
+
+    Utilities for SonarQube API
+
+"""
+from http import HTTPStatus
+import sys
+import os
+import contextlib
+import re
+import logging
+import argparse
+import json
+import datetime
+from typing import Union
+import pytz
+from sonar import options
+
+OPT_VERBOSE = "verbosity"
+OPT_MODE = "mode"
+DRY_RUN = "dryrun"
+CONFIRM = "confirm"
+BATCH = "batch"
+RUN_MODE = DRY_RUN
+ISO_DATE_FORMAT = "%04d-%02d-%02d"
+SQ_DATETIME_FORMAT = "%Y-%m-%dT%H:%M:%S%z"
+SQ_DATE_FORMAT = "%Y-%m-%d"
+SQ_TIME_FORMAT = "%H:%M:%S"
+
+CSV_SEPARATOR = ","
+
+logger = logging.getLogger("sonar-tools")
+formatter = logging.Formatter("%(asctime)s | %(name)s | %(levelname)-7s | %(threadName)-15s | %(message)s")
+fh = logging.FileHandler("sonar-tools.log")
+ch = logging.StreamHandler()
+logger.addHandler(fh)
+logger.addHandler(ch)
+fh.setFormatter(formatter)
+ch.setFormatter(formatter)
+
+
+def set_logger(name):
+    global logger
+    logger = logging.getLogger(name)
+    new_fh = logging.FileHandler(name + ".log")
+    new_ch = logging.StreamHandler()
+    logger.addHandler(new_fh)
+    logger.addHandler(new_ch)
+    new_fh.setFormatter(formatter)
+    new_ch.setFormatter(formatter)
+
+
+def set_common_args(desc):
+    """Parses options common to all sonar-tools scripts"""
+    parser = argparse.ArgumentParser(description=desc)
+    parser.add_argument(
+        "-t",
+        "--token",
+        required=False,
+        default=os.getenv("SONAR_TOKEN", None),
+        help="""Token to authenticate to the source SonarQube, default is environment variable $SONAR_TOKEN
+        - Unauthenticated usage is not possible""",
+    )
+    parser.add_argument(
+        "-u",
+        "--url",
+        required=False,
+        default=os.getenv("SONAR_HOST_URL", "http://localhost:9000"),
+        help="""Root URL of the source SonarQube server,
+        default is environment variable $SONAR_HOST_URL or http://localhost:9000 if not set""",
+    )
+    parser.add_argument(
+        "-v",
+        "--" + OPT_VERBOSE,
+        required=False,
+        choices=["WARN", "INFO", "DEBUG"],
+        default="INFO",
+        help="Logging verbosity level",
+    )
+    parser.add_argument(
+        "-c",
+        "--clientCert",
+        required=False,
+        default=None,
+        help="Optional client certificate file (as .pem file)",
+    )
+    return parser
+
+
+def set_key_arg(parser):
+    parser.add_argument(
+        "-k",
+        "--projectKeys",
+        "--keys",
+        "--projectKey",
+        required=False,
+        help="Commas separated keys of the objects to select",
+    )
+    return parser
+
+
+def set_target_sonar_args(parser):
+    parser.add_argument(
+        "-U",
+        "--urlTarget",
+        required=False,
+        help="Root URL of the target SonarQube server",
+    )
+    parser.add_argument(
+        "-T",
+        "--tokenTarget",
+        required=False,
+        help="Token to authenticate to target SonarQube - Unauthenticated usage is not possible",
+    )
+    return parser
+
+
+def set_output_file_args(parser, json_fmt=True, csv_fmt=True):
+    parser.add_argument(
+        "-f",
+        "--file",
+        required=False,
+        help="Output file for the report, stdout by default",
+    )
+    if json_fmt and csv_fmt:
+        parser.add_argument(
+            "--" + options.FORMAT,
+            choices=["csv", "json"],
+            required=False,
+            help="Output format for generated report.\nIf not specified, it is the output file extension if json or csv, then csv by default",
+        )
+    if csv_fmt:
+        parser.add_argument(
+            "--" + options.CSV_SEPARATOR,
+            required=False,
+            default=CSV_SEPARATOR,
+            help=f"CSV separator (for CSV output), default {CSV_SEPARATOR}",
+        )
+
+    return parser
+
+
+def set_what(parser, what_list, operation):
+    parser.add_argument(
+        "-w",
+        "--what",
+        required=False,
+        default="",
+        help=f"What to {operation} {','.join(what_list)}",
+    )
+    return parser
+
+
+def get_logging_level(level):
+    if level == "DEBUG":
+        lvl = logging.DEBUG
+    elif level in ("WARN", "WARNING"):
+        lvl = logging.WARNING
+    elif level == "ERROR":
+        lvl = logging.ERROR
+    elif level == "CRITICAL":
+        lvl = logging.CRITICAL
+    else:
+        lvl = logging.INFO
+    return lvl
+
+
+def set_debug_level(level):
+    logger.setLevel(get_logging_level(level))
+    logger.info("Set debug level to %s", level)
+
+
+def check_environment(kwargs):
+    set_debug_level(kwargs.pop(OPT_VERBOSE))
+
+
+def parse_and_check_token(parser):
+    args = parser.parse_args()
+    if args.token is None:
+        exit_fatal(
+            "Token is missing (Argument -t/--token)",
+            options.ERR_SONAR_API_AUTHENTICATION,
+        )
+    return args
+
+
+def token_type(token):
+    if token[0:4] == "sqa_":
+        return "global-analysis"
+    elif token[0:4] == "sqp_":
+        return "project-analysis"
+    else:
+        return "user"
+
+
+def check_token(token):
+    if token_type(token) != "user":
+        exit_fatal(
+            f"The provided token {redacted_token(token)} is a {token_type(token)} token, a user token is required for sonar-tools",
+            options.ERR_TOKEN_NOT_SUITED,
+        )
+
+
+def json_dump_debug(json_data, pre_string=""):
+    logger.debug("%s%s", pre_string, json_dump(json_data))
+
+
+def format_date_ymd(year, month, day):
+    return ISO_DATE_FORMAT % (year, month, day)
+
+
+def format_date(somedate):
+    return ISO_DATE_FORMAT % (somedate.year, somedate.month, somedate.day)
+
+
+def string_to_date(string):
+    try:
+        return datetime.datetime.strptime(string, SQ_DATETIME_FORMAT)
+    except (ValueError, TypeError):
+        return string
+
+
+def date_to_string(date, with_time=True):
+    return "" if date is None else date.strftime(SQ_DATETIME_FORMAT if with_time else SQ_DATE_FORMAT)
+
+
+def age(some_date, rounded=True):
+    """returns the age (in days) of a date
+
+    :param some_date: date
+    :type date: datetime
+    :param rounded: Whether to rounddown to nearest day
+    :type rounded: bool
+    :return: The age in days, or by the second if not rounded
+    :rtype: timedelta or int if rounded
+    """
+    if not some_date:
+        return None
+    delta = datetime.datetime.today().replace(tzinfo=pytz.UTC) - some_date
+    return delta.days if rounded else delta
+
+
+def get_setting(settings, key, default):
+    if settings is None:
+        return default
+    return settings.get(key, default)
+
+
+def redacted_token(token):
+    if token is None:
+        return "-"
+    if token[0:4] in ("squ_", "sqa_", "sqp_"):
+        return re.sub(r"(......).*(..)", r"\1***\2", token)
+    else:
+        return re.sub(r"(..).*(..)", r"\1***\2", token)
+
+
+def convert_to_type(value):
+    try:
+        newval = int(value)
+        return newval
+    except ValueError:
+        pass
+    try:
+        newval = float(value)
+        return newval
+    except ValueError:
+        pass
+    return value
+
+
+def remove_nones(d):
+    if isinstance(d, dict):
+        return {k: v for k, v in d.items() if v is not None}
+    else:
+        return d
+
+
+def dict_subset(d, subset_list):
+    """Returns the subset of dict only with subset_list keys"""
+    return {key: d[key] for key in subset_list if key in d}
+
+
+def allowed_values_string(original_str, allowed_values):
+    return list_to_csv([v for v in csv_to_list(original_str) if v in allowed_values])
+
+
+def json_dump(jsondata, indent=3):
+    return json.dumps(remove_nones(jsondata), indent=indent, sort_keys=True, separators=(",", ": "))
+
+
+def str_none(v):
+    if v is None:
+        return ""
+    else:
+        return str(v)
+
+
+def csv_to_list(string, separator=","):
+    if isinstance(string, list):
+        return string
+    if string is None or re.match(r"^\s*$", string):
+        return []
+    return [s.strip() for s in string.split(separator)]
+
+
+def list_to_csv(array, separator=",", check_for_separator=False):
+    if isinstance(array, str):
+        return csv_normalize(array, separator) if " " in array else array
+    if array is None:
+        return None
+    if check_for_separator:
+        # Don't convert to string if one array item contains the string separator
+        s = separator.strip()
+        for item in array:
+            if s in item:
+                return array
+    return separator.join([v.strip() for v in array])
+
+
+def csv_normalize(string, separator=","):
+    return list_to_csv(csv_to_list(string, separator))
+
+
+def intersection(list1, list2):
+    return [value for value in list1 if value in list2]
+
+
+def union(list1, list2):
+    return list1 + [value for value in list2 if value not in list1]
+
+
+def difference(list1, list2):
+    return [value for value in list1 if value not in list2]
+
+
+def quote(string, sep):
+    if sep in string:
+        string = '"' + string.replace('"', '""') + '"'
+    if "\n" in string:
+        string = string.replace("\n", " ")
+    return string
+
+
+def jvm_heap(cmdline):
+    for s in cmdline.split(" "):
+        if not re.match("-Xmx", s):
+            continue
+        try:
+            val = int(s[4:-1])
+            unit = s[-1].upper()
+            if unit == "M":
+                return val
+            elif unit == "G":
+                return val * 1024
+            elif unit == "K":
+                return val // 1024
+        except ValueError:
+            logger.warning("JVM -Xmx heap specified seems invalid in '%s'", cmdline)
+            return None
+    logger.warning("No JVM heap memory settings specified in '%s'", cmdline)
+    return None
+
+
+def int_memory(string):
+    (val, unit) = string.split(" ")
+    # For decimal separator in some countries
+    val = float(val.replace(",", "."))
+    if unit == "MB":
+        return int(val)
+    elif unit == "GB":
+        return int(val * 1024)
+    elif unit == "KB":
+        return val / 1024
+    elif unit == "bytes":
+        return val / 1024 / 1024
+    return None
+
+
+def dict_add(dict1, dict2):
+    for k in dict2:
+        if k not in dict1:
+            dict1[k] = 0
+        dict1[k] += dict2[k]
+    return dict1
+
+
+def exit_fatal(err_msg, exit_code):
+    logger.fatal(err_msg)
+    print(f"FATAL: {err_msg}", file=sys.stderr)
+    sys.exit(exit_code)
+
+
+def convert_string(value):
+    if not isinstance(value, str):
+        return value
+    if value.lower() in ("yes", "true", "on"):
+        value = True
+    elif value.lower() in ("no", "false", "off"):
+        value = False
+    else:
+        try:
+            value = int(value)
+        except ValueError:
+            try:
+                value = float(value)
+            except ValueError:
+                pass
+    return value
+
+
+def update_json(json_data, categ, subcateg, value):
+    if categ not in json_data:
+        if subcateg is None:
+            json_data[categ] = value
+        else:
+            json_data[categ] = {subcateg: value}
+    elif subcateg is not None:
+        if subcateg in json_data[categ]:
+            json_data[categ][subcateg].update(value)
+        else:
+            json_data[categ][subcateg] = value
+    else:
+        json_data[categ].update(value)
+    return json_data
+
+
+def int_div_ceil(number, divider):
+    return (number + divider - 1) // divider
+
+
+def nbr_pages(sonar_api_json):
+    if "total" in sonar_api_json:
+        return int_div_ceil(sonar_api_json["total"], sonar_api_json["ps"])
+    elif "paging" in sonar_api_json:
+        return int_div_ceil(sonar_api_json["paging"]["total"], sonar_api_json["paging"]["pageSize"])
+    else:
+        return 1
+
+
+@contextlib.contextmanager
+def open_file(file=None, mode="w"):
+    if file and file != "-":
+        logger.debug("Opening file '%s'", file)
+        fd = open(file=file, mode=mode, encoding="utf-8", newline="")
+    else:
+        logger.debug("Writing to stdout")
+        fd = sys.stdout
+    try:
+        yield fd
+    finally:
+        if fd is not sys.stdout:
+            fd.close()
+
+
+def load_json_file(file):
+    with open(file, "r", encoding="utf-8") as fd:
+        return json.loads(fd.read())
+
+
+def search_by_name(endpoint, name, api, returned_field, extra_params=None):
+    params = {"q": name}
+    if extra_params is not None:
+        params.update(extra_params)
+    data = json.loads(endpoint.get(api, params=params).text)
+    for d in data[returned_field]:
+        if d["name"] == name:
+            return d
+    return None
+
+
+def search_by_key(endpoint, key, api, returned_field, extra_params=None):
+    params = {"q": key}
+    if extra_params is not None:
+        params.update(extra_params)
+    data = json.loads(endpoint.get(api, params=params).text)
+    for d in data[returned_field]:
+        if d["key"] == key:
+            return d
+    return None
+
+
+def log_and_exit(response):
+    if response.ok:
+        return
+    tool_msg = f"For request URL {response.request.url}\n"
+    code = response.status_code
+    try:
+        sq_msg = " | ".join([e["msg"] for e in json.loads(response.text)["errors"]])
+    except json.decoder.JSONDecodeError:
+        sq_msg = ""
+
+    if code == HTTPStatus.UNAUTHORIZED:
+        tool_msg += f"HTTP error {code} - Authentication error. Is token valid ?"
+        err_code = options.ERR_SONAR_API_AUTHENTICATION
+    elif code == HTTPStatus.FORBIDDEN:
+        tool_msg += f"HTTP error {code} - Insufficient permissions to perform operation"
+        err_code = options.ERR_SONAR_API_AUTHORIZATION
+    else:
+        tool_msg += f"HTTP error {code} - Exiting"
+        err_code = options.ERR_SONAR_API
+    exit_fatal(f"{tool_msg}: {sq_msg}", err_code)
+
+
+def object_key(key_or_obj):
+    if isinstance(key_or_obj, str):
+        return key_or_obj
+    else:
+        return key_or_obj.key
+
+
+def check_what(what, allowed_values, operation="processed"):
+    if what == "":
+        return allowed_values
+    what = csv_to_list(what)
+    for w in what:
+        if w in allowed_values:
+            continue
+        exit_fatal(
+            f"'{w}' is not something that can be {operation}, chose among {','.join(allowed_values)}",
+            exit_code=options.ERR_ARGS_ERROR,
+        )
+    return what
+
+
+def __prefix(value):
+    if isinstance(value, dict):
+        return {f"_{k}": __prefix(v) for k, v in value.items()}
+    elif isinstance(value, list):
+        return [__prefix(v) for v in value]
+    else:
+        return value
+
+
+def filter_export(json_data, key_properties, full):
+    new_json_data = json_data.copy()
+    for k in json_data:
+        if k not in key_properties:
+            if full and k != "actions":
+                new_json_data[f"_{k}"] = __prefix(new_json_data.pop(k))
+            else:
+                new_json_data.pop(k)
+    return new_json_data
+
+
+def replace_keys(key_list, new_key, data):
+    for k in key_list:
+        if k in data:
+            data[new_key] = data.pop(k)
+    return data
+
+
+def edition_normalize(edition: str) -> Union[str, None]:
+    """Returns the SQ edition in a normalized way (community, developer, enterprise or datacenter)
+
+    :param str edition: The original non normalized edition string
+    :return: The normalized edition string
+    :rtype: str
+    """
+    if edition is None:
+        return None
+    return edition.lower().replace("edition", "").replace(" ", "")
+
+
+def string_to_version(sif_v: str, digits: int = 3, as_string: bool = False) -> Union[str, tuple[int], None]:
+    """Returns the normalized SQ version as string or tuple
+
+    :param str edition: The original non normalized edition string
+    :return: The normalized edition string
+    :rtype: str
+    """
+    if sif_v is None:
+        return None
+
+    split_version = sif_v.split(".")
+    if as_string:
+        return ".".join(split_version[0:digits])
+    else:
+        return tuple(int(n) for n in split_version[0:digits])
```

## sonar/version.py

```diff
@@ -1,20 +1,27 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-PACKAGE_VERSION = "2.8.2"
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+
+"""
+
+    sonar-tools project version
+
+"""
+
+PACKAGE_VERSION = "2.9"
```

## sonar/webhooks.py

 * *Ordering differences only*

```diff
@@ -1,155 +1,155 @@
-#
-# sonar-tools
-# Copyright (C) 2022-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-
-import json
-import sonar.utilities as util
-import sonar.sqobject as sq
-
-from sonar.audit import rules, problem
-
-_IMPORTABLE_PROPERTIES = ("name", "url", "secret")
-
-_OBJECTS = {}
-
-
-class WebHook(sq.SqObject):
-    """
-    Abstraction of the SonarQube "webhook" concept
-    """
-
-    def __init__(self, name, endpoint, url=None, secret=None, project=None, data=None):
-        super().__init__(name, endpoint)
-        if data is None:
-            params = util.remove_nones({"name": name, "url": url, "secret": secret, "project": project})
-            data = json.loads(self.post("webhooks/create", params=params).text)["webhook"]
-        self._json = data
-        self.name = data["name"]  #: Webhook name
-        self.key = data["key"]  #: Webhook key
-        self.webhook_url = data["url"]  #: Webhook URL
-        self.secret = data.get("secret", None)  #: Webhook secret
-        self.project = project  #: Webhook project if project specific webhook
-        self.last_delivery = data.get("latestDelivery", None)
-        _OBJECTS[self.uuid()] = self
-
-    def __str__(self):
-        return f"webhook '{self.name}'"
-
-    def url(self):
-        return f"{self.endpoint.url}/admin/webhooks"
-
-    def uuid(self):
-        """
-        :meta private:
-        """
-        return _uuid(self.name, self.project)
-
-    def update(self, **kwargs):
-        """Updates a webhook with new properties (name, url, secret)
-
-        :param kwargs: dict - "url", "name", "secret" are the looked up keys
-        :return: Nothing
-        """
-        params = util.remove_nones(kwargs)
-        params.update({"webhook": self.key})
-        self.post("webhooks/update", params=params)
-
-    def audit(self):
-        """
-        :meta private:
-        """
-        if self._json["latestDelivery"]["success"]:
-            return []
-        rule = rules.get_rule(rules.RuleId.FAILED_WEBHOOK)
-        return [problem.Problem(rule.type, rule.severity, rule.msg.format(str(self)), concerned_object=self)]
-
-    def to_json(self, full=False):
-        """Exports a Webhook configuration in JSON format
-
-        :param full: Whether to export all properties, including those that can't be set, or not, defaults to False
-        :type full: bool, optional
-        :return: The configuration of the DevOps platform (except secrets)
-        :rtype: dict
-        """
-        return util.filter_export(self._json, _IMPORTABLE_PROPERTIES, full)
-
-
-def search(endpoint, params=None):
-    """Searches webhooks
-
-    :param params: Filters to narrow down the search, can only be "project"
-    :return: List of webhooks
-    :rtype: dict{<key>: <WebHook>}
-    """
-    return sq.search_objects(api="webhooks/list", params=params, returned_field="webhooks", key_field="key", object_class=WebHook, endpoint=endpoint)
-
-
-def get_list(endpoint, project_key=None):
-    util.logger.debug("Getting webhooks for project key %s", str(project_key))
-    params = None
-    if project_key is not None:
-        params = {"project": project_key}
-    return search(endpoint, params)
-
-
-def export(endpoint, project_key=None, full=False):
-    json_data = {}
-    for wb in get_list(endpoint, project_key).values():
-        j = wb.to_json(full)
-        j.pop("name", None)
-        json_data[wb.name] = util.remove_nones(j)
-    return json_data if len(json_data) > 0 else None
-
-
-def create(endpoint, name, url, secret=None, project=None):
-    return WebHook(name, endpoint, url=url, secret=secret, project=project)
-
-
-def update(endpoint, name, **kwargs):
-    project_key = kwargs.pop("project", None)
-    get_list(endpoint, project_key)
-    if _uuid(name, project_key) not in _OBJECTS:
-        create(endpoint, name, kwargs["url"], kwargs["secret"], project=project_key)
-    else:
-        get_object(name, endpoint, project_key=project_key, data=kwargs).update(**kwargs)
-
-
-def get_object(name, endpoint, project_key=None, data=None):
-    util.logger.debug("Getting webhook name %s project key %s data = %s", name, str(project_key), str(data))
-    u = _uuid(name, project_key)
-    if u not in _OBJECTS:
-        _ = WebHook(name=name, endpoint=endpoint, data=data)
-    return _OBJECTS[u]
-
-
-def _uuid(name, project_key):
-    # FIXME: Make UUID really unique
-    p = "" if project_key is None else f":PROJECT:{project_key}"
-    return f"{name}{p}"
-
-
-def audit(endpoint):
-    """
-    :meta private:
-    """
-    util.logger.info("Auditing webhooks")
-    problems = []
-    for wh in search(endpoint=endpoint).values():
-        problems += wh.audit()
-    return problems
+#
+# sonar-tools
+# Copyright (C) 2022-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+
+import json
+import sonar.utilities as util
+import sonar.sqobject as sq
+
+from sonar.audit import rules, problem
+
+_IMPORTABLE_PROPERTIES = ("name", "url", "secret")
+
+_OBJECTS = {}
+
+
+class WebHook(sq.SqObject):
+    """
+    Abstraction of the SonarQube "webhook" concept
+    """
+
+    def __init__(self, name, endpoint, url=None, secret=None, project=None, data=None):
+        super().__init__(name, endpoint)
+        if data is None:
+            params = util.remove_nones({"name": name, "url": url, "secret": secret, "project": project})
+            data = json.loads(self.post("webhooks/create", params=params).text)["webhook"]
+        self._json = data
+        self.name = data["name"]  #: Webhook name
+        self.key = data["key"]  #: Webhook key
+        self.webhook_url = data["url"]  #: Webhook URL
+        self.secret = data.get("secret", None)  #: Webhook secret
+        self.project = project  #: Webhook project if project specific webhook
+        self.last_delivery = data.get("latestDelivery", None)
+        _OBJECTS[self.uuid()] = self
+
+    def __str__(self):
+        return f"webhook '{self.name}'"
+
+    def url(self):
+        return f"{self.endpoint.url}/admin/webhooks"
+
+    def uuid(self):
+        """
+        :meta private:
+        """
+        return _uuid(self.name, self.project)
+
+    def update(self, **kwargs):
+        """Updates a webhook with new properties (name, url, secret)
+
+        :param kwargs: dict - "url", "name", "secret" are the looked up keys
+        :return: Nothing
+        """
+        params = util.remove_nones(kwargs)
+        params.update({"webhook": self.key})
+        self.post("webhooks/update", params=params)
+
+    def audit(self):
+        """
+        :meta private:
+        """
+        if self._json["latestDelivery"]["success"]:
+            return []
+        rule = rules.get_rule(rules.RuleId.FAILED_WEBHOOK)
+        return [problem.Problem(rule.type, rule.severity, rule.msg.format(str(self)), concerned_object=self)]
+
+    def to_json(self, full=False):
+        """Exports a Webhook configuration in JSON format
+
+        :param full: Whether to export all properties, including those that can't be set, or not, defaults to False
+        :type full: bool, optional
+        :return: The configuration of the DevOps platform (except secrets)
+        :rtype: dict
+        """
+        return util.filter_export(self._json, _IMPORTABLE_PROPERTIES, full)
+
+
+def search(endpoint, params=None):
+    """Searches webhooks
+
+    :param params: Filters to narrow down the search, can only be "project"
+    :return: List of webhooks
+    :rtype: dict{<key>: <WebHook>}
+    """
+    return sq.search_objects(api="webhooks/list", params=params, returned_field="webhooks", key_field="key", object_class=WebHook, endpoint=endpoint)
+
+
+def get_list(endpoint, project_key=None):
+    util.logger.debug("Getting webhooks for project key %s", str(project_key))
+    params = None
+    if project_key is not None:
+        params = {"project": project_key}
+    return search(endpoint, params)
+
+
+def export(endpoint, project_key=None, full=False):
+    json_data = {}
+    for wb in get_list(endpoint, project_key).values():
+        j = wb.to_json(full)
+        j.pop("name", None)
+        json_data[wb.name] = util.remove_nones(j)
+    return json_data if len(json_data) > 0 else None
+
+
+def create(endpoint, name, url, secret=None, project=None):
+    return WebHook(name, endpoint, url=url, secret=secret, project=project)
+
+
+def update(endpoint, name, **kwargs):
+    project_key = kwargs.pop("project", None)
+    get_list(endpoint, project_key)
+    if _uuid(name, project_key) not in _OBJECTS:
+        create(endpoint, name, kwargs["url"], kwargs["secret"], project=project_key)
+    else:
+        get_object(name, endpoint, project_key=project_key, data=kwargs).update(**kwargs)
+
+
+def get_object(name, endpoint, project_key=None, data=None):
+    util.logger.debug("Getting webhook name %s project key %s data = %s", name, str(project_key), str(data))
+    u = _uuid(name, project_key)
+    if u not in _OBJECTS:
+        _ = WebHook(name=name, endpoint=endpoint, data=data)
+    return _OBJECTS[u]
+
+
+def _uuid(name, project_key):
+    # FIXME: Make UUID really unique
+    p = "" if project_key is None else f":PROJECT:{project_key}"
+    return f"{name}{p}"
+
+
+def audit(endpoint):
+    """
+    :meta private:
+    """
+    util.logger.info("Auditing webhooks")
+    problems = []
+    for wh in search(endpoint=endpoint).values():
+        problems += wh.audit()
+    return problems
```

## sonar/audit/__init__.py

 * *Ordering differences only*

```diff
@@ -1,27 +1,27 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-import sys
-from sonar.audit import rules
-from sonar import utilities, options
-
-try:
-    rules.load()
-except rules.RuleConfigError as e:
-    utilities.exit_fatal(e.message, options.ERR_RULES_LOADING_FAILED)
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+import sys
+from sonar.audit import rules
+from sonar import utilities, options
+
+try:
+    rules.load()
+except rules.RuleConfigError as e:
+    utilities.exit_fatal(e.message, options.ERR_RULES_LOADING_FAILED)
```

## sonar/audit/config.py

 * *Ordering differences only*

```diff
@@ -1,84 +1,84 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-import os
-import pathlib
-import jprops
-import sonar.utilities as util
-
-_CONFIG_SETTINGS = None
-
-
-def _load_properties_file(file):
-    settings = {}
-    try:
-        with open(file, "r", encoding="utf-8") as fp:
-            util.logger.info("Loading config file %s", file)
-            settings = jprops.load_properties(fp)
-    except FileNotFoundError:
-        pass
-    except PermissionError:
-        util.logger.warning(
-            "Insufficient permissions to open file %s, configuration will be skipped",
-            file,
-        )
-    return settings
-
-
-def load(config_name=None, settings=None):
-    global _CONFIG_SETTINGS
-
-    if settings is None:
-        settings = {}
-
-    default_conf = _load_properties_file(pathlib.Path(__file__).parent / f"{config_name}.properties")
-    home_conf = _load_properties_file(f"{os.path.expanduser('~')}{os.sep}.{config_name}.properties")
-    local_conf = _load_properties_file(f"{os.getcwd()}{os.sep}{config_name}.properties")
-
-    _CONFIG_SETTINGS = {**default_conf, **home_conf, **local_conf, **settings}
-
-    for key, value in _CONFIG_SETTINGS.items():
-        _CONFIG_SETTINGS[key] = util.convert_string(value)
-
-    util.logger.debug("Audit settings = %s", util.json_dump(_CONFIG_SETTINGS))
-    return _CONFIG_SETTINGS
-
-
-def get_property(name, settings=None):
-    if settings is None:
-        settings = _CONFIG_SETTINGS
-    return settings.get(name, "")
-
-
-def configure():
-    template_file = pathlib.Path(__file__).parent / "sonar-audit.properties"
-    with open(template_file, "r", encoding="utf-8") as fh:
-        text = fh.read()
-
-    config_file = f"{os.path.expanduser('~')}{os.sep}.sonar-audit.properties"
-    if os.path.isfile(config_file):
-        util.logger.info(
-            "Config file '%s' already exists, sending configuration to stdout",
-            config_file,
-        )
-        print(text)
-    else:
-        util.logger.info("Creating file '%s'", config_file)
-        with open(config_file, "w", encoding="utf-8") as fh:
-            print(text, file=fh)
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+import os
+import pathlib
+import jprops
+import sonar.utilities as util
+
+_CONFIG_SETTINGS = None
+
+
+def _load_properties_file(file):
+    settings = {}
+    try:
+        with open(file, "r", encoding="utf-8") as fp:
+            util.logger.info("Loading config file %s", file)
+            settings = jprops.load_properties(fp)
+    except FileNotFoundError:
+        pass
+    except PermissionError:
+        util.logger.warning(
+            "Insufficient permissions to open file %s, configuration will be skipped",
+            file,
+        )
+    return settings
+
+
+def load(config_name=None, settings=None):
+    global _CONFIG_SETTINGS
+
+    if settings is None:
+        settings = {}
+
+    default_conf = _load_properties_file(pathlib.Path(__file__).parent / f"{config_name}.properties")
+    home_conf = _load_properties_file(f"{os.path.expanduser('~')}{os.sep}.{config_name}.properties")
+    local_conf = _load_properties_file(f"{os.getcwd()}{os.sep}{config_name}.properties")
+
+    _CONFIG_SETTINGS = {**default_conf, **home_conf, **local_conf, **settings}
+
+    for key, value in _CONFIG_SETTINGS.items():
+        _CONFIG_SETTINGS[key] = util.convert_string(value)
+
+    util.logger.debug("Audit settings = %s", util.json_dump(_CONFIG_SETTINGS))
+    return _CONFIG_SETTINGS
+
+
+def get_property(name, settings=None):
+    if settings is None:
+        settings = _CONFIG_SETTINGS
+    return settings.get(name, "")
+
+
+def configure():
+    template_file = pathlib.Path(__file__).parent / "sonar-audit.properties"
+    with open(template_file, "r", encoding="utf-8") as fh:
+        text = fh.read()
+
+    config_file = f"{os.path.expanduser('~')}{os.sep}.sonar-audit.properties"
+    if os.path.isfile(config_file):
+        util.logger.info(
+            "Config file '%s' already exists, sending configuration to stdout",
+            config_file,
+        )
+        print(text)
+    else:
+        util.logger.info("Creating file '%s'", config_file)
+        with open(config_file, "w", encoding="utf-8") as fh:
+            print(text, file=fh)
```

## sonar/audit/problem.py

 * *Ordering differences only*

```diff
@@ -1,68 +1,68 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-
-import csv
-from sonar import utilities, options
-
-
-class Problem:
-    def __init__(self, problem_type, severity, msg, concerned_object=None):
-        # dict.__init__(type=problem_type, severity=severity, message=msg)
-        self.concerned_object = concerned_object
-        self.type = problem_type
-        self.severity = severity
-        self.message = msg
-        utilities.logger.warning(msg)
-
-    def __str__(self):
-        return f"Type: {self.type} - Severity: {self.severity} - Description: {self.message}"
-
-    def to_json(self, with_url=False):
-        d = vars(self).copy()
-        d.pop("concerned_object")
-        for k in ("severity", "type"):
-            d[k] = str(d[k])
-        if with_url:
-            try:
-                d["url"] = self.concerned_object.url()
-            except AttributeError:
-                d["url"] = str(self.concerned_object)
-        return d
-
-
-def dump_report(problems, file, **kwargs):
-    utilities.logger.info("Writing report to %s", f"file '{file}'" if file else "stdout")
-    if kwargs.get("format", "csv") == "json":
-        __dump_json(problems=problems, file=file, **kwargs)
-    else:
-        __dump_csv(problems=problems, file=file, **kwargs)
-
-
-def __dump_csv(problems, file, **kwargs):
-    with utilities.open_file(file, "w") as fd:
-        csvwriter = csv.writer(fd, delimiter=kwargs.get("separator", ","))
-        for p in problems:
-            csvwriter.writerow(list(p.to_json(kwargs.get(options.WITH_URL, False)).values()))
-
-
-def __dump_json(problems, file, **kwargs):
-    json = [p.to_json(kwargs.get(options.WITH_URL, False)) for p in problems]
-    with utilities.open_file(file) as fd:
-        print(utilities.json_dump(json), file=fd)
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+
+import csv
+from sonar import utilities, options
+
+
+class Problem:
+    def __init__(self, problem_type, severity, msg, concerned_object=None):
+        # dict.__init__(type=problem_type, severity=severity, message=msg)
+        self.concerned_object = concerned_object
+        self.type = problem_type
+        self.severity = severity
+        self.message = msg
+        utilities.logger.warning(msg)
+
+    def __str__(self):
+        return f"Type: {self.type} - Severity: {self.severity} - Description: {self.message}"
+
+    def to_json(self, with_url=False):
+        d = vars(self).copy()
+        d.pop("concerned_object")
+        for k in ("severity", "type"):
+            d[k] = str(d[k])
+        if with_url:
+            try:
+                d["url"] = self.concerned_object.url()
+            except AttributeError:
+                d["url"] = str(self.concerned_object)
+        return d
+
+
+def dump_report(problems, file, **kwargs):
+    utilities.logger.info("Writing report to %s", f"file '{file}'" if file else "stdout")
+    if kwargs.get("format", "csv") == "json":
+        __dump_json(problems=problems, file=file, **kwargs)
+    else:
+        __dump_csv(problems=problems, file=file, **kwargs)
+
+
+def __dump_csv(problems, file, **kwargs):
+    with utilities.open_file(file, "w") as fd:
+        csvwriter = csv.writer(fd, delimiter=kwargs.get("separator", ","))
+        for p in problems:
+            csvwriter.writerow(list(p.to_json(kwargs.get(options.WITH_URL, False)).values()))
+
+
+def __dump_json(problems, file, **kwargs):
+    json = [p.to_json(kwargs.get(options.WITH_URL, False)) for p in problems]
+    with utilities.open_file(file) as fd:
+        print(utilities.json_dump(json), file=fd)
```

## sonar/audit/rules.json

### Pretty-printed

 * *Similarity: 0.9486386138613861%*

 * *Differences: {"'BRANCH_LAST_ANALYSIS'": "{'severity': 'LOW', 'object': 'Branch', 'message': '{} is stale, has "*

 * *                           "not been analyzed since {} days, it could be deleted'}",*

 * * "'BRANCH_NEVER_ANALYZED'": "OrderedDict([('severity', 'LOW'), ('type', 'PERFORMANCE'), ('object', "*

 * *                            "'Branch'), ('message', '{} has never been analyzed and is kept even "*

 * *                            "if inactive, this is suspicious')])",*

 * * "'PROJ_SCM_UNDETECTED'": "OrderedDict([('severity', 'HIGH'), […]*

```diff
@@ -48,17 +48,23 @@
     },
     "BG_TASK_FAILED": {
         "message": "{} last background task FAILED",
         "severity": "MEDIUM",
         "type": "OPERATIONS"
     },
     "BRANCH_LAST_ANALYSIS": {
-        "message": "{} is stale, it has not been analyzed since {} days, it could be deleted",
-        "object": "Project",
-        "severity": "MEDIUM",
+        "message": "{} is stale, has not been analyzed since {} days, it could be deleted",
+        "object": "Branch",
+        "severity": "LOW",
+        "type": "PERFORMANCE"
+    },
+    "BRANCH_NEVER_ANALYZED": {
+        "message": "{} has never been analyzed and is kept even if inactive, this is suspicious",
+        "object": "Branch",
+        "severity": "LOW",
         "type": "PERFORMANCE"
     },
     "DCE_APP_CLUSTER_NOT_HA": {
         "message": "Only 1 app node is running, cluster will go down if this app node goes down",
         "severity": "CRITICAL",
         "type": "OPERATIONS"
     },
@@ -287,14 +293,20 @@
     },
     "PROJ_SCM_DISABLED": {
         "message": "{} has SCM integration disabled, issue timestamping may not be accurate for these project",
         "object": "Project",
         "severity": "MEDIUM",
         "type": "GOVERNANCE"
     },
+    "PROJ_SCM_UNDETECTED": {
+        "message": "{} SCM detection failed, issue timestamping may not be accurate for these project",
+        "object": "Project",
+        "severity": "HIGH",
+        "type": "GOVERNANCE"
+    },
     "PROJ_SUSPICIOUS_EXCLUSION": {
         "message": "{} has suspicious exclusions '{}', please review analysis properties",
         "object": "Project",
         "severity": "MEDIUM",
         "type": "GOVERNANCE"
     },
     "PROJ_UTILITY_LOCS": {
@@ -494,14 +506,24 @@
         "type": "PERFORMANCE"
     },
     "SETTING_WEB_NO_HEAP": {
         "message": "Web process heap ('sonar.web.javaOpts' -Xmx) memory setting is not specified",
         "severity": "HIGH",
         "type": "PERFORMANCE"
     },
+    "SETTING_WEB_WRONG_JAVA_VERSION": {
+        "message": "SonarQube {} is running on Java {} which is not a supported Java version",
+        "severity": "HIGH",
+        "type": "OPERATIONS"
+    },
+    "SIF_UNDETECTED_SCM": {
+        "message": "{} projects with undetected SCM, issue timestamping may not be accurate for those projects",
+        "severity": "LOW",
+        "type": "GOVERNANCE"
+    },
     "SONAR_USERS_WITH_ELEVATED_PERMS": {
         "message": "Group 'sonar-users' has global admin, admin QG, admin QP or create project permissions, this is a critical security risk",
         "severity": "CRITICAL",
         "type": "GOVERNANCE"
     },
     "TOKEN_NEVER_USED": {
         "message": "{} has been created {} days ago but never used, it should be revoked",
@@ -517,19 +539,14 @@
     },
     "TOKEN_UNUSED": {
         "message": "{} is not used since {} days, it should be revoked",
         "object": "Token",
         "severity": "MEDIUM",
         "type": "SECURITY"
     },
-    "UNDETECTED_SCM": {
-        "message": "{} projects with undetected SCM, issue timestamping may not be accurate for those projects",
-        "severity": "LOW",
-        "type": "GOVERNANCE"
-    },
     "USER_UNUSED": {
         "message": "{} did not connect since {} days, it should be deactivated",
         "object": "User",
         "severity": "MEDIUM",
         "type": "SECURITY"
     }
 }
```

## sonar/audit/rules.py

```diff
@@ -1,207 +1,211 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-import enum
-import json
-from sonar.audit import severities, types
-import sonar.utilities as util
-
-__RULES = {}
-
-
-class RuleId(enum.Enum):
-    DEFAULT_ADMIN_PASSWORD = 1
-    BELOW_LTS = 2
-    LOG4SHELL_WEB = 3
-    LOG4SHELL_CE = 4
-    LOG4SHELL_ES = 5
-    BELOW_LATEST = 6
-    LTS_PATCH_MISSING = 7
-
-    SETTING_FORCE_AUTH = 100
-    SETTING_PROJ_DEFAULT_VISIBILITY = 101
-    SETTING_CPD_CROSS_PROJECT = 102
-
-    SETTING_NOT_SET = 110
-    SETTING_SET = 111
-    SETTING_VALUE_INCORRECT = 112
-    SETTING_VALUE_OUT_OF_RANGE = 113
-
-    SETTING_BASE_URL = 120
-    SETTING_DB_CLEANER = 121
-    SETTING_MAINT_GRID = 122
-    SETTING_SLB_RETENTION = 123
-    SETTING_TD_LOC_COST = 124
-
-    SETTING_WEB_HEAP = 130
-    SETTING_ES_HEAP = 131
-    SETTING_CE_HEAP = 132
-    SETTING_CE_TOO_MANY_WORKERS = 133
-    SETTING_JDBC_URL_NOT_SET = 134
-    SETTING_DB_ON_SAME_HOST = 135
-
-    SETTING_WEB_NO_HEAP = 140
-    SETTING_ES_NO_HEAP = 141
-    SETTING_CE_NO_HEAP = 142
-
-    ANYONE_WITH_GLOBAL_PERMS = 150
-    SONAR_USERS_WITH_ELEVATED_PERMS = 151
-    FAILED_WEBHOOK = 152
-
-    DCE_DIFFERENT_APP_NODES_VERSIONS = 160
-    DCE_DIFFERENT_APP_NODES_PLUGINS = 161
-    DCE_APP_NODE_UNOFFICIAL_DISTRO = 162
-    DCE_APP_NODE_NOT_GREEN = 163
-    DCE_APP_CLUSTER_NOT_HA = 164
-    DCE_ES_CLUSTER_NOT_HA = 170
-    DCE_ES_UNBALANCED_INDEX = 171
-    DCE_ES_INDEX_EMPTY = 172
-    DCE_ES_CLUSTER_WRONG_NUMBER_OF_NODES = 173
-    DCE_ES_CLUSTER_EVEN_NUMBER_OF_NODES = 174
-
-    BACKGROUND_TASKS_FAILURE_RATE_HIGH = 200
-    BACKGROUND_TASKS_PENDING_QUEUE_LONG = 201
-    BACKGROUND_TASKS_PENDING_QUEUE_VERY_LONG = 202
-
-    PROJ_LAST_ANALYSIS = 1000
-    PROJ_NOT_ANALYZED = 1001
-    PROJ_VISIBILITY = 1002
-    PROJ_DUPLICATE = 1003
-
-    BRANCH_LAST_ANALYSIS = 1020
-    PULL_REQUEST_LAST_ANALYSIS = 1030
-
-    PROJ_PERM_MAX_USERS = 1100
-    PROJ_PERM_MAX_ADM_USERS = 1101
-    PROJ_PERM_MAX_ISSUE_ADM_USERS = 1102
-    PROJ_PERM_MAX_HOTSPOT_ADM_USERS = 1103
-    PROJ_PERM_MAX_SCAN_USERS = 1104
-
-    PROJ_PERM_MAX_GROUPS = 1200
-    PROJ_PERM_MAX_ADM_GROUPS = 1201
-    PROJ_PERM_MAX_ISSUE_ADM_GROUPS = 1202
-    PROJ_PERM_MAX_HOTSPOT_ADM_GROUPS = 1203
-    PROJ_PERM_MAX_SCAN_GROUPS = 1204
-    PROJ_PERM_SONAR_USERS_ELEVATED_PERMS = 1205
-    PROJ_PERM_ANYONE = 1206
-
-    PROJ_UTILITY_LOCS = 1300
-    PROJ_SUSPICIOUS_EXCLUSION = 1301
-    PROJ_DUPLICATE_BINDING = 1302
-    PROJ_INVALID_BINDING = 1303
-    PROJ_ZERO_LOC = 1304
-    PROJ_SCM_DISABLED = 1305
-    PROJ_MAIN_AND_MASTER = 1306
-    PROJ_ANALYSIS_WARNING = 1307
-    BG_TASK_FAILED = 1308
-    OBSOLETE_SCANNER = 1309
-    NOT_LATEST_SCANNER = 1310
-    ANT_SCANNER_DEPRECATED = 1311
-
-    NOT_USING_BRANCH_ANALYSIS = 1400
-    UNDETECTED_SCM = 1401
-
-    QG_NO_COND = 2000
-    QG_TOO_MANY_COND = 2001
-    QG_NOT_USED = 2002
-    QG_TOO_MANY_GATES = 2003
-    QG_WRONG_METRIC = 2004
-    QG_WRONG_THRESHOLD = 2005
-
-    QP_TOO_MANY_QP = 3000
-    QP_LAST_USED_DATE = 3001
-    QP_LAST_CHANGE_DATE = 3002
-    QP_TOO_FEW_RULES = 3003
-    QP_NOT_USED = 3004
-    QP_USE_DEPRECATED_RULES = 3005
-
-    TOKEN_TOO_OLD = 4000
-    TOKEN_UNUSED = 4001
-    TOKEN_NEVER_USED = 4002
-    USER_UNUSED = 4010
-
-    PORTFOLIO_EMPTY = 5000
-    PORTFOLIO_SINGLETON = 5001
-
-    APPLICATION_EMPTY = 5100
-    APPLICATION_SINGLETON = 5101
-
-    GROUP_EMPTY = 5200
-
-    def __str__(self):
-        return repr(self.name)[1:-1]
-
-
-class RuleConfigError(Exception):
-    def __init__(self, message):
-        super().__init__()
-        self.message = message
-
-
-class Rule:
-    def __init__(self, rule_id, severity, rule_type, concerned_object, message):
-        self.id = to_id(rule_id)
-        self.severity = severities.to_severity(severity)
-        self.type = types.to_type(rule_type)
-        self.object = concerned_object
-        self.msg = message
-
-
-def to_id(val):
-    for enum_val in RuleId:
-        if repr(enum_val.name)[1:-1] == val:
-            return enum_val
-    return None
-
-
-def load():
-    global __RULES
-    import pathlib
-
-    util.logger.info("Loading audit rules")
-    path = pathlib.Path(__file__).parent
-    with open(path / "rules.json", "r", encoding="utf-8") as rulefile:
-        rules = json.loads(rulefile.read())
-    rulefile.close()
-    __RULES = {}
-    for rule_id, rule in rules.items():
-        if to_id(rule_id) is None:
-            raise RuleConfigError(f"Rule '{rule_id}' from rules.json is not a legit ruleId")
-        if types.to_type(rule.get("type", "")) is None:
-            raise RuleConfigError(f"Rule '{rule_id}' from rules.json has no or incorrect type")
-        if severities.to_severity(rule.get("severity", "")) is None:
-            raise RuleConfigError(f"Rule '{rule_id}' from rules.json has no or incorrect severity")
-        if "message" not in rule:
-            raise RuleConfigError(f"Rule '{rule_id}' from rules.json has no message defined'")
-        __RULES[to_id(rule_id)] = Rule(
-            rule_id,
-            rule["severity"],
-            rule["type"],
-            rule.get("object", ""),
-            rule["message"],
-        )
-
-    # Cross check that all rule Ids are defined in the JSON
-    for rule in RuleId:
-        if rule not in __RULES:
-            raise RuleConfigError(f"Rule {rule} has no configuration defined in 'rules.json'")
-
-
-def get_rule(rule_id):
-    return __RULES[rule_id]
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+import enum
+import json
+from sonar.audit import severities, types
+import sonar.utilities as util
+
+__RULES = {}
+
+
+class RuleId(enum.Enum):
+    DEFAULT_ADMIN_PASSWORD = 1
+    BELOW_LTS = 2
+    LOG4SHELL_WEB = 3
+    LOG4SHELL_CE = 4
+    LOG4SHELL_ES = 5
+    BELOW_LATEST = 6
+    LTS_PATCH_MISSING = 7
+
+    SETTING_FORCE_AUTH = 100
+    SETTING_PROJ_DEFAULT_VISIBILITY = 101
+    SETTING_CPD_CROSS_PROJECT = 102
+
+    SETTING_NOT_SET = 110
+    SETTING_SET = 111
+    SETTING_VALUE_INCORRECT = 112
+    SETTING_VALUE_OUT_OF_RANGE = 113
+
+    SETTING_BASE_URL = 120
+    SETTING_DB_CLEANER = 121
+    SETTING_MAINT_GRID = 122
+    SETTING_SLB_RETENTION = 123
+    SETTING_TD_LOC_COST = 124
+
+    SETTING_WEB_HEAP = 130
+    SETTING_ES_HEAP = 131
+    SETTING_CE_HEAP = 132
+    SETTING_CE_TOO_MANY_WORKERS = 133
+    SETTING_JDBC_URL_NOT_SET = 134
+    SETTING_DB_ON_SAME_HOST = 135
+
+    SETTING_WEB_NO_HEAP = 140
+    SETTING_ES_NO_HEAP = 141
+    SETTING_CE_NO_HEAP = 142
+    SETTING_WEB_WRONG_JAVA_VERSION = 143
+
+    ANYONE_WITH_GLOBAL_PERMS = 150
+    SONAR_USERS_WITH_ELEVATED_PERMS = 151
+    FAILED_WEBHOOK = 152
+
+    DCE_DIFFERENT_APP_NODES_VERSIONS = 160
+    DCE_DIFFERENT_APP_NODES_PLUGINS = 161
+    DCE_APP_NODE_UNOFFICIAL_DISTRO = 162
+    DCE_APP_NODE_NOT_GREEN = 163
+    DCE_APP_CLUSTER_NOT_HA = 164
+    DCE_ES_CLUSTER_NOT_HA = 170
+    DCE_ES_UNBALANCED_INDEX = 171
+    DCE_ES_INDEX_EMPTY = 172
+    DCE_ES_CLUSTER_WRONG_NUMBER_OF_NODES = 173
+    DCE_ES_CLUSTER_EVEN_NUMBER_OF_NODES = 174
+
+    BACKGROUND_TASKS_FAILURE_RATE_HIGH = 200
+    BACKGROUND_TASKS_PENDING_QUEUE_LONG = 201
+    BACKGROUND_TASKS_PENDING_QUEUE_VERY_LONG = 202
+
+    PROJ_LAST_ANALYSIS = 1000
+    PROJ_NOT_ANALYZED = 1001
+    PROJ_VISIBILITY = 1002
+    PROJ_DUPLICATE = 1003
+
+    BRANCH_LAST_ANALYSIS = 1020
+    PULL_REQUEST_LAST_ANALYSIS = 1030
+
+    PROJ_PERM_MAX_USERS = 1100
+    PROJ_PERM_MAX_ADM_USERS = 1101
+    PROJ_PERM_MAX_ISSUE_ADM_USERS = 1102
+    PROJ_PERM_MAX_HOTSPOT_ADM_USERS = 1103
+    PROJ_PERM_MAX_SCAN_USERS = 1104
+
+    PROJ_PERM_MAX_GROUPS = 1200
+    PROJ_PERM_MAX_ADM_GROUPS = 1201
+    PROJ_PERM_MAX_ISSUE_ADM_GROUPS = 1202
+    PROJ_PERM_MAX_HOTSPOT_ADM_GROUPS = 1203
+    PROJ_PERM_MAX_SCAN_GROUPS = 1204
+    PROJ_PERM_SONAR_USERS_ELEVATED_PERMS = 1205
+    PROJ_PERM_ANYONE = 1206
+
+    PROJ_UTILITY_LOCS = 1300
+    PROJ_SUSPICIOUS_EXCLUSION = 1301
+    PROJ_DUPLICATE_BINDING = 1302
+    PROJ_INVALID_BINDING = 1303
+    PROJ_ZERO_LOC = 1304
+    PROJ_SCM_DISABLED = 1305
+    PROJ_MAIN_AND_MASTER = 1306
+    PROJ_ANALYSIS_WARNING = 1307
+    BG_TASK_FAILED = 1308
+    OBSOLETE_SCANNER = 1309
+    NOT_LATEST_SCANNER = 1310
+    ANT_SCANNER_DEPRECATED = 1311
+    PROJ_SCM_UNDETECTED = 1312
+
+    NOT_USING_BRANCH_ANALYSIS = 1400
+    SIF_UNDETECTED_SCM = 1401
+
+    BRANCH_NEVER_ANALYZED = 1600
+
+    QG_NO_COND = 2000
+    QG_TOO_MANY_COND = 2001
+    QG_NOT_USED = 2002
+    QG_TOO_MANY_GATES = 2003
+    QG_WRONG_METRIC = 2004
+    QG_WRONG_THRESHOLD = 2005
+
+    QP_TOO_MANY_QP = 3000
+    QP_LAST_USED_DATE = 3001
+    QP_LAST_CHANGE_DATE = 3002
+    QP_TOO_FEW_RULES = 3003
+    QP_NOT_USED = 3004
+    QP_USE_DEPRECATED_RULES = 3005
+
+    TOKEN_TOO_OLD = 4000
+    TOKEN_UNUSED = 4001
+    TOKEN_NEVER_USED = 4002
+    USER_UNUSED = 4010
+
+    PORTFOLIO_EMPTY = 5000
+    PORTFOLIO_SINGLETON = 5001
+
+    APPLICATION_EMPTY = 5100
+    APPLICATION_SINGLETON = 5101
+
+    GROUP_EMPTY = 5200
+
+    def __str__(self):
+        return repr(self.name)[1:-1]
+
+
+class RuleConfigError(Exception):
+    def __init__(self, message):
+        super().__init__()
+        self.message = message
+
+
+class Rule:
+    def __init__(self, rule_id, severity, rule_type, concerned_object, message):
+        self.id = to_id(rule_id)
+        self.severity = severities.to_severity(severity)
+        self.type = types.to_type(rule_type)
+        self.object = concerned_object
+        self.msg = message
+
+
+def to_id(val):
+    for enum_val in RuleId:
+        if repr(enum_val.name)[1:-1] == val:
+            return enum_val
+    return None
+
+
+def load():
+    global __RULES
+    import pathlib
+
+    util.logger.info("Loading audit rules")
+    path = pathlib.Path(__file__).parent
+    with open(path / "rules.json", "r", encoding="utf-8") as rulefile:
+        rules = json.loads(rulefile.read())
+    rulefile.close()
+    __RULES = {}
+    for rule_id, rule in rules.items():
+        if to_id(rule_id) is None:
+            raise RuleConfigError(f"Rule '{rule_id}' from rules.json is not a legit ruleId")
+        if types.to_type(rule.get("type", "")) is None:
+            raise RuleConfigError(f"Rule '{rule_id}' from rules.json has no or incorrect type")
+        if severities.to_severity(rule.get("severity", "")) is None:
+            raise RuleConfigError(f"Rule '{rule_id}' from rules.json has no or incorrect severity")
+        if "message" not in rule:
+            raise RuleConfigError(f"Rule '{rule_id}' from rules.json has no message defined'")
+        __RULES[to_id(rule_id)] = Rule(
+            rule_id,
+            rule["severity"],
+            rule["type"],
+            rule.get("object", ""),
+            rule["message"],
+        )
+
+    # Cross check that all rule Ids are defined in the JSON
+    for rule in RuleId:
+        if rule not in __RULES:
+            raise RuleConfigError(f"Rule {rule} has no configuration defined in 'rules.json'")
+
+
+def get_rule(rule_id):
+    return __RULES[rule_id]
```

## sonar/audit/severities.py

 * *Ordering differences only*

```diff
@@ -1,37 +1,37 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-import enum
-
-
-class Severity(enum.Enum):
-    CRITICAL = 1
-    HIGH = 2
-    MEDIUM = 3
-    LOW = 4
-
-    def __str__(self):
-        return repr(self.name)[1:-1]
-
-
-def to_severity(val):
-    for enum_val in Severity:
-        if repr(enum_val.name)[1:-1] == val:
-            return enum_val
-    return None
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+import enum
+
+
+class Severity(enum.Enum):
+    CRITICAL = 1
+    HIGH = 2
+    MEDIUM = 3
+    LOW = 4
+
+    def __str__(self):
+        return repr(self.name)[1:-1]
+
+
+def to_severity(val):
+    for enum_val in Severity:
+        if repr(enum_val.name)[1:-1] == val:
+            return enum_val
+    return None
```

## sonar/audit/sonar-audit.properties

```diff
@@ -1,292 +1,295 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-#======================== SELECT AUDIT SCOPE CONFIGURATION =======================
-
-# yes/no
-audit.globalSettings = yes
-audit.projects = yes
-audit.qualityGates = yes
-audit.qualityProfiles = yes
-audit.users = yes
-audit.groups = yes
-# Portfolios and application audit (for DE (Apps) and EE, DCE (Apps, and Portfolios))
-audit.portfolios = yes
-audit.applications = yes
-
-#===================== GLOBAL SETTINGS AUDIT CONFIGURATION ====================
-
-#----------------------- GLOBAL SETTINGS CONFIGURATION ------------------------
-
-# Audit (and warn) for default project public visibility
-audit.globalSettings.defaultProjectVisibility = private
-
-# Below settings audit structure is:
-# For allowed range settings:
-# audit.globalSettings.range.<auditSequenceNbr> = <setting>, <minAllowedValue>, <maxAllowedValue>, <auditSeverity>, <impactedArea>
-# For allowed value settings:
-# audit.globalSetting.value.<auditSequenceNbr> = <setting>, <allowedValue>, <auditSeverity>, <impactedArea>
-# To check if a setting is set:
-# audit.globalSettings.isSet.<auditSequenceNbr> = <setting>, <auditSeverity>, <impactedArea>
-
-# Audit (and warn) if cross project duplication is enabled
-audit.globalSettings.value.1 = sonar.cpd.cross_project, false, HIGH, PERFORMANCE
-
-# Audit (and warn) if force authentication is disabled
-audit.globalSettings.value.2 = sonar.forceAuthentication, true, HIGH, SECURITY
-
-# Audit (and warn) if server base URL is not set
-audit.globalSettings.isSet.1 = sonar.core.serverBaseURL, HIGH, OPERATIONS
-
-#----------------------- DB CLEANER AUDIT CONFIGURATION -----------------------
-
-# Audit (and warn) for suspicious DB cleaner settings
-audit.globalSettings.dbcleaner = yes
-
-# Audit DB Cleaner min/max time before purging issues
-audit.globalSettings.range.1 = sonar.dbcleaner.daysBeforeDeletingClosedIssues, 10, 60, MEDIUM, PERFORMANCE
-
-# Audit DB Cleaner min/max time before only keeping one analysis snapshot per day
-audit.globalSettings.range.2 = sonar.dbcleaner.hoursBeforeKeepingOnlyOneSnapshotByDay, 12, 240, MEDIUM, PERFORMANCE
-
-# Audit DB Cleaner min/max time before only keeping one analysis snapshot per week
-audit.globalSettings.range.3 = sonar.dbcleaner.weeksBeforeKeepingOnlyOneSnapshotByWeek, 2, 12, MEDIUM, PERFORMANCE
-
-# Audit DB Cleaner min/max time before only keeping one analysis snapshot per month
-audit.globalSettings.range.4 = sonar.dbcleaner.weeksBeforeKeepingOnlyOneSnapshotByMonth, 26, 104, MEDIUM, PERFORMANCE
-
-# Audit DB Cleaner min/max time before deleting all snapshots
-audit.globalSettings.range.5 = sonar.dbcleaner.weeksBeforeDeletingAllSnapshots, 104, 260, MEDIUM, PERFORMANCE
-
-# Audit DB Cleaner min/max time before deleting inactive branches (and PRs)
-audit.globalSettings.range.6 = sonar.dbcleaner.daysBeforeDeletingInactiveBranches, 10, 60, MEDIUM, PERFORMANCE
-audit.globalSettings.range.7 = sonar.dbcleaner.daysBeforeDeletingInactiveBranchesAndPRs, 10, 60, MEDIUM, PERFORMANCE
-
-
-#------------------- TECH DEBT SETTINGS AUDIT CONFIGURATION -------------------
-
-# Audit for suspicious technical debt thresholds, listed further below
-audit.globalSettings.technicalDebt = yes
-
-# Audit if dev cost of 1 line is not within expected range (affects Tech Debt ratio and Maintainability rating metrics)
-audit.globalSettings.range.7 = sonar.technicalDebt.developmentCost, 20, 30, MEDIUM, CONFIGURATION
-
-# Audit if maintainaibility rating thresholds are not within normal ranges
-audit.globalSettings.maintainabilityRating.A.range.1 = 0.03, 0.05, MEDIUM, CONFIGURATION
-audit.globalSettings.maintainabilityRating.A.range.2 = 0.02, 0.07, HIGH, CONFIGURATION
-audit.globalSettings.maintainabilityRating.B.range.1 = 0.07, 0.10, MEDIUM, CONFIGURATION
-audit.globalSettings.maintainabilityRating.B.range.2 = 0.05, 0.15, HIGH, CONFIGURATION
-audit.globalSettings.maintainabilityRating.C.range.1 = 0.15, 0.20, MEDIUM, CONFIGURATION
-audit.globalSettings.maintainabilityRating.C.range.2 = 0.10, 0.25, HIGH, CONFIGURATION
-audit.globalSettings.maintainabilityRating.D.range.1 = 0.40, 0.50, MEDIUM, CONFIGURATION
-audit.globalSettings.maintainabilityRating.D.range.2 = 0.30, 0.60, HIGH, CONFIGURATION
-
-# Min max heap allocated to the web process
-audit.web.heapMin = 1024
-audit.web.heapMax = 2048
-
-#======================= PERMISSIONS AUDIT CONFIGURATION ======================
-
-#----------------------------- GLOBAL PERMISSIONS -----------------------------
-# Audit (and warn) for suspicious global permissions
-audit.globalSettings.permissions = yes
-
-# Max allowed number of users/groups with global admin permission
-audit.globalSettings.permissions.maxAdminUsers = 3
-audit.globalSettings.permissions.maxAdminGroups = 2
-
-# Max allowed number of users/groups with quality gate admin permission
-audit.globalSettings.permissions.maxGateAdminUsers = 3
-audit.globalSettings.permissions.maxGateAdminGroups = 2
-
-# Max allowed number of users/groups with quality profile admin permission
-audit.globalSettings.permissions.maxProfileAdminUsers = 3
-audit.globalSettings.permissions.maxProfileAdminGroups = 2
-
-# Max allowed number of users/groups with execute analysis permission
-audit.globalSettings.permissions.maxScanUsers = 3
-audit.globalSettings.permissions.maxScanGroups = 2
-
-# Max allowed number of users/groups with create project permission
-audit.globalSettings.permissions.maxCreateProjectUsers = 3
-audit.globalSettings.permissions.maxCreateProjectGroups = 3
-
-#----------------------------- PROJECT PERMISSIONS ----------------------------
-# Project permission audit
-# Max sure there are not too many users/groups with given project permissions
-audit.projects.permissions.maxUsers = 5
-audit.projects.permissions.maxAdminUsers = 2
-audit.projects.permissions.maxGroups = 5
-audit.projects.permissions.maxAdminGroups = 2
-audit.projects.permissions.maxScanGroups = 1
-audit.projects.permissions.maxIssueAdminGroups = 2
-audit.projects.permissions.maxHotspotAdminGroups = 2
-# audit.projects.permissions.anyone = yes
-
-#========================= PROJECT AUDIT CONFIGURATION ========================
-
-# Audit and warn) for projects likely to be duplicates
-# Duplicate projects are detected from project keys that are similar
-audit.projects.duplicates = yes
-
-# Audit and warn) for projects that have been provisioned but never analyzed
-audit.projects.neverAnalyzed = yes
-
-# Audit (and warn) if project visibility is public
-audit.projects.visibility = yes
-
-# Audit (and warn) for suspicious projects permissions
-audit.projects.permissions = yes
-
-# Audit (and warn) for suspicious projects exclusions
-audit.projects.exclusions = yes
-# In the below:
-# - All * . and ? symbols that relate to the SonarQube exclusion pattern should be escaped with \\
-# - All symbols that have a special meaning for regex pattern matching shall not be escaped
-audit.projects.suspiciousExclusionsPatterns = \\*\\*/[^\/]+/\\*\\*, \\*\\*/\\*[\.\w]*, \\*\\*/\\*, \\*\\*/\\*\\.(java|jav|cs|csx|py|php|js|ts|sql|html|css|cpp|c|h|hpp)\\*?
-audit.projects.suspiciousExclusionsExceptions = \\*\\*/(__pycache__|libs|lib|vendor|node_modules)/\\*\\*
-
-# Audit (and warn) for projects whose last analysis date is older than maxLastAnalysisAge
-# Set property to 0 to turn off the check
-audit.projects.maxLastAnalysisAge = 180
-
-# Audit branches for zero LoC and last analysis date
-audit.projects.branches = yes
-
-# Audits for branches whose last analysis is older than a given number of days
-# This parameter is only considered for branches not marked as "keep when inactive" 
-# Set property to 0 to turn off the check
-audit.projects.branches.maxLastAnalysisAge = 30
-
-# Audits for PR whose last analysis is older than a given number of days
-# Set property to 0 to turn off the check
-audit.projects.pullRequests.maxLastAnalysisAge = 30
-
-# Audits duplicate projects bound to same DevOps platform repo
-audit.projects.bindings = yes
-
-# Audits that projects with bindings have valid bindings
-# Off by default since each project binding validation takes 1 to 3 seconds by project (with ALM bindings)
-# which can be too time consuming for platform with large number of bound projects
-audit.projects.bindings.validation = no
-
-# Audits projects for disabled SCM
-audit.project.scm.disabled = yes
-
-# Audits projects with suspiciously high proportion of utility LoCs (XML, JSON...)
-audit.projects.utilityLocs = yes
-
-# Audits projects for analysis warnings
-audit.projects.analysisWarnings = yes
-
-# Audits projects with last background task failed
-audit.projects.failedTasks = yes
-
-# Audits projects analyzed with too old scanner version, 2 years by default
-audit.projects.scannerMaxAge = 730
-
-#====================== QUALITY GATES AUDIT CONFIGURATION =====================
-
-# Audit that there are not too many quality gates, this defeats company common governance
-audit.qualitygates.maxNumber = 5
-
-# Audit that quality gates don't have too many criterias, it's too complex and
-# may prevent passing QG because of incorrect QG criteria
-audit.qualitygates.maxConditions = 8
-
-# Audits that QGs only use the meaningful metrics (those that make sense in a QG)
-audit.qualitygates.allowedMetrics = new_reliability_rating, new_security_rating, new_maintainability_rating, new_bugs, new_vulnerabilities, new_security_hotspots, new_security_hotspots_reviewed, new_blocker_violations, new_critical_violations, new_major_violations, new_duplicated_lines_density, reliability_rating, security_rating
-
-#------------------------ AUDIT OF METRICS ON NEW CODE ------------------------
-
-# Audit that reliability, security, maintainability, hotspot review ratings, if used, are A
-# if rating is used as a QG criteria
-audit.qualitygates.new_reliability_rating.value = 1
-audit.qualitygates.new_security_rating.value = 1
-audit.qualitygates.new_hotspot_rating.value = 1
-audit.qualitygates.new_maintainability_rating.value = 1
-
-# Audit that coverage on new code, if used, is between 20% and 90%
-audit.qualitygates.new_coverage.range = 20,90
-
-# Audit that new bugs, vulnerabilities, unreviewed hotspots metric, if used, is 0
-audit.qualitygates.new_bugs.value = 0
-audit.qualitygates.new_vulnerabilities.value = 0
-audit.qualitygates.new_security_hotspots.value = 0
-
-# Audit that % of hotspots review on new code, if used, is 100%
-audit.qualitygates.new_security_hotspots_reviewed.value = 100
-
-# Audit that new blockers/critical/major issues metric, if used, is 0
-audit.qualitygates.new_blocker_violations.value = 0
-audit.qualitygates.new_critical_violations.value = 0
-audit.qualitygates.new_major_violations.value = 0
-
-# Audit that duplication on new code, if used, is between 1% and 5%
-audit.qualitygates.new_duplicated_lines_density.range = 1, 5
-
-#---------------------- AUDIT OF METRICS ON OVERALL CODE ----------------------
-
-# Audit that reliability/security/hotspot rating on overall code, if used, is not too strict
-audit.qualitygates.reliability_rating.range = 4, 4
-audit.qualitygates.security_rating.range = 3, 4
-audit.qualitygates.hotspot_rating.range = 4, 4
-
-
-#===================== QUALITY PROFILES AUDIT CONFIGURATION ===================
-
-# Audit QP not changed since a given number of days
-audit.qualityProfiles.maxLastChangeAge = 180
-
-# Audit quality profiles with too few rules (0.5 = 50% of all rules)
-audit.qualityProfiles.minNumberOfRules = 0.5
-
-# Audit quality profiles not used for a given number of days
-audit.qualityProfiles.maxUnusedAge = 60
-
-# Audit quality profiles for usage of deprecated rules
-audit.qualityProfiles.checkDeprecatedRules = yes
-
-#========================= USERS AND GROUPS AUDIT CONFIGURATION =========================
-
-# Audit for users that have not logged in for a given number of days
-audit.users.maxLoginAge = 180
-
-# Audit for days after which a token should be revoked (and potentially renewed)
-audit.tokens.maxAge = 90
-
-# Audit  for days after which an unused token should be revoked (and potentially renewed)
-audit.tokens.maxUnusedAge = 30
-
-# Comma separated list of SonarQube users whose tokens are not considered for expiration
-audit.tokens.neverExpire =
-
-# Audit (and warn) for empty groups
-audit.groups.empty = yes
-
-#========================= PORTFOLIOS AND APPS AUDIT CONFIGURATION ========================
-
-
-# Audit (and warn) for portfolios composed of 0 or/and 1 projects
-audit.portfolios.empty = yes
-audit.portfolios.singleton = yes
-
-# Audit (and warn) for applications composed of 0 or/and 1 projects
-audit.applications.empty = yes
-audit.applications.singleton = yes
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+#======================== SELECT AUDIT SCOPE CONFIGURATION =======================
+
+# yes/no
+audit.globalSettings = yes
+audit.projects = yes
+audit.qualityGates = yes
+audit.qualityProfiles = yes
+audit.users = yes
+audit.groups = yes
+# Portfolios and application audit (for DE (Apps) and EE, DCE (Apps, and Portfolios))
+audit.portfolios = yes
+audit.applications = yes
+
+#===================== GLOBAL SETTINGS AUDIT CONFIGURATION ====================
+
+#----------------------- GLOBAL SETTINGS CONFIGURATION ------------------------
+
+# Audit (and warn) for default project public visibility
+audit.globalSettings.defaultProjectVisibility = private
+
+# Below settings audit structure is:
+# For allowed range settings:
+# audit.globalSettings.range.<auditSequenceNbr> = <setting>, <minAllowedValue>, <maxAllowedValue>, <auditSeverity>, <impactedArea>
+# For allowed value settings:
+# audit.globalSetting.value.<auditSequenceNbr> = <setting>, <allowedValue>, <auditSeverity>, <impactedArea>
+# To check if a setting is set:
+# audit.globalSettings.isSet.<auditSequenceNbr> = <setting>, <auditSeverity>, <impactedArea>
+
+# Audit (and warn) if cross project duplication is enabled
+audit.globalSettings.value.1 = sonar.cpd.cross_project, false, HIGH, PERFORMANCE
+
+# Audit (and warn) if force authentication is disabled
+audit.globalSettings.value.2 = sonar.forceAuthentication, true, HIGH, SECURITY
+
+# Audit (and warn) if server base URL is not set
+audit.globalSettings.isSet.1 = sonar.core.serverBaseURL, HIGH, OPERATIONS
+
+#----------------------- DB CLEANER AUDIT CONFIGURATION -----------------------
+
+# Audit (and warn) for suspicious DB cleaner settings
+audit.globalSettings.dbcleaner = yes
+
+# Audit DB Cleaner min/max time before purging issues
+audit.globalSettings.range.1 = sonar.dbcleaner.daysBeforeDeletingClosedIssues, 10, 60, MEDIUM, PERFORMANCE
+
+# Audit DB Cleaner min/max time before only keeping one analysis snapshot per day
+audit.globalSettings.range.2 = sonar.dbcleaner.hoursBeforeKeepingOnlyOneSnapshotByDay, 12, 240, MEDIUM, PERFORMANCE
+
+# Audit DB Cleaner min/max time before only keeping one analysis snapshot per week
+audit.globalSettings.range.3 = sonar.dbcleaner.weeksBeforeKeepingOnlyOneSnapshotByWeek, 2, 12, MEDIUM, PERFORMANCE
+
+# Audit DB Cleaner min/max time before only keeping one analysis snapshot per month
+audit.globalSettings.range.4 = sonar.dbcleaner.weeksBeforeKeepingOnlyOneSnapshotByMonth, 26, 104, MEDIUM, PERFORMANCE
+
+# Audit DB Cleaner min/max time before deleting all snapshots
+audit.globalSettings.range.5 = sonar.dbcleaner.weeksBeforeDeletingAllSnapshots, 104, 260, MEDIUM, PERFORMANCE
+
+# Audit DB Cleaner min/max time before deleting inactive branches (and PRs)
+audit.globalSettings.range.6 = sonar.dbcleaner.daysBeforeDeletingInactiveBranches, 10, 60, MEDIUM, PERFORMANCE
+audit.globalSettings.range.7 = sonar.dbcleaner.daysBeforeDeletingInactiveBranchesAndPRs, 10, 60, MEDIUM, PERFORMANCE
+
+
+#------------------- TECH DEBT SETTINGS AUDIT CONFIGURATION -------------------
+
+# Audit for suspicious technical debt thresholds, listed further below
+audit.globalSettings.technicalDebt = yes
+
+# Audit if dev cost of 1 line is not within expected range (affects Tech Debt ratio and Maintainability rating metrics)
+audit.globalSettings.range.7 = sonar.technicalDebt.developmentCost, 20, 30, MEDIUM, CONFIGURATION
+
+# Audit if maintainaibility rating thresholds are not within normal ranges
+audit.globalSettings.maintainabilityRating.A.range.1 = 0.03, 0.05, MEDIUM, CONFIGURATION
+audit.globalSettings.maintainabilityRating.A.range.2 = 0.02, 0.07, HIGH, CONFIGURATION
+audit.globalSettings.maintainabilityRating.B.range.1 = 0.07, 0.10, MEDIUM, CONFIGURATION
+audit.globalSettings.maintainabilityRating.B.range.2 = 0.05, 0.15, HIGH, CONFIGURATION
+audit.globalSettings.maintainabilityRating.C.range.1 = 0.15, 0.20, MEDIUM, CONFIGURATION
+audit.globalSettings.maintainabilityRating.C.range.2 = 0.10, 0.25, HIGH, CONFIGURATION
+audit.globalSettings.maintainabilityRating.D.range.1 = 0.40, 0.50, MEDIUM, CONFIGURATION
+audit.globalSettings.maintainabilityRating.D.range.2 = 0.30, 0.60, HIGH, CONFIGURATION
+
+# Min max heap allocated to the web process
+audit.web.heapMin = 1024
+audit.web.heapMax = 2048
+
+#======================= PERMISSIONS AUDIT CONFIGURATION ======================
+
+#----------------------------- GLOBAL PERMISSIONS -----------------------------
+# Audit (and warn) for suspicious global permissions
+audit.globalSettings.permissions = yes
+
+# Max allowed number of users/groups with global admin permission
+audit.globalSettings.permissions.maxAdminUsers = 3
+audit.globalSettings.permissions.maxAdminGroups = 2
+
+# Max allowed number of users/groups with quality gate admin permission
+audit.globalSettings.permissions.maxGateAdminUsers = 3
+audit.globalSettings.permissions.maxGateAdminGroups = 2
+
+# Max allowed number of users/groups with quality profile admin permission
+audit.globalSettings.permissions.maxProfileAdminUsers = 3
+audit.globalSettings.permissions.maxProfileAdminGroups = 2
+
+# Max allowed number of users/groups with execute analysis permission
+audit.globalSettings.permissions.maxScanUsers = 3
+audit.globalSettings.permissions.maxScanGroups = 2
+
+# Max allowed number of users/groups with create project permission
+audit.globalSettings.permissions.maxCreateProjectUsers = 3
+audit.globalSettings.permissions.maxCreateProjectGroups = 3
+
+#----------------------------- PROJECT PERMISSIONS ----------------------------
+# Project permission audit
+# Max sure there are not too many users/groups with given project permissions
+audit.projects.permissions.maxUsers = 5
+audit.projects.permissions.maxAdminUsers = 2
+audit.projects.permissions.maxGroups = 5
+audit.projects.permissions.maxAdminGroups = 2
+audit.projects.permissions.maxScanGroups = 1
+audit.projects.permissions.maxIssueAdminGroups = 2
+audit.projects.permissions.maxHotspotAdminGroups = 2
+# audit.projects.permissions.anyone = yes
+
+#========================= PROJECT AUDIT CONFIGURATION ========================
+
+# Audit and warn) for projects likely to be duplicates
+# Duplicate projects are detected from project keys that are similar
+audit.projects.duplicates = yes
+
+# Audit and warn) for projects that have been provisioned but never analyzed
+audit.projects.neverAnalyzed = yes
+
+# Audit (and warn) if project visibility is public
+audit.projects.visibility = yes
+
+# Audit (and warn) for suspicious projects permissions
+audit.projects.permissions = yes
+
+# Audit (and warn) for suspicious projects exclusions
+audit.projects.exclusions = yes
+# In the below:
+# - All * . and ? symbols that relate to the SonarQube exclusion pattern should be escaped with \\
+# - All symbols that have a special meaning for regex pattern matching shall not be escaped
+audit.projects.suspiciousExclusionsPatterns = \\*\\*/[^\/]+/\\*\\*, \\*\\*/\\*[\.\w]*, \\*\\*/\\*, \\*\\*/\\*\\.(java|jav|cs|csx|py|php|js|ts|sql|html|css|cpp|c|h|hpp)\\*?
+audit.projects.suspiciousExclusionsExceptions = \\*\\*/(__pycache__|libs|lib|vendor|node_modules)/\\*\\*
+
+# Audit (and warn) for projects whose last analysis date is older than maxLastAnalysisAge
+# Set property to 0 to turn off the check
+audit.projects.maxLastAnalysisAge = 180
+
+# Audit branches for zero LoC and last analysis date
+audit.projects.branches = yes
+
+# Audits for branches whose last analysis is older than a given number of days
+# This parameter is only considered for branches not marked as "keep when inactive" 
+# Set property to 0 to turn off the check
+audit.projects.branches.maxLastAnalysisAge = 30
+
+# Audits for PR whose last analysis is older than a given number of days
+# Set property to 0 to turn off the check
+audit.projects.pullRequests.maxLastAnalysisAge = 30
+
+# Audits duplicate projects bound to same DevOps platform repo
+audit.projects.bindings = yes
+
+# Audits that projects with bindings have valid bindings
+# Off by default since each project binding validation takes 1 to 3 seconds by project (with ALM bindings)
+# which can be too time consuming for platform with large number of bound projects
+audit.projects.bindings.validation = no
+
+# Audits projects for disabled SCM
+audit.project.scm.disabled = yes
+
+# Audits projects with suspiciously high proportion of utility LoCs (XML, JSON...)
+audit.projects.utilityLocs = yes
+
+# Audits projects for analysis warnings
+audit.projects.analysisWarnings = yes
+
+# Audits projects with last background task failed
+audit.projects.failedTasks = yes
+
+# Audits projects analyzed with too old scanner version, 2 years by default
+audit.projects.scannerMaxAge = 730
+
+# Audits projects branches
+audit.project.branches = yes
+
+#====================== QUALITY GATES AUDIT CONFIGURATION =====================
+
+# Audit that there are not too many quality gates, this defeats company common governance
+audit.qualitygates.maxNumber = 5
+
+# Audit that quality gates don't have too many criterias, it's too complex and
+# may prevent passing QG because of incorrect QG criteria
+audit.qualitygates.maxConditions = 8
+
+# Audits that QGs only use the meaningful metrics (those that make sense in a QG)
+audit.qualitygates.allowedMetrics = new_reliability_rating, new_security_rating, new_maintainability_rating, new_bugs, new_vulnerabilities, new_security_hotspots, new_security_hotspots_reviewed, new_blocker_violations, new_critical_violations, new_major_violations, new_duplicated_lines_density, reliability_rating, security_rating
+
+#------------------------ AUDIT OF METRICS ON NEW CODE ------------------------
+
+# Audit that reliability, security, maintainability, hotspot review ratings, if used, are A
+# if rating is used as a QG criteria
+audit.qualitygates.new_reliability_rating.value = 1
+audit.qualitygates.new_security_rating.value = 1
+audit.qualitygates.new_hotspot_rating.value = 1
+audit.qualitygates.new_maintainability_rating.value = 1
+
+# Audit that coverage on new code, if used, is between 20% and 90%
+audit.qualitygates.new_coverage.range = 20,90
+
+# Audit that new bugs, vulnerabilities, unreviewed hotspots metric, if used, is 0
+audit.qualitygates.new_bugs.value = 0
+audit.qualitygates.new_vulnerabilities.value = 0
+audit.qualitygates.new_security_hotspots.value = 0
+
+# Audit that % of hotspots review on new code, if used, is 100%
+audit.qualitygates.new_security_hotspots_reviewed.value = 100
+
+# Audit that new blockers/critical/major issues metric, if used, is 0
+audit.qualitygates.new_blocker_violations.value = 0
+audit.qualitygates.new_critical_violations.value = 0
+audit.qualitygates.new_major_violations.value = 0
+
+# Audit that duplication on new code, if used, is between 1% and 5%
+audit.qualitygates.new_duplicated_lines_density.range = 1, 5
+
+#---------------------- AUDIT OF METRICS ON OVERALL CODE ----------------------
+
+# Audit that reliability/security/hotspot rating on overall code, if used, is not too strict
+audit.qualitygates.reliability_rating.range = 4, 4
+audit.qualitygates.security_rating.range = 3, 4
+audit.qualitygates.hotspot_rating.range = 4, 4
+
+
+#===================== QUALITY PROFILES AUDIT CONFIGURATION ===================
+
+# Audit QP not changed since a given number of days
+audit.qualityProfiles.maxLastChangeAge = 180
+
+# Audit quality profiles with too few rules (0.5 = 50% of all rules)
+audit.qualityProfiles.minNumberOfRules = 0.5
+
+# Audit quality profiles not used for a given number of days
+audit.qualityProfiles.maxUnusedAge = 60
+
+# Audit quality profiles for usage of deprecated rules
+audit.qualityProfiles.checkDeprecatedRules = yes
+
+#========================= USERS AND GROUPS AUDIT CONFIGURATION =========================
+
+# Audit for users that have not logged in for a given number of days
+audit.users.maxLoginAge = 180
+
+# Audit for days after which a token should be revoked (and potentially renewed)
+audit.tokens.maxAge = 90
+
+# Audit  for days after which an unused token should be revoked (and potentially renewed)
+audit.tokens.maxUnusedAge = 30
+
+# Comma separated list of SonarQube users whose tokens are not considered for expiration
+audit.tokens.neverExpire =
+
+# Audit (and warn) for empty groups
+audit.groups.empty = yes
+
+#========================= PORTFOLIOS AND APPS AUDIT CONFIGURATION ========================
+
+
+# Audit (and warn) for portfolios composed of 0 or/and 1 projects
+audit.portfolios.empty = yes
+audit.portfolios.singleton = yes
+
+# Audit (and warn) for applications composed of 0 or/and 1 projects
+audit.applications.empty = yes
+audit.applications.singleton = yes
```

## sonar/audit/types.py

 * *Ordering differences only*

```diff
@@ -1,39 +1,39 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-import enum
-
-
-class Type(enum.Enum):
-    SECURITY = 1
-    GOVERNANCE = 2
-    CONFIGURATION = 3
-    PERFORMANCE = 4
-    BAD_PRACTICE = 5
-    OPERATIONS = 6
-
-    def __str__(self):
-        return repr(self.name)[1:-1]
-
-
-def to_type(val):
-    for enum_val in Type:
-        if repr(enum_val.name)[1:-1] == val:
-            return enum_val
-    return None
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+import enum
+
+
+class Type(enum.Enum):
+    SECURITY = 1
+    GOVERNANCE = 2
+    CONFIGURATION = 3
+    PERFORMANCE = 4
+    BAD_PRACTICE = 5
+    OPERATIONS = 6
+
+    def __str__(self):
+        return repr(self.name)[1:-1]
+
+
+def to_type(val):
+    for enum_val in Type:
+        if repr(enum_val.name)[1:-1] == val:
+            return enum_val
+    return None
```

## sonar/dce/__init__.py

 * *Ordering differences only*

```diff
@@ -1,19 +1,19 @@
-#
-# sonar-tools
-# Copyright (C) 2022-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
+#
+# sonar-tools
+# Copyright (C) 2022-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
```

## sonar/dce/app_nodes.py

```diff
@@ -1,254 +1,304 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-"""
-
-    Abstraction of the App Node concept
-
-"""
-
-import datetime
-from dateutil.relativedelta import relativedelta
-import sonar.utilities as util
-from sonar.audit import rules, severities, types
-import sonar.audit.problem as pb
-import sonar.dce.nodes as dce_nodes
-
-_RELEASE_DATE_6_7 = datetime.datetime(2017, 11, 8) + relativedelta(months=+6)
-_RELEASE_DATE_7_9 = datetime.datetime(2019, 7, 1) + relativedelta(months=+6)
-_RELEASE_DATE_8_9 = datetime.datetime(2021, 5, 4) + relativedelta(months=+6)
-
-_SYSTEM = "System"
-_SETTINGS = "Settings"
-_VERSION = "Version"
-
-
-class AppNode(dce_nodes.DceNode):
-    def __str__(self):
-        return f"App Node '{self.name()}'"
-
-    def plugins(self):
-        self.json.get("Plugins", None)
-
-    def health(self):
-        return self.json.get("Health", "RED")
-
-    def node_type(self):
-        return "APPLICATION"
-
-    def version(self, digits=3, as_string=False):
-        if _SETTINGS in self.json:
-            split_version = self.json[_SETTINGS][_VERSION].split(".")
-        elif _SYSTEM in self.json and _VERSION in self.json[_SYSTEM]:
-            split_version = self.json[_SYSTEM][_VERSION].split(".")
-        else:
-            return None
-        try:
-            if as_string:
-                return ".".join(split_version[0:digits])
-            else:
-                return tuple(int(n) for n in split_version[0:digits])
-        except ValueError:
-            return None
-
-    def log_level(self):
-        if "Web Logging" in self.json:
-            return self.json["Web Logging"]["Logs Level"]
-        else:
-            return None
-
-    def name(self):
-        return self.json["Name"]
-
-    def audit(self):
-        util.logger.info("Auditing %s", str(self))
-        return (
-            self.__audit_log_level()
-            + self.__audit_official()
-            + self.__audit_health()
-            + self.__audit_version()
-            + self.__audit_ce_settings()
-            + self.__audit_background_tasks()
-        )
-
-    def __audit_log_level(self):
-        util.logger.debug("Auditing log level")
-        log_level = self.log_level()
-        if log_level is None:
-            util.logger.warning("%s: log level is missing, audit of log level is skipped...", str(self))
-            return []
-        if log_level not in ("DEBUG", "TRACE"):
-            util.logger.info("Log level of '%s' is '%s', all good...", str(self), log_level)
-            return []
-        if log_level == "TRACE":
-            return [
-                pb.Problem(
-                    types.Type.PERFORMANCE,
-                    severities.Severity.CRITICAL,
-                    f"Log level of {str(self)} set to TRACE, this does very negatively affect platform performance, " "reverting to INFO is required",
-                )
-            ]
-        if log_level == "DEBUG":
-            return [
-                pb.Problem(
-                    types.Type.PERFORMANCE,
-                    severities.Severity.HIGH,
-                    f"Log level of {str(self)} is set to DEBUG, this may affect platform performance, " "reverting to INFO is recommended",
-                )
-            ]
-        util.logger.debug("%s: Node log level is %s", str(self), log_level)
-        return []
-
-    def __audit_health(self):
-        if self.health() != dce_nodes.HEALTH_GREEN:
-            rule = rules.get_rule(rules.RuleId.DCE_APP_NODE_NOT_GREEN)
-            return [pb.Problem(rule.type, rule.severity, rule.msg.format(str(self), self.health()))]
-        else:
-            util.logger.debug("%s: Node health is %s", str(self), dce_nodes.HEALTH_GREEN)
-            return []
-
-    def __audit_official(self):
-        if _SYSTEM not in self.json:
-            util.logger.warning(
-                "%s: Official distribution information missing, audit skipped...",
-                str(self),
-            )
-            return []
-        elif not self.json[_SYSTEM]["Official Distribution"]:
-            rule = rules.get_rule(rules.RuleId.DCE_APP_NODE_UNOFFICIAL_DISTRO)
-            return [pb.Problem(rule.type, rule.severity, rule.msg.format(str(self)))]
-        else:
-            util.logger.debug("%s: Node is official distribution", str(self))
-            return []
-
-    def __audit_version(self):
-        sq_version = self.version()
-        if sq_version is None:
-            util.logger.warning("%s: Version information is missing, audit on node vresion is skipped...")
-            return []
-        st_time = self.sif.start_time()
-        if (
-            (st_time > _RELEASE_DATE_6_7 and sq_version < (6, 7, 0))
-            or (st_time > _RELEASE_DATE_7_9 and sq_version < (7, 9, 0))
-            or (st_time > _RELEASE_DATE_8_9 and sq_version < (8, 9, 0))
-        ):
-            rule = rules.get_rule(rules.RuleId.BELOW_LTS)
-            return [pb.Problem(rule.type, rule.severity, rule.msg)]
-        else:
-            util.logger.debug(
-                "%s: Version %s is correct wrt LTS",
-                str(self),
-                self.version(as_string=True),
-            )
-            return []
-
-    def __audit_ce_settings(self):
-        util.logger.info("Auditing CE settings")
-        try:
-            ce_workers = self.json["Compute Engine Tasks"]["Worker Count"]
-        except KeyError:
-            util.logger.warning(
-                "%s: CE section missing from SIF, CE workers audit skipped...",
-                str(self),
-            )
-            return []
-        MAX_WORKERS = 2
-        if ce_workers > MAX_WORKERS:
-            rule = rules.get_rule(rules.RuleId.SETTING_CE_TOO_MANY_WORKERS)
-            return [pb.Problem(rule.type, rule.severity, rule.msg.format(ce_workers, MAX_WORKERS))]
-        else:
-            util.logger.debug(
-                "%s: %d CE workers configured, correct compared to the max %d recommended",
-                str(self),
-                ce_workers,
-                MAX_WORKERS,
-            )
-            return []
-
-    def __audit_background_tasks(self):
-        util.logger.debug("Auditing CE background tasks")
-        problems = []
-        try:
-            ce_tasks = self.json["Compute Engine Tasks"]
-        except KeyError:
-            util.logger.warning(
-                "%s: CE section missing from SIF, background tasks audit skipped...",
-                str(self),
-            )
-            return []
-
-        ce_success = ce_tasks["Processed With Success"]
-        ce_error = ce_tasks["Processed With Error"]
-        failure_rate = 0
-        if ce_success != 0 or ce_error != 0:
-            failure_rate = ce_error / (ce_success + ce_error)
-        if ce_error > 10 and failure_rate > 0.01:
-            rule = rules.get_rule(rules.RuleId.BACKGROUND_TASKS_FAILURE_RATE_HIGH)
-            problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(int(failure_rate * 100))))
-        else:
-            util.logger.debug(
-                "Number of failed background tasks (%d), and failure rate %d%% is OK",
-                ce_error,
-                int(failure_rate * 100),
-            )
-
-        ce_pending = ce_tasks["Pending"]
-        if ce_pending > 100:
-            rule = rules.get_rule(rules.RuleId.BACKGROUND_TASKS_PENDING_QUEUE_VERY_LONG)
-            problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(ce_pending)))
-        elif ce_pending > 20 and ce_pending > (10 * ce_tasks["Worker Count"]):
-            rule = rules.get_rule(rules.RuleId.BACKGROUND_TASKS_PENDING_QUEUE_LONG)
-            problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(ce_pending)))
-        else:
-            util.logger.debug("Number of pending background tasks (%d) is OK", ce_pending)
-        return problems
-
-
-def audit(sub_sif, sif):
-    nodes = []
-    problems = []
-    for n in sub_sif:
-        nodes.append(AppNode(n, sif))
-    if len(nodes) == 1:
-        rule = rules.get_rule(rules.RuleId.DCE_APP_CLUSTER_NOT_HA)
-        return [pb.Problem(rule.type, rule.severity, rule.msg)]
-    for i in range(len(nodes)):
-        problems += nodes[i].audit()
-        for j in range(i, len(nodes)):
-            v1 = nodes[i].version()
-            v2 = nodes[j].version()
-            if v1 is not None and v2 is not None and v1 != v2:
-                rule = rules.get_rule(rules.RuleId.DCE_DIFFERENT_APP_NODES_VERSIONS)
-                problems.append(
-                    pb.Problem(
-                        rule.type,
-                        rule.severity,
-                        rule.msg.format(str(nodes[i]), str(nodes[j])),
-                    )
-                )
-            if nodes[i].plugins() != nodes[j].plugins():
-                rule = rules.get_rule(rules.RuleId.DCE_DIFFERENT_APP_NODES_PLUGINS)
-                problems.append(
-                    pb.Problem(
-                        rule.type,
-                        rule.severity,
-                        rule.msg.format(str(nodes[i]), str(nodes[j])),
-                    )
-                )
-    return problems
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+"""
+
+    Abstraction of the App Node concept
+
+"""
+
+import datetime
+from dateutil.relativedelta import relativedelta
+import sonar.utilities as util
+from sonar.audit import rules, severities, types
+import sonar.audit.problem as pb
+import sonar.dce.nodes as dce_nodes
+
+_RELEASE_DATE_6_7 = datetime.datetime(2017, 11, 8) + relativedelta(months=+6)
+_RELEASE_DATE_7_9 = datetime.datetime(2019, 7, 1) + relativedelta(months=+6)
+_RELEASE_DATE_8_9 = datetime.datetime(2021, 5, 4) + relativedelta(months=+6)
+
+_SYSTEM = "System"
+_SETTINGS = "Settings"
+_VERSION = "Version"
+
+
+class AppNode(dce_nodes.DceNode):
+    def __str__(self):
+        return f"App Node '{self.name()}'"
+
+    def plugins(self):
+        self.json.get("Plugins", None)
+
+    def health(self):
+        return self.json.get("Health", "RED")
+
+    def node_type(self):
+        return "APPLICATION"
+
+    def version(self, digits=3, as_string=False):
+        if _SETTINGS in self.json:
+            split_version = self.json[_SETTINGS][_VERSION].split(".")
+        elif _SYSTEM in self.json and _VERSION in self.json[_SYSTEM]:
+            split_version = self.json[_SYSTEM][_VERSION].split(".")
+        else:
+            return None
+        try:
+            if as_string:
+                return ".".join(split_version[0:digits])
+            else:
+                return tuple(int(n) for n in split_version[0:digits])
+        except ValueError:
+            return None
+
+    def log_level(self):
+        if "Web Logging" in self.json:
+            return self.json["Web Logging"]["Logs Level"]
+        else:
+            return None
+
+    def name(self):
+        return self.json["Name"]
+
+    def audit(self, audit_settings: dict[str, str] = None):
+        util.logger.info("Auditing %s", str(self))
+        return (
+            self.__audit_log_level()
+            + self.__audit_official()
+            + self.__audit_health()
+            + self.__audit_version()
+            + self.__audit_web_settings(audit_settings)
+            + self.__audit_ce_settings()
+            + self.__audit_background_tasks()
+        )
+
+    def __audit_log_level(self):
+        util.logger.debug("Auditing log level")
+        log_level = self.log_level()
+        if log_level is None:
+            util.logger.warning("%s: log level is missing, audit of log level is skipped...", str(self))
+            return []
+        if log_level not in ("DEBUG", "TRACE"):
+            util.logger.info("Log level of '%s' is '%s', all good...", str(self), log_level)
+            return []
+        if log_level == "TRACE":
+            return [
+                pb.Problem(
+                    types.Type.PERFORMANCE,
+                    severities.Severity.CRITICAL,
+                    f"Log level of {str(self)} set to TRACE, this does very negatively affect platform performance, " "reverting to INFO is required",
+                )
+            ]
+        if log_level == "DEBUG":
+            return [
+                pb.Problem(
+                    types.Type.PERFORMANCE,
+                    severities.Severity.HIGH,
+                    f"Log level of {str(self)} is set to DEBUG, this may affect platform performance, " "reverting to INFO is recommended",
+                )
+            ]
+        util.logger.debug("%s: Node log level is %s", str(self), log_level)
+        return []
+
+    def __audit_health(self):
+        if self.health() != dce_nodes.HEALTH_GREEN:
+            rule = rules.get_rule(rules.RuleId.DCE_APP_NODE_NOT_GREEN)
+            return [pb.Problem(rule.type, rule.severity, rule.msg.format(str(self), self.health()))]
+        else:
+            util.logger.debug("%s: Node health is %s", str(self), dce_nodes.HEALTH_GREEN)
+            return []
+
+    def __audit_official(self):
+        if _SYSTEM not in self.json:
+            util.logger.warning(
+                "%s: Official distribution information missing, audit skipped...",
+                str(self),
+            )
+            return []
+        elif not self.json[_SYSTEM]["Official Distribution"]:
+            rule = rules.get_rule(rules.RuleId.DCE_APP_NODE_UNOFFICIAL_DISTRO)
+            return [pb.Problem(rule.type, rule.severity, rule.msg.format(str(self)))]
+        else:
+            util.logger.debug("%s: Node is official distribution", str(self))
+            return []
+
+    def __audit_version(self):
+        sq_version = self.version()
+        if sq_version is None:
+            util.logger.warning("%s: Version information is missing, audit on node vresion is skipped...")
+            return []
+        st_time = self.sif.start_time()
+        if (
+            (st_time > _RELEASE_DATE_6_7 and sq_version < (6, 7, 0))
+            or (st_time > _RELEASE_DATE_7_9 and sq_version < (7, 9, 0))
+            or (st_time > _RELEASE_DATE_8_9 and sq_version < (8, 9, 0))
+        ):
+            rule = rules.get_rule(rules.RuleId.BELOW_LTS)
+            return [pb.Problem(rule.type, rule.severity, rule.msg)]
+        else:
+            util.logger.debug(
+                "%s: Version %s is correct wrt LTS",
+                str(self),
+                self.version(as_string=True),
+            )
+            return []
+
+    def __audit_jvm_version(self) -> list[pb.Problem]:
+        try:
+            java_version = int(self.json["Web JVM Properties"]["java.specification.version"])
+        except KeyError:
+            util.logger.warning("Can't find Java version for %s in SIF, auditing this part is skipped", str(self))
+            return []
+        try:
+            sq_version = util.string_to_version(self.json["System"]["Version"])
+        except KeyError:
+            util.logger.warning("Can't find SonarQube version for %s in SIF, auditing this part is skipped", str(self))
+            return []
+        if sq_version >= (9, 9, 0) and java_version != 17:
+            rule = rules.get_rule(rules.RuleId.SETTING_WEB_WRONG_JAVA_VERSION)
+            return [pb.Problem(rule.type, rule.severity, rule.msg.format(str(self), java_version), concerned_object=self)]
+        else:
+            util.logger.debug("%s is running on the required java version (java %d)", str(self), java_version)
+        return []
+
+    def __audit_jvm_ram(self, audit_settings: dict[str, str]) -> list[pb.Problem]:
+        # On DCE we expect between 2 and 4 GB of RAM per App Node Web JVM
+        min_heap = audit_settings.get("audit.web.heapMin", 2024)
+        max_heap = audit_settings.get("audit.web.heapMax", 4096)
+        try:
+            web_heap = self.json["Web JVM State"]["Heap Max (MB)"]
+        except KeyError:
+            util.logger.warning("Can't find JVM Heap for %s in SIF, auditing this part is skipped", str(self))
+            return []
+        if web_heap is None:
+            rule = rules.get_rule(rules.RuleId.SETTING_WEB_NO_HEAP)
+            return [pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=self)]
+        elif web_heap < min_heap or web_heap > max_heap:
+            rule = rules.get_rule(rules.RuleId.SETTING_WEB_HEAP)
+            return [pb.Problem(rule.type, rule.severity, rule.msg.format(web_heap, min_heap, max_heap), concerned_object=self)]
+        else:
+            util.logger.debug("%s web heap of %d MB is within recommended range [%d-%d]", str(self), web_heap, min_heap, max_heap)
+
+    def __audit_web_settings(self, audit_settings: dict[str, str]) -> list[pb.Problem]:
+        return self.__audit_jvm_version() + self.__audit_jvm_ram(audit_settings)
+
+    def __audit_ce_settings(self):
+        util.logger.info("Auditing CE settings")
+        try:
+            ce_workers = self.json["Compute Engine Tasks"]["Worker Count"]
+        except KeyError:
+            util.logger.warning(
+                "%s: CE section missing from SIF, CE workers audit skipped...",
+                str(self),
+            )
+            return []
+        MAX_WORKERS = 2
+        if ce_workers > MAX_WORKERS:
+            rule = rules.get_rule(rules.RuleId.SETTING_CE_TOO_MANY_WORKERS)
+            return [pb.Problem(rule.type, rule.severity, rule.msg.format(ce_workers, MAX_WORKERS))]
+        else:
+            util.logger.debug(
+                "%s: %d CE workers configured, correct compared to the max %d recommended",
+                str(self),
+                ce_workers,
+                MAX_WORKERS,
+            )
+            return []
+
+    def __audit_background_tasks(self):
+        util.logger.debug("Auditing CE background tasks")
+        problems = []
+        try:
+            ce_tasks = self.json["Compute Engine Tasks"]
+        except KeyError:
+            util.logger.warning(
+                "%s: CE section missing from SIF, background tasks audit skipped...",
+                str(self),
+            )
+            return []
+
+        ce_success = ce_tasks["Processed With Success"]
+        ce_error = ce_tasks["Processed With Error"]
+        failure_rate = 0
+        if ce_success != 0 or ce_error != 0:
+            failure_rate = ce_error / (ce_success + ce_error)
+        if ce_error > 10 and failure_rate > 0.01:
+            rule = rules.get_rule(rules.RuleId.BACKGROUND_TASKS_FAILURE_RATE_HIGH)
+            problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(int(failure_rate * 100))))
+        else:
+            util.logger.debug(
+                "Number of failed background tasks (%d), and failure rate %d%% is OK",
+                ce_error,
+                int(failure_rate * 100),
+            )
+
+        ce_pending = ce_tasks["Pending"]
+        if ce_pending > 100:
+            rule = rules.get_rule(rules.RuleId.BACKGROUND_TASKS_PENDING_QUEUE_VERY_LONG)
+            problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(ce_pending)))
+        elif ce_pending > 20 and ce_pending > (10 * ce_tasks["Worker Count"]):
+            rule = rules.get_rule(rules.RuleId.BACKGROUND_TASKS_PENDING_QUEUE_LONG)
+            problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(ce_pending)))
+        else:
+            util.logger.debug("Number of pending background tasks (%d) is OK", ce_pending)
+        return problems
+
+
+def audit(sub_sif: dict[str, str], sif: object, audit_settings: dict[str, str] = None) -> list[pb.Problem]:
+    """Audits application nodes of a DCE instance
+
+    :param dict sub_sif: The JSON subsection of the SIF pertaining to the App Nodes
+    :param Sif sif: The Sif object
+    :param dict audit_settings: Config settings for audit
+    :return: List of Problems
+    :rtype: list
+    """
+    if audit_settings is None:
+        audit_settings = {}
+    nodes = []
+    problems = []
+    for n in sub_sif:
+        nodes.append(AppNode(n, sif))
+    if len(nodes) == 1:
+        rule = rules.get_rule(rules.RuleId.DCE_APP_CLUSTER_NOT_HA)
+        return [pb.Problem(rule.type, rule.severity, rule.msg)]
+    for i in range(len(nodes)):
+        problems += nodes[i].audit(audit_settings)
+        for j in range(i, len(nodes)):
+            v1 = nodes[i].version()
+            v2 = nodes[j].version()
+            if v1 is not None and v2 is not None and v1 != v2:
+                rule = rules.get_rule(rules.RuleId.DCE_DIFFERENT_APP_NODES_VERSIONS)
+                problems.append(
+                    pb.Problem(
+                        rule.type,
+                        rule.severity,
+                        rule.msg.format(str(nodes[i]), str(nodes[j])),
+                    )
+                )
+            if nodes[i].plugins() != nodes[j].plugins():
+                rule = rules.get_rule(rules.RuleId.DCE_DIFFERENT_APP_NODES_PLUGINS)
+                problems.append(
+                    pb.Problem(
+                        rule.type,
+                        rule.severity,
+                        rule.msg.format(str(nodes[i]), str(nodes[j])),
+                    )
+                )
+    return problems
```

## sonar/dce/nodes.py

 * *Ordering differences only*

```diff
@@ -1,37 +1,37 @@
-#
-# sonar-tools
-# Copyright (C) 2022-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-"""
-
-    Abstraction of the DCE Node concept
-
-"""
-
-HEALTH_GREEN = "GREEN"
-HEALTH_YELLOW = "YELLOW"
-HEALTH_RED = "RED"
-
-
-class DceNode:
-    def __init__(self, data, sif):
-        self.json = data
-        self.sif = sif
-
-    def audit(self):
-        return []
+#
+# sonar-tools
+# Copyright (C) 2022-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+"""
+
+    Abstraction of the DCE Node concept
+
+"""
+
+HEALTH_GREEN = "GREEN"
+HEALTH_YELLOW = "YELLOW"
+HEALTH_RED = "RED"
+
+
+class DceNode:
+    def __init__(self, data, sif):
+        self.json = data
+        self.sif = sif
+
+    def audit(self):
+        return []
```

## sonar/dce/search_nodes.py

```diff
@@ -1,113 +1,127 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-"""
-
-    Abstraction of the Search Node concept
-
-"""
-
-import sonar.utilities as util
-from sonar.audit import rules
-import sonar.audit.problem as pb
-from sonar.dce import nodes
-
-_STORE_SIZE = "Store Size"
-_ES_STATE = "Search State"
-
-
-class SearchNode(nodes.DceNode):
-    def __str__(self):
-        return f"Search Node '{self.name()}'"
-
-    def store_size(self):
-        return util.int_memory(self.json[_ES_STATE][_STORE_SIZE])
-
-    def name(self):
-        return self.json["Name"]
-
-    def node_type(self):
-        return "SEARCH"
-
-    def audit(self):
-        util.logger.info("Auditing %s", str(self))
-        return self.__audit_store_size()
-
-    def __audit_store_size(self):
-        es_heap = util.jvm_heap(self.sif.search_jvm_cmdline())
-        index_size = self.store_size()
-
-        if es_heap is None:
-            rule = rules.get_rule(rules.RuleId.SETTING_ES_NO_HEAP)
-            return [pb.Problem(rule.type, rule.severity, rule.msg)]
-        elif index_size is None:
-            util.logger.debug("Search server index size missing, audit of ES index vs heap skipped...")
-            return []
-        elif index_size == 0:
-            rule = rules.get_rule(rules.RuleId.DCE_ES_INDEX_EMPTY)
-            return [pb.Problem(rule.type, rule.severity, rule.msg.format(str(self)))]
-        elif es_heap < 2 * index_size and es_heap < index_size + 1000:
-            rule = rules.get_rule(rules.RuleId.SETTING_ES_HEAP)
-            return [pb.Problem(rule.type, rule.severity, rule.msg.format(es_heap, index_size))]
-        else:
-            util.logger.debug(
-                "Search server memory %d MB is correct wrt to index size of %d MB",
-                es_heap,
-                index_size,
-            )
-            return []
-
-
-def __audit_index_balance(searchnodes):
-    nbr_search_nodes = len(searchnodes)
-    for i in range(nbr_search_nodes):
-        size_i = searchnodes[i].store_size()
-        if size_i is None:
-            continue
-        for j in range(i + 1, nbr_search_nodes):
-            size_j = searchnodes[j].store_size()
-            if size_j is None or size_j == 0:
-                continue
-            store_ratio = size_i / size_j
-            if store_ratio >= 0.5 or store_ratio <= 2:
-                continue
-            rule = rules.get_rule(rules.RuleId.DCE_ES_UNBALANCED_INDEX)
-            return [pb.Problem(rule.type, rule.severity, rule.msg.format())]
-    return []
-
-
-def audit(sub_sif, sif):
-    searchnodes = []
-    problems = []
-    for n in sub_sif:
-        searchnodes.append(SearchNode(n, sif))
-    nbr_search_nodes = len(searchnodes)
-    if nbr_search_nodes < 3:
-        rule = rules.get_rule(rules.RuleId.DCE_ES_CLUSTER_NOT_HA)
-        problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format()))
-    elif nbr_search_nodes > 3:
-        if nbr_search_nodes % 2 == 0:
-            rule = rules.get_rule(rules.RuleId.DCE_ES_CLUSTER_EVEN_NUMBER_OF_NODES)
-        else:
-            rule = rules.get_rule(rules.RuleId.DCE_ES_CLUSTER_WRONG_NUMBER_OF_NODES)
-        problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(nbr_search_nodes)))
-    for i in range(nbr_search_nodes):
-        problems += searchnodes[i].audit()
-    problems += __audit_index_balance(searchnodes)
-    return problems
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+"""
+
+    Abstraction of the Search Node concept
+
+"""
+
+from typing import Union
+import sonar.utilities as util
+from sonar.audit import rules
+import sonar.audit.problem as pb
+from sonar.dce import nodes
+
+
+_STORE_SIZE = "Store Size"
+_ES_STATE = "Search State"
+
+
+class SearchNode(nodes.DceNode):
+    def __str__(self):
+        return f"Search Node '{self.name()}'"
+
+    def store_size(self):
+        return util.int_memory(self.json[_ES_STATE][_STORE_SIZE])
+
+    def name(self):
+        return self.json["Name"]
+
+    def node_type(self):
+        return "SEARCH"
+
+    def audit(self):
+        util.logger.info("Auditing %s", str(self))
+        return self.__audit_store_size()
+
+    def max_heap(self) -> Union[int, None]:
+        if self.sif.version() < (9, 0, 0):
+            return util.jvm_heap(self.sif.search_jvm_cmdline())
+        try:
+            sz = self.json["Search State"]["JVM Heap Max"]
+        except KeyError:
+            util.logger.warning("Can't retrieve heap allocated to %s", str(self))
+            return None
+        return int(float(sz.split(" ")[0]) * 1024)
+
+    def __audit_store_size(self):
+        es_heap = self.max_heap()
+        if es_heap is None:
+            util.logger.warning("No ES heap found for %s, audit of ES head is skipped", str(self))
+            rule = rules.get_rule(rules.RuleId.SETTING_ES_NO_HEAP)
+            return [pb.Problem(rule.type, rule.severity, rule.msg)]
+
+        index_size = self.store_size()
+
+        if index_size is None:
+            util.logger.debug("Search server index size missing, audit of ES index vs heap skipped...")
+            return []
+        elif index_size == 0:
+            rule = rules.get_rule(rules.RuleId.DCE_ES_INDEX_EMPTY)
+            return [pb.Problem(rule.type, rule.severity, rule.msg.format(str(self)))]
+        elif es_heap < 2 * index_size and es_heap < index_size + 1000:
+            rule = rules.get_rule(rules.RuleId.SETTING_ES_HEAP)
+            return [pb.Problem(rule.type, rule.severity, rule.msg.format(es_heap, index_size))]
+        else:
+            util.logger.debug(
+                "Search server memory %d MB is correct wrt to index size of %d MB",
+                es_heap,
+                index_size,
+            )
+            return []
+
+
+def __audit_index_balance(searchnodes):
+    nbr_search_nodes = len(searchnodes)
+    for i in range(nbr_search_nodes):
+        size_i = searchnodes[i].store_size()
+        if size_i is None:
+            continue
+        for j in range(i + 1, nbr_search_nodes):
+            size_j = searchnodes[j].store_size()
+            if size_j is None or size_j == 0:
+                continue
+            store_ratio = size_i / size_j
+            if store_ratio >= 0.5 or store_ratio <= 2:
+                continue
+            rule = rules.get_rule(rules.RuleId.DCE_ES_UNBALANCED_INDEX)
+            return [pb.Problem(rule.type, rule.severity, rule.msg.format())]
+    return []
+
+
+def audit(sub_sif, sif):
+    searchnodes = []
+    problems = []
+    for n in sub_sif:
+        searchnodes.append(SearchNode(n, sif))
+    nbr_search_nodes = len(searchnodes)
+    if nbr_search_nodes < 3:
+        rule = rules.get_rule(rules.RuleId.DCE_ES_CLUSTER_NOT_HA)
+        problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format()))
+    elif nbr_search_nodes > 3:
+        if nbr_search_nodes % 2 == 0:
+            rule = rules.get_rule(rules.RuleId.DCE_ES_CLUSTER_EVEN_NUMBER_OF_NODES)
+        else:
+            rule = rules.get_rule(rules.RuleId.DCE_ES_CLUSTER_WRONG_NUMBER_OF_NODES)
+        problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(nbr_search_nodes)))
+    for i in range(nbr_search_nodes):
+        problems += searchnodes[i].audit()
+    problems += __audit_index_balance(searchnodes)
+    return problems
```

## sonar/findings/__init__.py

 * *Ordering differences only*

```diff
@@ -1,19 +1,19 @@
-#
-# sonar-tools
-# Copyright (C) 2022-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
+#
+# sonar-tools
+# Copyright (C) 2022-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
```

## sonar/findings/changelog.py

 * *Ordering differences only*

```diff
@@ -1,203 +1,203 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-
-import sonar.utilities as util
-
-
-class Changelog:
-    def __init__(self, jsonlog):
-        self._json = jsonlog
-        self._change_type = None
-
-    def __str__(self):
-        return str(self._json)
-
-    def __is_resolve_as(self, resolve_reason):
-        cond1 = False
-        cond2 = False
-        for diff in self._json["diffs"]:
-            if diff["key"] == "resolution" and "newValue" in diff and diff["newValue"] == resolve_reason:
-                cond1 = True
-            if diff["key"] == "status" and "newValue" in diff and diff["newValue"] == "RESOLVED":
-                cond2 = True
-        return cond1 and cond2
-
-    def is_resolve_as_fixed(self):
-        return self.__is_resolve_as("FIXED")
-
-    def is_resolve_as_fp(self):
-        return self.__is_resolve_as("FALSE-POSITIVE")
-
-    def is_resolve_as_wf(self):
-        return self.__is_resolve_as("WONTFIX")
-
-    def is_closed(self):
-        """{'creationDate': '2022-02-01T19:15:24+0100', 'diffs': [
-        {'key': 'resolution', 'newValue': 'FIXED'},
-        {'key': 'status', 'newValue': 'CLOSED', 'oldValue': 'OPEN'}]}"""
-        for diff in self._json["diffs"]:
-            if diff["key"] == "status" and "newValue" in diff and diff["newValue"] == "CLOSED":
-                return True
-        return False
-
-    def __is_status(self, status):
-        for d in self._json["diffs"]:
-            if d.get("key", "") == "status" and d.get("newValue", "") == status:
-                return True
-        return False
-
-    def is_reopen(self):
-        for d in self._json["diffs"]:
-            if d.get("key", "") == "status" and (
-                (d.get("newValue", "") == "REOPENED" and d.get("oldValue", "") != "CONFIRMED")
-                or (d.get("newValue", "") == "OPEN" and d.get("oldValue", "") == "CLOSED")
-            ):
-                return True
-        return False
-
-    def is_confirm(self):
-        return self.__is_status("CONFIRMED")
-
-    def is_unconfirm(self):
-        for d in self._json["diffs"]:
-            if d.get("key", "") == "status" and d.get("newValue", "") == "REOPENED" and d.get("oldValue", "") == "CONFIRMED":
-                return True
-        return False
-
-    def is_mark_as_safe(self):
-        for d in self._json["diffs"]:
-            if d.get("key", "") == "resolution" and d.get("newValue", "") == "SAFE":
-                return True
-        return False
-
-    def is_mark_as_to_review(self):
-        for d in self._json["diffs"]:
-            if d.get("key", "") == "status" and d.get("newValue", "") == "TO_REVIEW":
-                return True
-        return False
-
-    def is_mark_as_fixed(self):
-        for d in self._json["diffs"]:
-            if d.get("key", "") == "resolution" and d.get("newValue", "") == "FIXED":
-                return True
-        return False
-
-    def is_mark_as_acknowledged(self):
-        for d in self._json["diffs"]:
-            if d.get("key", "") == "resolution" and d.get("newValue", "") == "ACKNOWLEDGED":
-                return True
-        return False
-
-    def is_change_severity(self):
-        d = self._json["diffs"][0]
-        return d.get("key", "") == "severity"
-
-    def new_severity(self):
-        if self.is_change_severity():
-            d = self._json["diffs"][0]
-            return d.get("newValue", None)
-        return None
-
-    def is_change_type(self):
-        d = self._json["diffs"][0]
-        return d.get("key", "") == "type"
-
-    def new_type(self):
-        if self.is_change_type():
-            d = self._json["diffs"][0]
-            return d.get("newValue", None)
-        return None
-
-    def is_technical_change(self):
-        d = self._json["diffs"][0]
-        key = d.get("key", "")
-        return key in ("from_short_branch", "from_branch", "effort")
-
-    def is_assignment(self):
-        d = self._json["diffs"][0]
-        return d.get("key", "") == "assignee"
-
-    def new_assignee(self):
-        d = self._json["diffs"][0]
-        return d.get("newValue", None)
-
-    def old_assignee(self):
-        d = self._json["diffs"][0]
-        return d.get("oldValue", None)
-
-    def previous_state(self):
-        for d in self._json["diffs"]:
-            if d.get("key", "") == "status":
-                return d.get("oldValue", "")
-        return ""
-
-    def date(self):
-        return self._json["creationDate"]
-
-    def author(self):
-        return self._json.get("user", None)
-
-    def is_tag(self):
-        d = self._json["diffs"][0]
-        return d.get("key", "") == "tag"
-
-    def tags(self):
-        if not self.is_tag():
-            return None
-        d = self._json["diffs"][0]
-        return d.get("newValue", "").replace(" ", ",")
-
-    def changelog_type(self):
-        ctype = (None, None)
-        if self.is_assignment():
-            ctype = ("ASSIGN", self.new_assignee())
-        elif self.is_reopen():
-            ctype = ("REOPEN", None)
-        elif self.is_confirm():
-            ctype = ("CONFIRM", None)
-        elif self.is_unconfirm():
-            ctype = ("UNCONFIRM", None)
-        elif self.is_change_severity():
-            ctype = ("SEVERITY", self.new_severity())
-        elif self.is_change_type():
-            ctype = ("TYPE", self.new_type())
-        elif self.is_resolve_as_fixed():
-            ctype = ("FIXED", None)
-        elif self.is_resolve_as_fp():
-            ctype = ("FALSE-POSITIVE", None)
-        elif self.is_resolve_as_wf():
-            ctype = ("WONT-FIX", None)
-        elif self.is_tag():
-            ctype = ("TAG", self.tags())
-        elif self.is_closed():
-            ctype = ("CLOSED", None)
-        elif self.is_mark_as_safe():
-            ctype = ("HOTSPOT_SAFE", None)
-        elif self.is_mark_as_fixed():
-            ctype = ("HOTSPOT_FIXED", None)
-        elif self.is_mark_as_to_review():
-            ctype = ("HOTSPOT_TO_REVIEW", None)
-        elif self.is_mark_as_acknowledged():
-            ctype = ("HOTSPOT_ACKNOWLEDGED", None)
-        elif self.is_technical_change():
-            ctype = ("INTERNAL", None)
-        else:
-            util.logger.warning("Could not determine changelog type for %s", str(self))
-        return ctype
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+
+import sonar.utilities as util
+
+
+class Changelog:
+    def __init__(self, jsonlog):
+        self._json = jsonlog
+        self._change_type = None
+
+    def __str__(self):
+        return str(self._json)
+
+    def __is_resolve_as(self, resolve_reason):
+        cond1 = False
+        cond2 = False
+        for diff in self._json["diffs"]:
+            if diff["key"] == "resolution" and "newValue" in diff and diff["newValue"] == resolve_reason:
+                cond1 = True
+            if diff["key"] == "status" and "newValue" in diff and diff["newValue"] == "RESOLVED":
+                cond2 = True
+        return cond1 and cond2
+
+    def is_resolve_as_fixed(self):
+        return self.__is_resolve_as("FIXED")
+
+    def is_resolve_as_fp(self):
+        return self.__is_resolve_as("FALSE-POSITIVE")
+
+    def is_resolve_as_wf(self):
+        return self.__is_resolve_as("WONTFIX")
+
+    def is_closed(self):
+        """{'creationDate': '2022-02-01T19:15:24+0100', 'diffs': [
+        {'key': 'resolution', 'newValue': 'FIXED'},
+        {'key': 'status', 'newValue': 'CLOSED', 'oldValue': 'OPEN'}]}"""
+        for diff in self._json["diffs"]:
+            if diff["key"] == "status" and "newValue" in diff and diff["newValue"] == "CLOSED":
+                return True
+        return False
+
+    def __is_status(self, status):
+        for d in self._json["diffs"]:
+            if d.get("key", "") == "status" and d.get("newValue", "") == status:
+                return True
+        return False
+
+    def is_reopen(self):
+        for d in self._json["diffs"]:
+            if d.get("key", "") == "status" and (
+                (d.get("newValue", "") == "REOPENED" and d.get("oldValue", "") != "CONFIRMED")
+                or (d.get("newValue", "") == "OPEN" and d.get("oldValue", "") == "CLOSED")
+            ):
+                return True
+        return False
+
+    def is_confirm(self):
+        return self.__is_status("CONFIRMED")
+
+    def is_unconfirm(self):
+        for d in self._json["diffs"]:
+            if d.get("key", "") == "status" and d.get("newValue", "") == "REOPENED" and d.get("oldValue", "") == "CONFIRMED":
+                return True
+        return False
+
+    def is_mark_as_safe(self):
+        for d in self._json["diffs"]:
+            if d.get("key", "") == "resolution" and d.get("newValue", "") == "SAFE":
+                return True
+        return False
+
+    def is_mark_as_to_review(self):
+        for d in self._json["diffs"]:
+            if d.get("key", "") == "status" and d.get("newValue", "") == "TO_REVIEW":
+                return True
+        return False
+
+    def is_mark_as_fixed(self):
+        for d in self._json["diffs"]:
+            if d.get("key", "") == "resolution" and d.get("newValue", "") == "FIXED":
+                return True
+        return False
+
+    def is_mark_as_acknowledged(self):
+        for d in self._json["diffs"]:
+            if d.get("key", "") == "resolution" and d.get("newValue", "") == "ACKNOWLEDGED":
+                return True
+        return False
+
+    def is_change_severity(self):
+        d = self._json["diffs"][0]
+        return d.get("key", "") == "severity"
+
+    def new_severity(self):
+        if self.is_change_severity():
+            d = self._json["diffs"][0]
+            return d.get("newValue", None)
+        return None
+
+    def is_change_type(self):
+        d = self._json["diffs"][0]
+        return d.get("key", "") == "type"
+
+    def new_type(self):
+        if self.is_change_type():
+            d = self._json["diffs"][0]
+            return d.get("newValue", None)
+        return None
+
+    def is_technical_change(self):
+        d = self._json["diffs"][0]
+        key = d.get("key", "")
+        return key in ("from_short_branch", "from_branch", "effort")
+
+    def is_assignment(self):
+        d = self._json["diffs"][0]
+        return d.get("key", "") == "assignee"
+
+    def new_assignee(self):
+        d = self._json["diffs"][0]
+        return d.get("newValue", None)
+
+    def old_assignee(self):
+        d = self._json["diffs"][0]
+        return d.get("oldValue", None)
+
+    def previous_state(self):
+        for d in self._json["diffs"]:
+            if d.get("key", "") == "status":
+                return d.get("oldValue", "")
+        return ""
+
+    def date(self):
+        return self._json["creationDate"]
+
+    def author(self):
+        return self._json.get("user", None)
+
+    def is_tag(self):
+        d = self._json["diffs"][0]
+        return d.get("key", "") == "tag"
+
+    def tags(self):
+        if not self.is_tag():
+            return None
+        d = self._json["diffs"][0]
+        return d.get("newValue", "").replace(" ", ",")
+
+    def changelog_type(self):
+        ctype = (None, None)
+        if self.is_assignment():
+            ctype = ("ASSIGN", self.new_assignee())
+        elif self.is_reopen():
+            ctype = ("REOPEN", None)
+        elif self.is_confirm():
+            ctype = ("CONFIRM", None)
+        elif self.is_unconfirm():
+            ctype = ("UNCONFIRM", None)
+        elif self.is_change_severity():
+            ctype = ("SEVERITY", self.new_severity())
+        elif self.is_change_type():
+            ctype = ("TYPE", self.new_type())
+        elif self.is_resolve_as_fixed():
+            ctype = ("FIXED", None)
+        elif self.is_resolve_as_fp():
+            ctype = ("FALSE-POSITIVE", None)
+        elif self.is_resolve_as_wf():
+            ctype = ("WONT-FIX", None)
+        elif self.is_tag():
+            ctype = ("TAG", self.tags())
+        elif self.is_closed():
+            ctype = ("CLOSED", None)
+        elif self.is_mark_as_safe():
+            ctype = ("HOTSPOT_SAFE", None)
+        elif self.is_mark_as_fixed():
+            ctype = ("HOTSPOT_FIXED", None)
+        elif self.is_mark_as_to_review():
+            ctype = ("HOTSPOT_TO_REVIEW", None)
+        elif self.is_mark_as_acknowledged():
+            ctype = ("HOTSPOT_ACKNOWLEDGED", None)
+        elif self.is_technical_change():
+            ctype = ("INTERNAL", None)
+        else:
+            util.logger.warning("Could not determine changelog type for %s", str(self))
+        return ctype
```

## sonar/findings/findings.py

 * *Ordering differences only*

```diff
@@ -1,356 +1,356 @@
-#
-# sonar-tools
-# Copyright (C) 2022-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-
-import re
-import sonar.sqobject as sq
-import sonar.utilities as util
-from sonar.projects import projects
-
-_JSON_FIELDS_REMAPPED = (("pull_request", "pullRequest"), ("_comments", "comments"))
-
-_JSON_FIELDS_PRIVATE = (
-    "endpoint",
-    "id",
-    "_json",
-    "_changelog",
-    "assignee",
-    "hash",
-    "sonarqube",
-    "creation_date",
-    "modification_date",
-    "_debt",
-    "component",
-    "language",
-    "resolution",
-)
-
-_CSV_FIELDS = (
-    "key",
-    "rule",
-    "type",
-    "severity",
-    "status",
-    "creationDate",
-    "updateDate",
-    "projectKey",
-    "projectName",
-    "branch",
-    "pullRequest",
-    "file",
-    "line",
-    "effort",
-    "message",
-)
-
-
-class Finding(sq.SqObject):
-    """
-    Abstraction of the SonarQube "findings" concept.
-    A finding is a general concept that can be either an issue or a security hotspot
-    """
-
-    def __init__(self, key, endpoint, data=None, from_export=False):
-        super().__init__(key, endpoint)
-        self.severity = None  #: Severity (str)
-        self.type = None  #: Type (str): VULNERABILITY, BUG, CODE_SMELL or SECURITY_HOTSPOT
-        self.author = None  #: Author (str)
-        self.assignee = None  #: Assignee (str)
-        self.status = None  #: Status (str)
-        self.resolution = None  #: Resolution (str)
-        self.rule = None  #: Rule Id (str)
-        self.projectKey = None  #: Project key (str)
-        self.language = None  #: Language (str)
-        self._changelog = None
-        self._comments = None
-        self.line = None  #: Line (int)
-        self.component = None
-        self.message = None  #: Message
-        self.creation_date = None  #: Creation date (datetime)
-        self.modification_date = None  #: Last modification date (datetime)
-        self.hash = None  #: Hash (str)
-        self.branch = None  #: Branch (str)
-        self.pull_request = None  #: Pull request (str)
-        self._load(data, from_export)
-
-    def _load(self, data, from_export=False):
-        if data is not None:
-            if from_export:
-                self._load_from_export(data)
-            else:
-                self._load_from_search(data)
-
-    def _load_common(self, jsondata):
-        if self._json is None:
-            self._json = jsondata
-        else:
-            self._json.update(jsondata)
-        self.author = jsondata.get("author", None)
-        self.type = jsondata.get("type", None)
-        self.severity = jsondata.get("severity", None)
-
-        self.message = jsondata.get("message", None)
-        self.status = jsondata["status"]
-        self.resolution = jsondata.get("resolution", None)
-        self.rule = jsondata.get("rule", jsondata.get("ruleReference", None))
-        self.line = jsondata.get("line", jsondata.get("lineNumber", None))
-        if self.line == "null":
-            self.line = None
-        if self.line is not None:
-            try:
-                self.line = int(self.line)
-            except ValueError:
-                pass
-
-    def _load_from_search(self, jsondata):
-        self._load_common(jsondata)
-        self.projectKey = jsondata["project"]
-        self.creation_date = util.string_to_date(jsondata["creationDate"])
-        self.modification_date = util.string_to_date(jsondata["updateDate"])
-        self.hash = jsondata.get("hash", None)
-        self.branch = jsondata.get("branch", None)
-        if self.branch is not None:
-            self.branch = re.sub("^BRANCH:", "", self.branch)
-        self.pull_request = jsondata.get("pullRequest", None)
-
-    def _load_from_export(self, jsondata):
-        self._load_common(jsondata)
-        self.projectKey = jsondata["projectKey"]
-        self.creation_date = util.string_to_date(jsondata["createdAt"])
-        self.modification_date = util.string_to_date(jsondata["updatedAt"])
-
-    def url(self):
-        # Must be implemented in sub classes
-        raise NotImplementedError()
-
-    def file(self):
-        """
-        :return: The finding full file path, relative to the rpoject root directory
-        :rtype: str or None if not found
-        """
-        if "component" in self._json:
-            comp = self._json["component"]
-            # Hack: Fix to adapt to the ugly component structure on branches and PR
-            # "component": "src:sonar/hot.py:BRANCH:somebranch"
-            m = re.search("(^.*):BRANCH:", comp)
-            if m:
-                comp = m.group(1)
-            m = re.search("(^.*):PULL_REQUEST:", comp)
-            if m:
-                comp = m.group(1)
-            return comp.split(":")[-1]
-        elif "path" in self._json:
-            return self._json["path"]
-        else:
-            util.logger.warning("Can't find file name for %s", str(self))
-            return None
-
-    def to_csv(self, separator=","):
-        """
-        :param separator: CSV separator, defaults to ","
-        :type separator: str, optional
-        :return: The finding as CSV
-        :rtype: str
-        """
-        data = self.to_json()
-        for field in _CSV_FIELDS:
-            if data.get(field, None) is None:
-                data[field] = ""
-        data["branch"] = util.quote(data["branch"], separator)
-        data["message"] = util.quote(data["message"], separator)
-        data["projectName"] = projects.Project.get_object(key=self.projectKey, endpoint=self.endpoint).name
-        return separator.join([str(data[field]) for field in _CSV_FIELDS])
-
-    def to_json(self):
-        """
-        :return: The finding as dict
-        :rtype: dict
-        """
-        data = vars(self).copy()
-        for old_name, new_name in _JSON_FIELDS_REMAPPED:
-            data[new_name] = data.pop(old_name, None)
-        data["effort"] = ""
-        data["file"] = self.file()
-        data["creationDate"] = self.creation_date.strftime(util.SQ_DATETIME_FORMAT)
-        data["updateDate"] = self.modification_date.strftime(util.SQ_DATETIME_FORMAT)
-        for field in _JSON_FIELDS_PRIVATE:
-            data.pop(field, None)
-        for k in data.copy():
-            if data[k] is None or data[k] == "":
-                data.pop(k)
-        return data
-
-    def is_vulnerability(self):
-        return self.type == "VULNERABILITY"
-
-    def is_hotspot(self):
-        return self.type == "SECURITY_HOTSPOT"
-
-    def is_bug(self):
-        return self.type == "BUG"
-
-    def is_code_smell(self):
-        return self.type == "CODE_SMELL"
-
-    def is_security_issue(self):
-        return self.is_vulnerability() or self.is_hotspot()
-
-    def is_closed(self):
-        return self.status == "CLOSED"
-
-    def changelog(self):
-        # Implemented in subclasses, should not reach this
-        raise NotImplementedError()
-
-    def comments(self):
-        # Implemented in subclasses, should not reach this
-        raise NotImplementedError()
-
-    def has_changelog(self):
-        """
-        :return: Whether the finding has a changelog
-        :rtype: bool
-        """
-        util.logger.debug("%s has %d changelogs", str(self), len(self.changelog()))
-        return len(self.changelog()) > 0
-
-    def has_comments(self):
-        """
-        :return: Whether the finding has comments
-        :rtype: bool
-        """
-        return len(self.comments()) > 0
-
-    def modifiers(self):
-        """
-        :return: the set of users that modified the finding
-        :rtype: set(str)
-        """
-        return set([c.author() for c in self.changelog().values()])
-
-    def commenters(self):
-        """
-        :return: the set of users that commented the finding
-        :rtype: set(str)
-        """
-        return set([v.get("user", None) for v in self.comments() if v.get("user", None)])
-
-    def can_be_synced(self, user_list):
-        """
-        :meta private:
-        """
-        util.logger.debug(
-            "Issue %s: Checking if modifiers %s are different from user %s",
-            str(self),
-            str(self.modifiers()),
-            str(user_list),
-        )
-        if user_list is None:
-            return not self.has_changelog()
-        for u in self.modifiers():
-            if u not in user_list:
-                return False
-        return True
-
-    def strictly_identical_to(self, another_finding, ignore_component=False):
-        """
-        :meta private:
-        """
-        return (
-            self.rule == another_finding.rule
-            and self.hash == another_finding.hash
-            and self.message == another_finding.message
-            and self.file() == another_finding.file()
-            and (self.component == another_finding.component or ignore_component)
-        )
-
-    def almost_identical_to(self, another_finding, ignore_component=False, **kwargs):
-        """
-        :meta private:
-        """
-        if self.rule != another_finding.rule or self.hash != another_finding.hash:
-            return False
-        score = 0
-        if self.message == another_finding.message or kwargs.get("ignore_message", False):
-            score += 2
-        if self.file() == another_finding.file():
-            score += 2
-        if self.line == another_finding.line or kwargs.get("ignore_line", False):
-            score += 1
-        if self.component == another_finding.component or ignore_component:
-            score += 1
-        if self.author == another_finding.author or kwargs.get("ignore_author", False):
-            score += 1
-        if self.type == another_finding.type or kwargs.get("ignore_type", False):
-            score += 1
-        if self.severity == another_finding.severity or kwargs.get("ignore_severity", False):
-            score += 1
-        # Need at least 7 / 9 to match
-        return score >= 7
-
-    def search_siblings(self, findings_list, allowed_users=None, ignore_component=False, **kwargs):
-        """
-        :meta private:
-        """
-        exact_matches = []
-        approx_matches = []
-        match_but_modified = []
-        for key, finding in findings_list.items():
-            if key == self.key:
-                continue
-            if finding.strictly_identical_to(self, ignore_component, **kwargs):
-                util.logger.debug("Issues %s and %s are strictly identical", self.key, key)
-                if finding.can_be_synced(allowed_users):
-                    exact_matches.append(finding)
-                else:
-                    match_but_modified.append(finding)
-            elif finding.almost_identical_to(self, ignore_component, **kwargs):
-                util.logger.debug("Issues %s and %s are almost identical", self.key, key)
-                if finding.can_be_synced(allowed_users):
-                    approx_matches.append(finding)
-                else:
-                    match_but_modified.append(finding)
-            else:
-                util.logger.debug("Issues %s and %s are not siblings", self.key, key)
-        return (exact_matches, approx_matches, match_but_modified)
-
-
-def export_findings(endpoint, project_key, branch=None, pull_request=None):
-    """Export all findings of a given project
-
-    :param endpoint: Reference to the SonarQube platform
-    :type endpoint: Platform
-    :param project_key: The project key
-    :type project_key: str
-    :param branch: Branch to select for export (exclusive of pull_request), defaults to None
-    :type branch: str, optional
-    :param pull_request: Pull request to select for export (exclusive of branch), default to None
-    :type pull_request: str, optional
-    :return: list of Findings (Issues or Hotspots)
-    :rtype: dict{<key>: <Finding>}
-    """
-    util.logger.info("Using new export findings to speed up issue export")
-    return projects.Project(key=project_key, endpoint=endpoint).get_findings(branch, pull_request)
-
-
-def to_csv_header(separator=","):
-    """
-    :meta private:
-    """
-    return "# " + separator.join(_CSV_FIELDS)
+#
+# sonar-tools
+# Copyright (C) 2022-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+
+import re
+import sonar.sqobject as sq
+import sonar.utilities as util
+from sonar.projects import projects
+
+_JSON_FIELDS_REMAPPED = (("pull_request", "pullRequest"), ("_comments", "comments"))
+
+_JSON_FIELDS_PRIVATE = (
+    "endpoint",
+    "id",
+    "_json",
+    "_changelog",
+    "assignee",
+    "hash",
+    "sonarqube",
+    "creation_date",
+    "modification_date",
+    "_debt",
+    "component",
+    "language",
+    "resolution",
+)
+
+_CSV_FIELDS = (
+    "key",
+    "rule",
+    "type",
+    "severity",
+    "status",
+    "creationDate",
+    "updateDate",
+    "projectKey",
+    "projectName",
+    "branch",
+    "pullRequest",
+    "file",
+    "line",
+    "effort",
+    "message",
+)
+
+
+class Finding(sq.SqObject):
+    """
+    Abstraction of the SonarQube "findings" concept.
+    A finding is a general concept that can be either an issue or a security hotspot
+    """
+
+    def __init__(self, key, endpoint, data=None, from_export=False):
+        super().__init__(key, endpoint)
+        self.severity = None  #: Severity (str)
+        self.type = None  #: Type (str): VULNERABILITY, BUG, CODE_SMELL or SECURITY_HOTSPOT
+        self.author = None  #: Author (str)
+        self.assignee = None  #: Assignee (str)
+        self.status = None  #: Status (str)
+        self.resolution = None  #: Resolution (str)
+        self.rule = None  #: Rule Id (str)
+        self.projectKey = None  #: Project key (str)
+        self.language = None  #: Language (str)
+        self._changelog = None
+        self._comments = None
+        self.line = None  #: Line (int)
+        self.component = None
+        self.message = None  #: Message
+        self.creation_date = None  #: Creation date (datetime)
+        self.modification_date = None  #: Last modification date (datetime)
+        self.hash = None  #: Hash (str)
+        self.branch = None  #: Branch (str)
+        self.pull_request = None  #: Pull request (str)
+        self._load(data, from_export)
+
+    def _load(self, data, from_export=False):
+        if data is not None:
+            if from_export:
+                self._load_from_export(data)
+            else:
+                self._load_from_search(data)
+
+    def _load_common(self, jsondata):
+        if self._json is None:
+            self._json = jsondata
+        else:
+            self._json.update(jsondata)
+        self.author = jsondata.get("author", None)
+        self.type = jsondata.get("type", None)
+        self.severity = jsondata.get("severity", None)
+
+        self.message = jsondata.get("message", None)
+        self.status = jsondata["status"]
+        self.resolution = jsondata.get("resolution", None)
+        self.rule = jsondata.get("rule", jsondata.get("ruleReference", None))
+        self.line = jsondata.get("line", jsondata.get("lineNumber", None))
+        if self.line == "null":
+            self.line = None
+        if self.line is not None:
+            try:
+                self.line = int(self.line)
+            except ValueError:
+                pass
+
+    def _load_from_search(self, jsondata):
+        self._load_common(jsondata)
+        self.projectKey = jsondata["project"]
+        self.creation_date = util.string_to_date(jsondata["creationDate"])
+        self.modification_date = util.string_to_date(jsondata["updateDate"])
+        self.hash = jsondata.get("hash", None)
+        self.branch = jsondata.get("branch", None)
+        if self.branch is not None:
+            self.branch = re.sub("^BRANCH:", "", self.branch)
+        self.pull_request = jsondata.get("pullRequest", None)
+
+    def _load_from_export(self, jsondata):
+        self._load_common(jsondata)
+        self.projectKey = jsondata["projectKey"]
+        self.creation_date = util.string_to_date(jsondata["createdAt"])
+        self.modification_date = util.string_to_date(jsondata["updatedAt"])
+
+    def url(self):
+        # Must be implemented in sub classes
+        raise NotImplementedError()
+
+    def file(self):
+        """
+        :return: The finding full file path, relative to the rpoject root directory
+        :rtype: str or None if not found
+        """
+        if "component" in self._json:
+            comp = self._json["component"]
+            # Hack: Fix to adapt to the ugly component structure on branches and PR
+            # "component": "src:sonar/hot.py:BRANCH:somebranch"
+            m = re.search("(^.*):BRANCH:", comp)
+            if m:
+                comp = m.group(1)
+            m = re.search("(^.*):PULL_REQUEST:", comp)
+            if m:
+                comp = m.group(1)
+            return comp.split(":")[-1]
+        elif "path" in self._json:
+            return self._json["path"]
+        else:
+            util.logger.warning("Can't find file name for %s", str(self))
+            return None
+
+    def to_csv(self, separator=","):
+        """
+        :param separator: CSV separator, defaults to ","
+        :type separator: str, optional
+        :return: The finding as CSV
+        :rtype: str
+        """
+        data = self.to_json()
+        for field in _CSV_FIELDS:
+            if data.get(field, None) is None:
+                data[field] = ""
+        data["branch"] = util.quote(data["branch"], separator)
+        data["message"] = util.quote(data["message"], separator)
+        data["projectName"] = projects.Project.get_object(key=self.projectKey, endpoint=self.endpoint).name
+        return separator.join([str(data[field]) for field in _CSV_FIELDS])
+
+    def to_json(self):
+        """
+        :return: The finding as dict
+        :rtype: dict
+        """
+        data = vars(self).copy()
+        for old_name, new_name in _JSON_FIELDS_REMAPPED:
+            data[new_name] = data.pop(old_name, None)
+        data["effort"] = ""
+        data["file"] = self.file()
+        data["creationDate"] = self.creation_date.strftime(util.SQ_DATETIME_FORMAT)
+        data["updateDate"] = self.modification_date.strftime(util.SQ_DATETIME_FORMAT)
+        for field in _JSON_FIELDS_PRIVATE:
+            data.pop(field, None)
+        for k in data.copy():
+            if data[k] is None or data[k] == "":
+                data.pop(k)
+        return data
+
+    def is_vulnerability(self):
+        return self.type == "VULNERABILITY"
+
+    def is_hotspot(self):
+        return self.type == "SECURITY_HOTSPOT"
+
+    def is_bug(self):
+        return self.type == "BUG"
+
+    def is_code_smell(self):
+        return self.type == "CODE_SMELL"
+
+    def is_security_issue(self):
+        return self.is_vulnerability() or self.is_hotspot()
+
+    def is_closed(self):
+        return self.status == "CLOSED"
+
+    def changelog(self):
+        # Implemented in subclasses, should not reach this
+        raise NotImplementedError()
+
+    def comments(self):
+        # Implemented in subclasses, should not reach this
+        raise NotImplementedError()
+
+    def has_changelog(self):
+        """
+        :return: Whether the finding has a changelog
+        :rtype: bool
+        """
+        util.logger.debug("%s has %d changelogs", str(self), len(self.changelog()))
+        return len(self.changelog()) > 0
+
+    def has_comments(self):
+        """
+        :return: Whether the finding has comments
+        :rtype: bool
+        """
+        return len(self.comments()) > 0
+
+    def modifiers(self):
+        """
+        :return: the set of users that modified the finding
+        :rtype: set(str)
+        """
+        return set([c.author() for c in self.changelog().values()])
+
+    def commenters(self):
+        """
+        :return: the set of users that commented the finding
+        :rtype: set(str)
+        """
+        return set([v.get("user", None) for v in self.comments() if v.get("user", None)])
+
+    def can_be_synced(self, user_list):
+        """
+        :meta private:
+        """
+        util.logger.debug(
+            "Issue %s: Checking if modifiers %s are different from user %s",
+            str(self),
+            str(self.modifiers()),
+            str(user_list),
+        )
+        if user_list is None:
+            return not self.has_changelog()
+        for u in self.modifiers():
+            if u not in user_list:
+                return False
+        return True
+
+    def strictly_identical_to(self, another_finding, ignore_component=False):
+        """
+        :meta private:
+        """
+        return (
+            self.rule == another_finding.rule
+            and self.hash == another_finding.hash
+            and self.message == another_finding.message
+            and self.file() == another_finding.file()
+            and (self.component == another_finding.component or ignore_component)
+        )
+
+    def almost_identical_to(self, another_finding, ignore_component=False, **kwargs):
+        """
+        :meta private:
+        """
+        if self.rule != another_finding.rule or self.hash != another_finding.hash:
+            return False
+        score = 0
+        if self.message == another_finding.message or kwargs.get("ignore_message", False):
+            score += 2
+        if self.file() == another_finding.file():
+            score += 2
+        if self.line == another_finding.line or kwargs.get("ignore_line", False):
+            score += 1
+        if self.component == another_finding.component or ignore_component:
+            score += 1
+        if self.author == another_finding.author or kwargs.get("ignore_author", False):
+            score += 1
+        if self.type == another_finding.type or kwargs.get("ignore_type", False):
+            score += 1
+        if self.severity == another_finding.severity or kwargs.get("ignore_severity", False):
+            score += 1
+        # Need at least 7 / 9 to match
+        return score >= 7
+
+    def search_siblings(self, findings_list, allowed_users=None, ignore_component=False, **kwargs):
+        """
+        :meta private:
+        """
+        exact_matches = []
+        approx_matches = []
+        match_but_modified = []
+        for key, finding in findings_list.items():
+            if key == self.key:
+                continue
+            if finding.strictly_identical_to(self, ignore_component, **kwargs):
+                util.logger.debug("Issues %s and %s are strictly identical", self.key, key)
+                if finding.can_be_synced(allowed_users):
+                    exact_matches.append(finding)
+                else:
+                    match_but_modified.append(finding)
+            elif finding.almost_identical_to(self, ignore_component, **kwargs):
+                util.logger.debug("Issues %s and %s are almost identical", self.key, key)
+                if finding.can_be_synced(allowed_users):
+                    approx_matches.append(finding)
+                else:
+                    match_but_modified.append(finding)
+            else:
+                util.logger.debug("Issues %s and %s are not siblings", self.key, key)
+        return (exact_matches, approx_matches, match_but_modified)
+
+
+def export_findings(endpoint, project_key, branch=None, pull_request=None):
+    """Export all findings of a given project
+
+    :param endpoint: Reference to the SonarQube platform
+    :type endpoint: Platform
+    :param project_key: The project key
+    :type project_key: str
+    :param branch: Branch to select for export (exclusive of pull_request), defaults to None
+    :type branch: str, optional
+    :param pull_request: Pull request to select for export (exclusive of branch), default to None
+    :type pull_request: str, optional
+    :return: list of Findings (Issues or Hotspots)
+    :rtype: dict{<key>: <Finding>}
+    """
+    util.logger.info("Using new export findings to speed up issue export")
+    return projects.Project(key=project_key, endpoint=endpoint).get_findings(branch, pull_request)
+
+
+def to_csv_header(separator=","):
+    """
+    :meta private:
+    """
+    return "# " + separator.join(_CSV_FIELDS)
```

## sonar/findings/hotspots.py

 * *Ordering differences only*

```diff
@@ -1,435 +1,435 @@
-#
-# sonar-tools
-# Copyright (C) 2022-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-"""Abstraction of the SonarQube "hotspot" concept"""
-
-import json
-import re
-import requests.utils
-import sonar.utilities as util
-from sonar import syncer, users
-from sonar.projects import projects
-from sonar.findings import findings, changelog
-
-SEARCH_CRITERIAS = (
-    "branch",
-    "cwe",
-    "files",
-    "hotspots",
-    "onlyMine",
-    "owaspTop10",
-    "owaspTop10-2021",
-    "p",
-    "ps",
-    "projectKey",
-    "pullRequest",
-    "resolution",
-    "sansTop25",
-    "sinceLeakPeriod",
-    "sonarsourceSecurity",
-    "status",
-)
-
-TYPES = ("SECURITY_HOTSPOT",)
-RESOLUTIONS = ("SAFE", "ACKNOWLEDGED", "FIXED")
-STATUSES = ("TO_REVIEW", "REVIEWED")
-SEVERITIES = ()
-
-_OBJECTS = {}
-
-
-class TooManyHotspotsError(Exception):
-    def __init__(self, nbr_issues, message):
-        super().__init__()
-        self.nbr_issues = nbr_issues
-        self.message = message
-
-
-class Hotspot(findings.Finding):
-    def __init__(self, key, endpoint, data=None, from_export=False):
-        super().__init__(key, endpoint, data, from_export)
-        self.vulnerabilityProbability = None  #:
-        self.category = data["securityCategory"]  #:
-        self.vulnerabilityProbability = data["vulnerabilityProbability"]  #:
-        self.securityCategory = None  #:
-        self.type = "SECURITY_HOTSPOT"
-        self.__details = None
-
-        # FIXME: Ugly hack to fix how hotspot branches are managed
-        m = re.match(r"^(.*):BRANCH:(.*)$", self.projectKey)
-        if m:
-            self.projectKey = m.group(1)
-            self.branch = m.group(2)
-        m = re.match(r"^(.*):PULL_REQUEST:(.*)$", self.projectKey)
-        if m:
-            self.projectKey = m.group(1)
-            self.branch = m.group(2)
-        _OBJECTS[self.uuid()] = self
-        if self.rule is None and self.refresh():
-            self.rule = self.__details["rule"]["key"]
-
-    def __str__(self):
-        """
-        :return: String representation of the hotspot
-        :rtype: str
-        """
-        return f"Hotspot key '{self.key}'"
-
-    def url(self):
-        """
-        :return: Permalink URL to the hotspot in the SonarQube platform
-        :rtype: str
-        """
-        branch = ""
-        if self.branch is not None:
-            branch = f"branch={requests.utils.quote(self.branch)}&"
-        elif self.pull_request is not None:
-            branch = f"pullRequest={requests.utils.quote(self.pull_request)}&"
-        return f"{self.endpoint.url}/security_hotspots?{branch}id={self.projectKey}&hotspots={self.key}"
-
-    def to_json(self):
-        """
-        :return: JSON representation of the hotspot
-        :rtype: dict
-        """
-        data = super().to_json()
-        data["url"] = self.url()
-        return data
-
-    def refresh(self):
-        """Refreshes and reads hotspots details in SonarQube
-        :return: The hotspot details
-        :rtype: Whether ther operation succeeded
-        """
-        resp = self.get("hotspots/show", {"hotspot": self.key})
-        if resp.ok:
-            self.__details = json.loads(resp.text)
-        return resp.ok
-
-    def __mark_as(self, resolution, comment=None):
-        params = {"hotspot": self.key, "status": "REVIEWED", "resolution": resolution}
-        if comment is not None:
-            params["comment"] = comment
-        return self.post("hotspots/change_status", params=params).ok
-
-    def mark_as_safe(self):
-        """Marks a hotspot as safe
-
-        :return: Whether the operation succeeded
-        :rtype: bool
-        """
-        return self.__mark_as("SAFE")
-
-    def mark_as_fixed(self):
-        """Marks a hotspot as fixed
-
-        :return: Whether the operation succeeded
-        :rtype: bool
-        """
-        return self.__mark_as("FIXED")
-
-    def mark_as_acknowledged(self):
-        """Marks a hotspot as acknowledged
-
-        :return: Whether the operation succeeded
-        :rtype: bool
-        """
-        if self.endpoint.version() < (9, 4, 0):
-            util.logger.warning("Platform version is < 9.4, can't acknowledge %s", str(self))
-            return False
-        return self.__mark_as("ACKNOWLEDGED")
-
-    def mark_as_to_review(self):
-        """Marks a hotspot as to review
-
-        :return: Whether the operation succeeded
-        :rtype: bool
-        """
-        return self.post("hotspots/change_status", params={"hotspot": self.key, "status": "TO_REVIEW"}).ok
-
-    def reopen(self):
-        """Reopens a hotspot as to review
-
-        :return: Whether the operation succeeded
-        :rtype: bool
-        """
-        return self.mark_as_to_review()
-
-    def add_comment(self, comment):
-        """Adds a comment to a hotspot
-
-        :param comment: Comment to add, in markdown format
-        :type comment: str
-        :return: Whether the operation succeeded
-        :rtype: bool
-        """
-        params = {"hotspot": self.key, "comment": comment}
-        return self.post("hotspots/add_comment", params=params).ok
-
-    def assign(self, assignee, comment=None):
-        """Assigns a hotspot (and optionally comment)
-
-        :param assignee: User login to assign the hotspot
-        :type assignee: str
-        :param comment: Comment to add, in markdown format, defaults to None
-        :type comment: str, optional
-        :return: Whether the operation succeeded
-        :rtype: bool
-        """
-        params = {"hotspot": self.key, "assignee": assignee}
-        if comment is not None:
-            params["comment"] = comment
-        return self.post("hotspots/assign", params=params)
-
-    def __apply_event(self, event, settings):
-        util.logger.debug("Applying event %s", str(event))
-        # origin = f"originally by *{event['userName']}* on original branch"
-        (event_type, data) = event.changelog_type()
-        if event_type == "HOTSPOT_SAFE":
-            self.mark_as_safe()
-            # self.add_comment(f"Hotspot review safe {origin}")
-        elif event_type == "HOTSPOT_FIXED":
-            self.mark_as_fixed()
-            # self.add_comment(f"Hotspot marked as fixed {origin}", settings[SYNC_ADD_COMMENTS])
-        elif event_type == "HOTSPOT_TO_REVIEW":
-            self.mark_as_to_review()
-            # self.add_comment(f"Hotspot marked as fixed {origin}", settings[SYNC_ADD_COMMENTS])
-        elif event_type == "HOTSPOT_ACKNOWLEDGED":
-            self.mark_as_acknowledged()
-            # self.add_comment(f"Hotspot marked as acknowledged {origin}", settings[SYNC_ADD_COMMENTS])
-        elif event_type == "ASSIGN":
-            if settings[syncer.SYNC_ASSIGN]:
-                u = users.get_login_from_name(data, endpoint=self.endpoint)
-                if u is None:
-                    u = settings[syncer.SYNC_SERVICE_ACCOUNTS][0]
-                self.assign(u)
-                # self.add_comment(f"Hotspot assigned assigned {origin}", settings[SYNC_ADD_COMMENTS])
-
-        elif event_type == "INTERNAL":
-            util.logger.info("Changelog %s is internal, it will not be applied...", str(event))
-            # self.add_comment(f"Change of issue type {origin}", settings[SYNC_ADD_COMMENTS])
-        else:
-            util.logger.error("Event %s can't be applied", str(event))
-            return False
-        return True
-
-    def apply_changelog(self, source_hotspot, settings):
-        """
-        :meta private:
-        """
-        events = source_hotspot.changelog()
-        if events is None or not events:
-            util.logger.debug("Sibling %s has no changelog, no action taken", str(source_hotspot))
-            return False
-
-        change_nbr = 0
-        start_change = len(self.changelog()) + 1
-        util.logger.debug("Applying changelog of %s to %s, from change %d", str(source_hotspot), str(self), start_change)
-        for key in sorted(events.keys()):
-            change_nbr += 1
-            if change_nbr < start_change:
-                util.logger.debug("Skipping change already applied in a previous sync: %s", str(events[key]))
-                continue
-            self.__apply_event(events[key], settings)
-
-        comments = source_hotspot.comments()
-        if len(self.comments()) == 0 and settings[syncer.SYNC_ADD_LINK]:
-            util.logger.info("Target %s has 0 comments, adding sync link comment", str(self))
-            start_change = 1
-            self.add_comment(f"Automatically synchronized from [this original issue]({source_hotspot.url()})")
-        else:
-            start_change = len(self.comments())
-            util.logger.info("Target %s already has %d comments", str(self), start_change)
-        util.logger.info(
-            "Applying comments of %s to %s, from comment %d",
-            str(source_hotspot),
-            str(self),
-            start_change,
-        )
-        change_nbr = 0
-        for key in sorted(comments.keys()):
-            change_nbr += 1
-            if change_nbr < start_change:
-                util.logger.debug(
-                    "Skipping comment already applied in a previous sync: %s",
-                    str(comments[key]),
-                )
-                continue
-            # origin = f"originally by *{event['userName']}* on original branch"
-            self.add_comment(comments[key]["value"])
-        return True
-
-    def changelog(self):
-        """
-        :return: The hotspot changelog
-        :rtype: dict
-        """
-        if self._changelog is not None:
-            return self._changelog
-        if not self.__details:
-            self.refresh()
-        util.json_dump_debug(self.__details, f"{str(self)} Details = ")
-        self._changelog = {}
-        seq = 1
-        for l in self.__details["changelog"]:
-            d = changelog.Changelog(l)
-            if d.is_technical_change():
-                # Skip automatic changelog events generated by SonarSource itself
-                util.logger.debug("Changelog is a technical change: %s", str(d))
-                continue
-            util.json_dump_debug(l, "Changelog item Changelog ADDED = ")
-            seq += 1
-            self._changelog[f"{d.date()}_{seq:03d}"] = d
-        return self._changelog
-
-    def comments(self):
-        """
-        :return: The hotspot comments
-        :rtype: dict
-        """
-        if self._comments is not None:
-            return self._comments
-        if not self.__details:
-            self.refresh()
-        self._comments = {}
-        seq = 0
-        for c in self.__details["comment"]:
-            seq += 1
-            self._comments[f"{c['createdAt']}_{seq:03d}"] = {
-                "date": c["createdAt"],
-                "event": "comment",
-                "value": c["markdown"],
-                "user": c["login"],
-                "userName": c["login"],
-                "commentKey": c["key"],
-            }
-        return self._comments
-
-
-def search_by_project(project_key, endpoint=None, params=None):
-    """Searches hotspots of a project
-
-    :param endpoint: Reference to the SonarQube platform
-    :type endpoint: Platform
-    :param project_key: Project key
-    :type project_key: str
-    :param params: Search filters to narrow down the search, defaults to None
-    :type params: dict
-    :return: List of found hotspots
-    :rtype: dict{<key>: <Hotspot>}
-    """
-    new_params = {} if params is None else params.copy()
-    if project_key is None:
-        key_list = projects.search(endpoint).keys()
-    else:
-        key_list = util.csv_to_list(project_key)
-    hotspots = {}
-    for k in key_list:
-        new_params["projectKey"] = k
-        project_hotspots = search(endpoint=endpoint, params=new_params)
-        util.logger.debug("Project '%s' has %d hotspots", k, len(project_hotspots))
-        hotspots.update(project_hotspots)
-    return hotspots
-
-
-def search(endpoint, page=None, params=None):
-    """Searches hotspots
-
-    :param endpoint: Reference to the SonarQube platform
-    :type endpoint: Platform
-    :param project_key: Project key
-    :type project_key: str
-    :param params: Search filters to narrow down the search, defaults to None
-    :type params: dict
-    :return: List of found hotspots
-    :rtype: dict{<key>: <Hotspot>}
-    """
-    hotspots_list = {}
-    new_params = {} if params is None else params.copy()
-    r_list = util.csv_to_list(params.get("resolution", None))
-    s_list = util.csv_to_list(params.get("status", None))
-    if len(r_list) > 1:
-        for r in r_list:
-            new_params["resolution"] = r
-            hotspots_list.update(search(endpoint, params=new_params))
-        return hotspots_list
-    elif len(s_list) > 1:
-        for s in s_list:
-            new_params["status"] = s
-            hotspots_list.update(search(endpoint, params=new_params))
-        return hotspots_list
-
-    new_params["ps"] = 500
-    p = 1
-    while True:
-        if page is None:
-            new_params["p"] = p
-        else:
-            new_params["p"] = page
-        resp = endpoint.get("hotspots/search", params=new_params)
-        data = json.loads(resp.text)
-        nbr_hotspots = data["paging"]["total"]
-        nbr_pages = (nbr_hotspots + 499) // 500
-        util.logger.debug(
-            "Number of issues: %d - Page: %d/%d",
-            nbr_hotspots,
-            new_params["p"],
-            nbr_pages,
-        )
-        if page is None and nbr_hotspots > 10000:
-            raise TooManyHotspotsError(
-                nbr_hotspots,
-                f"{nbr_hotspots} hotpots returned by api/hotspots/search, " "this is more than the max 10000 possible",
-            )
-
-        for i in data["hotspots"]:
-            if "branch" in params:
-                i["branch"] = params["branch"]
-            if "pullRequest" in params:
-                i["pullRequest"] = params["pullRequest"]
-            hotspots_list[i["key"]] = get_object(i["key"], endpoint=endpoint, data=i)
-        if page is not None or p >= nbr_pages:
-            break
-        p += 1
-    return hotspots_list
-
-
-def get_object(key, data=None, endpoint=None, from_export=False):
-    if key not in _OBJECTS:
-        _ = Hotspot(key=key, data=data, endpoint=endpoint, from_export=from_export)
-    return _OBJECTS[key]
-
-
-def get_search_criteria(params):
-    """Returns the filtered list of params that are allowed for api/issue/search"""
-    criterias = {} if params is None else params.copy()
-    for old, new in {
-        "resolutions": "resolution",
-        "componentsKey": "projectKey",
-        "statuses": "status",
-    }.items():
-        if old in params:
-            criterias[new] = params[old]
-    if criterias.get("status", None) is not None:
-        criterias["status"] = util.allowed_values_string(criterias["status"], STATUSES)
-    if criterias.get("resolution", None) is not None:
-        criterias["resolution"] = util.allowed_values_string(criterias["resolution"], RESOLUTIONS)
-        util.logger.error("hotspot 'status' criteria incompatible with 'resolution' criteria, ignoring 'status'")
-        criterias["status"] = "REVIEWED"
-    return util.dict_subset(util.remove_nones(criterias), SEARCH_CRITERIAS)
+#
+# sonar-tools
+# Copyright (C) 2022-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+"""Abstraction of the SonarQube "hotspot" concept"""
+
+import json
+import re
+import requests.utils
+import sonar.utilities as util
+from sonar import syncer, users
+from sonar.projects import projects
+from sonar.findings import findings, changelog
+
+SEARCH_CRITERIAS = (
+    "branch",
+    "cwe",
+    "files",
+    "hotspots",
+    "onlyMine",
+    "owaspTop10",
+    "owaspTop10-2021",
+    "p",
+    "ps",
+    "projectKey",
+    "pullRequest",
+    "resolution",
+    "sansTop25",
+    "sinceLeakPeriod",
+    "sonarsourceSecurity",
+    "status",
+)
+
+TYPES = ("SECURITY_HOTSPOT",)
+RESOLUTIONS = ("SAFE", "ACKNOWLEDGED", "FIXED")
+STATUSES = ("TO_REVIEW", "REVIEWED")
+SEVERITIES = ()
+
+_OBJECTS = {}
+
+
+class TooManyHotspotsError(Exception):
+    def __init__(self, nbr_issues, message):
+        super().__init__()
+        self.nbr_issues = nbr_issues
+        self.message = message
+
+
+class Hotspot(findings.Finding):
+    def __init__(self, key, endpoint, data=None, from_export=False):
+        super().__init__(key, endpoint, data, from_export)
+        self.vulnerabilityProbability = None  #:
+        self.category = data["securityCategory"]  #:
+        self.vulnerabilityProbability = data["vulnerabilityProbability"]  #:
+        self.securityCategory = None  #:
+        self.type = "SECURITY_HOTSPOT"
+        self.__details = None
+
+        # FIXME: Ugly hack to fix how hotspot branches are managed
+        m = re.match(r"^(.*):BRANCH:(.*)$", self.projectKey)
+        if m:
+            self.projectKey = m.group(1)
+            self.branch = m.group(2)
+        m = re.match(r"^(.*):PULL_REQUEST:(.*)$", self.projectKey)
+        if m:
+            self.projectKey = m.group(1)
+            self.branch = m.group(2)
+        _OBJECTS[self.uuid()] = self
+        if self.rule is None and self.refresh():
+            self.rule = self.__details["rule"]["key"]
+
+    def __str__(self):
+        """
+        :return: String representation of the hotspot
+        :rtype: str
+        """
+        return f"Hotspot key '{self.key}'"
+
+    def url(self):
+        """
+        :return: Permalink URL to the hotspot in the SonarQube platform
+        :rtype: str
+        """
+        branch = ""
+        if self.branch is not None:
+            branch = f"branch={requests.utils.quote(self.branch)}&"
+        elif self.pull_request is not None:
+            branch = f"pullRequest={requests.utils.quote(self.pull_request)}&"
+        return f"{self.endpoint.url}/security_hotspots?{branch}id={self.projectKey}&hotspots={self.key}"
+
+    def to_json(self):
+        """
+        :return: JSON representation of the hotspot
+        :rtype: dict
+        """
+        data = super().to_json()
+        data["url"] = self.url()
+        return data
+
+    def refresh(self):
+        """Refreshes and reads hotspots details in SonarQube
+        :return: The hotspot details
+        :rtype: Whether ther operation succeeded
+        """
+        resp = self.get("hotspots/show", {"hotspot": self.key})
+        if resp.ok:
+            self.__details = json.loads(resp.text)
+        return resp.ok
+
+    def __mark_as(self, resolution, comment=None):
+        params = {"hotspot": self.key, "status": "REVIEWED", "resolution": resolution}
+        if comment is not None:
+            params["comment"] = comment
+        return self.post("hotspots/change_status", params=params).ok
+
+    def mark_as_safe(self):
+        """Marks a hotspot as safe
+
+        :return: Whether the operation succeeded
+        :rtype: bool
+        """
+        return self.__mark_as("SAFE")
+
+    def mark_as_fixed(self):
+        """Marks a hotspot as fixed
+
+        :return: Whether the operation succeeded
+        :rtype: bool
+        """
+        return self.__mark_as("FIXED")
+
+    def mark_as_acknowledged(self):
+        """Marks a hotspot as acknowledged
+
+        :return: Whether the operation succeeded
+        :rtype: bool
+        """
+        if self.endpoint.version() < (9, 4, 0):
+            util.logger.warning("Platform version is < 9.4, can't acknowledge %s", str(self))
+            return False
+        return self.__mark_as("ACKNOWLEDGED")
+
+    def mark_as_to_review(self):
+        """Marks a hotspot as to review
+
+        :return: Whether the operation succeeded
+        :rtype: bool
+        """
+        return self.post("hotspots/change_status", params={"hotspot": self.key, "status": "TO_REVIEW"}).ok
+
+    def reopen(self):
+        """Reopens a hotspot as to review
+
+        :return: Whether the operation succeeded
+        :rtype: bool
+        """
+        return self.mark_as_to_review()
+
+    def add_comment(self, comment):
+        """Adds a comment to a hotspot
+
+        :param comment: Comment to add, in markdown format
+        :type comment: str
+        :return: Whether the operation succeeded
+        :rtype: bool
+        """
+        params = {"hotspot": self.key, "comment": comment}
+        return self.post("hotspots/add_comment", params=params).ok
+
+    def assign(self, assignee, comment=None):
+        """Assigns a hotspot (and optionally comment)
+
+        :param assignee: User login to assign the hotspot
+        :type assignee: str
+        :param comment: Comment to add, in markdown format, defaults to None
+        :type comment: str, optional
+        :return: Whether the operation succeeded
+        :rtype: bool
+        """
+        params = {"hotspot": self.key, "assignee": assignee}
+        if comment is not None:
+            params["comment"] = comment
+        return self.post("hotspots/assign", params=params)
+
+    def __apply_event(self, event, settings):
+        util.logger.debug("Applying event %s", str(event))
+        # origin = f"originally by *{event['userName']}* on original branch"
+        (event_type, data) = event.changelog_type()
+        if event_type == "HOTSPOT_SAFE":
+            self.mark_as_safe()
+            # self.add_comment(f"Hotspot review safe {origin}")
+        elif event_type == "HOTSPOT_FIXED":
+            self.mark_as_fixed()
+            # self.add_comment(f"Hotspot marked as fixed {origin}", settings[SYNC_ADD_COMMENTS])
+        elif event_type == "HOTSPOT_TO_REVIEW":
+            self.mark_as_to_review()
+            # self.add_comment(f"Hotspot marked as fixed {origin}", settings[SYNC_ADD_COMMENTS])
+        elif event_type == "HOTSPOT_ACKNOWLEDGED":
+            self.mark_as_acknowledged()
+            # self.add_comment(f"Hotspot marked as acknowledged {origin}", settings[SYNC_ADD_COMMENTS])
+        elif event_type == "ASSIGN":
+            if settings[syncer.SYNC_ASSIGN]:
+                u = users.get_login_from_name(data, endpoint=self.endpoint)
+                if u is None:
+                    u = settings[syncer.SYNC_SERVICE_ACCOUNTS][0]
+                self.assign(u)
+                # self.add_comment(f"Hotspot assigned assigned {origin}", settings[SYNC_ADD_COMMENTS])
+
+        elif event_type == "INTERNAL":
+            util.logger.info("Changelog %s is internal, it will not be applied...", str(event))
+            # self.add_comment(f"Change of issue type {origin}", settings[SYNC_ADD_COMMENTS])
+        else:
+            util.logger.error("Event %s can't be applied", str(event))
+            return False
+        return True
+
+    def apply_changelog(self, source_hotspot, settings):
+        """
+        :meta private:
+        """
+        events = source_hotspot.changelog()
+        if events is None or not events:
+            util.logger.debug("Sibling %s has no changelog, no action taken", str(source_hotspot))
+            return False
+
+        change_nbr = 0
+        start_change = len(self.changelog()) + 1
+        util.logger.debug("Applying changelog of %s to %s, from change %d", str(source_hotspot), str(self), start_change)
+        for key in sorted(events.keys()):
+            change_nbr += 1
+            if change_nbr < start_change:
+                util.logger.debug("Skipping change already applied in a previous sync: %s", str(events[key]))
+                continue
+            self.__apply_event(events[key], settings)
+
+        comments = source_hotspot.comments()
+        if len(self.comments()) == 0 and settings[syncer.SYNC_ADD_LINK]:
+            util.logger.info("Target %s has 0 comments, adding sync link comment", str(self))
+            start_change = 1
+            self.add_comment(f"Automatically synchronized from [this original issue]({source_hotspot.url()})")
+        else:
+            start_change = len(self.comments())
+            util.logger.info("Target %s already has %d comments", str(self), start_change)
+        util.logger.info(
+            "Applying comments of %s to %s, from comment %d",
+            str(source_hotspot),
+            str(self),
+            start_change,
+        )
+        change_nbr = 0
+        for key in sorted(comments.keys()):
+            change_nbr += 1
+            if change_nbr < start_change:
+                util.logger.debug(
+                    "Skipping comment already applied in a previous sync: %s",
+                    str(comments[key]),
+                )
+                continue
+            # origin = f"originally by *{event['userName']}* on original branch"
+            self.add_comment(comments[key]["value"])
+        return True
+
+    def changelog(self):
+        """
+        :return: The hotspot changelog
+        :rtype: dict
+        """
+        if self._changelog is not None:
+            return self._changelog
+        if not self.__details:
+            self.refresh()
+        util.json_dump_debug(self.__details, f"{str(self)} Details = ")
+        self._changelog = {}
+        seq = 1
+        for l in self.__details["changelog"]:
+            d = changelog.Changelog(l)
+            if d.is_technical_change():
+                # Skip automatic changelog events generated by SonarSource itself
+                util.logger.debug("Changelog is a technical change: %s", str(d))
+                continue
+            util.json_dump_debug(l, "Changelog item Changelog ADDED = ")
+            seq += 1
+            self._changelog[f"{d.date()}_{seq:03d}"] = d
+        return self._changelog
+
+    def comments(self):
+        """
+        :return: The hotspot comments
+        :rtype: dict
+        """
+        if self._comments is not None:
+            return self._comments
+        if not self.__details:
+            self.refresh()
+        self._comments = {}
+        seq = 0
+        for c in self.__details["comment"]:
+            seq += 1
+            self._comments[f"{c['createdAt']}_{seq:03d}"] = {
+                "date": c["createdAt"],
+                "event": "comment",
+                "value": c["markdown"],
+                "user": c["login"],
+                "userName": c["login"],
+                "commentKey": c["key"],
+            }
+        return self._comments
+
+
+def search_by_project(project_key, endpoint=None, params=None):
+    """Searches hotspots of a project
+
+    :param endpoint: Reference to the SonarQube platform
+    :type endpoint: Platform
+    :param project_key: Project key
+    :type project_key: str
+    :param params: Search filters to narrow down the search, defaults to None
+    :type params: dict
+    :return: List of found hotspots
+    :rtype: dict{<key>: <Hotspot>}
+    """
+    new_params = {} if params is None else params.copy()
+    if project_key is None:
+        key_list = projects.search(endpoint).keys()
+    else:
+        key_list = util.csv_to_list(project_key)
+    hotspots = {}
+    for k in key_list:
+        new_params["projectKey"] = k
+        project_hotspots = search(endpoint=endpoint, params=new_params)
+        util.logger.debug("Project '%s' has %d hotspots", k, len(project_hotspots))
+        hotspots.update(project_hotspots)
+    return hotspots
+
+
+def search(endpoint, page=None, params=None):
+    """Searches hotspots
+
+    :param endpoint: Reference to the SonarQube platform
+    :type endpoint: Platform
+    :param project_key: Project key
+    :type project_key: str
+    :param params: Search filters to narrow down the search, defaults to None
+    :type params: dict
+    :return: List of found hotspots
+    :rtype: dict{<key>: <Hotspot>}
+    """
+    hotspots_list = {}
+    new_params = {} if params is None else params.copy()
+    r_list = util.csv_to_list(params.get("resolution", None))
+    s_list = util.csv_to_list(params.get("status", None))
+    if len(r_list) > 1:
+        for r in r_list:
+            new_params["resolution"] = r
+            hotspots_list.update(search(endpoint, params=new_params))
+        return hotspots_list
+    elif len(s_list) > 1:
+        for s in s_list:
+            new_params["status"] = s
+            hotspots_list.update(search(endpoint, params=new_params))
+        return hotspots_list
+
+    new_params["ps"] = 500
+    p = 1
+    while True:
+        if page is None:
+            new_params["p"] = p
+        else:
+            new_params["p"] = page
+        resp = endpoint.get("hotspots/search", params=new_params)
+        data = json.loads(resp.text)
+        nbr_hotspots = data["paging"]["total"]
+        nbr_pages = (nbr_hotspots + 499) // 500
+        util.logger.debug(
+            "Number of issues: %d - Page: %d/%d",
+            nbr_hotspots,
+            new_params["p"],
+            nbr_pages,
+        )
+        if page is None and nbr_hotspots > 10000:
+            raise TooManyHotspotsError(
+                nbr_hotspots,
+                f"{nbr_hotspots} hotpots returned by api/hotspots/search, " "this is more than the max 10000 possible",
+            )
+
+        for i in data["hotspots"]:
+            if "branch" in params:
+                i["branch"] = params["branch"]
+            if "pullRequest" in params:
+                i["pullRequest"] = params["pullRequest"]
+            hotspots_list[i["key"]] = get_object(i["key"], endpoint=endpoint, data=i)
+        if page is not None or p >= nbr_pages:
+            break
+        p += 1
+    return hotspots_list
+
+
+def get_object(key, data=None, endpoint=None, from_export=False):
+    if key not in _OBJECTS:
+        _ = Hotspot(key=key, data=data, endpoint=endpoint, from_export=from_export)
+    return _OBJECTS[key]
+
+
+def get_search_criteria(params):
+    """Returns the filtered list of params that are allowed for api/issue/search"""
+    criterias = {} if params is None else params.copy()
+    for old, new in {
+        "resolutions": "resolution",
+        "componentsKey": "projectKey",
+        "statuses": "status",
+    }.items():
+        if old in params:
+            criterias[new] = params[old]
+    if criterias.get("status", None) is not None:
+        criterias["status"] = util.allowed_values_string(criterias["status"], STATUSES)
+    if criterias.get("resolution", None) is not None:
+        criterias["resolution"] = util.allowed_values_string(criterias["resolution"], RESOLUTIONS)
+        util.logger.error("hotspot 'status' criteria incompatible with 'resolution' criteria, ignoring 'status'")
+        criterias["status"] = "REVIEWED"
+    return util.dict_subset(util.remove_nones(criterias), SEARCH_CRITERIAS)
```

## sonar/findings/issues.py

 * *Ordering differences only*

```diff
@@ -1,839 +1,839 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-
-import datetime
-import json
-import re
-from queue import Queue
-from threading import Thread
-import requests.utils
-
-from sonar.projects import projects
-from sonar import users, syncer
-from sonar.findings import findings, changelog
-import sonar.utilities as util
-
-API_SET_TAGS = "issues/set_tags"
-API_SET_TYPE = "issues/set_type"
-
-SEARCH_CRITERIAS = (
-    "componentKeys",
-    "types",
-    "severities",
-    "createdAfter",
-    "createdBefore",
-    "createdInLast",
-    "createdAt",
-    "branch",
-    "pullRequest",
-    "statuses",
-    "tags",
-    "inNewCodePeriod",
-    "sinceLeakPeriod",
-    "p",
-    "page",
-    "facets",
-    "onComponentOnly",
-    "s",
-    "timeZone",
-    "cwe",
-    "owaspTop10",
-    "owaspTop10-21",
-    "sansTop25",
-    "sonarsourceSecurity",
-    "additionalFields",
-    "asc",
-    "assigned",
-    "assignees",
-    "author",
-    "issues",
-    "languages",
-    "resolutions",
-    "resolved",
-    "rules",
-    "scopes",
-)
-
-TYPES = ("BUG", "VULNERABILITY", "CODE_SMELL")
-SEVERITIES = ("BLOCKER", "CRITICAL", "MAJOR", "MINOR", "INFO")
-STATUSES = ("OPEN", "CONFIRMED", "REOPENED", "RESOLVED", "CLOSED")
-RESOLUTIONS = ("FALSE-POSITIVE", "WONTFIX", "FIXED", "REMOVED")
-
-_TOO_MANY_ISSUES_MSG = "Too many issues, recursing..."
-_ISSUES = {}
-
-
-class TooManyIssuesError(Exception):
-    """When a call to api/issues/search returns too many issues."""
-
-    def __init__(self, nbr_issues, message):
-        super().__init__()
-        self.nbr_issues = nbr_issues
-        self.message = message
-
-
-class Issue(findings.Finding):
-    """
-    Abstraction of the SonarQube 'issue' concept
-    """
-
-    SEARCH_API = "issues/search"
-    MAX_PAGE_SIZE = 500
-    MAX_SEARCH = 10000
-    OPTIONS_SEARCH = [
-        "additionalFields",
-        "asc",
-        "assigned",
-        "assignees",
-        "authors",
-        "componentKeys",
-        "createdAfter",
-        "createdAt",
-        "createdBefore",
-        "createdInLast",
-        "directories",
-        "facetMode",
-        "facets",
-        "files",
-        "branch",
-        "fileUuids",
-        "issues",
-        "languages",
-        "onComponentOnly",
-        "p",
-        "ps",
-        "resolutions",
-        "resolved",
-        "rules",
-        "s",
-        "severities",
-        "sinceLeakPeriod",
-        "statuses",
-        "tags",
-        "types",
-    ]
-
-    def __init__(self, key, endpoint, data=None, from_export=False):
-        super().__init__(key, endpoint, data, from_export)
-        self._debt = None
-        self.tags = []  #: Issue tags
-        if data is not None:
-            self.component = data.get("component", None)
-        # util.logger.debug("Loaded issue: %s", util.json_dump(data))
-        _ISSUES[self.uuid()] = self
-
-    def __str__(self):
-        """
-        :return: String representation of the issue
-        :rtype: str
-        """
-        return f"Issue key '{self.key}'"
-
-    def __format__(self, format_spec=""):
-        return (
-            f"Key: {self.key} - Type: {self.type} - Severity: {self.severity}"
-            f" - File/Line: {self.component}/{self.line} - Rule: {self.rule} - Project: {self.projectKey}"
-        )
-
-    def url(self):
-        """
-        :return: A permalink URL to the issue in the SonarQube platform
-        :rtype: str
-        """
-        branch = ""
-        if self.branch is not None:
-            branch = f"&branch={requests.utils.quote(self.branch)}"
-        elif self.pull_request is not None:
-            branch = f"pullRequest={requests.utils.quote(self.pull_request)}&"
-        return f"{self.endpoint.url}/project/issues?id={self.projectKey}{branch}&issues={self.key}"
-
-    def debt(self):
-        """
-        :return: The remediation effort of the issue, in minutes
-        :rtype: int
-        """
-        if self._debt is not None:
-            return self._debt
-        if "debt" in self._json:
-            kdays, days, hours, minutes = 0, 0, 0, 0
-            debt = self._json["debt"]
-            m = re.search(r"(\d+)kd", debt)
-            if m:
-                kdays = int(m.group(1))
-            m = re.search(r"(\d+)d", debt)
-            if m:
-                days = int(m.group(1))
-            m = re.search(r"(\d+)h", debt)
-            if m:
-                hours = int(m.group(1))
-            m = re.search(r"(\d+)min", debt)
-            if m:
-                minutes = int(m.group(1))
-            self._debt = ((kdays * 1000 + days) * 24 + hours) * 60 + minutes
-        elif "effort" in self._json:
-            self._debt = 0
-            if self._json["effort"] != "null":
-                self._debt = int(self._json["effort"])
-        return self._debt
-
-    def to_json(self):
-        """
-        :return: The issue attributes as JSON
-        :rtype: dict
-        """
-        data = super().to_json()
-        data["url"] = self.url()
-        data["effort"] = self.debt()
-        return data
-
-    def refresh(self):
-        """Refreshes an issue from the SonarQube platform live data
-        :return: whether the refresh was successful
-        :rtype: bool
-        """
-        resp = self.get(Issue.SEARCH_API, params={"issues": self.key, "additionalFields": "_all"})
-        if resp.ok:
-            self._load(resp.issues[0])
-        return resp.ok
-
-    def changelog(self):
-        """
-        :return: The issue changelog
-        :rtype: dict{"<date>_<sequence_nbr>": <event>}
-        """
-        if self._changelog is None:
-            data = json.loads(self.get("issues/changelog", {"issue": self.key, "format": "json"}).text)
-            util.json_dump_debug(data["changelog"], f"{str(self)} Changelog = ")
-            self._changelog = {}
-            seq = 1
-            for l in data["changelog"]:
-                d = changelog.Changelog(l)
-                if d.is_technical_change():
-                    # Skip automatic changelog events generated by SonarSource itself
-                    util.logger.debug("Changelog is a technical change: %s", str(d))
-                    continue
-                util.json_dump_debug(l, "Changelog item Changelog ADDED = ")
-                seq += 1
-                self._changelog[f"{d.date()}_{seq:03d}"] = d
-        return self._changelog
-
-    def comments(self):
-        """
-        :return: The issue comments
-        :rtype: dict{"<date>_<sequence_nbr>": <comment>}
-        """
-        if "comments" not in self._json:
-            self._comments = {}
-        elif self._comments is None:
-            self._comments = {}
-            seq = 0
-            for c in self._json["comments"]:
-                seq += 1
-                self._comments[f"{c['createdAt']}_{seq:03}"] = {
-                    "date": c["createdAt"],
-                    "event": "comment",
-                    "value": c["markdown"],
-                    "user": c["login"],
-                    "userName": c["login"],
-                }
-        return self._comments
-
-    def add_comment(self, comment):
-        """Adds a comment to an issue
-
-        :param comment: The comment to add
-        :type comment: str
-        :return: Whether the operation succeeded
-        :rtype: bool
-        """
-        util.logger.debug("Adding comment '%s' to %s", comment, str(self))
-        r = self.post("issues/add_comment", {"issue": self.key, "text": comment})
-        return r.ok
-
-    def set_severity(self, severity):
-        """Changes the severity of an issue
-
-        :param severity: The comment to add
-        :type severity: str
-        :return: Whether the operation succeeded
-        :rtype: bool
-        """
-        if severity != self.severity:
-            util.logger.debug("Changing severity of %s from '%s' to '%s'", str(self), self.severity, severity)
-            return self.post("issues/set_severity", {"issue": self.key, "severity": severity}).ok
-        return False
-
-    def assign(self, assignee):
-        """Assigns an issue to a user
-
-        :param assignee: The user login
-        :type assignee: str
-        :return: Whether the operation succeeded
-        :rtype: bool
-        """
-        if assignee != self.assignee:
-            util.logger.debug("Assigning %s to '%s'", str(self), assignee)
-            return self.post("issues/assign", {"issue": self.key, "assignee": assignee}).ok
-        return False
-
-    def set_tags(self, tags):
-        """Sets tags to an issue (Replacing all previous tags)
-        :param tags: Tags to set
-        :type tags: list
-        :return: Whether the operation succeeded
-        :rtype: bool
-        """
-        util.logger.debug("Setting tags %s to %s", tags, str(self))
-        if not self.post(API_SET_TAGS, {"issue": self.key, "tags": util.list_to_csv(tags)}).ok:
-            return False
-        self.tags = tags
-        return True
-
-    def add_tag(self, tag):
-        """Adds a tag to an issue
-        :param tag: Tags to add
-        :type tag: str
-        :return: Whether the operation succeeded
-        :rtype: bool
-        """
-        util.logger.debug("Adding tag '%s' to %s", tag, str(self))
-        tags = self.tags.copy()
-        if tag not in self.tags:
-            tags.append(tag)
-        return self.set_tags(tags)
-
-    def remove_tag(self, tag):
-        """Removes a tag from an issue
-        :param tag: Tags to remove
-        :type tag: str
-        :return: Whether the operation succeeded
-        :rtype: bool
-        """
-        util.logger.debug("Removing tag '%s' from %s", tag, str(self))
-        tags = self.tags.copy()
-        if tag in self.tags:
-            tags.remove(tag)
-        return self.set_tags(tags)
-
-    def set_type(self, new_type):
-        """Sets an issue type
-        :param new_type: New type of the issue (Can be BUG, VULNERABILITY or CODE_SMELL)
-        :type tag: str
-        :return: Whether the operation succeeded
-        :rtype: bool
-        """
-        util.logger.debug("Changing type of issue %s from %s to %s", self.key, self.type, new_type)
-        return self.post(API_SET_TYPE, {"issue": self.key, "type": new_type}).ok
-
-    def is_wont_fix(self):
-        """
-        :return: Whether the issue is won't fix
-        :rtype: bool
-        """
-        return self.resolution == "WONT-FIX"
-
-    def is_false_positive(self):
-        """
-        :return: Whether the issue is a false positive
-        :rtype: bool
-        """
-        return self.resolution == "FALSE-POSITIVE"
-
-    def strictly_identical_to(self, another_finding, ignore_component=False):
-        """
-        :meta private:
-        """
-        return super().strictly_identical_to(another_finding, ignore_component) and (self.debt() == another_finding.debt())
-
-    def almost_identical_to(self, another_finding, ignore_component=False, **kwargs):
-        """
-        :meta private:
-        """
-        return super().almost_identical_to(another_finding, ignore_component, **kwargs) and (
-            self.debt() == another_finding.debt() or kwargs.get("ignore_debt", False)
-        )
-
-    def __do_transition(self, transition):
-        return self.post("issues/do_transition", {"issue": self.key, "transition": transition}).ok
-
-    def reopen(self):
-        """Re-opens an issue
-
-        :return: Whether the operation succeeded
-        :rtype: bool
-        """
-        util.logger.debug("Reopening %s", str(self))
-        return self.__do_transition("reopen")
-
-    def mark_as_false_positive(self):
-        """Sets an issue as false positive
-
-        :return: Whether the operation succeeded
-        :rtype: bool
-        """
-        util.logger.debug("Marking %s as false positive", str(self))
-        return self.__do_transition("falsepositive")
-
-    def confirm(self):
-        """Confirms an issue
-
-        :return: Whether the operation succeeded
-        :rtype: bool
-        """
-        util.logger.debug("Confirming %s", str(self))
-        return self.__do_transition("confirm")
-
-    def unconfirm(self):
-        """Unconfirms an issue
-
-        :return: Whether the operation succeeded
-        :rtype: bool
-        """
-        util.logger.debug("Unconfirming %s", str(self))
-        return self.__do_transition("unconfirm")
-
-    def resolve_as_fixed(self):
-        """Marks an issue as resolved as fixed
-
-        :return: Whether the operation succeeded
-        :rtype: bool
-        """
-        util.logger.debug("Marking %s as fixed", str(self))
-        return self.__do_transition("resolve")
-
-    def mark_as_wont_fix(self):
-        """Marks an issue as resolved as won't fix
-
-        :return: Whether the operation succeeded
-        :rtype: bool
-        """
-        util.logger.debug("Marking %s as won't fix", str(self))
-        return self.__do_transition("wontfix")
-
-    def __apply_event(self, event, settings):
-        util.logger.debug("Applying event %s", str(event))
-        # origin = f"originally by *{event['userName']}* on original branch"
-        (event_type, data) = event.changelog_type()
-        if event_type == "SEVERITY":
-            self.set_severity(data)
-            # self.add_comment(f"Change of severity {origin}", settings[SYNC_ADD_COMMENTS])
-        elif event_type == "TYPE":
-            self.set_type(data)
-            # self.add_comment(f"Change of issue type {origin}", settings[SYNC_ADD_COMMENTS])
-        elif event_type == "REOPEN":
-            if event.previous_state() == "CLOSED":
-                util.logger.info("Reopen from closed issue won't be applied, issue was never closed")
-            else:
-                self.reopen()
-            # self.add_comment(f"Issue re-open {origin}", settings[SYNC_ADD_COMMENTS])
-        elif event_type == "FALSE-POSITIVE":
-            self.mark_as_false_positive()
-            # self.add_comment(f"False positive {origin}", settings[SYNC_ADD_COMMENTS])
-        elif event_type == "WONT-FIX":
-            self.mark_as_wont_fix()
-            # self.add_comment(f"Won't fix {origin}", settings[SYNC_ADD_COMMENTS])
-        elif event_type == "CONFIRM":
-            self.confirm()
-            # self.add_comment(f"Won't fix {origin}", settings[SYNC_ADD_COMMENTS])
-        elif event_type == "UNCONFIRM":
-            self.unconfirm()
-            # self.add_comment(f"Won't fix {origin}", settings[SYNC_ADD_COMMENTS])
-        elif event_type == "ASSIGN":
-            if settings[syncer.SYNC_ASSIGN]:
-                u = users.get_login_from_name(data, endpoint=self.endpoint)
-                if u is None:
-                    u = settings[syncer.SYNC_SERVICE_ACCOUNTS][0]
-                self.assign(u)
-                # self.add_comment(f"Issue assigned {origin}", settings[SYNC_ADD_COMMENTS])
-        elif event_type == "TAG":
-            self.set_tags(data)
-            # self.add_comment(f"Tag change {origin}", settings[SYNC_ADD_COMMENTS])
-        elif event_type == "FIXED":
-            self.resolve_as_fixed()
-            # self.add_comment(f"Change of issue type {origin}", settings[SYNC_ADD_COMMENTS])
-        elif event_type == "CLOSED":
-            util.logger.info(
-                "Changelog event is a CLOSE issue, it cannot be applied... %s",
-                str(event),
-            )
-            # self.add_comment(f"Change of issue type {origin}", settings[SYNC_ADD_COMMENTS])
-        elif event_type == "INTERNAL":
-            util.logger.info("Changelog %s is internal, it will not be applied...", str(event))
-            # self.add_comment(f"Change of issue type {origin}", settings[SYNC_ADD_COMMENTS])
-        else:
-            util.logger.error("Event %s can't be applied", str(event))
-            return False
-        return True
-
-    def apply_changelog(self, source_issue, settings):
-        """
-        :meta private:
-        """
-        events = source_issue.changelog()
-        if events is None or not events:
-            util.logger.debug("Sibling %s has no changelog, no action taken", source_issue.key)
-            return False
-
-        change_nbr = 0
-        start_change = len(self.changelog()) + 1
-        util.logger.debug("Issue %s: Changelog = %s", str(self), str(self.changelog()))
-        util.logger.info(
-            "Applying changelog of issue %s to issue %s, from change %d",
-            source_issue.key,
-            self.key,
-            start_change,
-        )
-        for key in sorted(events.keys()):
-            change_nbr += 1
-            if change_nbr < start_change:
-                util.logger.debug(
-                    "Skipping change already applied in a previous sync: %s",
-                    str(events[key]),
-                )
-                continue
-            self.__apply_event(events[key], settings)
-
-        comments = source_issue.comments()
-        if len(self.comments()) == 0 and settings[syncer.SYNC_ADD_LINK]:
-            util.logger.info("Target %s has 0 comments, adding sync link comment", str(self))
-            start_change = 1
-            self.add_comment(f"Automatically synchronized from [this original issue]({source_issue.url()})")
-        else:
-            start_change = len(self.comments())
-            util.logger.info("Target %s already has %d comments", str(self), start_change)
-        util.logger.info(
-            "Applying comments of %s to %s, from comment %d",
-            str(source_issue),
-            str(self),
-            start_change,
-        )
-        change_nbr = 0
-        for key in sorted(comments.keys()):
-            change_nbr += 1
-            if change_nbr < start_change:
-                util.logger.debug(
-                    "Skipping comment already applied in a previous sync: %s",
-                    str(comments[key]),
-                )
-                continue
-            # origin = f"originally by *{event['userName']}* on original branch"
-            self.add_comment(comments[key]["value"])
-        return True
-
-
-# ------------------------------- Static methods --------------------------------------
-
-
-def __search_all_by_directories(params, endpoint=None):
-    new_params = params.copy()
-    facets = _get_facets(new_params["componentKeys"], facets="directories", params=new_params, endpoint=endpoint)
-    issue_list = {}
-    util.logger.info("Splitting search by directories")
-    for d in facets["directories"]:
-        new_params["directories"] = d["val"]
-        issue_list.update(search(endpoint=endpoint, params=new_params, raise_error=False))
-    util.logger.debug("Search by directory ALL: %d issues found", len(issue_list))
-    return issue_list
-
-
-def __search_all_by_types(params, endpoint=None):
-    issue_list = {}
-    new_params = params.copy()
-    util.logger.info("Splitting search by issue types")
-    for issue_type in ("BUG", "VULNERABILITY", "CODE_SMELL"):
-        try:
-            new_params["types"] = issue_type
-            issue_list.update(search(endpoint=endpoint, params=new_params))
-        except TooManyIssuesError:
-            util.logger.info(_TOO_MANY_ISSUES_MSG)
-            issue_list.update(__search_all_by_directories(params=new_params, endpoint=endpoint))
-    util.logger.debug("Search by type ALL: %d issues found", len(issue_list))
-    return issue_list
-
-
-def __search_all_by_severities(params, endpoint=None):
-    issue_list = {}
-    new_params = params.copy()
-    util.logger.info("Splitting search by severities")
-    for sev in ("BLOCKER", "CRITICAL", "MAJOR", "MINOR", "INFO"):
-        try:
-            new_params["severities"] = sev
-            issue_list.update(search(endpoint=endpoint, params=new_params))
-        except TooManyIssuesError:
-            util.logger.info(_TOO_MANY_ISSUES_MSG)
-            issue_list.update(__search_all_by_types(params=new_params, endpoint=endpoint))
-    util.logger.debug("Search by severity ALL: %d issues found", len(issue_list))
-    return issue_list
-
-
-def __search_all_by_date(params, date_start=None, date_stop=None, endpoint=None):
-    new_params = params.copy()
-    if date_start is None:
-        date_start = get_oldest_issue(endpoint=endpoint, params=new_params).replace(hour=0, minute=0, second=0, microsecond=0)
-    if date_stop is None:
-        date_stop = get_newest_issue(endpoint=endpoint, params=new_params).replace(hour=0, minute=0, second=0, microsecond=0)
-    util.logger.info("Splitting search by date between [%s - %s]", util.date_to_string(date_start, False), util.date_to_string(date_stop, False))
-    issue_list = {}
-    new_params.update({"createdAfter": date_start, "createdBefore": date_stop})
-    try:
-        issue_list = search(endpoint=endpoint, params=new_params)
-    except TooManyIssuesError as e:
-        util.logger.debug("Too many issues (%d), splitting time window", e.nbr_issues)
-        diff = (date_stop - date_start).days
-        if diff == 0:
-            util.logger.info(_TOO_MANY_ISSUES_MSG)
-            issue_list = __search_all_by_severities(new_params, endpoint=endpoint)
-        elif diff == 1:
-            issue_list.update(__search_all_by_date(new_params, date_start=date_start, date_stop=date_start, endpoint=endpoint))
-            issue_list.update(__search_all_by_date(new_params, date_start=date_stop, date_stop=date_stop, endpoint=endpoint))
-        else:
-            date_middle = date_start + datetime.timedelta(days=diff // 2)
-            issue_list.update(__search_all_by_date(new_params, date_start=date_start, date_stop=date_middle, endpoint=endpoint))
-            date_middle = date_middle + datetime.timedelta(days=1)
-            issue_list.update(__search_all_by_date(new_params, date_start=date_middle, date_stop=date_stop, endpoint=endpoint))
-    if date_start is not None and date_stop is not None:
-        util.logger.debug(
-            "Project %s has %d issues between %s and %s",
-            new_params["componentKeys"],
-            len(issue_list),
-            util.date_to_string(date_start, False),
-            util.date_to_string(date_stop, False),
-        )
-    return issue_list
-
-
-def __search_all_by_project(project_key, params, endpoint=None):
-    new_params = {} if params is None else params.copy()
-    if project_key is None:
-        key_list = projects.search(endpoint).keys()
-    else:
-        key_list = util.csv_to_list(project_key)
-    issue_list = {}
-    for k in key_list:
-        new_params["componentKeys"] = k
-        util.logger.debug("Searching for issues of project '%s'", k)
-        try:
-            issue_list.update(search(endpoint=endpoint, params=new_params))
-        except TooManyIssuesError:
-            util.logger.info(_TOO_MANY_ISSUES_MSG)
-            issue_list.update(__search_all_by_date(params=new_params, endpoint=endpoint))
-    return issue_list
-
-
-def search_by_project(project_key, endpoint, params=None, search_findings=False):
-    """Search all issues of a given project
-
-    :param project_key: The project key
-    :type project_key: str
-    :param endpoint: Reference to the SonarQube platform
-    :type endpoint: Platform
-    :param params: List of search filters to narrow down the search, defaults to None
-    :type params: dict
-    :param search_findings: Whether to use the api/project_search/findings API or not, defaults to False
-    :type search_findings: bool, optional
-    :return: list of Issues
-    :rtype: dict{<key>: <Issue>}
-    """
-    if params is None:
-        params = {}
-    if project_key is None:
-        key_list = projects.search(endpoint).keys()
-    else:
-        key_list = util.csv_to_list(project_key)
-    issue_list = {}
-    for k in key_list:
-        util.logger.info("Project '%s' issue search", k)
-        if endpoint.version() >= (9, 1, 0) and endpoint.edition() in ("enterprise", "datacenter") and search_findings:
-            util.logger.info("Using new export findings to speed up issue export")
-            issue_list.update(findings.export_findings(endpoint, k, params.get("branch", None), params.get("pullRequest", None)))
-        else:
-            issue_list.update(__search_all_by_project(k, params=params, endpoint=endpoint))
-        util.logger.info("Project '%s' has %d issues", k, len(issue_list))
-    return issue_list
-
-
-def search_all(endpoint, params=None):
-    """Returns all issues of the platforms
-
-    :param endpoint: Reference to the SonarQube platform
-    :type endpoint: Platform
-    :param params: List of search filters to narrow down the search, defaults to None
-    :type params: dict
-    :return: list of Issues
-    :rtype: dict{<key>: <Issue>}
-    """
-    new_params = {} if params is None else params.copy()
-    util.logger.info("Issue search all with %s", str(params))
-    issue_list = {}
-    try:
-        issue_list = search(endpoint=endpoint, params=params)
-    except TooManyIssuesError:
-        util.logger.info(_TOO_MANY_ISSUES_MSG)
-        for k in projects.search(endpoint):
-            issue_list.update(__search_all_by_project(k, params=new_params, endpoint=endpoint))
-    return issue_list
-
-
-def __search_thread(queue):
-    while not queue.empty():
-        (endpoint, api, issue_list, params, page) = queue.get()
-        page_params = params.copy()
-        page_params["p"] = page
-        util.logger.debug("Threaded issue search params = %s", str(page_params))
-        data = json.loads(endpoint.get(api, params=page_params).text)
-        for i in data["issues"]:
-            i["branch"] = page_params.get("branch", None)
-            i["pullRequest"] = page_params.get("pullRequest", None)
-            issue_list[i["key"]] = get_object(i["key"], endpoint=endpoint, data=i)
-        util.logger.debug("Added %d issues in threaded search page %d", len(data["issues"]), page)
-        queue.task_done()
-
-
-def search_first(endpoint, **params):
-    """
-    :return: The first issue of a search, for instance the oldest, if params = s="CREATION_DATE", asc=asc_sort
-    :rtype: Issue or None if not issue found
-    """
-    params["ps"] = 1
-    data = json.loads(endpoint.get(Issue.SEARCH_API, params=params).text)
-    if len(data) == 0:
-        return None
-    i = data["issues"][0]
-    return get_object(i["key"], endpoint=endpoint, data=i)
-
-
-def search(endpoint, params=None, raise_error=True, threads=8):
-    """Multi-threaded search of issues
-
-    :param params: Search filter criteria to narrow down the search
-    :type params: dict
-    :param raise_error: Whether to raise exception if more than 10'000 issues returned, defaults to True
-    :type raise_error: bool
-    :param threads: Nbr of parallel threads for search, defaults to 8
-    :type threads: int
-    :return: List of issues found
-    :rtype: dict{<key>: <Issue>}
-    :raises: TooManyIssuesError if more than 10'000 issues found
-    """
-    new_params = {} if params is None else params.copy()
-    util.logger.debug("Search params = %s", str(new_params))
-    if "ps" not in new_params:
-        new_params["ps"] = Issue.MAX_PAGE_SIZE
-    issue_list = {}
-
-    data = json.loads(endpoint.get(Issue.SEARCH_API, params=new_params).text)
-    nbr_issues = data["paging"]["total"]
-    nbr_pages = util.nbr_pages(data)
-    util.logger.debug("Number of issues: %d - Nbr pages: %d", nbr_issues, nbr_pages)
-    if nbr_pages > 20 and raise_error:
-        raise TooManyIssuesError(
-            nbr_issues,
-            f"{nbr_issues} issues returned by api/{Issue.SEARCH_API}, this is more than the max {Issue.MAX_SEARCH} possible",
-        )
-    for i in data["issues"]:
-        i["branch"] = new_params.get("branch", None)
-        i["pullRequest"] = new_params.get("pullRequest", None)
-        issue_list[i["key"]] = get_object(i["key"], endpoint=endpoint, data=i)
-    if nbr_pages == 1:
-        return issue_list
-    q = Queue(maxsize=0)
-    for page in range(2, nbr_pages + 1):
-        q.put((endpoint, Issue.SEARCH_API, issue_list, new_params, page))
-    for i in range(threads):
-        util.logger.debug("Starting issue search thread %d", i)
-        worker = Thread(target=__search_thread, args=[q])
-        worker.setDaemon(True)
-        worker.start()
-    q.join()
-    return issue_list
-
-
-def _get_facets(project_key, facets="directories", endpoint=None, params=None):
-    new_params = {} if params is None else params.copy()
-    new_params.update({"componentKeys": project_key, "facets": facets, "ps": 500})
-    new_params = __get_issues_search_params(new_params)
-    data = json.loads(endpoint.get(Issue.SEARCH_API, params=new_params).text)
-    l = {}
-    facets_list = util.csv_to_list(facets)
-    for f in data["facets"]:
-        if f["property"] in facets_list:
-            l[f["property"]] = f["values"]
-    return l
-
-
-def __get_one_issue_date(endpoint=None, asc_sort="false", params=None):
-    """Returns the date of one issue found"""
-    issue = search_first(endpoint=endpoint, s="CREATION_DATE", asc=asc_sort, **params)
-    if not issue:
-        return None
-    return issue.creation_date
-
-
-def get_oldest_issue(endpoint=None, params=None):
-    """Returns the oldest date of all issues found"""
-    return __get_one_issue_date(endpoint=endpoint, asc_sort="true", params=params)
-
-
-def get_newest_issue(endpoint=None, params=None):
-    """Returns the newest date of all issues found"""
-    return __get_one_issue_date(endpoint=endpoint, asc_sort="false", params=params)
-
-
-def count(endpoint=None, **kwargs):
-    """Returns number of issues of a search"""
-    returned_data = search(endpoint=endpoint, params=kwargs.copy().update({"ps": 1}))
-    util.logger.debug("Issue search %s would return %d issues", str(kwargs), returned_data["total"])
-    return returned_data["total"]
-
-
-def identical_attributes(o1, o2, key_list):
-    for key in key_list:
-        if o1[key] != o2[key]:
-            return False
-    return True
-
-
-def __get_issues_search_params(params):
-    outparams = {"additionalFields": "comments"}
-    for key in params:
-        if params[key] is not None and key in Issue.OPTIONS_SEARCH:
-            outparams[key] = params[key]
-    return outparams
-
-
-def get_object(key, data=None, endpoint=None, from_export=False):
-    if key not in _ISSUES:
-        _ = Issue(key=key, data=data, endpoint=endpoint, from_export=from_export)
-    return _ISSUES[key]
-
-
-def get_search_criteria(params):
-    """Returns the filtered list of params that are allowed for api/issue/search"""
-    criterias = params.copy()
-    if criterias.get("types", None) is not None:
-        criterias["types"] = util.allowed_values_string(criterias["types"], TYPES)
-    if criterias.get("severities", None) is not None:
-        criterias["severities"] = util.allowed_values_string(criterias["severities"], SEVERITIES)
-    if criterias.get("statuses", None) is not None:
-        criterias["statuses"] = util.allowed_values_string(criterias["statuses"], STATUSES)
-    if criterias.get("resolutions", None) is not None:
-        criterias["resolutions"] = util.allowed_values_string(criterias["resolutions"], RESOLUTIONS)
-    criterias = util.dict_subset(util.remove_nones(criterias), SEARCH_CRITERIAS)
-    return criterias
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+
+import datetime
+import json
+import re
+from queue import Queue
+from threading import Thread
+import requests.utils
+
+from sonar.projects import projects
+from sonar import users, syncer
+from sonar.findings import findings, changelog
+import sonar.utilities as util
+
+API_SET_TAGS = "issues/set_tags"
+API_SET_TYPE = "issues/set_type"
+
+SEARCH_CRITERIAS = (
+    "componentKeys",
+    "types",
+    "severities",
+    "createdAfter",
+    "createdBefore",
+    "createdInLast",
+    "createdAt",
+    "branch",
+    "pullRequest",
+    "statuses",
+    "tags",
+    "inNewCodePeriod",
+    "sinceLeakPeriod",
+    "p",
+    "page",
+    "facets",
+    "onComponentOnly",
+    "s",
+    "timeZone",
+    "cwe",
+    "owaspTop10",
+    "owaspTop10-21",
+    "sansTop25",
+    "sonarsourceSecurity",
+    "additionalFields",
+    "asc",
+    "assigned",
+    "assignees",
+    "author",
+    "issues",
+    "languages",
+    "resolutions",
+    "resolved",
+    "rules",
+    "scopes",
+)
+
+TYPES = ("BUG", "VULNERABILITY", "CODE_SMELL")
+SEVERITIES = ("BLOCKER", "CRITICAL", "MAJOR", "MINOR", "INFO")
+STATUSES = ("OPEN", "CONFIRMED", "REOPENED", "RESOLVED", "CLOSED")
+RESOLUTIONS = ("FALSE-POSITIVE", "WONTFIX", "FIXED", "REMOVED")
+
+_TOO_MANY_ISSUES_MSG = "Too many issues, recursing..."
+_ISSUES = {}
+
+
+class TooManyIssuesError(Exception):
+    """When a call to api/issues/search returns too many issues."""
+
+    def __init__(self, nbr_issues, message):
+        super().__init__()
+        self.nbr_issues = nbr_issues
+        self.message = message
+
+
+class Issue(findings.Finding):
+    """
+    Abstraction of the SonarQube 'issue' concept
+    """
+
+    SEARCH_API = "issues/search"
+    MAX_PAGE_SIZE = 500
+    MAX_SEARCH = 10000
+    OPTIONS_SEARCH = [
+        "additionalFields",
+        "asc",
+        "assigned",
+        "assignees",
+        "authors",
+        "componentKeys",
+        "createdAfter",
+        "createdAt",
+        "createdBefore",
+        "createdInLast",
+        "directories",
+        "facetMode",
+        "facets",
+        "files",
+        "branch",
+        "fileUuids",
+        "issues",
+        "languages",
+        "onComponentOnly",
+        "p",
+        "ps",
+        "resolutions",
+        "resolved",
+        "rules",
+        "s",
+        "severities",
+        "sinceLeakPeriod",
+        "statuses",
+        "tags",
+        "types",
+    ]
+
+    def __init__(self, key, endpoint, data=None, from_export=False):
+        super().__init__(key, endpoint, data, from_export)
+        self._debt = None
+        self.tags = []  #: Issue tags
+        if data is not None:
+            self.component = data.get("component", None)
+        # util.logger.debug("Loaded issue: %s", util.json_dump(data))
+        _ISSUES[self.uuid()] = self
+
+    def __str__(self):
+        """
+        :return: String representation of the issue
+        :rtype: str
+        """
+        return f"Issue key '{self.key}'"
+
+    def __format__(self, format_spec=""):
+        return (
+            f"Key: {self.key} - Type: {self.type} - Severity: {self.severity}"
+            f" - File/Line: {self.component}/{self.line} - Rule: {self.rule} - Project: {self.projectKey}"
+        )
+
+    def url(self):
+        """
+        :return: A permalink URL to the issue in the SonarQube platform
+        :rtype: str
+        """
+        branch = ""
+        if self.branch is not None:
+            branch = f"&branch={requests.utils.quote(self.branch)}"
+        elif self.pull_request is not None:
+            branch = f"pullRequest={requests.utils.quote(self.pull_request)}&"
+        return f"{self.endpoint.url}/project/issues?id={self.projectKey}{branch}&issues={self.key}"
+
+    def debt(self):
+        """
+        :return: The remediation effort of the issue, in minutes
+        :rtype: int
+        """
+        if self._debt is not None:
+            return self._debt
+        if "debt" in self._json:
+            kdays, days, hours, minutes = 0, 0, 0, 0
+            debt = self._json["debt"]
+            m = re.search(r"(\d+)kd", debt)
+            if m:
+                kdays = int(m.group(1))
+            m = re.search(r"(\d+)d", debt)
+            if m:
+                days = int(m.group(1))
+            m = re.search(r"(\d+)h", debt)
+            if m:
+                hours = int(m.group(1))
+            m = re.search(r"(\d+)min", debt)
+            if m:
+                minutes = int(m.group(1))
+            self._debt = ((kdays * 1000 + days) * 24 + hours) * 60 + minutes
+        elif "effort" in self._json:
+            self._debt = 0
+            if self._json["effort"] != "null":
+                self._debt = int(self._json["effort"])
+        return self._debt
+
+    def to_json(self):
+        """
+        :return: The issue attributes as JSON
+        :rtype: dict
+        """
+        data = super().to_json()
+        data["url"] = self.url()
+        data["effort"] = self.debt()
+        return data
+
+    def refresh(self):
+        """Refreshes an issue from the SonarQube platform live data
+        :return: whether the refresh was successful
+        :rtype: bool
+        """
+        resp = self.get(Issue.SEARCH_API, params={"issues": self.key, "additionalFields": "_all"})
+        if resp.ok:
+            self._load(resp.issues[0])
+        return resp.ok
+
+    def changelog(self):
+        """
+        :return: The issue changelog
+        :rtype: dict{"<date>_<sequence_nbr>": <event>}
+        """
+        if self._changelog is None:
+            data = json.loads(self.get("issues/changelog", {"issue": self.key, "format": "json"}).text)
+            util.json_dump_debug(data["changelog"], f"{str(self)} Changelog = ")
+            self._changelog = {}
+            seq = 1
+            for l in data["changelog"]:
+                d = changelog.Changelog(l)
+                if d.is_technical_change():
+                    # Skip automatic changelog events generated by SonarSource itself
+                    util.logger.debug("Changelog is a technical change: %s", str(d))
+                    continue
+                util.json_dump_debug(l, "Changelog item Changelog ADDED = ")
+                seq += 1
+                self._changelog[f"{d.date()}_{seq:03d}"] = d
+        return self._changelog
+
+    def comments(self):
+        """
+        :return: The issue comments
+        :rtype: dict{"<date>_<sequence_nbr>": <comment>}
+        """
+        if "comments" not in self._json:
+            self._comments = {}
+        elif self._comments is None:
+            self._comments = {}
+            seq = 0
+            for c in self._json["comments"]:
+                seq += 1
+                self._comments[f"{c['createdAt']}_{seq:03}"] = {
+                    "date": c["createdAt"],
+                    "event": "comment",
+                    "value": c["markdown"],
+                    "user": c["login"],
+                    "userName": c["login"],
+                }
+        return self._comments
+
+    def add_comment(self, comment):
+        """Adds a comment to an issue
+
+        :param comment: The comment to add
+        :type comment: str
+        :return: Whether the operation succeeded
+        :rtype: bool
+        """
+        util.logger.debug("Adding comment '%s' to %s", comment, str(self))
+        r = self.post("issues/add_comment", {"issue": self.key, "text": comment})
+        return r.ok
+
+    def set_severity(self, severity):
+        """Changes the severity of an issue
+
+        :param severity: The comment to add
+        :type severity: str
+        :return: Whether the operation succeeded
+        :rtype: bool
+        """
+        if severity != self.severity:
+            util.logger.debug("Changing severity of %s from '%s' to '%s'", str(self), self.severity, severity)
+            return self.post("issues/set_severity", {"issue": self.key, "severity": severity}).ok
+        return False
+
+    def assign(self, assignee):
+        """Assigns an issue to a user
+
+        :param assignee: The user login
+        :type assignee: str
+        :return: Whether the operation succeeded
+        :rtype: bool
+        """
+        if assignee != self.assignee:
+            util.logger.debug("Assigning %s to '%s'", str(self), assignee)
+            return self.post("issues/assign", {"issue": self.key, "assignee": assignee}).ok
+        return False
+
+    def set_tags(self, tags):
+        """Sets tags to an issue (Replacing all previous tags)
+        :param tags: Tags to set
+        :type tags: list
+        :return: Whether the operation succeeded
+        :rtype: bool
+        """
+        util.logger.debug("Setting tags %s to %s", tags, str(self))
+        if not self.post(API_SET_TAGS, {"issue": self.key, "tags": util.list_to_csv(tags)}).ok:
+            return False
+        self.tags = tags
+        return True
+
+    def add_tag(self, tag):
+        """Adds a tag to an issue
+        :param tag: Tags to add
+        :type tag: str
+        :return: Whether the operation succeeded
+        :rtype: bool
+        """
+        util.logger.debug("Adding tag '%s' to %s", tag, str(self))
+        tags = self.tags.copy()
+        if tag not in self.tags:
+            tags.append(tag)
+        return self.set_tags(tags)
+
+    def remove_tag(self, tag):
+        """Removes a tag from an issue
+        :param tag: Tags to remove
+        :type tag: str
+        :return: Whether the operation succeeded
+        :rtype: bool
+        """
+        util.logger.debug("Removing tag '%s' from %s", tag, str(self))
+        tags = self.tags.copy()
+        if tag in self.tags:
+            tags.remove(tag)
+        return self.set_tags(tags)
+
+    def set_type(self, new_type):
+        """Sets an issue type
+        :param new_type: New type of the issue (Can be BUG, VULNERABILITY or CODE_SMELL)
+        :type tag: str
+        :return: Whether the operation succeeded
+        :rtype: bool
+        """
+        util.logger.debug("Changing type of issue %s from %s to %s", self.key, self.type, new_type)
+        return self.post(API_SET_TYPE, {"issue": self.key, "type": new_type}).ok
+
+    def is_wont_fix(self):
+        """
+        :return: Whether the issue is won't fix
+        :rtype: bool
+        """
+        return self.resolution == "WONT-FIX"
+
+    def is_false_positive(self):
+        """
+        :return: Whether the issue is a false positive
+        :rtype: bool
+        """
+        return self.resolution == "FALSE-POSITIVE"
+
+    def strictly_identical_to(self, another_finding, ignore_component=False):
+        """
+        :meta private:
+        """
+        return super().strictly_identical_to(another_finding, ignore_component) and (self.debt() == another_finding.debt())
+
+    def almost_identical_to(self, another_finding, ignore_component=False, **kwargs):
+        """
+        :meta private:
+        """
+        return super().almost_identical_to(another_finding, ignore_component, **kwargs) and (
+            self.debt() == another_finding.debt() or kwargs.get("ignore_debt", False)
+        )
+
+    def __do_transition(self, transition):
+        return self.post("issues/do_transition", {"issue": self.key, "transition": transition}).ok
+
+    def reopen(self):
+        """Re-opens an issue
+
+        :return: Whether the operation succeeded
+        :rtype: bool
+        """
+        util.logger.debug("Reopening %s", str(self))
+        return self.__do_transition("reopen")
+
+    def mark_as_false_positive(self):
+        """Sets an issue as false positive
+
+        :return: Whether the operation succeeded
+        :rtype: bool
+        """
+        util.logger.debug("Marking %s as false positive", str(self))
+        return self.__do_transition("falsepositive")
+
+    def confirm(self):
+        """Confirms an issue
+
+        :return: Whether the operation succeeded
+        :rtype: bool
+        """
+        util.logger.debug("Confirming %s", str(self))
+        return self.__do_transition("confirm")
+
+    def unconfirm(self):
+        """Unconfirms an issue
+
+        :return: Whether the operation succeeded
+        :rtype: bool
+        """
+        util.logger.debug("Unconfirming %s", str(self))
+        return self.__do_transition("unconfirm")
+
+    def resolve_as_fixed(self):
+        """Marks an issue as resolved as fixed
+
+        :return: Whether the operation succeeded
+        :rtype: bool
+        """
+        util.logger.debug("Marking %s as fixed", str(self))
+        return self.__do_transition("resolve")
+
+    def mark_as_wont_fix(self):
+        """Marks an issue as resolved as won't fix
+
+        :return: Whether the operation succeeded
+        :rtype: bool
+        """
+        util.logger.debug("Marking %s as won't fix", str(self))
+        return self.__do_transition("wontfix")
+
+    def __apply_event(self, event, settings):
+        util.logger.debug("Applying event %s", str(event))
+        # origin = f"originally by *{event['userName']}* on original branch"
+        (event_type, data) = event.changelog_type()
+        if event_type == "SEVERITY":
+            self.set_severity(data)
+            # self.add_comment(f"Change of severity {origin}", settings[SYNC_ADD_COMMENTS])
+        elif event_type == "TYPE":
+            self.set_type(data)
+            # self.add_comment(f"Change of issue type {origin}", settings[SYNC_ADD_COMMENTS])
+        elif event_type == "REOPEN":
+            if event.previous_state() == "CLOSED":
+                util.logger.info("Reopen from closed issue won't be applied, issue was never closed")
+            else:
+                self.reopen()
+            # self.add_comment(f"Issue re-open {origin}", settings[SYNC_ADD_COMMENTS])
+        elif event_type == "FALSE-POSITIVE":
+            self.mark_as_false_positive()
+            # self.add_comment(f"False positive {origin}", settings[SYNC_ADD_COMMENTS])
+        elif event_type == "WONT-FIX":
+            self.mark_as_wont_fix()
+            # self.add_comment(f"Won't fix {origin}", settings[SYNC_ADD_COMMENTS])
+        elif event_type == "CONFIRM":
+            self.confirm()
+            # self.add_comment(f"Won't fix {origin}", settings[SYNC_ADD_COMMENTS])
+        elif event_type == "UNCONFIRM":
+            self.unconfirm()
+            # self.add_comment(f"Won't fix {origin}", settings[SYNC_ADD_COMMENTS])
+        elif event_type == "ASSIGN":
+            if settings[syncer.SYNC_ASSIGN]:
+                u = users.get_login_from_name(data, endpoint=self.endpoint)
+                if u is None:
+                    u = settings[syncer.SYNC_SERVICE_ACCOUNTS][0]
+                self.assign(u)
+                # self.add_comment(f"Issue assigned {origin}", settings[SYNC_ADD_COMMENTS])
+        elif event_type == "TAG":
+            self.set_tags(data)
+            # self.add_comment(f"Tag change {origin}", settings[SYNC_ADD_COMMENTS])
+        elif event_type == "FIXED":
+            self.resolve_as_fixed()
+            # self.add_comment(f"Change of issue type {origin}", settings[SYNC_ADD_COMMENTS])
+        elif event_type == "CLOSED":
+            util.logger.info(
+                "Changelog event is a CLOSE issue, it cannot be applied... %s",
+                str(event),
+            )
+            # self.add_comment(f"Change of issue type {origin}", settings[SYNC_ADD_COMMENTS])
+        elif event_type == "INTERNAL":
+            util.logger.info("Changelog %s is internal, it will not be applied...", str(event))
+            # self.add_comment(f"Change of issue type {origin}", settings[SYNC_ADD_COMMENTS])
+        else:
+            util.logger.error("Event %s can't be applied", str(event))
+            return False
+        return True
+
+    def apply_changelog(self, source_issue, settings):
+        """
+        :meta private:
+        """
+        events = source_issue.changelog()
+        if events is None or not events:
+            util.logger.debug("Sibling %s has no changelog, no action taken", source_issue.key)
+            return False
+
+        change_nbr = 0
+        start_change = len(self.changelog()) + 1
+        util.logger.debug("Issue %s: Changelog = %s", str(self), str(self.changelog()))
+        util.logger.info(
+            "Applying changelog of issue %s to issue %s, from change %d",
+            source_issue.key,
+            self.key,
+            start_change,
+        )
+        for key in sorted(events.keys()):
+            change_nbr += 1
+            if change_nbr < start_change:
+                util.logger.debug(
+                    "Skipping change already applied in a previous sync: %s",
+                    str(events[key]),
+                )
+                continue
+            self.__apply_event(events[key], settings)
+
+        comments = source_issue.comments()
+        if len(self.comments()) == 0 and settings[syncer.SYNC_ADD_LINK]:
+            util.logger.info("Target %s has 0 comments, adding sync link comment", str(self))
+            start_change = 1
+            self.add_comment(f"Automatically synchronized from [this original issue]({source_issue.url()})")
+        else:
+            start_change = len(self.comments())
+            util.logger.info("Target %s already has %d comments", str(self), start_change)
+        util.logger.info(
+            "Applying comments of %s to %s, from comment %d",
+            str(source_issue),
+            str(self),
+            start_change,
+        )
+        change_nbr = 0
+        for key in sorted(comments.keys()):
+            change_nbr += 1
+            if change_nbr < start_change:
+                util.logger.debug(
+                    "Skipping comment already applied in a previous sync: %s",
+                    str(comments[key]),
+                )
+                continue
+            # origin = f"originally by *{event['userName']}* on original branch"
+            self.add_comment(comments[key]["value"])
+        return True
+
+
+# ------------------------------- Static methods --------------------------------------
+
+
+def __search_all_by_directories(params, endpoint=None):
+    new_params = params.copy()
+    facets = _get_facets(new_params["componentKeys"], facets="directories", params=new_params, endpoint=endpoint)
+    issue_list = {}
+    util.logger.info("Splitting search by directories")
+    for d in facets["directories"]:
+        new_params["directories"] = d["val"]
+        issue_list.update(search(endpoint=endpoint, params=new_params, raise_error=False))
+    util.logger.debug("Search by directory ALL: %d issues found", len(issue_list))
+    return issue_list
+
+
+def __search_all_by_types(params, endpoint=None):
+    issue_list = {}
+    new_params = params.copy()
+    util.logger.info("Splitting search by issue types")
+    for issue_type in ("BUG", "VULNERABILITY", "CODE_SMELL"):
+        try:
+            new_params["types"] = issue_type
+            issue_list.update(search(endpoint=endpoint, params=new_params))
+        except TooManyIssuesError:
+            util.logger.info(_TOO_MANY_ISSUES_MSG)
+            issue_list.update(__search_all_by_directories(params=new_params, endpoint=endpoint))
+    util.logger.debug("Search by type ALL: %d issues found", len(issue_list))
+    return issue_list
+
+
+def __search_all_by_severities(params, endpoint=None):
+    issue_list = {}
+    new_params = params.copy()
+    util.logger.info("Splitting search by severities")
+    for sev in ("BLOCKER", "CRITICAL", "MAJOR", "MINOR", "INFO"):
+        try:
+            new_params["severities"] = sev
+            issue_list.update(search(endpoint=endpoint, params=new_params))
+        except TooManyIssuesError:
+            util.logger.info(_TOO_MANY_ISSUES_MSG)
+            issue_list.update(__search_all_by_types(params=new_params, endpoint=endpoint))
+    util.logger.debug("Search by severity ALL: %d issues found", len(issue_list))
+    return issue_list
+
+
+def __search_all_by_date(params, date_start=None, date_stop=None, endpoint=None):
+    new_params = params.copy()
+    if date_start is None:
+        date_start = get_oldest_issue(endpoint=endpoint, params=new_params).replace(hour=0, minute=0, second=0, microsecond=0)
+    if date_stop is None:
+        date_stop = get_newest_issue(endpoint=endpoint, params=new_params).replace(hour=0, minute=0, second=0, microsecond=0)
+    util.logger.info("Splitting search by date between [%s - %s]", util.date_to_string(date_start, False), util.date_to_string(date_stop, False))
+    issue_list = {}
+    new_params.update({"createdAfter": date_start, "createdBefore": date_stop})
+    try:
+        issue_list = search(endpoint=endpoint, params=new_params)
+    except TooManyIssuesError as e:
+        util.logger.debug("Too many issues (%d), splitting time window", e.nbr_issues)
+        diff = (date_stop - date_start).days
+        if diff == 0:
+            util.logger.info(_TOO_MANY_ISSUES_MSG)
+            issue_list = __search_all_by_severities(new_params, endpoint=endpoint)
+        elif diff == 1:
+            issue_list.update(__search_all_by_date(new_params, date_start=date_start, date_stop=date_start, endpoint=endpoint))
+            issue_list.update(__search_all_by_date(new_params, date_start=date_stop, date_stop=date_stop, endpoint=endpoint))
+        else:
+            date_middle = date_start + datetime.timedelta(days=diff // 2)
+            issue_list.update(__search_all_by_date(new_params, date_start=date_start, date_stop=date_middle, endpoint=endpoint))
+            date_middle = date_middle + datetime.timedelta(days=1)
+            issue_list.update(__search_all_by_date(new_params, date_start=date_middle, date_stop=date_stop, endpoint=endpoint))
+    if date_start is not None and date_stop is not None:
+        util.logger.debug(
+            "Project %s has %d issues between %s and %s",
+            new_params["componentKeys"],
+            len(issue_list),
+            util.date_to_string(date_start, False),
+            util.date_to_string(date_stop, False),
+        )
+    return issue_list
+
+
+def __search_all_by_project(project_key, params, endpoint=None):
+    new_params = {} if params is None else params.copy()
+    if project_key is None:
+        key_list = projects.search(endpoint).keys()
+    else:
+        key_list = util.csv_to_list(project_key)
+    issue_list = {}
+    for k in key_list:
+        new_params["componentKeys"] = k
+        util.logger.debug("Searching for issues of project '%s'", k)
+        try:
+            issue_list.update(search(endpoint=endpoint, params=new_params))
+        except TooManyIssuesError:
+            util.logger.info(_TOO_MANY_ISSUES_MSG)
+            issue_list.update(__search_all_by_date(params=new_params, endpoint=endpoint))
+    return issue_list
+
+
+def search_by_project(project_key, endpoint, params=None, search_findings=False):
+    """Search all issues of a given project
+
+    :param project_key: The project key
+    :type project_key: str
+    :param endpoint: Reference to the SonarQube platform
+    :type endpoint: Platform
+    :param params: List of search filters to narrow down the search, defaults to None
+    :type params: dict
+    :param search_findings: Whether to use the api/project_search/findings API or not, defaults to False
+    :type search_findings: bool, optional
+    :return: list of Issues
+    :rtype: dict{<key>: <Issue>}
+    """
+    if params is None:
+        params = {}
+    if project_key is None:
+        key_list = projects.search(endpoint).keys()
+    else:
+        key_list = util.csv_to_list(project_key)
+    issue_list = {}
+    for k in key_list:
+        util.logger.info("Project '%s' issue search", k)
+        if endpoint.version() >= (9, 1, 0) and endpoint.edition() in ("enterprise", "datacenter") and search_findings:
+            util.logger.info("Using new export findings to speed up issue export")
+            issue_list.update(findings.export_findings(endpoint, k, params.get("branch", None), params.get("pullRequest", None)))
+        else:
+            issue_list.update(__search_all_by_project(k, params=params, endpoint=endpoint))
+        util.logger.info("Project '%s' has %d issues", k, len(issue_list))
+    return issue_list
+
+
+def search_all(endpoint, params=None):
+    """Returns all issues of the platforms
+
+    :param endpoint: Reference to the SonarQube platform
+    :type endpoint: Platform
+    :param params: List of search filters to narrow down the search, defaults to None
+    :type params: dict
+    :return: list of Issues
+    :rtype: dict{<key>: <Issue>}
+    """
+    new_params = {} if params is None else params.copy()
+    util.logger.info("Issue search all with %s", str(params))
+    issue_list = {}
+    try:
+        issue_list = search(endpoint=endpoint, params=params)
+    except TooManyIssuesError:
+        util.logger.info(_TOO_MANY_ISSUES_MSG)
+        for k in projects.search(endpoint):
+            issue_list.update(__search_all_by_project(k, params=new_params, endpoint=endpoint))
+    return issue_list
+
+
+def __search_thread(queue):
+    while not queue.empty():
+        (endpoint, api, issue_list, params, page) = queue.get()
+        page_params = params.copy()
+        page_params["p"] = page
+        util.logger.debug("Threaded issue search params = %s", str(page_params))
+        data = json.loads(endpoint.get(api, params=page_params).text)
+        for i in data["issues"]:
+            i["branch"] = page_params.get("branch", None)
+            i["pullRequest"] = page_params.get("pullRequest", None)
+            issue_list[i["key"]] = get_object(i["key"], endpoint=endpoint, data=i)
+        util.logger.debug("Added %d issues in threaded search page %d", len(data["issues"]), page)
+        queue.task_done()
+
+
+def search_first(endpoint, **params):
+    """
+    :return: The first issue of a search, for instance the oldest, if params = s="CREATION_DATE", asc=asc_sort
+    :rtype: Issue or None if not issue found
+    """
+    params["ps"] = 1
+    data = json.loads(endpoint.get(Issue.SEARCH_API, params=params).text)
+    if len(data) == 0:
+        return None
+    i = data["issues"][0]
+    return get_object(i["key"], endpoint=endpoint, data=i)
+
+
+def search(endpoint, params=None, raise_error=True, threads=8):
+    """Multi-threaded search of issues
+
+    :param params: Search filter criteria to narrow down the search
+    :type params: dict
+    :param raise_error: Whether to raise exception if more than 10'000 issues returned, defaults to True
+    :type raise_error: bool
+    :param threads: Nbr of parallel threads for search, defaults to 8
+    :type threads: int
+    :return: List of issues found
+    :rtype: dict{<key>: <Issue>}
+    :raises: TooManyIssuesError if more than 10'000 issues found
+    """
+    new_params = {} if params is None else params.copy()
+    util.logger.debug("Search params = %s", str(new_params))
+    if "ps" not in new_params:
+        new_params["ps"] = Issue.MAX_PAGE_SIZE
+    issue_list = {}
+
+    data = json.loads(endpoint.get(Issue.SEARCH_API, params=new_params).text)
+    nbr_issues = data["paging"]["total"]
+    nbr_pages = util.nbr_pages(data)
+    util.logger.debug("Number of issues: %d - Nbr pages: %d", nbr_issues, nbr_pages)
+    if nbr_pages > 20 and raise_error:
+        raise TooManyIssuesError(
+            nbr_issues,
+            f"{nbr_issues} issues returned by api/{Issue.SEARCH_API}, this is more than the max {Issue.MAX_SEARCH} possible",
+        )
+    for i in data["issues"]:
+        i["branch"] = new_params.get("branch", None)
+        i["pullRequest"] = new_params.get("pullRequest", None)
+        issue_list[i["key"]] = get_object(i["key"], endpoint=endpoint, data=i)
+    if nbr_pages == 1:
+        return issue_list
+    q = Queue(maxsize=0)
+    for page in range(2, nbr_pages + 1):
+        q.put((endpoint, Issue.SEARCH_API, issue_list, new_params, page))
+    for i in range(threads):
+        util.logger.debug("Starting issue search thread %d", i)
+        worker = Thread(target=__search_thread, args=[q])
+        worker.setDaemon(True)
+        worker.start()
+    q.join()
+    return issue_list
+
+
+def _get_facets(project_key, facets="directories", endpoint=None, params=None):
+    new_params = {} if params is None else params.copy()
+    new_params.update({"componentKeys": project_key, "facets": facets, "ps": 500})
+    new_params = __get_issues_search_params(new_params)
+    data = json.loads(endpoint.get(Issue.SEARCH_API, params=new_params).text)
+    l = {}
+    facets_list = util.csv_to_list(facets)
+    for f in data["facets"]:
+        if f["property"] in facets_list:
+            l[f["property"]] = f["values"]
+    return l
+
+
+def __get_one_issue_date(endpoint=None, asc_sort="false", params=None):
+    """Returns the date of one issue found"""
+    issue = search_first(endpoint=endpoint, s="CREATION_DATE", asc=asc_sort, **params)
+    if not issue:
+        return None
+    return issue.creation_date
+
+
+def get_oldest_issue(endpoint=None, params=None):
+    """Returns the oldest date of all issues found"""
+    return __get_one_issue_date(endpoint=endpoint, asc_sort="true", params=params)
+
+
+def get_newest_issue(endpoint=None, params=None):
+    """Returns the newest date of all issues found"""
+    return __get_one_issue_date(endpoint=endpoint, asc_sort="false", params=params)
+
+
+def count(endpoint=None, **kwargs):
+    """Returns number of issues of a search"""
+    returned_data = search(endpoint=endpoint, params=kwargs.copy().update({"ps": 1}))
+    util.logger.debug("Issue search %s would return %d issues", str(kwargs), returned_data["total"])
+    return returned_data["total"]
+
+
+def identical_attributes(o1, o2, key_list):
+    for key in key_list:
+        if o1[key] != o2[key]:
+            return False
+    return True
+
+
+def __get_issues_search_params(params):
+    outparams = {"additionalFields": "comments"}
+    for key in params:
+        if params[key] is not None and key in Issue.OPTIONS_SEARCH:
+            outparams[key] = params[key]
+    return outparams
+
+
+def get_object(key, data=None, endpoint=None, from_export=False):
+    if key not in _ISSUES:
+        _ = Issue(key=key, data=data, endpoint=endpoint, from_export=from_export)
+    return _ISSUES[key]
+
+
+def get_search_criteria(params):
+    """Returns the filtered list of params that are allowed for api/issue/search"""
+    criterias = params.copy()
+    if criterias.get("types", None) is not None:
+        criterias["types"] = util.allowed_values_string(criterias["types"], TYPES)
+    if criterias.get("severities", None) is not None:
+        criterias["severities"] = util.allowed_values_string(criterias["severities"], SEVERITIES)
+    if criterias.get("statuses", None) is not None:
+        criterias["statuses"] = util.allowed_values_string(criterias["statuses"], STATUSES)
+    if criterias.get("resolutions", None) is not None:
+        criterias["resolutions"] = util.allowed_values_string(criterias["resolutions"], RESOLUTIONS)
+    criterias = util.dict_subset(util.remove_nones(criterias), SEARCH_CRITERIAS)
+    return criterias
```

## sonar/permissions/__init__.py

 * *Ordering differences only*

```diff
@@ -1,19 +1,19 @@
-#
-# sonar-tools
-# Copyright (C) 2022-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
+#
+# sonar-tools
+# Copyright (C) 2022-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
```

## sonar/permissions/aggregation_permissions.py

 * *Ordering differences only*

```diff
@@ -1,52 +1,52 @@
-#
-# sonar-tools
-# Copyright (C) 2022-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-
-from sonar.permissions import permissions, project_permissions
-
-AGGREGATION_PERMISSIONS = {
-    "user": "Browse",
-    "admin": "Administer Project",
-}
-
-
-class AggregationPermissions(project_permissions.ProjectPermissions):
-    """
-    Abstraction of aggregations (Portfolios and Applications) permissions
-    """
-
-    def read(self):
-        """
-        :return: Permissions associated to the aggregation
-        :rtype: self
-        """
-        super().read()
-        # Hack: SonarQube return permissions for aggregations that do not exist
-        self.white_list(AGGREGATION_PERMISSIONS)
-        return self
-
-    def set(self, new_perms):
-        """Sets permissions of an aggregation
-
-        :param new_perms:
-        :type new_perms: dict {"users": [<user>, <user>, ...], "groups": [<group>, <group>, ...]}
-        :return: Permissions associated to the aggregation
-        :rtype: self
-        """
-        return super().set(permissions.white_list(new_perms, AGGREGATION_PERMISSIONS))
+#
+# sonar-tools
+# Copyright (C) 2022-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+
+from sonar.permissions import permissions, project_permissions
+
+AGGREGATION_PERMISSIONS = {
+    "user": "Browse",
+    "admin": "Administer Project",
+}
+
+
+class AggregationPermissions(project_permissions.ProjectPermissions):
+    """
+    Abstraction of aggregations (Portfolios and Applications) permissions
+    """
+
+    def read(self):
+        """
+        :return: Permissions associated to the aggregation
+        :rtype: self
+        """
+        super().read()
+        # Hack: SonarQube return permissions for aggregations that do not exist
+        self.white_list(AGGREGATION_PERMISSIONS)
+        return self
+
+    def set(self, new_perms):
+        """Sets permissions of an aggregation
+
+        :param new_perms:
+        :type new_perms: dict {"users": [<user>, <user>, ...], "groups": [<group>, <group>, ...]}
+        :return: Permissions associated to the aggregation
+        :rtype: self
+        """
+        return super().set(permissions.white_list(new_perms, AGGREGATION_PERMISSIONS))
```

## sonar/permissions/application_permissions.py

 * *Ordering differences only*

```diff
@@ -1,27 +1,27 @@
-#
-# sonar-tools
-# Copyright (C) 2022-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-
-from sonar.permissions import aggregation_permissions
-
-
-class ApplicationPermissions(aggregation_permissions.AggregationPermissions):
-    """
-    Abstraction of Application permission
-    """
+#
+# sonar-tools
+# Copyright (C) 2022-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+
+from sonar.permissions import aggregation_permissions
+
+
+class ApplicationPermissions(aggregation_permissions.AggregationPermissions):
+    """
+    Abstraction of Application permission
+    """
```

## sonar/permissions/global_permissions.py

 * *Ordering differences only*

```diff
@@ -1,74 +1,74 @@
-#
-# sonar-tools
-# Copyright (C) 2022-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-
-from sonar import utilities
-from sonar.permissions import permissions
-
-
-class GlobalPermissions(permissions.Permissions):
-    API_GET = {"users": "permissions/users", "groups": "permissions/groups"}
-    API_SET = {"users": "permissions/add_user", "groups": "permissions/add_group"}
-    API_REMOVE = {"users": "permissions/remove_user", "groups": "permissions/remove_group"}
-    API_GET_FIELD = {"users": "login", "groups": "name"}
-    API_SET_FIELD = {"users": "login", "groups": "groupName"}
-
-    def __str__(self):
-        return "global permissions"
-
-    def read(self):
-        self.permissions = permissions.NO_PERMISSIONS
-        for ptype in permissions.PERMISSION_TYPES:
-            self.permissions[ptype] = self._get_api(
-                GlobalPermissions.API_GET[ptype], ptype, GlobalPermissions.API_GET_FIELD[ptype], ps=permissions.MAX_PERMS
-            )
-        return self
-
-    def set(self, new_perms):
-        utilities.logger.debug("Setting %s to %s", str(self), str(new_perms))
-        if self.permissions is None:
-            self.read()
-        ed = self.endpoint.edition()
-        for perm_type in permissions.PERMISSION_TYPES:
-            if new_perms is None or perm_type not in new_perms:
-                continue
-            decoded_perms = {k: permissions.decode(v) for k, v in new_perms[perm_type].items()}
-            to_remove = edition_filter(permissions.diff(self.permissions[perm_type], decoded_perms), ed)
-            self._post_api(GlobalPermissions.API_REMOVE[perm_type], GlobalPermissions.API_SET_FIELD[perm_type], to_remove)
-            to_add = edition_filter(permissions.diff(decoded_perms, self.permissions[perm_type]), ed)
-            self._post_api(GlobalPermissions.API_SET[perm_type], GlobalPermissions.API_SET_FIELD[perm_type], to_add)
-        return self.read()
-
-
-def import_config(endpoint, config_data):
-    my_permissions = config_data.get("permissions", {})
-    if len(my_permissions) == 0:
-        utilities.logger.info("No global permissions in config, skipping import...")
-        return
-    utilities.logger.info("Importing global permissions")
-    global_perms = GlobalPermissions(endpoint)
-    global_perms.set(my_permissions)
-
-
-def edition_filter(perms, ed):
-    for p in perms.copy():
-        if ed == "community" and p in ("portfoliocreator", "applicationcreator") or ed == "developer" and p == "portfoliocreator":
-            utilities.logger.warning("Can't remove permission '%s' on a %s edition", p, ed)
-            perms.remove(p)
-    return perms
+#
+# sonar-tools
+# Copyright (C) 2022-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+
+from sonar import utilities
+from sonar.permissions import permissions
+
+
+class GlobalPermissions(permissions.Permissions):
+    API_GET = {"users": "permissions/users", "groups": "permissions/groups"}
+    API_SET = {"users": "permissions/add_user", "groups": "permissions/add_group"}
+    API_REMOVE = {"users": "permissions/remove_user", "groups": "permissions/remove_group"}
+    API_GET_FIELD = {"users": "login", "groups": "name"}
+    API_SET_FIELD = {"users": "login", "groups": "groupName"}
+
+    def __str__(self):
+        return "global permissions"
+
+    def read(self):
+        self.permissions = permissions.NO_PERMISSIONS
+        for ptype in permissions.PERMISSION_TYPES:
+            self.permissions[ptype] = self._get_api(
+                GlobalPermissions.API_GET[ptype], ptype, GlobalPermissions.API_GET_FIELD[ptype], ps=permissions.MAX_PERMS
+            )
+        return self
+
+    def set(self, new_perms):
+        utilities.logger.debug("Setting %s to %s", str(self), str(new_perms))
+        if self.permissions is None:
+            self.read()
+        ed = self.endpoint.edition()
+        for perm_type in permissions.PERMISSION_TYPES:
+            if new_perms is None or perm_type not in new_perms:
+                continue
+            decoded_perms = {k: permissions.decode(v) for k, v in new_perms[perm_type].items()}
+            to_remove = edition_filter(permissions.diff(self.permissions[perm_type], decoded_perms), ed)
+            self._post_api(GlobalPermissions.API_REMOVE[perm_type], GlobalPermissions.API_SET_FIELD[perm_type], to_remove)
+            to_add = edition_filter(permissions.diff(decoded_perms, self.permissions[perm_type]), ed)
+            self._post_api(GlobalPermissions.API_SET[perm_type], GlobalPermissions.API_SET_FIELD[perm_type], to_add)
+        return self.read()
+
+
+def import_config(endpoint, config_data):
+    my_permissions = config_data.get("permissions", {})
+    if len(my_permissions) == 0:
+        utilities.logger.info("No global permissions in config, skipping import...")
+        return
+    utilities.logger.info("Importing global permissions")
+    global_perms = GlobalPermissions(endpoint)
+    global_perms.set(my_permissions)
+
+
+def edition_filter(perms, ed):
+    for p in perms.copy():
+        if ed == "community" and p in ("portfoliocreator", "applicationcreator") or ed == "developer" and p == "portfoliocreator":
+            utilities.logger.warning("Can't remove permission '%s' on a %s edition", p, ed)
+            perms.remove(p)
+    return perms
```

## sonar/permissions/permission_templates.py

 * *Ordering differences only*

```diff
@@ -1,247 +1,247 @@
-#
-# sonar-tools
-# Copyright (C) 2022-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-
-import json
-from sonar import sqobject, utilities
-from sonar.permissions import template_permissions
-
-_OBJECTS = {}
-_MAP = {}
-_DEFAULT_TEMPLATES = {}
-_QUALIFIER_REVERSE_MAP = {"projects": "TRK", "applications": "APP", "portfolios": "VW"}
-_SEARCH_API = "permissions/search_templates"
-_CREATE_API = "permissions/create_template"
-_UPDATE_API = "permissions/update_template"
-
-_IMPORTABLE_PROPERTIES = ("name", "description", "pattern", "permissions", "defaultFor")
-
-
-class PermissionTemplate(sqobject.SqObject):
-    def __init__(self, endpoint, name, data=None, create_data=None):
-        super().__init__(name, endpoint)
-        self.key = None
-        self.name = name
-        self.description = None
-        self.project_key_pattern = None
-        self._permissions = None
-        if create_data is not None:
-            utilities.logger.info("Creating permission template '%s'", name)
-            utilities.logger.debug("from create_data %s", utilities.json_dump(create_data))
-            create_data["name"] = name
-            self.post(_CREATE_API, params=create_data)
-            data = search_by_name(endpoint, name)
-            self.key = data.get("id", None)
-            self.set_pattern(create_data.pop("pattern", None))
-            self.set_permissions(create_data.pop("permissions", None))
-        elif data is None:
-            data = search_by_name(endpoint, name)
-            self.key = data.get("id", None)
-            self.permissions().read()
-            utilities.logger.info("Creating permission template '%s'", name)
-            utilities.logger.debug("from sync data %s", utilities.json_dump(data))
-        self._json = data
-        self.name = name
-        data.pop("name")
-        self.key = data.pop("id", None)
-        self.description = data.get("description", None)
-        self.project_key_pattern = data.pop("projectKeyPattern", "")
-        self.creation_date = utilities.string_to_date(data.pop("createdAt", None))
-        self.last_update = utilities.string_to_date(data.pop("updatedAt", None))
-        self.__set_hash()
-        _OBJECTS[self.key] = self
-        _MAP[self.name.lower()] = self.key
-
-    def __str__(self):
-        return f"permission template '{self.name}'"
-
-    def __set_hash(self):
-        _OBJECTS[self.key] = self
-        _MAP[self.name] = self.key
-
-    def is_default_for(self, qualifier):
-        return qualifier in _DEFAULT_TEMPLATES and _DEFAULT_TEMPLATES[qualifier] == self.key
-
-    def is_projects_default(self):
-        return self.is_default_for("TRK")
-
-    def is_applications_default(self):
-        return self.is_default_for("APP")
-
-    def is_portfolios_default(self):
-        return self.is_default_for("VW")
-
-    def set_permissions(self, perms):
-        if perms is None or len(perms) == 0:
-            return
-        self.permissions().set(perms)
-
-    def update(self, **pt_data):
-        params = {"id": self.key}
-        # Hack: On SQ 8.9 if you pass all params otherwise SQ does NPE
-        params["name"] = pt_data.get("name", self.name if self.name else "")
-        params["description"] = pt_data.get("description", self.description if self.description else "")
-        params["projectKeyPattern"] = pt_data.get("pattern", self.project_key_pattern)
-        utilities.logger.info("Updating %s with %s", str(self), str(params))
-        self.post(_UPDATE_API, params=params)
-        _MAP.pop(self.name, None)
-        self.name = params["name"]
-        _MAP[self.name] = self.key
-        self.description = params["description"]
-        self.project_key_pattern = params["projectKeyPattern"]
-        self.permissions().set(pt_data.get("permissions", None))
-        return self
-
-    def permissions(self):
-        if self._permissions is None:
-            self._permissions = template_permissions.TemplatePermissions(self)
-        return self._permissions
-
-    def set_as_default(self, what_list):
-        utilities.logger.debug("Setting %s as default for %s", str(self), str(what_list))
-        ed = self.endpoint.edition()
-        for d in what_list:
-            qual = _QUALIFIER_REVERSE_MAP.get(d, d)
-            if (ed == "community" and qual in ("VW", "APP")) or (ed == "developer" and qual == "VW"):
-                utilities.logger.warning("Can't set permission template as default for %s on a %s edition", qual, ed)
-                continue
-            self.post("permissions/set_default_template", params={"templateId": self.key, "qualifier": qual})
-
-    def set_pattern(self, pattern):
-        if pattern is None:
-            return None
-        return self.update(pattern=pattern)
-
-    def to_json(self, full=False):
-        json_data = self._json.copy()
-        json_data.update(
-            {
-                "key": self.key,
-                "name": self.name,
-                "description": self.description if self.description != "" else None,
-                "pattern": self.project_key_pattern,
-                "permissions": self.permissions().export(),
-            }
-        )
-
-        defaults = []
-        if self.is_projects_default():
-            defaults.append("projects")
-        if self.is_applications_default():
-            defaults.append("applications")
-        if self.is_portfolios_default():
-            defaults.append("portfolios")
-        if len(defaults) > 0:
-            json_data["defaultFor"] = utilities.list_to_csv(defaults, ", ")
-
-        json_data["creationDate"] = utilities.date_to_string(self.creation_date)
-        json_data["lastUpdate"] = utilities.date_to_string(self.last_update)
-        return utilities.remove_nones(utilities.filter_export(json_data, _IMPORTABLE_PROPERTIES, full))
-
-    def audit(self, audit_settings):
-        utilities.logger.debug("Auditing %s", str(self))
-        return self.permissions().audit(audit_settings)
-
-
-def get_object(name, endpoint=None):
-    if len(_OBJECTS) == 0:
-        get_list(endpoint)
-    lowername = name.lower()
-    if lowername not in _MAP:
-        return None
-    return _OBJECTS.get(_MAP[lowername], None)
-
-
-def create_or_update(name, endpoint, kwargs):
-    utilities.logger.debug("Create or update permission template '%s'", name)
-    o = get_object(endpoint=endpoint, name=name)
-    if o is None:
-        utilities.logger.debug("Permission template '%s' does not exist, creating...", name)
-        return create(name, endpoint, create_data=kwargs)
-    else:
-        return o.update(name=name, **kwargs)
-
-
-def create(name, endpoint=None, create_data=None):
-    o = get_object(name=name, endpoint=endpoint)
-    if o is None:
-        o = PermissionTemplate(name=name, endpoint=endpoint, create_data=create_data)
-    else:
-        utilities.logger.info("%s already exists, skipping creation...", str(o))
-    return o
-
-
-def search(endpoint, params=None):
-    utilities.logger.debug("Searching all permission templates")
-    objects_list = {}
-    data = json.loads(endpoint.get(_SEARCH_API, params=params).text)
-    for obj in data["permissionTemplates"]:
-        o = PermissionTemplate(name=obj["name"], endpoint=endpoint, data=obj)
-        objects_list[o.key] = o
-    _load_default_templates(data=data)
-    return objects_list
-
-
-def search_by_name(endpoint, name):
-    return utilities.search_by_name(endpoint, name, _SEARCH_API, "permissionTemplates")
-
-
-def get_list(endpoint):
-    return search(endpoint, None)
-
-
-def _load_default_templates(data=None, endpoint=None):
-    if data is None:
-        data = json.loads(endpoint.get(_SEARCH_API).text)
-    for d in data["defaultTemplates"]:
-        _DEFAULT_TEMPLATES[d["qualifier"]] = d["templateId"]
-
-
-def export(endpoint, full=False):
-    utilities.logger.info("Exporting permission templates")
-    pt_list = get_list(endpoint)
-    json_data = {}
-    for pt in pt_list.values():
-        json_data[pt.name] = pt.to_json(full)
-        if not full:
-            for k in ("name", "id", "key"):
-                json_data[pt.name].pop(k, None)
-    return json_data
-
-
-def import_config(endpoint, config_data):
-    if "permissionTemplates" not in config_data:
-        utilities.logger.info("No permissions templates in config, skipping import...")
-        return
-    utilities.logger.info("Importing permission templates")
-    get_list(endpoint)
-    for name, data in config_data["permissionTemplates"].items():
-        utilities.json_dump_debug(data, f"Importing: {name}:")
-        o = create_or_update(name, endpoint, data)
-        defs = data.get("defaultFor", None)
-        if defs is not None and defs != "":
-            o.set_as_default(utilities.csv_to_list(data.get("defaultFor", None)))
-
-
-def audit(endpoint, audit_settings):
-    utilities.logger.info("--- Auditing permission templates ---")
-    problems = []
-    for pt in get_list(endpoint=endpoint).values():
-        problems += pt.audit(audit_settings)
-    return problems
+#
+# sonar-tools
+# Copyright (C) 2022-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+
+import json
+from sonar import sqobject, utilities
+from sonar.permissions import template_permissions
+
+_OBJECTS = {}
+_MAP = {}
+_DEFAULT_TEMPLATES = {}
+_QUALIFIER_REVERSE_MAP = {"projects": "TRK", "applications": "APP", "portfolios": "VW"}
+_SEARCH_API = "permissions/search_templates"
+_CREATE_API = "permissions/create_template"
+_UPDATE_API = "permissions/update_template"
+
+_IMPORTABLE_PROPERTIES = ("name", "description", "pattern", "permissions", "defaultFor")
+
+
+class PermissionTemplate(sqobject.SqObject):
+    def __init__(self, endpoint, name, data=None, create_data=None):
+        super().__init__(name, endpoint)
+        self.key = None
+        self.name = name
+        self.description = None
+        self.project_key_pattern = None
+        self._permissions = None
+        if create_data is not None:
+            utilities.logger.info("Creating permission template '%s'", name)
+            utilities.logger.debug("from create_data %s", utilities.json_dump(create_data))
+            create_data["name"] = name
+            self.post(_CREATE_API, params=create_data)
+            data = search_by_name(endpoint, name)
+            self.key = data.get("id", None)
+            self.set_pattern(create_data.pop("pattern", None))
+            self.set_permissions(create_data.pop("permissions", None))
+        elif data is None:
+            data = search_by_name(endpoint, name)
+            self.key = data.get("id", None)
+            self.permissions().read()
+            utilities.logger.info("Creating permission template '%s'", name)
+            utilities.logger.debug("from sync data %s", utilities.json_dump(data))
+        self._json = data
+        self.name = name
+        data.pop("name")
+        self.key = data.pop("id", None)
+        self.description = data.get("description", None)
+        self.project_key_pattern = data.pop("projectKeyPattern", "")
+        self.creation_date = utilities.string_to_date(data.pop("createdAt", None))
+        self.last_update = utilities.string_to_date(data.pop("updatedAt", None))
+        self.__set_hash()
+        _OBJECTS[self.key] = self
+        _MAP[self.name.lower()] = self.key
+
+    def __str__(self):
+        return f"permission template '{self.name}'"
+
+    def __set_hash(self):
+        _OBJECTS[self.key] = self
+        _MAP[self.name] = self.key
+
+    def is_default_for(self, qualifier):
+        return qualifier in _DEFAULT_TEMPLATES and _DEFAULT_TEMPLATES[qualifier] == self.key
+
+    def is_projects_default(self):
+        return self.is_default_for("TRK")
+
+    def is_applications_default(self):
+        return self.is_default_for("APP")
+
+    def is_portfolios_default(self):
+        return self.is_default_for("VW")
+
+    def set_permissions(self, perms):
+        if perms is None or len(perms) == 0:
+            return
+        self.permissions().set(perms)
+
+    def update(self, **pt_data):
+        params = {"id": self.key}
+        # Hack: On SQ 8.9 if you pass all params otherwise SQ does NPE
+        params["name"] = pt_data.get("name", self.name if self.name else "")
+        params["description"] = pt_data.get("description", self.description if self.description else "")
+        params["projectKeyPattern"] = pt_data.get("pattern", self.project_key_pattern)
+        utilities.logger.info("Updating %s with %s", str(self), str(params))
+        self.post(_UPDATE_API, params=params)
+        _MAP.pop(self.name, None)
+        self.name = params["name"]
+        _MAP[self.name] = self.key
+        self.description = params["description"]
+        self.project_key_pattern = params["projectKeyPattern"]
+        self.permissions().set(pt_data.get("permissions", None))
+        return self
+
+    def permissions(self):
+        if self._permissions is None:
+            self._permissions = template_permissions.TemplatePermissions(self)
+        return self._permissions
+
+    def set_as_default(self, what_list):
+        utilities.logger.debug("Setting %s as default for %s", str(self), str(what_list))
+        ed = self.endpoint.edition()
+        for d in what_list:
+            qual = _QUALIFIER_REVERSE_MAP.get(d, d)
+            if (ed == "community" and qual in ("VW", "APP")) or (ed == "developer" and qual == "VW"):
+                utilities.logger.warning("Can't set permission template as default for %s on a %s edition", qual, ed)
+                continue
+            self.post("permissions/set_default_template", params={"templateId": self.key, "qualifier": qual})
+
+    def set_pattern(self, pattern):
+        if pattern is None:
+            return None
+        return self.update(pattern=pattern)
+
+    def to_json(self, full=False):
+        json_data = self._json.copy()
+        json_data.update(
+            {
+                "key": self.key,
+                "name": self.name,
+                "description": self.description if self.description != "" else None,
+                "pattern": self.project_key_pattern,
+                "permissions": self.permissions().export(),
+            }
+        )
+
+        defaults = []
+        if self.is_projects_default():
+            defaults.append("projects")
+        if self.is_applications_default():
+            defaults.append("applications")
+        if self.is_portfolios_default():
+            defaults.append("portfolios")
+        if len(defaults) > 0:
+            json_data["defaultFor"] = utilities.list_to_csv(defaults, ", ")
+
+        json_data["creationDate"] = utilities.date_to_string(self.creation_date)
+        json_data["lastUpdate"] = utilities.date_to_string(self.last_update)
+        return utilities.remove_nones(utilities.filter_export(json_data, _IMPORTABLE_PROPERTIES, full))
+
+    def audit(self, audit_settings):
+        utilities.logger.debug("Auditing %s", str(self))
+        return self.permissions().audit(audit_settings)
+
+
+def get_object(name, endpoint=None):
+    if len(_OBJECTS) == 0:
+        get_list(endpoint)
+    lowername = name.lower()
+    if lowername not in _MAP:
+        return None
+    return _OBJECTS.get(_MAP[lowername], None)
+
+
+def create_or_update(name, endpoint, kwargs):
+    utilities.logger.debug("Create or update permission template '%s'", name)
+    o = get_object(endpoint=endpoint, name=name)
+    if o is None:
+        utilities.logger.debug("Permission template '%s' does not exist, creating...", name)
+        return create(name, endpoint, create_data=kwargs)
+    else:
+        return o.update(name=name, **kwargs)
+
+
+def create(name, endpoint=None, create_data=None):
+    o = get_object(name=name, endpoint=endpoint)
+    if o is None:
+        o = PermissionTemplate(name=name, endpoint=endpoint, create_data=create_data)
+    else:
+        utilities.logger.info("%s already exists, skipping creation...", str(o))
+    return o
+
+
+def search(endpoint, params=None):
+    utilities.logger.debug("Searching all permission templates")
+    objects_list = {}
+    data = json.loads(endpoint.get(_SEARCH_API, params=params).text)
+    for obj in data["permissionTemplates"]:
+        o = PermissionTemplate(name=obj["name"], endpoint=endpoint, data=obj)
+        objects_list[o.key] = o
+    _load_default_templates(data=data)
+    return objects_list
+
+
+def search_by_name(endpoint, name):
+    return utilities.search_by_name(endpoint, name, _SEARCH_API, "permissionTemplates")
+
+
+def get_list(endpoint):
+    return search(endpoint, None)
+
+
+def _load_default_templates(data=None, endpoint=None):
+    if data is None:
+        data = json.loads(endpoint.get(_SEARCH_API).text)
+    for d in data["defaultTemplates"]:
+        _DEFAULT_TEMPLATES[d["qualifier"]] = d["templateId"]
+
+
+def export(endpoint, full=False):
+    utilities.logger.info("Exporting permission templates")
+    pt_list = get_list(endpoint)
+    json_data = {}
+    for pt in pt_list.values():
+        json_data[pt.name] = pt.to_json(full)
+        if not full:
+            for k in ("name", "id", "key"):
+                json_data[pt.name].pop(k, None)
+    return json_data
+
+
+def import_config(endpoint, config_data):
+    if "permissionTemplates" not in config_data:
+        utilities.logger.info("No permissions templates in config, skipping import...")
+        return
+    utilities.logger.info("Importing permission templates")
+    get_list(endpoint)
+    for name, data in config_data["permissionTemplates"].items():
+        utilities.json_dump_debug(data, f"Importing: {name}:")
+        o = create_or_update(name, endpoint, data)
+        defs = data.get("defaultFor", None)
+        if defs is not None and defs != "":
+            o.set_as_default(utilities.csv_to_list(data.get("defaultFor", None)))
+
+
+def audit(endpoint, audit_settings):
+    utilities.logger.info("--- Auditing permission templates ---")
+    problems = []
+    for pt in get_list(endpoint=endpoint).values():
+        problems += pt.audit(audit_settings)
+    return problems
```

## sonar/permissions/permissions.py

```diff
@@ -1,361 +1,360 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-
-import json
-from http import HTTPStatus
-from abc import ABC, abstractmethod
-from sonar import utilities, options
-
-COMMUNITY_GLOBAL_PERMISSIONS = {
-    "admin": "Administer System",
-    "gateadmin": "Administer Quality Gates",
-    "profileadmin": "Administer Quality Profiles",
-    "provisioning": "Create Projects",
-    "scan": "Execute Analysis",
-}
-DEVELOPER_GLOBAL_PERMISSIONS = {**COMMUNITY_GLOBAL_PERMISSIONS, **{"applicationcreator": "Create Applications"}}
-ENTERPRISE_GLOBAL_PERMISSIONS = {**DEVELOPER_GLOBAL_PERMISSIONS, **{"portfoliocreator": "Create Portfolios"}}
-
-PROJECT_PERMISSIONS = {
-    "user": "Browse",
-    "codeviewer": "See source code",
-    "issueadmin": "Administer Issues",
-    "securityhotspotadmin": "Create Projects",
-    "scan": "Execute Analysis",
-    "admin": "Administer Project",
-}
-
-_GLOBAL = 0
-_PROJECTS = 1
-_TEMPLATES = 2
-_QG = 3
-_QP = 4
-_APPS = 5
-_PORTFOLIOS = 6
-
-OBJECTS_WITH_PERMISSIONS = (_GLOBAL, _PROJECTS, _TEMPLATES, _QG, _QP, _APPS, _PORTFOLIOS)
-PERMISSION_TYPES = ("users", "groups")
-NO_PERMISSIONS = {"users": None, "groups": None}
-
-MAX_PERMS = 100
-
-
-class Permissions(ABC):
-    """
-    Abstraction of sonar objects permissions
-    """
-
-    def __init__(self, endpoint):
-        self.endpoint = endpoint
-        self.permissions = None
-        self.read()
-
-    def to_json(self, perm_type=None, csv=False):
-        """
-        :return: The permissions as dict
-        :rtype: dict {"users": {<login>: [<perm>, <perm>, ...], ...}, "groups": {<name>: [<perm>, <perm>, ...], ...}}
-        """
-        if not csv:
-            return self.permissions[perm_type] if is_valid(perm_type) else self.permissions
-        perms = {}
-        for p in normalize(perm_type):
-            dperms = self.permissions.get(p, None)
-            if dperms is not None and len(dperms) > 0:
-                perms[p] = simplify(dperms)
-        return perms if len(perms) > 0 else None
-        # return {p: simplify(self.permissions.get(p, None)) for p in _normalize(perm_type) if self.permissions.get(p, None) is not None}
-
-    def export(self):
-        """
-        :return: The permissions as dict
-        :rtype: dict {"users": {<login>: [<perm>, <perm>, ...], ...}, "groups": {<name>: [<perm>, <perm>, ...], ...}}
-        """
-        return self.to_json(csv=True)
-
-    @abstractmethod
-    def __str__(self):
-        pass
-
-    @abstractmethod
-    def read(self):
-        """
-        :return: The concerned object permissions
-        :rtype: Permissions
-        """
-
-    @abstractmethod
-    def set(self, new_perms):
-        """Sets permissions of an object
-
-        :param dict new_perms: The permissions as dict
-        :rtype: self
-        """
-
-    def set_user_permissions(self, user_perms):
-        """Sets user permissions of an object
-
-        :param dict new_perms: The user permissions
-        :rtype: self
-        """
-        self.set({"users": user_perms})
-
-    def set_group_permissions(self, group_perms):
-        self.set({"groups": group_perms})
-
-    """
-    @abstractmethod
-    def remove_user_permissions(self, user_perms_dict):
-        pass
-
-    @abstractmethod
-    def remove_group_permissions(self, group_perms_dict):
-        pass
-
-
-    def remove_permissions(self, perms_dict):
-        self.remove_user_permissions(perms_dict.get("users", None))
-        self.remove_group_permissions(perms_dict.get("groups", None))
-    """
-
-    def clear(self):
-        """Clears all permissions of an object
-        :return: self
-        :rtype: Permissions
-        """
-        self.set({"users": {}, "groups": {}})
-
-    def users(self):
-        """
-        :return: User permissions of an object
-        :rtype: list (for QualityGate and QualityProfile) or dict (for other objects)
-        """
-        if self.permissions is None:
-            self.read()
-        return self.to_json(perm_type="users")
-
-    def groups(self):
-        """
-        :return: Group permissions of an object
-        :rtype: list (for QualityGate and QualityProfile) or dict (for other objects)
-        """
-        if self.permissions is None:
-            self.read()
-        return self.to_json(perm_type="groups")
-
-    def added_permissions(self, other_perms):
-        return diff(self.permissions, other_perms)
-
-    def removed_permissions(self, other_perms):
-        return diff(other_perms, self.permissions)
-
-    def compare(self, other_perms):
-        return {"added": diff(self.permissions, other_perms), "removed": diff(other_perms, self.permissions)}
-
-    def black_list(self, disallowed_perms):
-        """
-        :meta private:
-        """
-        for p in PERMISSION_TYPES:
-            for u, perms in self.permissions[p].items():
-                self.permissions[p][u] = black_list(perms, disallowed_perms)
-
-    def white_list(self, allowed_perms):
-        """
-        :meta private:
-        """
-        for p in PERMISSION_TYPES:
-            for u, perms in self.permissions[p].items():
-                self.permissions[p][u] = white_list(perms, allowed_perms)
-
-    def _filter_permissions_for_edition(self, perms):
-        ed = self.endpoint.edition()
-        allowed_perms = list(PROJECT_PERMISSIONS.keys())
-        if ed == "community":
-            allowed_perms += list(COMMUNITY_GLOBAL_PERMISSIONS.keys())
-        elif ed == "developer":
-            allowed_perms += list(DEVELOPER_GLOBAL_PERMISSIONS.keys())
-        else:
-            allowed_perms += list(ENTERPRISE_GLOBAL_PERMISSIONS.keys())
-        for p in perms.copy():
-            if p not in allowed_perms:
-                utilities.logger.warning("Can't set permission '%s' on a %s edition", ENTERPRISE_GLOBAL_PERMISSIONS[p], ed)
-                perms.remove(p)
-        return perms
-
-    def count(self, perm_type=None, perm_filter=None):
-        """Counts number of permissions of an object
-
-        :param perm_type: Optional "users" or "groups", both assumed if not specified.
-        :type perm_type: str, optional
-        :param perm_filter: Optional filter to count only specific types of permissions, defaults to None.
-        :type perm_type: str, Optional
-        :return: The number of permissions.
-        :rtype: int
-        """
-        perms = PERMISSION_TYPES if perm_type is None else (perm_type)
-        elem_counter, perm_counter = 0, 0
-        for ptype in perms:
-            for elem_perms in self.permissions.get(ptype, {}).values():
-                elem_counter += 1
-                if perm_filter is None:
-                    continue
-                for p in elem_perms:
-                    if p in perm_filter:
-                        perm_counter += 1
-        return elem_counter if perm_filter is None else perm_counter
-
-    def _get_api(self, api, perm_type, ret_field, **extra_params):
-        perms = {}
-        params = extra_params.copy()
-        page, nbr_pages = 1, 1
-        counter = 0
-        while page <= nbr_pages:
-            params["p"] = page
-            resp = self.endpoint.get(api, params=params)
-            if resp.ok:
-                data = json.loads(resp.text)
-                # perms.update({p[ret_field]: p["permissions"] for p in data[perm_type]})
-                for p in data[perm_type]:
-                    if len(p["permissions"]) > 0:
-                        perms[p[ret_field]] = p["permissions"]
-                        counter = 0
-                    else:
-                        counter += 1
-            elif resp.status_code not in (HTTPStatus.BAD_REQUEST, HTTPStatus.NOT_FOUND):
-                # Hack: Different versions of SonarQube return different codes (400 or 404)
-                utilities.exit_fatal(f"HTTP error {resp.status_code} - Exiting", options.ERR_SONAR_API)
-            page, nbr_pages = page + 1, utilities.nbr_pages(data)
-            if counter > 5 or not resp.ok:
-                break
-        return perms
-
-    def _post_api(self, api, set_field, perms_dict, **extra_params):
-        if perms_dict is None:
-            return True
-        result = False
-        params = extra_params.copy()
-        for u, perms in perms_dict.items():
-            params[set_field] = u
-            filtered_perms = self._filter_permissions_for_edition(perms)
-            for p in filtered_perms:
-                params["permission"] = p
-                r = self.endpoint.post(api, params=params)
-                result = result and r.ok
-        return result
-
-
-def simplify(perms_dict):
-    if perms_dict is None or len(perms_dict) == 0:
-        return None
-    return {k: encode(v) for k, v in perms_dict.items() if len(v) > 0}
-
-
-def encode(perms_array):
-    """
-    :meta private:
-    """
-    return utilities.list_to_csv(perms_array, ", ")
-
-
-def decode(encoded_perms):
-    """
-    :meta private:
-    """
-    return utilities.csv_to_list(encoded_perms)
-
-
-def is_valid(perm_type):
-    """
-    :param str perm_type:
-    :return: Whether that permission type exists
-    :rtype: bool
-    """
-    return perm_type and perm_type in PERMISSION_TYPES
-
-
-def normalize(perm_type):
-    """
-    :meta private:
-    """
-    return (perm_type) if is_valid(perm_type) else PERMISSION_TYPES
-
-
-def apply_api(endpoint, api, ufield, uvalue, ofield, ovalue, perm_list):
-    """
-    :meta private:
-    """
-    for p in perm_list:
-        endpoint.post(api, params={ufield: uvalue, ofield: ovalue, "permission": p})
-
-
-def diff_full(perms_1, perms_2):
-    """
-    :meta private:
-    """
-    diff_perms = perms_1.copy()
-    for perm_type in ("users", "groups"):
-        for elem, perms in perms_2:
-            if elem not in perms_1:
-                continue
-            for p in perms:
-                if p not in diff_perms[perm_type][elem]:
-                    continue
-                diff_perms[perm_type][elem].remove(p)
-    return diff_perms
-
-
-def diff(perms_1, perms_2):
-    """
-    :meta private:
-    """
-    diff_perms = perms_1.copy()
-    for elem, perms in perms_2.items():
-        if elem not in perms_1:
-            continue
-        for p in perms:
-            if p not in diff_perms[elem]:
-                continue
-            diff_perms[elem].remove(p)
-    return diff_perms
-
-
-def diffarray(perms_1, perms_2):
-    """
-    :meta private:
-    """
-    diff_perms = perms_1.copy()
-    for elem in perms_2:
-        if elem in diff_perms:
-            diff_perms.remove(elem)
-    return diff_perms
-
-
-def white_list(perms, allowed_perms):
-    """
-    :meta private:
-    """
-    return [p for p in perms if p in allowed_perms]
-
-
-def black_list(perms, disallowed_perms):
-    """
-    :meta private:
-    """
-    return [p for p in perms if p not in disallowed_perms]
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+
+import json
+from http import HTTPStatus
+from abc import ABC, abstractmethod
+from sonar import utilities, options
+
+COMMUNITY_GLOBAL_PERMISSIONS = {
+    "admin": "Administer System",
+    "gateadmin": "Administer Quality Gates",
+    "profileadmin": "Administer Quality Profiles",
+    "provisioning": "Create Projects",
+    "scan": "Execute Analysis",
+}
+DEVELOPER_GLOBAL_PERMISSIONS = {**COMMUNITY_GLOBAL_PERMISSIONS, **{"applicationcreator": "Create Applications"}}
+ENTERPRISE_GLOBAL_PERMISSIONS = {**DEVELOPER_GLOBAL_PERMISSIONS, **{"portfoliocreator": "Create Portfolios"}}
+
+PROJECT_PERMISSIONS = {
+    "user": "Browse",
+    "codeviewer": "See source code",
+    "issueadmin": "Administer Issues",
+    "securityhotspotadmin": "Create Projects",
+    "scan": "Execute Analysis",
+    "admin": "Administer Project",
+}
+
+_GLOBAL = 0
+_PROJECTS = 1
+_TEMPLATES = 2
+_QG = 3
+_QP = 4
+_APPS = 5
+_PORTFOLIOS = 6
+
+OBJECTS_WITH_PERMISSIONS = (_GLOBAL, _PROJECTS, _TEMPLATES, _QG, _QP, _APPS, _PORTFOLIOS)
+PERMISSION_TYPES = ("users", "groups")
+NO_PERMISSIONS = {"users": None, "groups": None}
+
+MAX_PERMS = 100
+
+
+class Permissions(ABC):
+    """
+    Abstraction of sonar objects permissions
+    """
+
+    def __init__(self, endpoint):
+        self.endpoint = endpoint
+        self.permissions = None
+        self.read()
+
+    def to_json(self, perm_type=None, csv=False):
+        """
+        :return: The permissions as dict
+        :rtype: dict {"users": {<login>: [<perm>, <perm>, ...], ...}, "groups": {<name>: [<perm>, <perm>, ...], ...}}
+        """
+        if not csv:
+            return self.permissions[perm_type] if is_valid(perm_type) else self.permissions
+        perms = {}
+        for p in normalize(perm_type):
+            dperms = self.permissions.get(p, None)
+            if dperms is not None and len(dperms) > 0:
+                perms[p] = simplify(dperms)
+        return perms if len(perms) > 0 else None
+        # return {p: simplify(self.permissions.get(p, None)) for p in _normalize(perm_type) if self.permissions.get(p, None) is not None}
+
+    def export(self):
+        """
+        :return: The permissions as dict
+        :rtype: dict {"users": {<login>: [<perm>, <perm>, ...], ...}, "groups": {<name>: [<perm>, <perm>, ...], ...}}
+        """
+        return self.to_json(csv=True)
+
+    @abstractmethod
+    def __str__(self):
+        pass
+
+    @abstractmethod
+    def read(self):
+        """
+        :return: The concerned object permissions
+        :rtype: Permissions
+        """
+
+    @abstractmethod
+    def set(self, new_perms):
+        """Sets permissions of an object
+
+        :param dict new_perms: The permissions as dict
+        :rtype: self
+        """
+
+    def set_user_permissions(self, user_perms):
+        """Sets user permissions of an object
+
+        :param dict new_perms: The user permissions
+        :rtype: self
+        """
+        self.set({"users": user_perms})
+
+    def set_group_permissions(self, group_perms):
+        self.set({"groups": group_perms})
+
+    """
+    @abstractmethod
+    def remove_user_permissions(self, user_perms_dict):
+        pass
+
+    @abstractmethod
+    def remove_group_permissions(self, group_perms_dict):
+        pass
+
+
+    def remove_permissions(self, perms_dict):
+        self.remove_user_permissions(perms_dict.get("users", None))
+        self.remove_group_permissions(perms_dict.get("groups", None))
+    """
+
+    def clear(self):
+        """Clears all permissions of an object
+        :return: self
+        :rtype: Permissions
+        """
+        self.set({"users": {}, "groups": {}})
+
+    def users(self):
+        """
+        :return: User permissions of an object
+        :rtype: list (for QualityGate and QualityProfile) or dict (for other objects)
+        """
+        if self.permissions is None:
+            self.read()
+        return self.to_json(perm_type="users")
+
+    def groups(self):
+        """
+        :return: Group permissions of an object
+        :rtype: list (for QualityGate and QualityProfile) or dict (for other objects)
+        """
+        if self.permissions is None:
+            self.read()
+        return self.to_json(perm_type="groups")
+
+    def added_permissions(self, other_perms):
+        return diff(self.permissions, other_perms)
+
+    def removed_permissions(self, other_perms):
+        return diff(other_perms, self.permissions)
+
+    def compare(self, other_perms):
+        return {"added": diff(self.permissions, other_perms), "removed": diff(other_perms, self.permissions)}
+
+    def black_list(self, disallowed_perms):
+        """
+        :meta private:
+        """
+        for p in PERMISSION_TYPES:
+            for u, perms in self.permissions[p].items():
+                self.permissions[p][u] = black_list(perms, disallowed_perms)
+
+    def white_list(self, allowed_perms):
+        """
+        :meta private:
+        """
+        for p in PERMISSION_TYPES:
+            for u, perms in self.permissions[p].items():
+                self.permissions[p][u] = white_list(perms, allowed_perms)
+
+    def _filter_permissions_for_edition(self, perms):
+        ed = self.endpoint.edition()
+        allowed_perms = list(PROJECT_PERMISSIONS.keys())
+        if ed == "community":
+            allowed_perms += list(COMMUNITY_GLOBAL_PERMISSIONS.keys())
+        elif ed == "developer":
+            allowed_perms += list(DEVELOPER_GLOBAL_PERMISSIONS.keys())
+        else:
+            allowed_perms += list(ENTERPRISE_GLOBAL_PERMISSIONS.keys())
+        for p in perms.copy():
+            if p not in allowed_perms:
+                utilities.logger.warning("Can't set permission '%s' on a %s edition", ENTERPRISE_GLOBAL_PERMISSIONS[p], ed)
+                perms.remove(p)
+        return perms
+
+    def count(self, perm_type=None, perm_filter=None):
+        """Counts number of permissions of an object
+
+        :param perm_type: Optional "users" or "groups", both assumed if not specified.
+        :type perm_type: str, optional
+        :param perm_filter: Optional filter to count only specific types of permissions, defaults to None.
+        :type perm_type: str, Optional
+        :return: The number of permissions.
+        :rtype: int
+        """
+        perms = PERMISSION_TYPES if perm_type is None else (perm_type,)
+        elem_counter, perm_counter = 0, 0
+        for ptype in perms:
+            for elem_perms in self.permissions.get(ptype, {}).values():
+                elem_counter += 1
+                if perm_filter is None:
+                    continue
+                perm_counter += len([1 for p in elem_perms if p in perm_filter])
+        utilities.logger.debug("Perm counts = %d", (elem_counter if perm_filter is None else perm_counter))
+        return elem_counter if perm_filter is None else perm_counter
+
+    def _get_api(self, api, perm_type, ret_field, **extra_params):
+        perms = {}
+        params = extra_params.copy()
+        page, nbr_pages = 1, 1
+        counter = 0
+        while page <= nbr_pages:
+            params["p"] = page
+            resp = self.endpoint.get(api, params=params)
+            if resp.ok:
+                data = json.loads(resp.text)
+                # perms.update({p[ret_field]: p["permissions"] for p in data[perm_type]})
+                for p in data[perm_type]:
+                    if len(p["permissions"]) > 0:
+                        perms[p[ret_field]] = p["permissions"]
+                        counter = 0
+                    else:
+                        counter += 1
+            elif resp.status_code not in (HTTPStatus.BAD_REQUEST, HTTPStatus.NOT_FOUND):
+                # Hack: Different versions of SonarQube return different codes (400 or 404)
+                utilities.exit_fatal(f"HTTP error {resp.status_code} - Exiting", options.ERR_SONAR_API)
+            page, nbr_pages = page + 1, utilities.nbr_pages(data)
+            if counter > 5 or not resp.ok:
+                break
+        return perms
+
+    def _post_api(self, api, set_field, perms_dict, **extra_params):
+        if perms_dict is None:
+            return True
+        result = False
+        params = extra_params.copy()
+        for u, perms in perms_dict.items():
+            params[set_field] = u
+            filtered_perms = self._filter_permissions_for_edition(perms)
+            for p in filtered_perms:
+                params["permission"] = p
+                r = self.endpoint.post(api, params=params)
+                result = result and r.ok
+        return result
+
+
+def simplify(perms_dict):
+    if perms_dict is None or len(perms_dict) == 0:
+        return None
+    return {k: encode(v) for k, v in perms_dict.items() if len(v) > 0}
+
+
+def encode(perms_array):
+    """
+    :meta private:
+    """
+    return utilities.list_to_csv(perms_array, ", ")
+
+
+def decode(encoded_perms):
+    """
+    :meta private:
+    """
+    return utilities.csv_to_list(encoded_perms)
+
+
+def is_valid(perm_type):
+    """
+    :param str perm_type:
+    :return: Whether that permission type exists
+    :rtype: bool
+    """
+    return perm_type and perm_type in PERMISSION_TYPES
+
+
+def normalize(perm_type):
+    """
+    :meta private:
+    """
+    return (perm_type) if is_valid(perm_type) else PERMISSION_TYPES
+
+
+def apply_api(endpoint, api, ufield, uvalue, ofield, ovalue, perm_list):
+    """
+    :meta private:
+    """
+    for p in perm_list:
+        endpoint.post(api, params={ufield: uvalue, ofield: ovalue, "permission": p})
+
+
+def diff_full(perms_1, perms_2):
+    """
+    :meta private:
+    """
+    diff_perms = perms_1.copy()
+    for perm_type in ("users", "groups"):
+        for elem, perms in perms_2:
+            if elem not in perms_1:
+                continue
+            for p in perms:
+                if p not in diff_perms[perm_type][elem]:
+                    continue
+                diff_perms[perm_type][elem].remove(p)
+    return diff_perms
+
+
+def diff(perms_1, perms_2):
+    """
+    :meta private:
+    """
+    diff_perms = perms_1.copy()
+    for elem, perms in perms_2.items():
+        if elem not in perms_1:
+            continue
+        for p in perms:
+            if p not in diff_perms[elem]:
+                continue
+            diff_perms[elem].remove(p)
+    return diff_perms
+
+
+def diffarray(perms_1, perms_2):
+    """
+    :meta private:
+    """
+    diff_perms = perms_1.copy()
+    for elem in perms_2:
+        if elem in diff_perms:
+            diff_perms.remove(elem)
+    return diff_perms
+
+
+def white_list(perms, allowed_perms):
+    """
+    :meta private:
+    """
+    return [p for p in perms if p in allowed_perms]
+
+
+def black_list(perms, disallowed_perms):
+    """
+    :meta private:
+    """
+    return [p for p in perms if p not in disallowed_perms]
```

## sonar/permissions/portfolio_permissions.py

 * *Ordering differences only*

```diff
@@ -1,27 +1,27 @@
-#
-# sonar-tools
-# Copyright (C) 2022-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-
-from sonar.permissions import aggregation_permissions
-
-
-class PortfolioPermissions(aggregation_permissions.AggregationPermissions):
-    """
-    Abstraction of Portfolio permissions
-    """
+#
+# sonar-tools
+# Copyright (C) 2022-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+
+from sonar.permissions import aggregation_permissions
+
+
+class PortfolioPermissions(aggregation_permissions.AggregationPermissions):
+    """
+    Abstraction of Portfolio permissions
+    """
```

## sonar/permissions/project_permissions.py

```diff
@@ -1,165 +1,165 @@
-#
-# sonar-tools
-# Copyright (C) 2022-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-
-from sonar import utilities
-from sonar.permissions import permissions
-from sonar.audit import rules, problem
-
-PROJECT_PERMISSIONS = {
-    "user": "Browse",
-    "codeviewer": "See source code",
-    "issueadmin": "Administer Issues",
-    "securityhotspotadmin": "Create Projects",
-    "scan": "Execute Analysis",
-    "admin": "Administer Project",
-}
-
-
-class ProjectPermissions(permissions.Permissions):
-    APIS = {
-        "get": {"users": "permissions/users", "groups": "permissions/groups"},
-        "add": {"users": "permissions/add_user", "groups": "permissions/add_group"},
-        "remove": {"users": "permissions/remove_user", "groups": "permissions/remove_group"},
-    }
-    API_GET_FIELD = {"users": "login", "groups": "name"}
-    API_SET_FIELD = {"users": "login", "groups": "groupName"}
-
-    def __init__(self, concerned_object):
-        self.concerned_object = concerned_object
-        super().__init__(concerned_object.endpoint)
-
-    def __str__(self):
-        return f"permissions of {str(self.concerned_object)}"
-
-    def read(self):
-        self.permissions = permissions.NO_PERMISSIONS.copy()
-        for p in permissions.PERMISSION_TYPES:
-            self.permissions[p] = self._get_api(
-                ProjectPermissions.APIS["get"][p],
-                p,
-                ProjectPermissions.API_GET_FIELD[p],
-                projectKey=self.concerned_object.key,
-                ps=permissions.MAX_PERMS,
-            )
-        # Hack: SonarQube returns application/portfoliocreator even for objects that don't have this permission
-        # so these perms needs to be removed manually
-        self.white_list(tuple(PROJECT_PERMISSIONS.keys()))
-        return self
-
-    def _set_perms(self, new_perms, apis, field, diff_func, **kwargs):
-        utilities.logger.debug("Setting %s with %s", str(self), str(new_perms))
-        if self.permissions is None:
-            self.read()
-        for p in permissions.PERMISSION_TYPES:
-            if new_perms is None or p not in new_perms:
-                continue
-            to_remove = diff_func(self.permissions[p], new_perms[p])
-            self._post_api(apis["remove"][p], field[p], to_remove, **kwargs)
-            to_add = diff_func(new_perms[p], self.permissions[p])
-            self._post_api(apis["add"][p], field[p], to_add, **kwargs)
-        return self.read()
-
-    def set(self, new_perms):
-        """Sets permissions of a project
-
-        :param new_perms:
-        :type new_perms: dict {"users": {<user>: [<perm>, ...], <user>: [], ...}, "groups": {<group>: [<perm>, ...], <group>:[], ...}}
-        :return: Permissions associated to the aggregation
-        :rtype: self
-        """
-        return self._set_perms(
-            new_perms, ProjectPermissions.APIS, ProjectPermissions.API_SET_FIELD, permissions.diff, projectKey=self.concerned_object.key
-        )
-
-    def audit(self, audit_settings):
-        if not audit_settings["audit.projects.permissions"]:
-            utilities.logger.debug("Auditing project permissions is disabled by configuration, skipping")
-            return []
-        utilities.logger.debug("Auditing %s", str(self))
-        return self.__audit_user_permissions(audit_settings) + self.__audit_group_permissions(audit_settings)
-
-    def __audit_user_permissions(self, audit_settings):
-        problems = []
-        user_count = self.count("users")
-        max_users = audit_settings["audit.projects.permissions.maxUsers"]
-        if user_count > max_users:
-            rule = rules.get_rule(rules.RuleId.PROJ_PERM_MAX_USERS)
-            msg = rule.msg.format(str(self.concerned_object), user_count)
-            problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
-
-        max_admins = audit_settings["audit.projects.permissions.maxAdminUsers"]
-        admin_count = self.count("users", ("admin"))
-        if admin_count > max_admins:
-            rule = rules.get_rule(rules.RuleId.PROJ_PERM_MAX_ADM_USERS)
-            msg = rule.msg.format(str(self.concerned_object), admin_count, max_admins)
-            problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
-
-        return problems
-
-    def __audit_group_permissions(self, audit_settings):
-        problems = []
-        groups = self.read().to_json(perm_type="groups")
-        for gr_name, gr_perms in groups.items():
-            if gr_name == "Anyone":
-                rule = rules.get_rule(rules.RuleId.PROJ_PERM_ANYONE)
-                problems.append(problem.Problem(rule.type, rule.severity, rule.msg.format(str(self.concerned_object)), concerned_object=self))
-            if gr_name == "sonar-users" and (
-                "issueadmin" in gr_perms or "scan" in gr_perms or "securityhotspotadmin" in gr_perms or "admin" in gr_perms
-            ):
-                rule = rules.get_rule(rules.RuleId.PROJ_PERM_SONAR_USERS_ELEVATED_PERMS)
-                problems.append(
-                    problem.Problem(rule.type, rule.severity, rule.msg.format(str(self.concerned_object)), concerned_object=self.concerned_object)
-                )
-
-        max_perms = audit_settings["audit.projects.permissions.maxGroups"]
-        counter = self.count(perm_type="groups", perm_filter=permissions.PROJECT_PERMISSIONS)
-        if counter > max_perms:
-            rule = rules.get_rule(rules.RuleId.PROJ_PERM_MAX_GROUPS)
-            msg = rule.msg.format(str(self.concerned_object), counter, max_perms)
-            problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
-
-        max_scan = audit_settings["audit.projects.permissions.maxScanGroups"]
-        counter = self.count(perm_type="groups", perm_filter=("scan"))
-        if counter > max_scan:
-            rule = rules.get_rule(rules.RuleId.PROJ_PERM_MAX_SCAN_GROUPS)
-            msg = rule.msg.format(str(self.concerned_object), counter, max_scan)
-            problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
-
-        max_issue_adm = audit_settings["audit.projects.permissions.maxIssueAdminGroups"]
-        counter = self.count(perm_type="groups", perm_filter=("issueadmin"))
-        if counter > max_issue_adm:
-            rule = rules.get_rule(rules.RuleId.PROJ_PERM_MAX_ISSUE_ADM_GROUPS)
-            msg = rule.msg.format(str(self.concerned_object), counter, max_issue_adm)
-            problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
-
-        max_spots_adm = audit_settings["audit.projects.permissions.maxHotspotAdminGroups"]
-        counter = self.count(perm_type="groups", perm_filter=("securityhotspotadmin"))
-        if counter > max_spots_adm:
-            rule = rules.get_rule(rules.RuleId.PROJ_PERM_MAX_HOTSPOT_ADM_GROUPS)
-            msg = rule.msg.format(str(self.concerned_object), counter, max_spots_adm)
-            problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
-
-        max_admins = audit_settings["audit.projects.permissions.maxAdminGroups"]
-        counter = self.count(perm_type="groups", perm_filter=("admin"))
-        if counter > max_admins:
-            rule = rules.get_rule(rules.RuleId.PROJ_PERM_MAX_ADM_GROUPS)
-            msg = rule.msg.format(str(self.concerned_object), counter, max_admins)
-            problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
-        return problems
+#
+# sonar-tools
+# Copyright (C) 2022-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+
+from sonar import utilities
+from sonar.permissions import permissions
+from sonar.audit import rules, problem
+
+PROJECT_PERMISSIONS = {
+    "user": "Browse",
+    "codeviewer": "See source code",
+    "issueadmin": "Administer Issues",
+    "securityhotspotadmin": "Create Projects",
+    "scan": "Execute Analysis",
+    "admin": "Administer Project",
+}
+
+
+class ProjectPermissions(permissions.Permissions):
+    APIS = {
+        "get": {"users": "permissions/users", "groups": "permissions/groups"},
+        "add": {"users": "permissions/add_user", "groups": "permissions/add_group"},
+        "remove": {"users": "permissions/remove_user", "groups": "permissions/remove_group"},
+    }
+    API_GET_FIELD = {"users": "login", "groups": "name"}
+    API_SET_FIELD = {"users": "login", "groups": "groupName"}
+
+    def __init__(self, concerned_object):
+        self.concerned_object = concerned_object
+        super().__init__(concerned_object.endpoint)
+
+    def __str__(self):
+        return f"permissions of {str(self.concerned_object)}"
+
+    def read(self):
+        self.permissions = permissions.NO_PERMISSIONS.copy()
+        for p in permissions.PERMISSION_TYPES:
+            self.permissions[p] = self._get_api(
+                ProjectPermissions.APIS["get"][p],
+                p,
+                ProjectPermissions.API_GET_FIELD[p],
+                projectKey=self.concerned_object.key,
+                ps=permissions.MAX_PERMS,
+            )
+        # Hack: SonarQube returns application/portfoliocreator even for objects that don't have this permission
+        # so these perms needs to be removed manually
+        self.white_list(tuple(PROJECT_PERMISSIONS.keys()))
+        return self
+
+    def _set_perms(self, new_perms, apis, field, diff_func, **kwargs):
+        utilities.logger.debug("Setting %s with %s", str(self), str(new_perms))
+        if self.permissions is None:
+            self.read()
+        for p in permissions.PERMISSION_TYPES:
+            if new_perms is None or p not in new_perms:
+                continue
+            to_remove = diff_func(self.permissions[p], new_perms[p])
+            self._post_api(apis["remove"][p], field[p], to_remove, **kwargs)
+            to_add = diff_func(new_perms[p], self.permissions[p])
+            self._post_api(apis["add"][p], field[p], to_add, **kwargs)
+        return self.read()
+
+    def set(self, new_perms):
+        """Sets permissions of a project
+
+        :param new_perms:
+        :type new_perms: dict {"users": {<user>: [<perm>, ...], <user>: [], ...}, "groups": {<group>: [<perm>, ...], <group>:[], ...}}
+        :return: Permissions associated to the aggregation
+        :rtype: self
+        """
+        return self._set_perms(
+            new_perms, ProjectPermissions.APIS, ProjectPermissions.API_SET_FIELD, permissions.diff, projectKey=self.concerned_object.key
+        )
+
+    def audit(self, audit_settings):
+        if not audit_settings.get("audit.projects.permissions", True):
+            utilities.logger.debug("Auditing project permissions is disabled by configuration, skipping")
+            return []
+        utilities.logger.debug("Auditing %s", str(self))
+        return self.__audit_user_permissions(audit_settings) + self.__audit_group_permissions(audit_settings)
+
+    def __audit_user_permissions(self, audit_settings):
+        problems = []
+        user_count = self.count("users")
+        max_users = audit_settings.get("audit.projects.permissions.maxUsers", 5)
+        if user_count > max_users:
+            rule = rules.get_rule(rules.RuleId.PROJ_PERM_MAX_USERS)
+            msg = rule.msg.format(str(self.concerned_object), user_count)
+            problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
+
+        max_admins = audit_settings.get("audit.projects.permissions.maxAdminUsers", 2)
+        admin_count = self.count("users", ("admin"))
+        if admin_count > max_admins:
+            rule = rules.get_rule(rules.RuleId.PROJ_PERM_MAX_ADM_USERS)
+            msg = rule.msg.format(str(self.concerned_object), admin_count, max_admins)
+            problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
+
+        return problems
+
+    def __audit_group_permissions(self, audit_settings):
+        problems = []
+        groups = self.read().to_json(perm_type="groups")
+        for gr_name, gr_perms in groups.items():
+            if gr_name == "Anyone":
+                rule = rules.get_rule(rules.RuleId.PROJ_PERM_ANYONE)
+                problems.append(problem.Problem(rule.type, rule.severity, rule.msg.format(str(self.concerned_object)), concerned_object=self))
+            if gr_name == "sonar-users" and (
+                "issueadmin" in gr_perms or "scan" in gr_perms or "securityhotspotadmin" in gr_perms or "admin" in gr_perms
+            ):
+                rule = rules.get_rule(rules.RuleId.PROJ_PERM_SONAR_USERS_ELEVATED_PERMS)
+                problems.append(
+                    problem.Problem(rule.type, rule.severity, rule.msg.format(str(self.concerned_object)), concerned_object=self.concerned_object)
+                )
+
+        max_perms = audit_settings.get("audit.projects.permissions.maxGroups", 5)
+        counter = self.count(perm_type="groups", perm_filter=permissions.PROJECT_PERMISSIONS)
+        if counter > max_perms:
+            rule = rules.get_rule(rules.RuleId.PROJ_PERM_MAX_GROUPS)
+            msg = rule.msg.format(str(self.concerned_object), counter, max_perms)
+            problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
+
+        max_scan = audit_settings.get("audit.projects.permissions.maxScanGroups", 1)
+        counter = self.count(perm_type="groups", perm_filter=("scan"))
+        if counter > max_scan:
+            rule = rules.get_rule(rules.RuleId.PROJ_PERM_MAX_SCAN_GROUPS)
+            msg = rule.msg.format(str(self.concerned_object), counter, max_scan)
+            problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
+
+        max_issue_adm = audit_settings.get("audit.projects.permissions.maxIssueAdminGroups", 2)
+        counter = self.count(perm_type="groups", perm_filter=("issueadmin"))
+        if counter > max_issue_adm:
+            rule = rules.get_rule(rules.RuleId.PROJ_PERM_MAX_ISSUE_ADM_GROUPS)
+            msg = rule.msg.format(str(self.concerned_object), counter, max_issue_adm)
+            problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
+
+        max_spots_adm = audit_settings.get("audit.projects.permissions.maxHotspotAdminGroups", 2)
+        counter = self.count(perm_type="groups", perm_filter=("securityhotspotadmin"))
+        if counter > max_spots_adm:
+            rule = rules.get_rule(rules.RuleId.PROJ_PERM_MAX_HOTSPOT_ADM_GROUPS)
+            msg = rule.msg.format(str(self.concerned_object), counter, max_spots_adm)
+            problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
+
+        max_admins = audit_settings.get("audit.projects.permissions.maxAdminGroups", 2)
+        counter = self.count(perm_type="groups", perm_filter=("admin"))
+        if counter > max_admins:
+            rule = rules.get_rule(rules.RuleId.PROJ_PERM_MAX_ADM_GROUPS)
+            msg = rule.msg.format(str(self.concerned_object), counter, max_admins)
+            problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
+        return problems
```

## sonar/permissions/quality_permissions.py

 * *Ordering differences only*

```diff
@@ -1,105 +1,105 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-
-import json
-from http import HTTPStatus
-from sonar import utilities, options
-from sonar.permissions import permissions
-
-MAX_PERMS = 25
-
-
-class QualityPermissions(permissions.Permissions):
-    def __init__(self, concerned_object):
-        self.concerned_object = concerned_object
-        super().__init__(concerned_object.endpoint)
-
-    def __str__(self):
-        return f"permissions of {str(self.concerned_object)}"
-
-    def _post_api(self, api, set_field, perms_dict, **extra_params):
-        if perms_dict is None:
-            return True
-        result = False
-        params = extra_params.copy()
-        for u in perms_dict:
-            params[set_field] = u
-            r = self.endpoint.post(api, params=params)
-            result = result and r.ok
-        return result
-
-    def to_json(self, perm_type=None, csv=False):
-        if not csv:
-            return self.permissions[perm_type] if permissions.is_valid(perm_type) else self.permissions
-        perms = {}
-        if not self.permissions:
-            return None
-        for p in permissions.normalize(perm_type):
-            dperms = self.permissions.get(p, None)
-            if dperms is not None and len(dperms) > 0:
-                perms[p] = permissions.encode(self.permissions.get(p, None))
-        return perms if len(perms) > 0 else None
-
-    def _get_api(self, api, perm_type, ret_field, **extra_params):
-        perms = []
-        params = extra_params.copy()
-        params["ps"] = MAX_PERMS
-        page, nbr_pages = 1, 1
-        while page <= nbr_pages:
-            params["p"] = page
-            resp = self.endpoint.get(api, params=params)
-            if resp.ok:
-                data = json.loads(resp.text)
-                perms += [p[ret_field] for p in data[perm_type]]
-            elif resp.status_code not in (HTTPStatus.BAD_REQUEST, HTTPStatus.NOT_FOUND):
-                # Hack: Different versions of SonarQube return different codes (400 or 404)
-                utilities.exit_fatal(f"HTTP error {resp.status_code} - Exiting", options.ERR_SONAR_API)
-            else:
-                break
-            page, nbr_pages = page + 1, utilities.nbr_pages(data)
-        return perms
-
-    def _set_perms(self, new_perms, apis, field, diff_func, **kwargs):
-        if self.concerned_object.is_built_in:
-            utilities.logger.debug("Can't set %s because it's built-in", str(self))
-            self.permissions = {p: [] for p in permissions.PERMISSION_TYPES}
-            return self
-        utilities.logger.debug("Setting %s with %s", str(self), str(new_perms))
-        if self.permissions is None:
-            self.read()
-        for p in permissions.PERMISSION_TYPES:
-            if new_perms is None or p not in new_perms:
-                continue
-            decoded_perms = permissions.decode(new_perms[p])
-            to_remove = diff_func(self.permissions[p], decoded_perms)
-            self._post_api(apis["remove"][p], field[p], to_remove, **kwargs)
-            to_add = diff_func(decoded_perms, self.permissions[p])
-            self._post_api(apis["add"][p], field[p], to_add, **kwargs)
-        self.read()
-        return True
-
-    def _read_perms(self, apis, field, **kwargs):
-        self.permissions = {p: [] for p in permissions.PERMISSION_TYPES}
-        if self.concerned_object.is_built_in:
-            utilities.logger.debug("No permissions for %s because it's built-in", str(self))
-        else:
-            for p in permissions.PERMISSION_TYPES:
-                self.permissions[p] = self._get_api(apis["get"][p], p, field[p], **kwargs)
-        return self.permissions
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+
+import json
+from http import HTTPStatus
+from sonar import utilities, options
+from sonar.permissions import permissions
+
+MAX_PERMS = 25
+
+
+class QualityPermissions(permissions.Permissions):
+    def __init__(self, concerned_object):
+        self.concerned_object = concerned_object
+        super().__init__(concerned_object.endpoint)
+
+    def __str__(self):
+        return f"permissions of {str(self.concerned_object)}"
+
+    def _post_api(self, api, set_field, perms_dict, **extra_params):
+        if perms_dict is None:
+            return True
+        result = False
+        params = extra_params.copy()
+        for u in perms_dict:
+            params[set_field] = u
+            r = self.endpoint.post(api, params=params)
+            result = result and r.ok
+        return result
+
+    def to_json(self, perm_type=None, csv=False):
+        if not csv:
+            return self.permissions[perm_type] if permissions.is_valid(perm_type) else self.permissions
+        perms = {}
+        if not self.permissions:
+            return None
+        for p in permissions.normalize(perm_type):
+            dperms = self.permissions.get(p, None)
+            if dperms is not None and len(dperms) > 0:
+                perms[p] = permissions.encode(self.permissions.get(p, None))
+        return perms if len(perms) > 0 else None
+
+    def _get_api(self, api, perm_type, ret_field, **extra_params):
+        perms = []
+        params = extra_params.copy()
+        params["ps"] = MAX_PERMS
+        page, nbr_pages = 1, 1
+        while page <= nbr_pages:
+            params["p"] = page
+            resp = self.endpoint.get(api, params=params)
+            if resp.ok:
+                data = json.loads(resp.text)
+                perms += [p[ret_field] for p in data[perm_type]]
+            elif resp.status_code not in (HTTPStatus.BAD_REQUEST, HTTPStatus.NOT_FOUND):
+                # Hack: Different versions of SonarQube return different codes (400 or 404)
+                utilities.exit_fatal(f"HTTP error {resp.status_code} - Exiting", options.ERR_SONAR_API)
+            else:
+                break
+            page, nbr_pages = page + 1, utilities.nbr_pages(data)
+        return perms
+
+    def _set_perms(self, new_perms, apis, field, diff_func, **kwargs):
+        if self.concerned_object.is_built_in:
+            utilities.logger.debug("Can't set %s because it's built-in", str(self))
+            self.permissions = {p: [] for p in permissions.PERMISSION_TYPES}
+            return self
+        utilities.logger.debug("Setting %s with %s", str(self), str(new_perms))
+        if self.permissions is None:
+            self.read()
+        for p in permissions.PERMISSION_TYPES:
+            if new_perms is None or p not in new_perms:
+                continue
+            decoded_perms = permissions.decode(new_perms[p])
+            to_remove = diff_func(self.permissions[p], decoded_perms)
+            self._post_api(apis["remove"][p], field[p], to_remove, **kwargs)
+            to_add = diff_func(decoded_perms, self.permissions[p])
+            self._post_api(apis["add"][p], field[p], to_add, **kwargs)
+        self.read()
+        return True
+
+    def _read_perms(self, apis, field, **kwargs):
+        self.permissions = {p: [] for p in permissions.PERMISSION_TYPES}
+        if self.concerned_object.is_built_in:
+            utilities.logger.debug("No permissions for %s because it's built-in", str(self))
+        else:
+            for p in permissions.PERMISSION_TYPES:
+                self.permissions[p] = self._get_api(apis["get"][p], p, field[p], **kwargs)
+        return self.permissions
```

## sonar/permissions/qualitygate_permissions.py

 * *Ordering differences only*

```diff
@@ -1,50 +1,50 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-
-from sonar import utilities, exceptions
-from sonar.permissions import permissions, quality_permissions
-
-
-class QualityGatePermissions(quality_permissions.QualityPermissions):
-    APIS = {
-        "get": {"users": "qualitygates/search_users", "groups": "qualitygates/search_groups"},
-        "add": {"users": "qualitygates/add_user", "groups": "qualitygates/add_group"},
-        "remove": {"users": "qualitygates/remove_user", "groups": "qualitygates/remove_group"},
-    }
-    API_GET_FIELD = {"users": "login", "groups": "name"}
-    API_SET_FIELD = {"users": "login", "groups": "groupName"}
-
-    def __str__(self):
-        return f"permissions of {str(self.concerned_object)}"
-
-    def read(self):
-        if self.endpoint.version() < (9, 2, 0):
-            utilities.logger.debug("Can't read %s on SonarQube < 9.2", str(self))
-            return self
-        self._read_perms(QualityGatePermissions.APIS, QualityGatePermissions.API_GET_FIELD, gateName=self.concerned_object.name)
-        return self
-
-    def set(self, new_perms):
-        if self.endpoint.version() < (9, 2, 0):
-            raise exceptions.UnsupportedOperation(f"Can't set {str(self)} on SonarQube < 9.2")
-
-        return self._set_perms(
-            new_perms, QualityGatePermissions.APIS, QualityGatePermissions.API_SET_FIELD, permissions.diffarray, gateName=self.concerned_object.name
-        )
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+
+from sonar import utilities, exceptions
+from sonar.permissions import permissions, quality_permissions
+
+
+class QualityGatePermissions(quality_permissions.QualityPermissions):
+    APIS = {
+        "get": {"users": "qualitygates/search_users", "groups": "qualitygates/search_groups"},
+        "add": {"users": "qualitygates/add_user", "groups": "qualitygates/add_group"},
+        "remove": {"users": "qualitygates/remove_user", "groups": "qualitygates/remove_group"},
+    }
+    API_GET_FIELD = {"users": "login", "groups": "name"}
+    API_SET_FIELD = {"users": "login", "groups": "groupName"}
+
+    def __str__(self):
+        return f"permissions of {str(self.concerned_object)}"
+
+    def read(self):
+        if self.endpoint.version() < (9, 2, 0):
+            utilities.logger.debug("Can't read %s on SonarQube < 9.2", str(self))
+            return self
+        self._read_perms(QualityGatePermissions.APIS, QualityGatePermissions.API_GET_FIELD, gateName=self.concerned_object.name)
+        return self
+
+    def set(self, new_perms):
+        if self.endpoint.version() < (9, 2, 0):
+            raise exceptions.UnsupportedOperation(f"Can't set {str(self)} on SonarQube < 9.2")
+
+        return self._set_perms(
+            new_perms, QualityGatePermissions.APIS, QualityGatePermissions.API_SET_FIELD, permissions.diffarray, gateName=self.concerned_object.name
+        )
```

## sonar/permissions/qualityprofile_permissions.py

 * *Ordering differences only*

```diff
@@ -1,58 +1,58 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-
-from sonar import utilities
-from sonar.permissions import permissions, quality_permissions
-
-
-class QualityProfilePermissions(quality_permissions.QualityPermissions):
-
-    APIS = {
-        "get": {"users": "qualityprofiles/search_users", "groups": "qualityprofiles/search_groups"},
-        "add": {"users": "qualityprofiles/add_user", "groups": "qualityprofiles/add_group"},
-        "remove": {"users": "qualityprofiles/remove_user", "groups": "qualityprofiles/remove_group"},
-    }
-    API_GET_FIELD = {"users": "login", "groups": "name"}
-    API_SET_FIELD = {"users": "login", "groups": "group"}
-
-    def read(self):
-        if self.endpoint.version() < (9, 2, 0):
-            utilities.logger.debug("Can't read %s on SonarQube < 9.2", str(self))
-            return self
-        self._read_perms(
-            QualityProfilePermissions.APIS,
-            QualityProfilePermissions.API_GET_FIELD,
-            qualityProfile=self.concerned_object.name,
-            language=self.concerned_object.language,
-        )
-        return self
-
-    def set(self, new_perms):
-        if self.endpoint.version() < (6, 6, 0):
-            utilities.logger.debug("Can set %s on SonarQube < 6.6", str(self))
-            return self
-        return self._set_perms(
-            new_perms,
-            QualityProfilePermissions.APIS,
-            QualityProfilePermissions.API_SET_FIELD,
-            permissions.diffarray,
-            qualityProfile=self.concerned_object.name,
-            language=self.concerned_object.language,
-        )
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+
+from sonar import utilities
+from sonar.permissions import permissions, quality_permissions
+
+
+class QualityProfilePermissions(quality_permissions.QualityPermissions):
+
+    APIS = {
+        "get": {"users": "qualityprofiles/search_users", "groups": "qualityprofiles/search_groups"},
+        "add": {"users": "qualityprofiles/add_user", "groups": "qualityprofiles/add_group"},
+        "remove": {"users": "qualityprofiles/remove_user", "groups": "qualityprofiles/remove_group"},
+    }
+    API_GET_FIELD = {"users": "login", "groups": "name"}
+    API_SET_FIELD = {"users": "login", "groups": "group"}
+
+    def read(self):
+        if self.endpoint.version() < (9, 2, 0):
+            utilities.logger.debug("Can't read %s on SonarQube < 9.2", str(self))
+            return self
+        self._read_perms(
+            QualityProfilePermissions.APIS,
+            QualityProfilePermissions.API_GET_FIELD,
+            qualityProfile=self.concerned_object.name,
+            language=self.concerned_object.language,
+        )
+        return self
+
+    def set(self, new_perms):
+        if self.endpoint.version() < (6, 6, 0):
+            utilities.logger.debug("Can set %s on SonarQube < 6.6", str(self))
+            return self
+        return self._set_perms(
+            new_perms,
+            QualityProfilePermissions.APIS,
+            QualityProfilePermissions.API_SET_FIELD,
+            permissions.diffarray,
+            qualityProfile=self.concerned_object.name,
+            language=self.concerned_object.language,
+        )
```

## sonar/permissions/template_permissions.py

 * *Ordering differences only*

```diff
@@ -1,59 +1,59 @@
-#
-# sonar-tools
-# Copyright (C) 2022-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-
-from sonar import utilities
-from sonar.permissions import permissions, project_permissions
-
-
-class TemplatePermissions(project_permissions.ProjectPermissions):
-    API_GET = {"users": "permissions/template_users", "groups": "permissions/template_groups"}
-    API_SET = {"users": "permissions/add_user_to_template", "groups": "permissions/add_group_to_template"}
-    API_REMOVE = {"users": "permissions/remove_user_from_template", "groups": "permissions/remove_group_from_template"}
-    API_GET_FIELD = {"users": "login", "groups": "name"}
-    API_SET_FIELD = {"users": "login", "groups": "groupName"}
-
-    def read(self):
-        self.permissions = permissions.NO_PERMISSIONS
-        for p in permissions.PERMISSION_TYPES:
-            self.permissions[p] = self._get_api(
-                TemplatePermissions.API_GET[p],
-                p,
-                TemplatePermissions.API_GET_FIELD[p],
-                templateId=self.concerned_object.key,
-                ps=permissions.MAX_PERMS,
-            )
-        # Hack: SonarQube returns application/portfoliocreator even for objects that don't have this permission
-        # so these perms needs to be removed manually
-        self.white_list(tuple(project_permissions.PROJECT_PERMISSIONS.keys()))
-        return self
-
-    def set(self, new_perms):
-        utilities.logger.debug("Setting %s with %s", str(self), str(new_perms))
-        if self.permissions is None:
-            self.read()
-        for p in permissions.PERMISSION_TYPES:
-            if new_perms is None or p not in new_perms:
-                continue
-            decoded_perms = {k: permissions.decode(v) for k, v in new_perms[p].items()}
-            to_remove = permissions.diff(self.permissions[p], decoded_perms)
-            self._post_api(TemplatePermissions.API_REMOVE[p], TemplatePermissions.API_SET_FIELD[p], to_remove, templateId=self.concerned_object.key)
-            to_add = permissions.diff(decoded_perms, self.permissions[p])
-            self._post_api(TemplatePermissions.API_SET[p], TemplatePermissions.API_SET_FIELD[p], to_add, templateId=self.concerned_object.key)
-        return self.read()
+#
+# sonar-tools
+# Copyright (C) 2022-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+
+from sonar import utilities
+from sonar.permissions import permissions, project_permissions
+
+
+class TemplatePermissions(project_permissions.ProjectPermissions):
+    API_GET = {"users": "permissions/template_users", "groups": "permissions/template_groups"}
+    API_SET = {"users": "permissions/add_user_to_template", "groups": "permissions/add_group_to_template"}
+    API_REMOVE = {"users": "permissions/remove_user_from_template", "groups": "permissions/remove_group_from_template"}
+    API_GET_FIELD = {"users": "login", "groups": "name"}
+    API_SET_FIELD = {"users": "login", "groups": "groupName"}
+
+    def read(self):
+        self.permissions = permissions.NO_PERMISSIONS
+        for p in permissions.PERMISSION_TYPES:
+            self.permissions[p] = self._get_api(
+                TemplatePermissions.API_GET[p],
+                p,
+                TemplatePermissions.API_GET_FIELD[p],
+                templateId=self.concerned_object.key,
+                ps=permissions.MAX_PERMS,
+            )
+        # Hack: SonarQube returns application/portfoliocreator even for objects that don't have this permission
+        # so these perms needs to be removed manually
+        self.white_list(tuple(project_permissions.PROJECT_PERMISSIONS.keys()))
+        return self
+
+    def set(self, new_perms):
+        utilities.logger.debug("Setting %s with %s", str(self), str(new_perms))
+        if self.permissions is None:
+            self.read()
+        for p in permissions.PERMISSION_TYPES:
+            if new_perms is None or p not in new_perms:
+                continue
+            decoded_perms = {k: permissions.decode(v) for k, v in new_perms[p].items()}
+            to_remove = permissions.diff(self.permissions[p], decoded_perms)
+            self._post_api(TemplatePermissions.API_REMOVE[p], TemplatePermissions.API_SET_FIELD[p], to_remove, templateId=self.concerned_object.key)
+            to_add = permissions.diff(decoded_perms, self.permissions[p])
+            self._post_api(TemplatePermissions.API_SET[p], TemplatePermissions.API_SET_FIELD[p], to_add, templateId=self.concerned_object.key)
+        return self.read()
```

## sonar/projects/__init__.py

 * *Ordering differences only*

```diff
@@ -1,19 +1,19 @@
-#
-# sonar-tools
-# Copyright (C) 2022-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
+#
+# sonar-tools
+# Copyright (C) 2022-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
```

## sonar/projects/branches.py

```diff
@@ -1,437 +1,448 @@
-#
-# sonar-tools
-# Copyright (C) 2022-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-
-from http import HTTPStatus
-import json
-from urllib.parse import unquote
-from requests.exceptions import HTTPError
-import requests.utils
-import sonar.sqobject as sq
-from sonar import measures, components, syncer, settings, exceptions
-from sonar.projects import projects
-from sonar.findings import issues, hotspots
-import sonar.utilities as util
-
-from sonar.audit import rules, problem
-
-_OBJECTS = {}
-
-#: APIs used for branch management
-APIS = {
-    "list": "project_branches/list",
-    "rename": "project_branches/rename",
-    "get_new_code": "new_code_periods/list",
-    "delete": "project_branches/delete",
-}
-
-_UNSUPPORTED_IN_CE = "Branches not available in Community Edition"
-
-
-class Branch(components.Component):
-    """
-    Abstraction of the SonarQube "project branch" concept
-    """
-
-    @classmethod
-    def get_object(cls, concerned_object, branch_name):
-        """Gets a SonarQube Branch object
-
-        :param concerned_object: Object concerned by the branch (Project or Application)
-        :type concerned_object: Project or Application
-        :param str branch_name: The branch name
-        :raises UnsupportedOperation: If trying to manipulate branches on a community edition
-        :raises ObjectNotFound: If project key or branch name not found in SonarQube
-        :return: The Branch object
-        :rtype: Branch
-        """
-        branch_name = unquote(branch_name)
-        _uuid = uuid(concerned_object.key, branch_name)
-        if _uuid in _OBJECTS:
-            return _OBJECTS[_uuid]
-        try:
-            data = json.loads(concerned_object.endpoint.get(APIS["list"], params={"project": concerned_object.key}).text)
-        except HTTPError as e:
-            if e.response.status_code == HTTPStatus.NOT_FOUND:
-                raise exceptions.ObjectNotFound(concerned_object.key, f"Project '{concerned_object.key}' not found")
-        for br in data.get("branches", []):
-            if br["name"] == branch_name:
-                return cls.load(concerned_object, branch_name, data)
-        raise exceptions.ObjectNotFound(branch_name, f"Branch '{branch_name}' of project '{concerned_object.key}' not found")
-
-    @classmethod
-    def load(cls, concerned_object, branch_name, data):
-        """Gets a Branch object from JSON data gotten from a list API call
-
-        :param concerned_object: Object concerned by the branch (Project or Application)
-        :type concerned_object: Project or Application
-        :param str branch_name:
-        :param dict data:
-        :raises UnsupportedOperation: If trying to manipulate branches on a community edition
-        :raises ObjectNotFound: If project key or branch name not found in SonarQube
-        :return: The Branch object
-        :rtype: Branch
-        """
-        branch_name = unquote(branch_name)
-        _uuid = uuid(concerned_object.key, branch_name)
-        o = _OBJECTS[_uuid] if _uuid in _OBJECTS else cls(concerned_object, branch_name)
-        o._load(data)
-        return o
-
-    def __init__(self, concerned_object, name):
-        """Don't use this, use class methods to create Branch objects
-
-        :raises UnsupportedOperation: When attempting to branches on Community Edition
-        """
-        if concerned_object.endpoint.edition() == "community":
-            raise exceptions.UnsupportedOperation(_UNSUPPORTED_IN_CE)
-        name = unquote(name)
-        super().__init__(name, concerned_object.endpoint)
-        self.name = name
-        self.concerned_object = concerned_object
-        self._is_main = None
-        self._new_code = None
-        self._last_analysis = None
-        self._keep_when_inactive = None
-        _OBJECTS[self.uuid()] = self
-        util.logger.debug("Created object %s", str(self))
-
-    def __str__(self):
-        return f"branch '{self.name}' of {str(self.concerned_object)}"
-
-    def refresh(self):
-        """Reads a branch in SonarQube (refresh with latest data)
-
-        :raises ObjectNotFound: Branch not found in SonarQube
-        :return: itself
-        :rtype: Branch
-        """
-        try:
-            data = json.loads(self.get(APIS["list"], params={"project": self.concerned_object.key}).text)
-        except HTTPError as e:
-            if e.response.status_code == HTTPStatus.NOT_FOUND:
-                raise exceptions.ObjectNotFound(self.key, f"{str(self)} not found in SonarQube")
-        for br in data.get("branches", []):
-            if br["name"] == self.name:
-                self._load(br)
-            else:
-                # While we're there let's load other branches with up to date branch data
-                Branch.load(self.concerned_object, br["name"], data)
-        return self
-
-    def _load(self, data):
-        if self._json is None:
-            self._json = data
-        else:
-            self._json.update(data)
-        self._is_main = self._json["isMain"]
-        self._last_analysis = util.string_to_date(self._json.get("analysisDate", None))
-        self._keep_when_inactive = self._json.get("excludedFromPurge", False)
-        self._is_main = self._json.get("isMain", False)
-
-    def uuid(self):
-        """Computes a uuid for the branch that can serve as index
-        :return: the UUID
-        :rtype: str
-        """
-        return uuid(self.concerned_object.key, self.name)
-
-    def last_analysis(self):
-        """
-        :param include_branches: Unused, present for inheritance reasons
-        :type include_branches: bool, optional
-        :return: Datetime of last analysis
-        :rtype: datetime
-        """
-        if self._last_analysis is None:
-            self.refresh()
-        return self._last_analysis
-
-    def is_kept_when_inactive(self):
-        """
-        :return: Whether the branch is kept when inactive
-        :rtype: bool
-        """
-        if self._keep_when_inactive is None or self._json is None:
-            self.refresh()
-        return self._keep_when_inactive
-
-    def is_main(self):
-        """
-        :return: Whether the branch is the project main branch
-        :rtype: bool
-        """
-        if self._is_main is None or self._json is None:
-            self.refresh()
-        return self._is_main
-
-    def delete(self):
-        """Deletes a branch
-
-        :raises ObjectNotFound: Branch not found for deletion
-        :return: Whether the deletion was successful
-        :rtype: bool
-        """
-        try:
-            return sq.delete_object(self, APIS["delete"], {"branch": self.name, "project": self.concerned_object.key}, _OBJECTS)
-        except HTTPError as e:
-            if e.response.status_code == HTTPStatus.BAD_REQUEST:
-                util.logger.warning("Can't delete %s, it's the main branch", str(self))
-            return False
-
-    def new_code(self):
-        """
-        :return: The branch new code period definition
-        :rtype: str
-        """
-        if self._new_code is None:
-            try:
-                data = json.loads(self.get(api=APIS["get_new_code"], params={"project": self.concerned_object.key}).text)
-            except HTTPError as e:
-                if e.response.status_code == HTTPStatus.NOT_FOUND:
-                    raise exceptions.ObjectNotFound(self.concerned_object.key, f"str{self.concerned_object} not found")
-            for b in data["newCodePeriods"]:
-                new_code = settings.new_code_to_string(b)
-                if b["branchKey"] == self.name:
-                    self._new_code = new_code
-                else:
-                    # While we're there let's store the new code of other branches
-                    Branch.get_object(self.concerned_object, b["branchKey"])._new_code = new_code
-        return self._new_code
-
-    def export(self, full_export=True):
-        """Exports a branch configuration (is main, keep when inactive, optionally name, project)
-
-        :param full_export: Also export branches attributes that are not needed for import, defaults to True
-        :type include_branches: bool, optional
-        :return: The branch new code period definition
-        :rtype: str
-        """
-        util.logger.debug("Exporting %s", str(self))
-        data = {settings.NEW_CODE_PERIOD: self.new_code()}
-        if self.is_main():
-            data["isMain"] = True
-        if self.is_kept_when_inactive() and not self.is_main():
-            data["keepWhenInactive"] = True
-        if self.new_code():
-            data[settings.NEW_CODE_PERIOD] = self.new_code()
-        if full_export:
-            data.update({"name": self.name, "project": self.concerned_object.key})
-        data = util.remove_nones(data)
-        return None if len(data) == 0 else data
-
-    def url(self):
-        """
-        :return: The branch URL in SonarQube as permalink
-        :rtype: str
-        """
-        return f"{self.endpoint.url}/dashboard?id={self.concerned_object.key}&branch={requests.utils.quote(self.name)}"
-
-    def rename(self, new_name):
-        """Renames a branch
-
-        :param new_name: New branch name
-        :type new_name: str
-        :raises UnsupportedOperation: If trying to rename anything than the main branch
-        :raises ObjectNotFound: Concerned object (project) not found
-        :return: Whether the branch was renamed
-        :rtype: bool
-        """
-        if not self.is_main():
-            raise exceptions.UnsupportedOperation(f"{str(self)} can't be renamed since it's not the main branch")
-
-        if self.name == new_name:
-            util.logger.debug("Skipping rename %s with same new name", str(self))
-            return False
-        util.logger.info("Renaming main branch of %s from '%s' to '%s'", str(self.concerned_object), self.name, new_name)
-        try:
-            self.post(APIS["rename"], params={"project": self.concerned_object.key, "name": new_name})
-        except HTTPError as e:
-            if e.response.status_code == HTTPStatus.NOT_FOUND:
-                raise exceptions.ObjectNotFound(self.concerned_object.key, f"str{self.concerned_object} not found")
-        _OBJECTS.pop(uuid(self.concerned_object.key, self.name), None)
-        self.name = new_name
-        _OBJECTS[uuid(self.concerned_object.key, self.name)] = self
-        return True
-
-    def __audit_zero_loc(self):
-        if self.last_analysis() and self.loc() == 0:
-            rule = rules.get_rule(rules.RuleId.PROJ_ZERO_LOC)
-            return [problem.Problem(rule.type, rule.severity, rule.msg.format(str(self)), concerned_object=self)]
-        return []
-
-    def get_measures(self, metrics_list):
-        """Retrieves a branch list of measures
-
-        :param metrics_list: List of metrics to return
-        :type metrics_list: str (comma separated)
-        :return: List of measures of a projects
-        :rtype: dict
-        """
-        return measures.get(self, metrics_list)
-
-    def get_issues(self):
-        """Returns a branch list of issues
-
-        :return: dict of Issues, with issue key as key
-        :rtype: dict{key: Issue}
-        """
-        return issues.search_all(
-            endpoint=self.endpoint,
-            params={
-                "componentKeys": self.concerned_object.key,
-                "branch": self.name,
-                "additionalFields": "comments",
-            },
-        )
-
-    def get_hotspots(self):
-        """Returns a branch list of hotspots
-
-        :return: dict of Hotspots, with hotspot key as key
-        :rtype: dict{key: Hotspot}
-        """
-        return hotspots.search(
-            endpoint=self.endpoint,
-            params={
-                "projectKey": self.concerned_object.key,
-                "branch": self.name,
-                "additionalFields": "comments",
-            },
-        )
-
-    def get_findings(self):
-        """Returns a branch list of findings
-
-        :return: dict of Findings, with finding key as key
-        :rtype: dict{key: Finding}
-        """
-        return self.get_issues() + self.get_hotspots()
-
-    def sync(self, another_branch, sync_settings):
-        """Syncs branch findings with another branch
-
-        :param another_branch: other branch to sync issues into (not necesssarily of same project)
-        :type another_branch: Branch
-        :param sync_settings: Parameters to configure the sync
-        :type sync_settings: dict
-        :return: sync report as tuple, with counts of successful and unsuccessful issue syncs
-        :rtype: tuple(report, counters)
-        """
-        report, counters = [], {}
-        (report, counters) = syncer.sync_lists(
-            self.get_issues(),
-            another_branch.get_issues(),
-            self,
-            another_branch,
-            sync_settings=sync_settings,
-        )
-        (tmp_report, tmp_counts) = syncer.sync_lists(
-            self.get_hotspots(),
-            another_branch.get_hotspots(),
-            self,
-            another_branch,
-            sync_settings=sync_settings,
-        )
-        report += tmp_report
-        counters = util.dict_add(counters, tmp_counts)
-        return (report, counters)
-
-    def __audit_last_analysis(self, audit_settings):
-        age = util.age(self.last_analysis())
-        if self.is_main() or age is None:
-            # Main branch (not purgeable) or branch not analyzed yet
-            return []
-        max_age = audit_settings["audit.projects.branches.maxLastAnalysisAge"]
-        problems = []
-        if self.is_main():
-            util.logger.debug("%s is main (not purgeable)", str(self))
-        elif self.is_kept_when_inactive():
-            util.logger.debug("%s is kept when inactive (not purgeable)", str(self))
-        elif age > max_age:
-            rule = rules.get_rule(rules.RuleId.BRANCH_LAST_ANALYSIS)
-            msg = rule.msg.format(str(self), age)
-            problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
-        else:
-            util.logger.debug("%s age is %d days", str(self), age)
-        return problems
-
-    def audit(self, audit_settings):
-        """Audits a branch and return list of problems found
-
-        :meta private:
-        :param audit_settings: Options of what to audit and thresholds to raise problems
-        :type audit_settings: dict
-        :return: List of problems found, or empty list
-        :rtype: list[Problem]
-        """
-        util.logger.debug("Auditing %s", str(self))
-        return self.__audit_last_analysis(audit_settings) + self.__audit_zero_loc()
-
-    def search_params(self):
-        """Return params used to search for that object
-
-        :meta private:
-        """
-        return {"project": self.concerned_object.key, "branch": self.name}
-
-
-def uuid(project_key, branch_name):
-    """Computes a uuid for the branch that can serve as index
-
-    :param str project_key: The project key
-    :param str branch_name: The branch name
-    :return: the UUID
-    :rtype: str
-    """
-    return f"{project_key} {branch_name}"
-
-
-def get_list(project):
-    """Retrieves the list of branches of a project
-
-    :param Project project: Project the branch belongs to
-    :raises UnsupportedOperation: Branches not supported in Community Edition
-    :return: List of project branches
-    :rtype: dict{branch_name: Branch}
-    """
-    if project.endpoint.edition() == "community":
-        util.logger.debug(_UNSUPPORTED_IN_CE)
-        raise exceptions.UnsupportedOperation(_UNSUPPORTED_IN_CE)
-
-    util.logger.debug("Reading all branches of %s", str(project))
-    data = json.loads(project.endpoint.get(APIS["list"], params={"project": project.key}).text)
-    return {branch["name"]: Branch.load(project, branch["name"], data=branch) for branch in data.get("branches", {})}
-
-
-def exists(endpoint, branch_name, project_key):
-    """Checks if a branch exists
-
-    :param Platform endpoint: Reference to the SonarQube platform
-    :param str branch_name: Branch name
-    :param str project_key: Project key
-    :raises UnsupportedOperation: Branches not supported in Community Edition
-    :return: Whether the branch exists in SonarQube
-    :rtype: bool
-    """
-    try:
-        project = projects.Project.get_object(endpoint, project_key)
-    except exceptions.ObjectNotFound:
-        return False
-    return branch_name in get_list(project)
+#
+# sonar-tools
+# Copyright (C) 2022-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+
+from http import HTTPStatus
+import json
+from urllib.parse import unquote
+from requests.exceptions import HTTPError
+import requests.utils
+import sonar.sqobject as sq
+from sonar import measures, components, syncer, settings, exceptions
+from sonar.projects import projects
+from sonar.findings import issues, hotspots
+import sonar.utilities as util
+
+from sonar.audit import rules, problem
+
+_OBJECTS = {}
+
+#: APIs used for branch management
+APIS = {
+    "list": "project_branches/list",
+    "rename": "project_branches/rename",
+    "get_new_code": "new_code_periods/list",
+    "delete": "project_branches/delete",
+}
+
+_UNSUPPORTED_IN_CE = "Branches not available in Community Edition"
+
+
+class Branch(components.Component):
+    """
+    Abstraction of the SonarQube "project branch" concept
+    """
+
+    @classmethod
+    def get_object(cls, concerned_object, branch_name):
+        """Gets a SonarQube Branch object
+
+        :param concerned_object: Object concerned by the branch (Project or Application)
+        :type concerned_object: Project or Application
+        :param str branch_name: The branch name
+        :raises UnsupportedOperation: If trying to manipulate branches on a community edition
+        :raises ObjectNotFound: If project key or branch name not found in SonarQube
+        :return: The Branch object
+        :rtype: Branch
+        """
+        branch_name = unquote(branch_name)
+        _uuid = uuid(concerned_object.key, branch_name)
+        if _uuid in _OBJECTS:
+            return _OBJECTS[_uuid]
+        try:
+            data = json.loads(concerned_object.endpoint.get(APIS["list"], params={"project": concerned_object.key}).text)
+        except HTTPError as e:
+            if e.response.status_code == HTTPStatus.NOT_FOUND:
+                raise exceptions.ObjectNotFound(concerned_object.key, f"Project '{concerned_object.key}' not found")
+        for br in data.get("branches", []):
+            if br["name"] == branch_name:
+                return cls.load(concerned_object, branch_name, data)
+        raise exceptions.ObjectNotFound(branch_name, f"Branch '{branch_name}' of project '{concerned_object.key}' not found")
+
+    @classmethod
+    def load(cls, concerned_object, branch_name, data):
+        """Gets a Branch object from JSON data gotten from a list API call
+
+        :param concerned_object: Object concerned by the branch (Project or Application)
+        :type concerned_object: Project or Application
+        :param str branch_name:
+        :param dict data:
+        :raises UnsupportedOperation: If trying to manipulate branches on a community edition
+        :raises ObjectNotFound: If project key or branch name not found in SonarQube
+        :return: The Branch object
+        :rtype: Branch
+        """
+        branch_name = unquote(branch_name)
+        _uuid = uuid(concerned_object.key, branch_name)
+        o = _OBJECTS[_uuid] if _uuid in _OBJECTS else cls(concerned_object, branch_name)
+        o._load(data)
+        return o
+
+    def __init__(self, concerned_object, name):
+        """Don't use this, use class methods to create Branch objects
+
+        :raises UnsupportedOperation: When attempting to branches on Community Edition
+        """
+        if concerned_object.endpoint.edition() == "community":
+            raise exceptions.UnsupportedOperation(_UNSUPPORTED_IN_CE)
+        name = unquote(name)
+        super().__init__(name, concerned_object.endpoint)
+        self.name = name
+        self.concerned_object = concerned_object
+        self._is_main = None
+        self._new_code = None
+        self._last_analysis = None
+        self._keep_when_inactive = None
+        _OBJECTS[self.uuid()] = self
+        util.logger.debug("Created object %s", str(self))
+
+    def __str__(self):
+        return f"branch '{self.name}' of {str(self.concerned_object)}"
+
+    def refresh(self):
+        """Reads a branch in SonarQube (refresh with latest data)
+
+        :raises ObjectNotFound: Branch not found in SonarQube
+        :return: itself
+        :rtype: Branch
+        """
+        try:
+            data = json.loads(self.get(APIS["list"], params={"project": self.concerned_object.key}).text)
+        except HTTPError as e:
+            if e.response.status_code == HTTPStatus.NOT_FOUND:
+                raise exceptions.ObjectNotFound(self.key, f"{str(self)} not found in SonarQube")
+        for br in data.get("branches", []):
+            if br["name"] == self.name:
+                self._load(br)
+            else:
+                # While we're there let's load other branches with up to date branch data
+                Branch.load(self.concerned_object, br["name"], data)
+        return self
+
+    def _load(self, data):
+        if self._json is None:
+            self._json = data
+        else:
+            self._json.update(data)
+        self._is_main = self._json["isMain"]
+        self._last_analysis = util.string_to_date(self._json.get("analysisDate", None))
+        self._keep_when_inactive = self._json.get("excludedFromPurge", False)
+        self._is_main = self._json.get("isMain", False)
+
+    def uuid(self):
+        """Computes a uuid for the branch that can serve as index
+        :return: the UUID
+        :rtype: str
+        """
+        return uuid(self.concerned_object.key, self.name)
+
+    def last_analysis(self):
+        """
+        :param include_branches: Unused, present for inheritance reasons
+        :type include_branches: bool, optional
+        :return: Datetime of last analysis
+        :rtype: datetime
+        """
+        if self._last_analysis is None:
+            self.refresh()
+        return self._last_analysis
+
+    def is_kept_when_inactive(self):
+        """
+        :return: Whether the branch is kept when inactive
+        :rtype: bool
+        """
+        if self._keep_when_inactive is None or self._json is None:
+            self.refresh()
+        return self._keep_when_inactive
+
+    def is_main(self):
+        """
+        :return: Whether the branch is the project main branch
+        :rtype: bool
+        """
+        if self._is_main is None or self._json is None:
+            self.refresh()
+        return self._is_main
+
+    def delete(self):
+        """Deletes a branch
+
+        :raises ObjectNotFound: Branch not found for deletion
+        :return: Whether the deletion was successful
+        :rtype: bool
+        """
+        try:
+            return sq.delete_object(self, APIS["delete"], {"branch": self.name, "project": self.concerned_object.key}, _OBJECTS)
+        except HTTPError as e:
+            if e.response.status_code == HTTPStatus.BAD_REQUEST:
+                util.logger.warning("Can't delete %s, it's the main branch", str(self))
+            return False
+
+    def new_code(self):
+        """
+        :return: The branch new code period definition
+        :rtype: str
+        """
+        if self._new_code is None:
+            try:
+                data = json.loads(self.get(api=APIS["get_new_code"], params={"project": self.concerned_object.key}).text)
+            except HTTPError as e:
+                if e.response.status_code == HTTPStatus.NOT_FOUND:
+                    raise exceptions.ObjectNotFound(self.concerned_object.key, f"str{self.concerned_object} not found")
+            for b in data["newCodePeriods"]:
+                new_code = settings.new_code_to_string(b)
+                if b["branchKey"] == self.name:
+                    self._new_code = new_code
+                else:
+                    # While we're there let's store the new code of other branches
+                    Branch.get_object(self.concerned_object, b["branchKey"])._new_code = new_code
+        return self._new_code
+
+    def export(self, full_export=True):
+        """Exports a branch configuration (is main, keep when inactive, optionally name, project)
+
+        :param full_export: Also export branches attributes that are not needed for import, defaults to True
+        :type include_branches: bool, optional
+        :return: The branch new code period definition
+        :rtype: str
+        """
+        util.logger.debug("Exporting %s", str(self))
+        data = {settings.NEW_CODE_PERIOD: self.new_code()}
+        if self.is_main():
+            data["isMain"] = True
+        if self.is_kept_when_inactive() and not self.is_main():
+            data["keepWhenInactive"] = True
+        if self.new_code():
+            data[settings.NEW_CODE_PERIOD] = self.new_code()
+        if full_export:
+            data.update({"name": self.name, "project": self.concerned_object.key})
+        data = util.remove_nones(data)
+        return None if len(data) == 0 else data
+
+    def url(self):
+        """
+        :return: The branch URL in SonarQube as permalink
+        :rtype: str
+        """
+        return f"{self.endpoint.url}/dashboard?id={self.concerned_object.key}&branch={requests.utils.quote(self.name)}"
+
+    def rename(self, new_name):
+        """Renames a branch
+
+        :param new_name: New branch name
+        :type new_name: str
+        :raises UnsupportedOperation: If trying to rename anything than the main branch
+        :raises ObjectNotFound: Concerned object (project) not found
+        :return: Whether the branch was renamed
+        :rtype: bool
+        """
+        if not self.is_main():
+            raise exceptions.UnsupportedOperation(f"{str(self)} can't be renamed since it's not the main branch")
+
+        if self.name == new_name:
+            util.logger.debug("Skipping rename %s with same new name", str(self))
+            return False
+        util.logger.info("Renaming main branch of %s from '%s' to '%s'", str(self.concerned_object), self.name, new_name)
+        try:
+            self.post(APIS["rename"], params={"project": self.concerned_object.key, "name": new_name})
+        except HTTPError as e:
+            if e.response.status_code == HTTPStatus.NOT_FOUND:
+                raise exceptions.ObjectNotFound(self.concerned_object.key, f"str{self.concerned_object} not found")
+        _OBJECTS.pop(uuid(self.concerned_object.key, self.name), None)
+        self.name = new_name
+        _OBJECTS[uuid(self.concerned_object.key, self.name)] = self
+        return True
+
+    def __audit_zero_loc(self):
+        if self.last_analysis() and self.loc() == 0:
+            rule = rules.get_rule(rules.RuleId.PROJ_ZERO_LOC)
+            return [problem.Problem(rule.type, rule.severity, rule.msg.format(str(self)), concerned_object=self)]
+        return []
+
+    def __audit_never_analyzed(self) -> list[problem.Problem]:
+        """Detects branches that have never been analyzed are are kept when inactive"""
+        if not self.last_analysis() and self.is_kept_when_inactive():
+            rule = rules.get_rule(rules.RuleId.BRANCH_NEVER_ANALYZED)
+            return [problem.Problem(rule.type, rule.severity, rule.msg.format(str(self)), concerned_object=self)]
+        return []
+
+    def get_measures(self, metrics_list):
+        """Retrieves a branch list of measures
+
+        :param metrics_list: List of metrics to return
+        :type metrics_list: str (comma separated)
+        :return: List of measures of a projects
+        :rtype: dict
+        """
+        return measures.get(self, metrics_list)
+
+    def get_issues(self):
+        """Returns a branch list of issues
+
+        :return: dict of Issues, with issue key as key
+        :rtype: dict{key: Issue}
+        """
+        return issues.search_all(
+            endpoint=self.endpoint,
+            params={
+                "componentKeys": self.concerned_object.key,
+                "branch": self.name,
+                "additionalFields": "comments",
+            },
+        )
+
+    def get_hotspots(self):
+        """Returns a branch list of hotspots
+
+        :return: dict of Hotspots, with hotspot key as key
+        :rtype: dict{key: Hotspot}
+        """
+        return hotspots.search(
+            endpoint=self.endpoint,
+            params={
+                "projectKey": self.concerned_object.key,
+                "branch": self.name,
+                "additionalFields": "comments",
+            },
+        )
+
+    def get_findings(self):
+        """Returns a branch list of findings
+
+        :return: dict of Findings, with finding key as key
+        :rtype: dict{key: Finding}
+        """
+        return self.get_issues() + self.get_hotspots()
+
+    def sync(self, another_branch, sync_settings):
+        """Syncs branch findings with another branch
+
+        :param another_branch: other branch to sync issues into (not necesssarily of same project)
+        :type another_branch: Branch
+        :param sync_settings: Parameters to configure the sync
+        :type sync_settings: dict
+        :return: sync report as tuple, with counts of successful and unsuccessful issue syncs
+        :rtype: tuple(report, counters)
+        """
+        report, counters = [], {}
+        (report, counters) = syncer.sync_lists(
+            self.get_issues(),
+            another_branch.get_issues(),
+            self,
+            another_branch,
+            sync_settings=sync_settings,
+        )
+        (tmp_report, tmp_counts) = syncer.sync_lists(
+            self.get_hotspots(),
+            another_branch.get_hotspots(),
+            self,
+            another_branch,
+            sync_settings=sync_settings,
+        )
+        report += tmp_report
+        counters = util.dict_add(counters, tmp_counts)
+        return (report, counters)
+
+    def __audit_last_analysis(self, audit_settings):
+        age = util.age(self.last_analysis())
+        if self.is_main() or age is None:
+            # Main branch (not purgeable) or branch not analyzed yet
+            return []
+        max_age = audit_settings.get("audit.projects.branches.maxLastAnalysisAge", 30)
+        problems = []
+        if self.is_main():
+            util.logger.debug("%s is main (not purgeable)", str(self))
+        elif self.is_kept_when_inactive():
+            util.logger.debug("%s is kept when inactive (not purgeable)", str(self))
+        elif age > max_age:
+            rule = rules.get_rule(rules.RuleId.BRANCH_LAST_ANALYSIS)
+            msg = rule.msg.format(str(self), age)
+            problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
+        else:
+            util.logger.debug("%s age is %d days", str(self), age)
+        return problems
+
+    def audit(self, audit_settings):
+        """Audits a branch and return list of problems found
+
+        :meta private:
+        :param audit_settings: Options of what to audit and thresholds to raise problems
+        :type audit_settings: dict
+        :return: List of problems found, or empty list
+        :rtype: list[Problem]
+        """
+        if audit_settings.get("audit.project.branches", True):
+            util.logger.debug("Auditing %s", str(self))
+            return self.__audit_last_analysis(audit_settings) + self.__audit_zero_loc() + self.__audit_never_analyzed()
+        else:
+            util.logger.debug("Branch audit disabled, skipping audit of %s", str(self))
+            return []
+
+    def search_params(self):
+        """Return params used to search for that object
+
+        :meta private:
+        """
+        return {"project": self.concerned_object.key, "branch": self.name}
+
+
+def uuid(project_key, branch_name):
+    """Computes a uuid for the branch that can serve as index
+
+    :param str project_key: The project key
+    :param str branch_name: The branch name
+    :return: the UUID
+    :rtype: str
+    """
+    return f"{project_key} {branch_name}"
+
+
+def get_list(project):
+    """Retrieves the list of branches of a project
+
+    :param Project project: Project the branch belongs to
+    :raises UnsupportedOperation: Branches not supported in Community Edition
+    :return: List of project branches
+    :rtype: dict{branch_name: Branch}
+    """
+    if project.endpoint.edition() == "community":
+        util.logger.debug(_UNSUPPORTED_IN_CE)
+        raise exceptions.UnsupportedOperation(_UNSUPPORTED_IN_CE)
+
+    util.logger.debug("Reading all branches of %s", str(project))
+    data = json.loads(project.endpoint.get(APIS["list"], params={"project": project.key}).text)
+    return {branch["name"]: Branch.load(project, branch["name"], data=branch) for branch in data.get("branches", {})}
+
+
+def exists(endpoint, branch_name, project_key):
+    """Checks if a branch exists
+
+    :param Platform endpoint: Reference to the SonarQube platform
+    :param str branch_name: Branch name
+    :param str project_key: Project key
+    :raises UnsupportedOperation: Branches not supported in Community Edition
+    :return: Whether the branch exists in SonarQube
+    :rtype: bool
+    """
+    try:
+        project = projects.Project.get_object(endpoint, project_key)
+    except exceptions.ObjectNotFound:
+        return False
+    return branch_name in get_list(project)
```

## sonar/projects/projects.py

```diff
@@ -1,1368 +1,1377 @@
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-"""
-
-    Abstraction of the SonarQube "project" concept
-
-"""
-import os
-import re
-import json
-from http import HTTPStatus
-from threading import Thread, Lock
-from queue import Queue
-from requests.exceptions import HTTPError
-from sonar import sqobject, components, qualitygates, qualityprofiles, tasks, options, settings, webhooks, devops, measures, exceptions, syncer
-import sonar.permissions.permissions as perms
-from sonar.projects import pull_requests, branches
-from sonar.findings import issues, hotspots
-import sonar.utilities as util
-import sonar.permissions.project_permissions as pperms
-
-from sonar.audit import rules, severities
-import sonar.audit.problem as pb
-
-_OBJECTS = {}
-_CLASS_LOCK = Lock()
-
-MAX_PAGE_SIZE = 500
-_SEARCH_API = "projects/search"
-_CREATE_API = "projects/create"
-_NAV_API = "navigation/component"
-PRJ_QUALIFIER = "TRK"
-APP_QUALIFIER = "APP"
-
-_BIND_SEP = ":::"
-
-_IMPORTABLE_PROPERTIES = (
-    "key",
-    "name",
-    "binding",
-    settings.NEW_CODE_PERIOD,
-    "qualityProfiles",
-    "links",
-    "permissions",
-    "branches",
-    "tags",
-    "visibility",
-    "qualityGate",
-    "webhooks",
-)
-
-
-class Project(components.Component):
-    """
-    Abstraction of the SonarQube project concept
-    """
-
-    @classmethod
-    def get_object(cls, endpoint, key):
-        """Creates a project from a search in SonarQube
-
-        :param Platform endpoint: Reference to the SonarQube platform
-        :param str key: Project key to search
-        :raises ObjectNotFound: if project key not found
-        :return: The Project
-        :rtype: Project
-        """
-        if endpoint.url in _OBJECTS and key in _OBJECTS[endpoint.url]:
-            return _OBJECTS[endpoint.url][key]
-        try:
-            data = json.loads(endpoint.get(_SEARCH_API, params={"projects": key}, mute=(HTTPStatus.FORBIDDEN,)).text)
-            if len(data["components"]) == 0:
-                raise exceptions.ObjectNotFound(key, f"Project key {key} not found")
-            return cls.load(endpoint, data["components"][0])
-        except HTTPError as e:
-            if e.response.status_code != HTTPStatus.FORBIDDEN:
-                raise
-            data = json.loads(endpoint.get(_NAV_API, params={"component": key}).text)
-            if "errors" in data:
-                raise exceptions.ObjectNotFound(key, f"Project key {key} not found")
-            return cls.load(endpoint, data)
-
-    @classmethod
-    def load(cls, endpoint, data):
-        """Creates a project loaded with JSON data coming from api/components/search request
-
-        :param Platform endpoint: Reference to the SonarQube platform
-        :param str key: Project key to search
-        :param dict data: Project data entry in the search results
-        :return: The Project
-        :rtype: Project
-        """
-        key = data["key"]
-        if endpoint.url in _OBJECTS and key in _OBJECTS[endpoint.url]:
-            o = _OBJECTS[endpoint.url][key]
-        else:
-            o = cls(endpoint, key)
-        o.reload(data)
-        return o
-
-    @classmethod
-    def create(cls, endpoint, key, name):
-        """Creates a Project object after creating it in SonarQube
-
-        :param Platform endpoint: Reference to the SonarQube platform
-        :param str key: Project key to create
-        :param str name: Project name
-        :return: The Project
-        :rtype: Project
-        """
-        try:
-            endpoint.post(_CREATE_API, params={"project": key, "name": name})
-        except HTTPError as e:
-            if e.response.status_code == HTTPStatus.BAD_REQUEST:
-                raise exceptions.ObjectAlreadyExists(key, e.response.text)
-        o = cls(endpoint, key)
-        o.name = name
-        return o
-
-    def __init__(self, endpoint, key):
-        super().__init__(key, endpoint)
-        self._last_analysis = "undefined"
-        self._branches_last_analysis = "undefined"
-        self._permissions = None
-        self._branches = None
-        self._pull_requests = None
-        self._ncloc_with_branches = None
-        self._binding = {"has_binding": True, "binding": None}
-        self._new_code = None
-        super().__init__(key, endpoint)
-        if endpoint.url not in _OBJECTS:
-            _OBJECTS[endpoint.url] = {}
-        _OBJECTS[endpoint.url][key] = self
-        util.logger.debug("Created object %s", str(self))
-
-    def __str__(self):
-        """
-        :return: String formatting of the object
-        :rtype: str
-        """
-        return f"project '{self.key}'"
-
-    def refresh(self):
-        """Refresh a project from SonarQube
-
-        :raises ObjectNotFound: if project key not found
-        :return: self
-        :rtype: Project
-        """
-        data = json.loads(self.get(_SEARCH_API, params={"projects": self.key}).text)
-        if len(data["components"]) == 0:
-            _OBJECTS[self.endpoint.url].pop(self.uuid(), None)
-            raise exceptions.ObjectNotFound(self.key, f"Project key {self.key} not found")
-        return self.reload(data["components"][0])
-
-    def reload(self, data):
-        """Reloads a project with JSON data coming from api/components/search request
-
-        :param dict data: Data to load
-        :return: self
-        :rtype: Project
-        """
-        """Loads a project object with contents of an api/projects/search call"""
-        if self._json is None:
-            self._json = data
-        else:
-            self._json.update(data)
-        self.name = data["name"]
-        self._visibility = data["visibility"]
-        if "lastAnalysisDate" in data:
-            self._last_analysis = util.string_to_date(data["lastAnalysisDate"])
-        elif "analysisDate" in data:
-            self._last_analysis = util.string_to_date(data["analysisDate"])
-        else:
-            self._last_analysis = None
-        self.revision = data.get("revision", None)
-        return self
-
-    def url(self):
-        """
-        :return: the SonarQube permalink to the project
-        :rtype: str
-        """
-        return f"{self.endpoint.url}/dashboard?id={self.key}"
-
-    def last_analysis(self, include_branches=False):
-        """
-        :param include_branches: Take into account branch to determine last analysis, defaults to False
-        :type include_branches: bool, optional
-        :return: Project last analysis date
-        :rtype: datetime
-        """
-        if self._last_analysis == "undefined":
-            self.refresh()
-        if not include_branches:
-            return self._last_analysis
-        if self._branches_last_analysis != "undefined":
-            return self._branches_last_analysis
-
-        self._branches_last_analysis = self._last_analysis
-        if self.endpoint.version() >= (9, 2, 0):
-            # Starting from 9.2 project last analysis date takes into account branches and PR
-            return self._branches_last_analysis
-
-        util.logger.debug("Branches = %s", str(self.branches().values()))
-        util.logger.debug("PR = %s", str(self.pull_requests().values()))
-        for b in list(self.branches().values()) + list(self.pull_requests().values()):
-            if b.last_analysis() is None:
-                continue
-            b_ana_date = b.last_analysis()
-            if self._branches_last_analysis is None or b_ana_date > self._branches_last_analysis:
-                self._branches_last_analysis = b_ana_date
-        return self._branches_last_analysis
-
-    def loc(self):
-        """
-        :return: Number of LoCs of the project, taking into account branches and pull requests, if any
-        :rtype: int
-        """
-        if self._ncloc_with_branches is not None:
-            return self._ncloc_with_branches
-        if self.endpoint.edition() == "community":
-            self._ncloc_with_branches = super().loc()
-        else:
-            self._ncloc_with_branches = max([b.loc() for b in list(self.branches().values()) + list(self.pull_requests().values())])
-        return self._ncloc_with_branches
-
-    def get_measures(self, metrics_list):
-        """Retrieves a project list of measures
-
-        :param list metrics_list: List of metrics to return
-        :return: List of measures of a projects
-        :rtype: dict
-        """
-        m = measures.get(self, metrics_list)
-        if "ncloc" in m:
-            self.ncloc = 0 if not m["ncloc"].value else int(m["ncloc"].value)
-        return m
-
-    def branches(self, use_cache=True):
-        """
-        :return: Dict of branches of the project
-        :param use_cache: Whether to use local cache or query SonarQube, default True (use cache)
-        :type use_cache: bool
-        :rtype: dict{<branchName>: <Branch>}
-        """
-        if not self._branches or not use_cache:
-            try:
-                self._branches = branches.get_list(self)
-            except exceptions.UnsupportedOperation:
-                self._branches = {}
-        return self._branches
-
-    def main_branch(self):
-        """
-        :return: Main branch of the project
-        :rtype: Branch
-        """
-        for b in self.branches().values():
-            if b.is_main():
-                return b
-        if self.endpoint.edition() != "community":
-            util.logger.warning("Could not find main branch for %s", str(self))
-        return None
-
-    def pull_requests(self, use_cache=True):
-        """
-        :return: List of pull requests of the project
-        :param use_cache: Whether to use local cache or query SonarQube, default True (use cache)
-        :type use_cache: bool
-        :rtype: dict{PR_ID: PullRequest}
-        """
-        if self._pull_requests is None or not use_cache:
-            try:
-                self._pull_requests = pull_requests.get_list(self)
-            except exceptions.UnsupportedOperation:
-                self._pull_requests = {}
-        return self._pull_requests
-
-    def delete(self):
-        """Deletes a project in SonarQube
-
-        :raises ObjectNotFound: If object to delete was not found in SonarQube
-        :raises request.HTTPError: In all other cases of HTTP Errors
-        :return: Nothing
-        """
-        loc = int(self.get_measure("ncloc", fallback="0"))
-        util.logger.info("Deleting %s, name '%s' with %d LoCs", str(self), self.name, loc)
-        ok = sqobject.delete_object(self, "projects/delete", {"project": self.key}, _OBJECTS[self.endpoint.url])
-        util.logger.info("Successfully deleted %s - %d LoCs", str(self), loc)
-        return ok
-
-    def has_binding(self):
-        """
-        :return: Whether the project has a DevOps platform binding
-        :rtype: bool
-        """
-        _ = self.binding()
-        return self._binding["has_binding"]
-
-    def binding(self):
-        """
-        :return: The project DevOps platform binding
-        :rtype: dict
-        """
-        if self._binding["has_binding"] and self._binding["binding"] is None:
-            try:
-                resp = self.get("alm_settings/get_binding", params={"project": self.key}, mute=(HTTPStatus.NOT_FOUND,))
-                self._binding["has_binding"] = True
-                self._binding["binding"] = json.loads(resp.text)
-            except HTTPError as e:
-                if e.response.status_code in (HTTPStatus.NOT_FOUND, HTTPStatus.BAD_REQUEST):
-                    # Hack: 8.9 returns 404, 9.x returns 400
-                    self._binding["has_binding"] = False
-                else:
-                    util.exit_fatal(
-                        f"alm_settings/get_binding returning status code {e.response.status_code}, exiting",
-                        options.ERR_SONAR_API,
-                    )
-        return self._binding["binding"]
-
-    def is_part_of_monorepo(self):
-        """
-        :return: From the DevOps binding, Whether the project is part of a monorepo
-        :rtype: bool
-        """
-        if self.binding() is None:
-            return False
-        return self.binding()["monorepo"]
-
-    def binding_key(self):
-        """Computes a unique project binding key
-
-        :meta private:
-        """
-        p_bind = self.binding()
-        if p_bind is None:
-            return None
-        key = p_bind["alm"] + _BIND_SEP + p_bind["repository"]
-        if p_bind["alm"] in ("azure", "bitbucket"):
-            key += _BIND_SEP + p_bind["slug"]
-        return key
-
-    def __audit_last_analysis(self, audit_settings):
-        """Audits whether the last analysis of the project is too old or not
-
-        :param audit_settings: Settings (thresholds) to raise problems
-        :type audit_settings: dict
-        :return: List of problems found, or empty list
-        :rtype: list[Problem]
-        """
-        util.logger.debug("Auditing %s last analysis date", str(self))
-        problems = []
-        age = util.age(self.last_analysis(include_branches=True), True)
-        if age is None:
-            if not audit_settings["audit.projects.neverAnalyzed"]:
-                util.logger.debug("Auditing of never analyzed projects is disabled, skipping")
-            else:
-                rule = rules.get_rule(rules.RuleId.PROJ_NOT_ANALYZED)
-                msg = rule.msg.format(str(self))
-                problems.append(pb.Problem(rule.type, rule.severity, msg, concerned_object=self))
-            return problems
-
-        max_age = audit_settings["audit.projects.maxLastAnalysisAge"]
-        if max_age == 0:
-            util.logger.debug("Auditing of projects with old analysis date is disabled, skipping")
-        elif age > max_age:
-            rule = rules.get_rule(rules.RuleId.PROJ_LAST_ANALYSIS)
-            severity = severities.Severity.HIGH if age > 365 else rule.severity
-            loc = self.get_measure("ncloc", fallback="0")
-            msg = rule.msg.format(str(self), loc, age)
-            problems.append(pb.Problem(rule.type, severity, msg, concerned_object=self))
-
-        util.logger.debug("%s last analysis is %d days old", str(self), age)
-        return problems
-
-    def __audit_branches(self, audit_settings):
-        """Audits project branches
-
-        :param audit_settings: Settings (thresholds) to raise problems
-        :type audit_settings: dict
-        :return: List of problems found, or empty list
-        :rtype: list[Problem]
-        """
-        if not audit_settings["audit.projects.branches"]:
-            util.logger.debug("Auditing of branchs is disabled, skipping...")
-            return []
-        util.logger.debug("Auditing %s branches", str(self))
-        problems = []
-        main_br_count = 0
-        for branch in self.branches().values():
-            problems += branch.audit(audit_settings)
-            if branch.name in ("main", "master"):
-                main_br_count += 1
-                if main_br_count > 1:
-                    rule = rules.get_rule(rules.RuleId.PROJ_MAIN_AND_MASTER)
-                    problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(str(self)), concerned_object=self))
-        return problems
-
-    def __audit_pull_requests(self, audit_settings):
-        """Audits project pul requests
-
-        :param audit_settings: Settings (thresholds) to raise problems
-        :type audit_settings: dict
-        :return: List of problems found, or empty list
-        :rtype: list[Problem]
-        """
-        max_age = audit_settings["audit.projects.pullRequests.maxLastAnalysisAge"]
-        if max_age == 0:
-            util.logger.debug("Auditing of pull request last analysis age is disabled, skipping...")
-            return []
-        problems = []
-        for pr in self.pull_requests().values():
-            problems += pr.audit(audit_settings)
-        return problems
-
-    def __audit_visibility(self, audit_settings):
-        """Audits project visibility and return problems if project is public
-
-        :param audit_settings: Options and Settings (thresholds) to raise problems
-        :type audit_settings: dict
-        :return: List of problems found, or empty list
-        :rtype: list[Problem]
-        """
-        if not audit_settings.get("audit.projects.visibility", True):
-            util.logger.debug("Project visibility audit is disabled by configuration, skipping...")
-            return []
-        util.logger.debug("Auditing %s visibility", str(self))
-        visi = self.visibility()
-        if visi != "private":
-            rule = rules.get_rule(rules.RuleId.PROJ_VISIBILITY)
-            return [pb.Problem(rule.type, rule.severity, rule.msg.format(str(self), visi), concerned_object=self)]
-        util.logger.debug("%s visibility is 'private'", str(self))
-        return []
-
-    def __audit_languages(self, audit_settings):
-        """Audits project utility languages and returns problems if too many LoCs of these
-
-        :param audit_settings: Settings (thresholds) to raise problems
-        :type audit_settings: dict
-        :return: List of problems found, or empty list
-        :rtype: list[Problem]
-        """
-        if not audit_settings.get("audit.projects.utilityLocs", False):
-            util.logger.debug("Utility LoCs audit disabled by configuration, skipping")
-            return []
-        util.logger.debug("Auditing %s utility LoC count", str(self))
-
-        total_locs = 0
-        languages = {}
-        resp = self.get_measure("ncloc_language_distribution")
-        if resp is None:
-            return []
-        for lang in self.get_measure("ncloc_language_distribution").split(";"):
-            (lang, ncloc) = lang.split("=")
-            languages[lang] = int(ncloc)
-            total_locs += int(ncloc)
-        utility_locs = sum(lcount for lang, lcount in languages.items() if lang in ("xml", "json"))
-        if total_locs > 100000 and (utility_locs / total_locs) > 0.5:
-            rule = rules.get_rule(rules.RuleId.PROJ_UTILITY_LOCS)
-            return [pb.Problem(rule.type, rule.severity, rule.msg.format(str(self), utility_locs), concerned_object=self)]
-        util.logger.debug("%s utility LoCs count (%d) seems reasonable", str(self), utility_locs)
-        return []
-
-    def __audit_zero_loc(self, audit_settings):
-        """Audits project utility projects with 0 LoCs
-
-        :param audit_settings: Settings (thresholds) to raise problems
-        :type audit_settings: dict
-        :return: List of problems found, or empty list
-        :rtype: list[Problem]
-        """
-        if (
-            (not audit_settings["audit.projects.branches"] or self.endpoint.edition() == "community")
-            and self.last_analysis() is not None
-            and self.loc() == 0
-        ):
-            rule = rules.get_rule(rules.RuleId.PROJ_ZERO_LOC)
-            return [pb.Problem(rule.type, rule.severity, rule.msg.format(str(self)), concerned_object=self)]
-        return []
-
-    def __audit_binding_valid(self, audit_settings):
-        if self.endpoint.edition() == "community" or not audit_settings["audit.projects.bindings.validation"] or not self.has_binding():
-            util.logger.info(
-                "Community edition, binding validation disabled or %s has no binding, skipping binding validation...",
-                str(self),
-            )
-            return []
-        try:
-            _ = self.get("alm_settings/validate_binding", params={"project": self.key})
-            util.logger.debug("%s binding is valid", str(self))
-            return []
-        except HTTPError as e:
-            # Hack: 8.9 returns 404, 9.x returns 400
-            if e.response.status_code in (HTTPStatus.BAD_REQUEST, HTTPStatus.NOT_FOUND):
-                rule = rules.get_rule(rules.RuleId.PROJ_INVALID_BINDING)
-                return [pb.Problem(rule.type, rule.severity, rule.msg.format(str(self)), concerned_object=self)]
-            else:
-                util.exit_fatal(f"alm_settings/validate_binding returning status code {e.response.status_code}, exiting", options.ERR_SONAR_API)
-
-    def audit(self, audit_settings):
-        """Audits a project and returns the list of problems found
-
-        :param dict audit_settings: Options of what to audit and thresholds to raise problems
-        :return: List of problems found, or empty list
-        :rtype: list[Problem]
-        """
-        util.logger.debug("Auditing %s", str(self))
-        return (
-            self.__audit_last_analysis(audit_settings)
-            + self.__audit_branches(audit_settings)
-            + self.__audit_pull_requests(audit_settings)
-            + self.__audit_visibility(audit_settings)
-            + self.__audit_languages(audit_settings)
-            + self.permissions().audit(audit_settings)
-            + self._audit_bg_task(audit_settings)
-            + self.__audit_binding_valid(audit_settings)
-            + self.__audit_zero_loc(audit_settings)
-        )
-
-    def export_zip(self, timeout=180):
-        """Exports project as zip file, synchronously
-
-        :param timeout: timeout in seconds to complete the export operation
-        :type timeout: int
-        :return: export status (success/failure/timeout), and zip file path
-        :rtype: dict
-        """
-        util.logger.info("Exporting %s (synchronously)", str(self))
-        if self.endpoint.version() < (9, 2, 0) and self.endpoint.edition() not in ("enterprise", "datacenter"):
-            raise exceptions.UnsupportedOperation(
-                "Project export is only available with Enterprise and Datacenter Edition, or with SonarQube 9.2 or higher for any Edition"
-            )
-        try:
-            resp = self.post("project_dump/export", params={"key": self.key})
-        except HTTPError as e:
-            return {"status": f"HTTP_ERROR {e.response.status_code}"}
-        data = json.loads(resp.text)
-        status = tasks.Task(data["taskId"], endpoint=self.endpoint, concerned_object=self, data=data).wait_for_completion(timeout=timeout)
-        if status != tasks.SUCCESS:
-            util.logger.error("%s export %s", str(self), status)
-            return {"status": status}
-        dump_file = json.loads(self.get("project_dump/status", params={"key": self.key}).text)["exportedDump"]
-        util.logger.debug("%s export %s, dump file %s", str(self), status, dump_file)
-        return {"status": status, "file": dump_file}
-
-    def export_async(self):
-        """Export project as zip file, synchronously
-
-        :return: export taskId or None if starting the export failed
-        :rtype: str or None
-        """
-        util.logger.info("Exporting %s (asynchronously)", str(self))
-        try:
-            return json.loads(self.post("project_dump/export", params={"key": self.key}).text)["taskId"]
-        except HTTPError:
-            return None
-
-    def import_zip(self):
-        """Imports a project zip file in SonarQube
-
-        :raises http.HTTPError:
-        :return: Whether the operation succeeded
-        :rtype: bool
-        """
-        util.logger.info("Importing %s (asynchronously)", str(self))
-        if self.endpoint.edition() not in ("enterprise", "datacenter"):
-            raise exceptions.UnsupportedOperation("Project import is only available with Enterprise and Datacenter Edition")
-        return self.post("project_dump/import", params={"key": self.key}).ok
-
-    def get_findings(self, branch=None, pr=None):
-        """Returns a project list of findings (issues and hotspots)
-
-        :param branch: branch name to consider, if any
-        :type branch: str, optional
-        :param pr: PR key to consider, if any
-        :type pr: str, optional
-        :return: JSON of all findings, with finding key as key
-        :rtype: dict{key: Finding}
-        """
-        if self.endpoint.version() < (9, 1, 0) or self.endpoint.edition() not in ("enterprise", "datacenter"):
-            util.logger.warning("export_findings only available in EE and DCE starting from SonarQube 9.1, returning no issues")
-            return {}
-        util.logger.info("Exporting findings for %s", str(self))
-        findings_list = {}
-        params = {"project": self.key}
-        if branch is not None:
-            params["branch"] = branch
-        elif pr is not None:
-            params["pullRequest"] = pr
-
-        data = json.loads(self.get("projects/export_findings", params=params).text)["export_findings"]
-        findings_conflicts = {"SECURITY_HOTSPOT": 0, "BUG": 0, "CODE_SMELL": 0, "VULNERABILITY": 0}
-        nbr_findings = {"SECURITY_HOTSPOT": 0, "BUG": 0, "CODE_SMELL": 0, "VULNERABILITY": 0}
-        util.logger.debug(util.json_dump(data))
-        for i in data:
-            key = i["key"]
-            if key in findings_list:
-                util.logger.warning("Finding %s (%s) already in past findings", i["key"], i["type"])
-                findings_conflicts[i["type"]] += 1
-            # FIXME - Hack for wrong projectKey returned in PR
-            # m = re.search(r"(\w+):PULL_REQUEST:(\w+)", i['projectKey'])
-            i["projectKey"] = self.key
-            i["branch"] = branch
-            i["pullRequest"] = pr
-            nbr_findings[i["type"]] += 1
-            if i["type"] == "SECURITY_HOTSPOT":
-                findings_list[key] = hotspots.get_object(key, endpoint=self.endpoint, data=i, from_export=True)
-            else:
-                findings_list[key] = issues.get_object(key, endpoint=self.endpoint, data=i, from_export=True)
-        for t in ("SECURITY_HOTSPOT", "BUG", "CODE_SMELL", "VULNERABILITY"):
-            if findings_conflicts[t] > 0:
-                util.logger.warning("%d %s findings missed because of JSON conflict", findings_conflicts[t], t)
-        util.logger.info("%d findings exported for %s branch %s PR %s", len(findings_list), str(self), branch, pr)
-        for t in ("SECURITY_HOTSPOT", "BUG", "CODE_SMELL", "VULNERABILITY"):
-            util.logger.info("%d %s exported", nbr_findings[t], t)
-
-        return findings_list
-
-    def get_hotspots(self):
-        """Returns a project main branch list of hotspots
-
-        :return: dict of Hotspots, with hotspot key as key
-        :rtype: dict{key: Hotspot}
-        """
-        return hotspots.search(
-            endpoint=self.endpoint,
-            params={
-                "projectKey": self.key,
-                "additionalFields": "comments",
-            },
-        )
-
-    def sync(self, another_project, sync_settings):
-        """Syncs project issues with another project
-
-        :param Project another_project: other project to sync issues into
-        :type another_project:
-        :param dict sync_settings: Parameters to configure the sync
-        :return: sync report as tuple, with counts of successful and unsuccessful issue syncs
-        :rtype: tuple(report, counters)
-        """
-        if self.endpoint.edition() == "community":
-            report, counters = [], {}
-            util.logger.info("Syncing %s issues", str(self))
-            (report, counters) = syncer.sync_lists(
-                self.get_issues(),
-                another_project.get_issues(),
-                self,
-                another_project,
-                sync_settings=sync_settings,
-            )
-            util.logger.info("Syncing %s issues", str(self))
-            (tmp_report, tmp_counts) = syncer.sync_lists(
-                self.get_hotspots(),
-                another_project.get_hotspots(),
-                self,
-                another_project,
-                sync_settings=sync_settings,
-            )
-            report += tmp_report
-            counters = util.dict_add(counters, tmp_counts)
-        else:
-            tgt_branches = another_project.branches().values()
-            report = []
-            counters = {}
-            for b_src in self.branches().values():
-                for b_tgt in tgt_branches:
-                    if b_src.name == b_tgt.name:
-                        (tmp_report, tmp_counts) = b_src.sync(b_tgt, sync_settings=sync_settings)
-                        report += tmp_report
-                        counters = util.dict_add(counters, tmp_counts)
-        return (report, counters)
-
-    def sync_branches(self, sync_settings):
-        """Syncs project issues across all its branches
-
-        :param dict sync_settings: Parameters to configure the sync
-        :return: sync report as tuple, with counts of successful and unsuccessful issue syncs
-        :rtype: tuple(report, counters)
-        """
-        my_branches = self.branches()
-        report = []
-        counters = {}
-        for b_src in my_branches.values():
-            for b_tgt in my_branches.values():
-                if b_src.name == b_tgt.name:
-                    continue
-                (tmp_report, tmp_counts) = b_src.sync(b_tgt, sync_settings=sync_settings)
-                report += tmp_report
-                counters = util.dict_add(counters, tmp_counts)
-        return (report, counters)
-
-    def quality_profiles(self):
-        """Returns the project quality profiles
-
-        :return: dict of quality profiles indexed by language
-        :rtype: dict{language: QualityProfile}
-        """
-        util.logger.debug("Getting %s quality profiles", str(self))
-        qp_list = qualityprofiles.get_list(self.endpoint)
-        return {qp.language: qp for qp in qp_list.values() if qp.used_by_project(self)}
-
-    def quality_gate(self):
-        """Returns the project quality gate
-
-        :return: name of quality gate and whether it's the default
-        :rtype: tuple(name, is_default)
-        """
-        data = json.loads(self.get(api="qualitygates/get_by_project", params={"project": self.key}).text)
-        return (data["qualityGate"]["name"], data["qualityGate"]["default"])
-
-    def webhooks(self):
-        """
-        :return: Project webhooks indexed by their key
-        :rtype: dict{key: WebHook}
-        """
-        util.logger.debug("Getting %s webhooks", str(self))
-        return webhooks.get_list(endpoint=self.endpoint, project_key=self.key)
-
-    def links(self):
-        """
-        :return: list of project links
-        :rtype: list[{type, name, url}]
-        """
-        data = json.loads(self.get(api="project_links/search", params={"projectKey": self.key}).text)
-        link_list = None
-        for link in data["links"]:
-            if link_list is None:
-                link_list = []
-            link_list.append({"type": link["type"], "name": link.get("name", link["type"]), "url": link["url"]})
-        return link_list
-
-    def __export_get_binding(self):
-        binding = self.binding()
-        if binding:
-            # Remove redundant fields
-            binding.pop("alm", None)
-            binding.pop("url", None)
-            if not binding["monorepo"]:
-                binding.pop("monorepo")
-        return binding
-
-    def __export_get_qp(self):
-        qp_json = {qp.language: f"{qp.name}" for qp in self.quality_profiles().values()}
-        if len(qp_json) == 0:
-            return None
-        return qp_json
-
-    def __get_branch_export(self):
-        branch_data = {}
-        my_branches = self.branches().values()
-        for branch in my_branches:
-            exp = branch.export(full_export=False)
-            if len(my_branches) == 1 and branch.is_main() and len(exp) <= 1:
-                # Don't export main branch with no data
-                continue
-            branch_data[branch.name] = exp
-        # If there is only 1 branch with no specific config except being main, don't return anything
-        if len(branch_data) == 0 or (len(branch_data) == 1 and len(exp) <= 1):
-            return None
-        return util.remove_nones(branch_data)
-
-    def export(self, settings_list=None, include_inherited=False, full=False):
-        """Exports the entire project configuration as JSON
-
-        :return: All project configuration settings
-        :rtype: dict
-        """
-        util.logger.info("Exporting %s", str(self))
-        json_data = self._json.copy()
-        json_data.update({"key": self.key, "name": self.name})
-        json_data["binding"] = self.__export_get_binding()
-        nc = self.new_code()
-        if nc != "":
-            json_data[settings.NEW_CODE_PERIOD] = nc
-        json_data["qualityProfiles"] = self.__export_get_qp()
-        json_data["links"] = self.links()
-        json_data["permissions"] = self.permissions().to_json(csv=True)
-        json_data["branches"] = self.__get_branch_export()
-        json_data["tags"] = util.list_to_csv(self.tags(), separator=", ")
-        json_data["visibility"] = self.visibility()
-        (json_data["qualityGate"], qg_is_default) = self.quality_gate()
-        if qg_is_default:
-            json_data.pop("qualityGate")
-
-        json_data["webhooks"] = webhooks.export(self.endpoint, self.key)
-        json_data = util.filter_export(json_data, _IMPORTABLE_PROPERTIES, full)
-        settings_dict = settings.get_bulk(endpoint=self, component=self, settings_list=settings_list, include_not_set=False)
-        # json_data.update({s.to_json() for s in settings_dict.values() if include_inherited or not s.inherited})
-        for s in settings_dict.values():
-            if not include_inherited and s.inherited:
-                continue
-            json_data.update(s.to_json())
-        return util.remove_nones(json_data)
-
-    def new_code(self):
-        """
-        :return: The project new code definition
-        :rtype: str
-        """
-        if self._new_code is None:
-            new_code = settings.Setting.read(settings.NEW_CODE_PERIOD, self.endpoint, component=self)
-            self._new_code = new_code.value if new_code else ""
-        return self._new_code
-
-    def permissions(self):
-        """
-        :return: The project permissions
-        :rtype: ProjectPermissions
-        """
-        if self._permissions is None:
-            self._permissions = pperms.ProjectPermissions(self)
-        return self._permissions
-
-    def set_permissions(self, desired_permissions):
-        """Sets project permissions
-
-        :param desired_permissions: dict describing permissions
-        :type desired_permissions: dict
-        :return: Nothing
-        """
-        self.permissions().set(desired_permissions)
-
-    def set_links(self, desired_links):
-        """Sets project links
-
-        :param desired_links: dict describing links
-        :type desired_links: dict
-        :return: Whether the operation was successful
-        """
-        params = {"projectKey": self.key}
-        ok = True
-        for link in desired_links.get("links", {}):
-            if link.get("type", "") != "custom":
-                continue
-            params.update(link)
-            ok = ok and self.post("project_links/create", params=params).ok
-
-    def set_tags(self, tags):
-        """Sets project tags
-
-        :param list tags: list of tags
-        :return: Whether the operation was successful
-        """
-        if tags is None:
-            return
-        my_tags = util.list_to_csv(tags) if isinstance(tags, list) else util.csv_normalize(tags)
-        r = self.post("project_tags/set", params={"project": self.key, "tags": my_tags})
-        self._tags = util.csv_to_list(my_tags)
-        return r.ok
-
-    def set_quality_gate(self, quality_gate):
-        """Sets project quality gate
-
-        :param quality_gate: quality gate name
-        :type quality_gate: str
-        :return: Whether the operation was successful
-        :rtype: bool
-        """
-        if quality_gate is None:
-            return False
-        try:
-            _ = qualitygates.QualityGate.get_object(self.endpoint, quality_gate)
-        except exceptions.ObjectNotFound:
-            util.logger.warning("Quality gate '%s' not found, can't set it for %s", quality_gate, str(self))
-            return False
-        util.logger.debug("Setting quality gate '%s' for %s", quality_gate, str(self))
-        r = self.post("qualitygates/select", params={"projectKey": self.key, "gateName": quality_gate})
-        return r.ok
-
-    def set_quality_profile(self, language, quality_profile):
-        """Sets project quality profile for a given language
-
-        :param language: Language mnemonic, following SonarQube convention
-        :type language: str
-        :param quality_profile: Name of the quality profile in the language
-        :type quality_profile: str
-        :return: Whether the operation was successful
-        :rtype: bool
-        """
-        if not qualityprofiles.exists(endpoint=self.endpoint, language=language, name=quality_profile):
-            util.logger.warning("Quality profile '%s' in language '%s' does not exist, can't set it for %s", quality_profile, language, str(self))
-            return False
-        util.logger.debug("Setting quality profile '%s' of language '%s' for %s", quality_profile, language, str(self))
-        r = self.post("qualityprofiles/add_project", params={"project": self.key, "qualityProfile": quality_profile, "language": language})
-        return r.ok
-
-    def rename_main_branch(self, main_branch_name):
-        """Renames the project main branch
-
-        :param main_branch_name: New main branch name
-        :type main_branch_name: str
-        :return: Whether the operation was successful
-        :rtype: bool
-        """
-        br = self.main_branch()
-        if br:
-            return br.rename(main_branch_name)
-        util.logger.warning("No main branch to rename found for %s", str(self))
-        return False
-
-    def set_webhooks(self, webhook_data):
-        """Sets project webhooks
-
-        :param dict webhook_data: JSON describing the webhooks
-        :return: Nothing
-        """
-        current_wh = self.webhooks()
-        current_wh_names = [wh.name for wh in current_wh.values()]
-        wh_map = {wh.name: k for k, wh in current_wh.items()}
-        # FIXME: Handle several webhooks with same name
-        for wh_name, wh in webhook_data.items():
-            if wh_name in current_wh_names:
-                current_wh[wh_map[wh_name]].update(name=wh_name, **wh)
-            else:
-                webhooks.update(name=wh_name, endpoint=self.endpoint, project=self.key, **wh)
-
-    def set_settings(self, data):
-        """Sets project settings (webhooks, settings, new code period)
-
-        :param dict data: JSON describing the settings
-        :return: Nothing
-        """
-        util.logger.debug("Setting %s settings with %s", str(self), util.json_dump(data))
-        for key, value in data.items():
-            if key in ("branches", settings.NEW_CODE_PERIOD):
-                continue
-            if key == "webhooks":
-                self.set_webhooks(value)
-            else:
-                settings.set_setting(endpoint=self.endpoint, key=key, value=value, component=self)
-
-        nc = data.get(settings.NEW_CODE_PERIOD, None)
-        if nc is not None:
-            (nc_type, nc_val) = settings.decode(settings.NEW_CODE_PERIOD, nc)
-            settings.set_new_code_period(self.endpoint, nc_type, nc_val, project_key=self.key)
-        # TODO: Update branches (main, new code definition, keepWhenInactive)
-        # util.logger.debug("Checking main branch")
-        # for branch, branch_data in data.get("branches", {}).items():
-        #    if branches.exists(branch_name=branch, project_key=self.key, endpoint=self.endpoint):
-        #        branches.get_object(branch, self, endpoint=self.endpoint).update(branch_data)()
-
-    def set_devops_binding(self, data):
-        """Sets project devops binding settings
-
-        :param dict data: JSON describing the devops binding
-        :return: Nothing
-        """
-        util.logger.debug("Setting devops binding of %s to %s", str(self), util.json_dump(data))
-        alm_key = data["key"]
-        if not devops.platform_exists(alm_key, self.endpoint):
-            util.logger.warning("DevOps platform '%s' does not exists, can't set it for %s", alm_key, str(self))
-            return False
-        alm_type = devops.devops_type(platform_key=alm_key, endpoint=self.endpoint)
-        mono = data.get("monorepo", False)
-        repo = data["repository"]
-        if alm_type == "github":
-            self.set_binding_github(alm_key, repository=repo, monorepo=mono, summary_comment=data.get("summaryComment", True))
-        elif alm_type == "gitlab":
-            self.set_binding_gitlab(alm_key, repository=repo, monorepo=mono)
-        elif alm_type == "azure":
-            self.set_binding_azure_devops(alm_key, repository=repo, monorepo=mono, slug=data["slug"])
-        elif alm_type == "bitbucket":
-            self.set_binding_bitbucket_server(alm_key, repository=repo, monorepo=mono, slug=data["slug"])
-        elif alm_type == "bitbucketcloud":
-            self.set_binding_bitbucket_cloud(alm_key, repository=repo, monorepo=mono)
-        else:
-            util.logger.error("Invalid devops platform type '%s' for %s, setting skipped", alm_key, str(self))
-            return False
-        return True
-
-    def __std_binding_params(self, alm_key, repo, monorepo):
-        return {"almSetting": alm_key, "project": self.key, "repository": repo, "monorepo": str(monorepo).lower()}
-
-    def set_binding_github(self, devops_platform_key, repository, monorepo=False, summary_comment=True):
-        """Sets project devops binding for github
-
-        :param str devops_platform_key: key of the platform in the global admin devops configuration
-        :param str repository: project repository name in github
-        :param monorepo: Whether the project is part of a monorepo, defaults to False
-        :type monorepo: bool, optional
-        :param summary_comment: Whether summary comments should be posted, defaults to True
-        :type summary_comment: bool, optional
-        :return: Nothing
-        """
-        params = self.__std_binding_params(devops_platform_key, repository, monorepo)
-        params["summaryCommentEnabled"] = str(summary_comment).lower()
-        self.post("alm_settings/set_github_binding", params=params)
-
-    def set_binding_gitlab(self, devops_platform_key, repository, monorepo=False):
-        """Sets project devops binding for gitlab
-
-        :param str devops_platform_key: key of the platform in the global admin devops configuration
-        :param str repository: project repository name in gitlab
-        :param monorepo: Whether the project is part of a monorepo, defaults to False
-        :type monorepo: bool, optional
-        :return: Nothing
-        """
-        params = self.__std_binding_params(devops_platform_key, repository, monorepo)
-        self.post("alm_settings/set_gitlab_binding", params=params)
-
-    def set_binding_bitbucket_server(self, devops_platform_key, repository, slug, monorepo=False):
-        """Sets project devops binding for bitbucket server
-
-        :param str devops_platform_key: key of the platform in the global admin devops configuration
-        :param str repository: project repository name in bitbucket server
-        :param str slug: project repository SLUG
-        :param monorepo: Whether the project is part of a monorepo, defaults to False
-        :type monorepo: bool, optional
-        :return: Nothing
-        """
-        params = self.__std_binding_params(devops_platform_key, repository, monorepo)
-        params["slug"] = slug
-        self.post("alm_settings/set_bitbucket_binding", params=params)
-
-    def set_binding_bitbucket_cloud(self, devops_platform_key, repository, monorepo=False):
-        """Sets project devops binding for bitbucket cloud
-
-        :param str devops_platform_key: key of the platform in the global admin devops configuration
-        :param str repository: project repository name in bitbucket cloud
-        :param str slug: project repository SLUG
-        :param monorepo: Whether the project is part of a monorepo, defaults to False
-        :type monorepo: bool, optional
-        :return: Nothing
-        """
-        params = self.__std_binding_params(devops_platform_key, repository, monorepo)
-        self.post("alm_settings/set_bitbucketcloud_binding", params=params)
-
-    def set_binding_azure_devops(self, devops_platform_key, slug, repository, monorepo=False):
-        """Sets project devops binding for azure devops
-
-        :param str devops_platform_key: key of the platform in the global admin devops configuration
-        :param str slug: project SLUG in Azure DevOps
-        :param str repository: project repository name in azure devops
-        :param monorepo: Whether the project is part of a monorepo, defaults to False
-        :type monorepo: bool, optional
-        :return: Nothing
-        """
-        params = self.__std_binding_params(devops_platform_key, repository, monorepo)
-        params["projectName"] = slug
-        params["repositoryName"] = params.pop("repository")
-        self.post("alm_settings/set_azure_binding", params=params)
-
-    def update(self, data):
-        """Updates a project with a whole configuration set
-
-        :param dict data: JSON of configuration settings
-        :return: Nothing
-        """
-        if "permissions" in data:
-            decoded_perms = {}
-            for ptype in perms.PERMISSION_TYPES:
-                if ptype not in data["permissions"]:
-                    continue
-                decoded_perms[ptype] = {u: perms.decode(v) for u, v in data["permissions"][ptype].items()}
-            self.set_permissions(decoded_perms)
-        self.set_links(data)
-        self.set_tags(data.get("tags", None))
-        self.set_quality_gate(data.get("qualityGate", None))
-        for lang, qp_name in data.get("qualityProfiles", {}).items():
-            self.set_quality_profile(language=lang, quality_profile=qp_name)
-        for bname, bdata in data.get("branches", {}).items():
-            if bdata.get("isMain", False):
-                self.rename_main_branch(bname)
-                break
-        if "binding" in data:
-            self.set_devops_binding(data["binding"])
-        else:
-            util.logger.debug("%s has no devops binding, skipped", str(self))
-        settings_to_apply = {
-            k: v for k, v in data.items() if k not in ("permissions", "tags", "links", "qualityGate", "qualityProfiles", "binding", "name")
-        }
-        # TODO: Set branch settings
-        self.set_settings(settings_to_apply)
-
-    def search_params(self):
-        """Return params used to search for that object
-
-        :meta private:
-        """
-        return {"project": self.key}
-
-
-def count(endpoint, params=None):
-    """Counts projects
-
-    :param params: list of parameters to filter projects to search
-    :type params: dict
-    :return: Count of projects
-    :rtype: int
-    """
-    new_params = {} if params is None else params.copy()
-    new_params.update({"ps": 1, "p": 1})
-    data = json.loads(endpoint.get(_SEARCH_API, params=params).text)
-    return data["paging"]["total"]
-
-
-def search(endpoint, params=None):
-    """Searches projects in SonarQube
-
-    :param endpoint: Reference to the SonarQube platform
-    :type endpoint: Platform
-    :param params: list of parameters to narrow down the search
-    :type params: dict
-    :return: list of projects
-    :rtype: dict{key: Project}
-    """
-    new_params = {} if params is None else params.copy()
-    new_params["qualifiers"] = "TRK"
-    return sqobject.search_objects(
-        api="projects/search",
-        params=new_params,
-        key_field="key",
-        returned_field="components",
-        endpoint=endpoint,
-        object_class=Project,
-    )
-
-
-def get_list(endpoint, key_list=None, use_cache=True):
-    """
-    :param endpoint: Reference to the SonarQube platform
-    :type endpoint: Platform
-    :param key_list: List of portfolios keys to get, if None or empty all portfolios are returned
-    :param use_cache: Whether to use local cache or query SonarQube, default True (use cache)
-    :type use_cache: bool
-    :return: the list of all quality profiles
-    :rtype: dict{key: QualityProfile}
-    """
-    with _CLASS_LOCK:
-        if key_list is None or len(key_list) == 0 or not use_cache:
-            util.logger.info("Listing projects")
-            return search(endpoint=endpoint)
-    return {key: Project.get_object(endpoint, key) for key in util.csv_to_list(key_list)}
-
-
-def __audit_thread(queue, results, audit_settings, bindings):
-    audit_bindings = audit_settings["audit.projects.bindings"]
-    while not queue.empty():
-        util.logger.debug("Picking from the queue")
-        project = queue.get()
-        results += project.audit(audit_settings)
-        if project.endpoint.edition() == "community" or not audit_bindings or project.is_part_of_monorepo():
-            queue.task_done()
-            util.logger.debug("%s audit done", str(project))
-            continue
-        bindkey = project.binding_key()
-        if bindkey and bindkey in bindings:
-            rule = rules.get_rule(rules.RuleId.PROJ_DUPLICATE_BINDING)
-            results.append(pb.Problem(rule.type, rule.severity, rule.msg.format(str(project), str(bindings[bindkey])), concerned_object=project))
-        else:
-            bindings[bindkey] = project
-        queue.task_done()
-        util.logger.debug("%s audit done", str(project))
-    util.logger.debug("Queue empty, exiting thread")
-
-
-def audit(endpoint, audit_settings, key_list=None):
-    """Audits all or a list of projects
-
-    :param endpoint: reference to the SonarQube platform
-    :type endpoint: Platform
-    :param audit_settings: Configuration of audit
-    :type audit_settings: dict
-    :param key_list: List of project keys to audit, defaults to None (all projects)
-    :type key_list: str, optional
-    :return: list of problems found
-    :rtype: list[Problem]
-    """
-    util.logger.info("--- Auditing projects ---")
-    plist = get_list(endpoint, key_list)
-    problems = []
-    q = Queue(maxsize=0)
-    for p in plist.values():
-        q.put(p)
-    bindings = {}
-    for i in range(audit_settings["threads"]):
-        util.logger.debug("Starting project audit thread %d", i)
-        worker = Thread(target=__audit_thread, args=(q, problems, audit_settings, bindings))
-        worker.setDaemon(True)
-        worker.setName(f"ProjectAudit{i}")
-        worker.start()
-    q.join()
-    if not audit_settings["audit.projects.duplicates"]:
-        util.logger.info("Project duplicates auditing was disabled by configuration")
-        return problems
-    for key, p in plist.items():
-        util.logger.debug("Auditing for potential duplicate projects")
-        for key2 in plist:
-            if key2 != key and re.match(key2, key):
-                rule = rules.get_rule(rules.RuleId.PROJ_DUPLICATE)
-                problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(str(p), key2), concerned_object=p))
-    return problems
-
-
-def __export_thread(queue, results, full):
-    while not queue.empty():
-        project = queue.get()
-        results[project.key] = project.export(full=full)
-        results[project.key].pop("key")
-        queue.task_done()
-
-
-def export(endpoint, key_list=None, full=False, threads=8):
-    """Exports all or a list of projects configuration as dict
-
-    :param endpoint: reference to the SonarQube platform
-    :type endpoint: Platform
-    :param key_list: List of project keys to export, defaults to None (all projects)
-    :type key_list: str
-    :param full: Whether to export all settings including those useless for re-import, defaults to False
-    :type full: bool, optional
-    :param threads: Number of parallel threads for export, defaults to 8
-    :type threads: int, optional
-    :return: list of projects
-    :rtype: dict{key: Project}
-    """
-    for qp in qualityprofiles.get_list(endpoint).values():
-        qp.projects()
-
-    q = Queue(maxsize=0)
-    for p in get_list(endpoint=endpoint, key_list=key_list).values():
-        q.put(p)
-    project_settings = {}
-    for i in range(threads):
-        util.logger.debug("Starting project export thread %d", i)
-        worker = Thread(target=__export_thread, args=(q, project_settings, full))
-        worker.setDaemon(True)
-        worker.setName(f"ProjectExport{i}")
-        worker.start()
-    q.join()
-    return project_settings
-
-
-def exists(key, endpoint):
-    """
-    :param key: project key to check
-    :type key: str
-    :param endpoint: reference to the SonarQube platform
-    :type endpoint: Platform
-    :return: whether the project exists
-    :rtype: bool
-    """
-    try:
-        Project.get_object(endpoint, key)
-        return True
-    except exceptions.ObjectNotFound:
-        return False
-
-
-def loc_csv_header(**kwargs):
-    arr = ["# Project Key"]
-    if kwargs[options.WITH_NAME]:
-        arr.append("Project name")
-    arr.append("LoC")
-    if kwargs[options.WITH_LAST_ANALYSIS]:
-        arr.append("Last analysis")
-    if kwargs[options.WITH_URL]:
-        arr.append("URL")
-    return arr
-
-
-def import_config(endpoint, config_data, key_list=None):
-    """Imports a configuration in SonarQube
-
-    :param endpoint: reference to the SonarQube platform
-    :type endpoint: Platform
-    :param config_data: the configuration to import
-    :type config_data: dict
-    :param key_list: List of project keys to be considered for the import, defaults to None (all projects)
-    :type key_list: str
-    :return: Nothing
-    """
-    if "projects" not in config_data:
-        util.logger.info("No projects to import")
-        return
-    util.logger.info("Importing projects")
-    get_list(endpoint=endpoint)
-    nb_projects = len(config_data["projects"])
-    i = 0
-    new_key_list = util.csv_to_list(key_list)
-    for key, data in config_data["projects"].items():
-        if new_key_list and key not in new_key_list:
-            continue
-        util.logger.info("Importing project key '%s'", key)
-        try:
-            o = Project.get_object(endpoint, key)
-        except exceptions.ObjectNotFound:
-            o = Project.create(endpoint, key, data["name"])
-        o.update(data)
-        i += 1
-        if i % 20 == 0 or i == nb_projects:
-            util.logger.info("Imported %d/%d projects (%d%%)", i, nb_projects, (i * 100 // nb_projects))
-
-
-def __export_zip_thread(queue, results, statuses, export_timeout):
-    while not queue.empty():
-        project = queue.get()
-        try:
-            dump = project.export_zip(timeout=export_timeout)
-        except exceptions.UnsupportedOperation:
-            queue.task_done()
-            util.exit_fatal("Zip export unsupported on your SonarQube version", options.ERR_UNSUPPORTED_OPERATION)
-        status = dump["status"]
-        statuses[status] = 1 if status not in statuses else statuses[status] + 1
-        data = {"key": project.key, "status": status}
-        if status == "SUCCESS":
-            data["file"] = os.path.basename(dump["file"])
-            data["path"] = dump["file"]
-        results.append(data)
-        util.logger.info("%s", ", ".join([f"{k}:{v}" for k, v in statuses.items()]))
-        queue.task_done()
-
-
-def export_zip(endpoint, key_list=None, threads=8, export_timeout=30):
-    """Export as zip all or a list of projects
-
-    :param endpoint: reference to the SonarQube platform
-    :type endpoint: Platform
-    :param key_list: List of project keys to export, defaults to None (all projects)
-    :type key_list: str, optional
-    :param threads: Number of parallel threads for export, defaults to 8
-    :type threads: int, optional
-    :param export_timeout: Tiemout to export the project, defaults to 30
-    :type export_timeout: int, optional
-    :return: list of exported projects and platform version
-    :rtype: dict
-    """
-    statuses, exports = {}, []
-    projects_list = get_list(endpoint, key_list)
-    nbr_projects = len(projects_list)
-    util.logger.info("Exporting %d projects to export", nbr_projects)
-    q = Queue(maxsize=0)
-    for p in projects_list.values():
-        q.put(p)
-    for i in range(threads):
-        util.logger.debug("Starting project export thread %d", i)
-        worker = Thread(target=__export_zip_thread, args=(q, exports, statuses, export_timeout))
-        worker.setDaemon(True)
-        worker.setName(f"ZipExport{i}")
-        worker.start()
-    q.join()
-
-    return {
-        "sonarqube_environment": {
-            "version": endpoint.version(digits=2, as_string=True),
-            "plugins": endpoint.plugins(),
-        },
-        "project_exports": exports,
-    }
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+"""
+
+    Abstraction of the SonarQube "project" concept
+
+"""
+import os
+import re
+import json
+from http import HTTPStatus
+from threading import Thread, Lock
+from queue import Queue
+from requests.exceptions import HTTPError
+from sonar import sqobject, components, qualitygates, qualityprofiles, tasks, options, settings, webhooks, devops, measures, exceptions, syncer
+import sonar.permissions.permissions as perms
+from sonar.projects import pull_requests, branches
+from sonar.findings import issues, hotspots
+import sonar.utilities as util
+import sonar.permissions.project_permissions as pperms
+
+from sonar.audit import rules, severities
+import sonar.audit.problem as pb
+
+_OBJECTS = {}
+_CLASS_LOCK = Lock()
+
+MAX_PAGE_SIZE = 500
+_SEARCH_API = "projects/search"
+_CREATE_API = "projects/create"
+_NAV_API = "navigation/component"
+PRJ_QUALIFIER = "TRK"
+APP_QUALIFIER = "APP"
+
+_BIND_SEP = ":::"
+_AUDIT_BRANCHES_PARAM = "audit.projects.branches"
+
+_IMPORTABLE_PROPERTIES = (
+    "key",
+    "name",
+    "binding",
+    settings.NEW_CODE_PERIOD,
+    "qualityProfiles",
+    "links",
+    "permissions",
+    "branches",
+    "tags",
+    "visibility",
+    "qualityGate",
+    "webhooks",
+)
+
+
+class Project(components.Component):
+    """
+    Abstraction of the SonarQube project concept
+    """
+
+    @classmethod
+    def get_object(cls, endpoint, key):
+        """Creates a project from a search in SonarQube
+
+        :param Platform endpoint: Reference to the SonarQube platform
+        :param str key: Project key to search
+        :raises ObjectNotFound: if project key not found
+        :return: The Project
+        :rtype: Project
+        """
+        if endpoint.url in _OBJECTS and key in _OBJECTS[endpoint.url]:
+            return _OBJECTS[endpoint.url][key]
+        try:
+            data = json.loads(endpoint.get(_SEARCH_API, params={"projects": key}, mute=(HTTPStatus.FORBIDDEN,)).text)
+            if len(data["components"]) == 0:
+                raise exceptions.ObjectNotFound(key, f"Project key {key} not found")
+            return cls.load(endpoint, data["components"][0])
+        except HTTPError as e:
+            if e.response.status_code != HTTPStatus.FORBIDDEN:
+                raise
+            data = json.loads(endpoint.get(_NAV_API, params={"component": key}).text)
+            if "errors" in data:
+                raise exceptions.ObjectNotFound(key, f"Project key {key} not found")
+            return cls.load(endpoint, data)
+
+    @classmethod
+    def load(cls, endpoint, data):
+        """Creates a project loaded with JSON data coming from api/components/search request
+
+        :param Platform endpoint: Reference to the SonarQube platform
+        :param str key: Project key to search
+        :param dict data: Project data entry in the search results
+        :return: The Project
+        :rtype: Project
+        """
+        key = data["key"]
+        if endpoint.url in _OBJECTS and key in _OBJECTS[endpoint.url]:
+            o = _OBJECTS[endpoint.url][key]
+        else:
+            o = cls(endpoint, key)
+        o.reload(data)
+        return o
+
+    @classmethod
+    def create(cls, endpoint, key, name):
+        """Creates a Project object after creating it in SonarQube
+
+        :param Platform endpoint: Reference to the SonarQube platform
+        :param str key: Project key to create
+        :param str name: Project name
+        :return: The Project
+        :rtype: Project
+        """
+        try:
+            endpoint.post(_CREATE_API, params={"project": key, "name": name})
+        except HTTPError as e:
+            if e.response.status_code == HTTPStatus.BAD_REQUEST:
+                raise exceptions.ObjectAlreadyExists(key, e.response.text)
+        o = cls(endpoint, key)
+        o.name = name
+        return o
+
+    def __init__(self, endpoint, key):
+        super().__init__(key, endpoint)
+        self._last_analysis = "undefined"
+        self._branches_last_analysis = "undefined"
+        self._permissions = None
+        self._branches = None
+        self._pull_requests = None
+        self._ncloc_with_branches = None
+        self._binding = {"has_binding": True, "binding": None}
+        self._new_code = None
+        super().__init__(key, endpoint)
+        if endpoint.url not in _OBJECTS:
+            _OBJECTS[endpoint.url] = {}
+        _OBJECTS[endpoint.url][key] = self
+        util.logger.debug("Created object %s", str(self))
+
+    def __str__(self):
+        """
+        :return: String formatting of the object
+        :rtype: str
+        """
+        return f"project '{self.key}'"
+
+    def refresh(self):
+        """Refresh a project from SonarQube
+
+        :raises ObjectNotFound: if project key not found
+        :return: self
+        :rtype: Project
+        """
+        data = json.loads(self.get(_SEARCH_API, params={"projects": self.key}).text)
+        if len(data["components"]) == 0:
+            _OBJECTS[self.endpoint.url].pop(self.uuid(), None)
+            raise exceptions.ObjectNotFound(self.key, f"Project key {self.key} not found")
+        return self.reload(data["components"][0])
+
+    def reload(self, data):
+        """Reloads a project with JSON data coming from api/components/search request
+
+        :param dict data: Data to load
+        :return: self
+        :rtype: Project
+        """
+        """Loads a project object with contents of an api/projects/search call"""
+        if self._json is None:
+            self._json = data
+        else:
+            self._json.update(data)
+        self.name = data["name"]
+        self._visibility = data["visibility"]
+        if "lastAnalysisDate" in data:
+            self._last_analysis = util.string_to_date(data["lastAnalysisDate"])
+        elif "analysisDate" in data:
+            self._last_analysis = util.string_to_date(data["analysisDate"])
+        else:
+            self._last_analysis = None
+        self.revision = data.get("revision", None)
+        return self
+
+    def url(self):
+        """
+        :return: the SonarQube permalink to the project
+        :rtype: str
+        """
+        return f"{self.endpoint.url}/dashboard?id={self.key}"
+
+    def last_analysis(self, include_branches=False):
+        """
+        :param include_branches: Take into account branch to determine last analysis, defaults to False
+        :type include_branches: bool, optional
+        :return: Project last analysis date
+        :rtype: datetime
+        """
+        if self._last_analysis == "undefined":
+            self.refresh()
+        if not include_branches:
+            return self._last_analysis
+        if self._branches_last_analysis != "undefined":
+            return self._branches_last_analysis
+
+        self._branches_last_analysis = self._last_analysis
+        if self.endpoint.version() >= (9, 2, 0):
+            # Starting from 9.2 project last analysis date takes into account branches and PR
+            return self._branches_last_analysis
+
+        util.logger.debug("Branches = %s", str(self.branches().values()))
+        util.logger.debug("PR = %s", str(self.pull_requests().values()))
+        for b in list(self.branches().values()) + list(self.pull_requests().values()):
+            if b.last_analysis() is None:
+                continue
+            b_ana_date = b.last_analysis()
+            if self._branches_last_analysis is None or b_ana_date > self._branches_last_analysis:
+                self._branches_last_analysis = b_ana_date
+        return self._branches_last_analysis
+
+    def loc(self):
+        """
+        :return: Number of LoCs of the project, taking into account branches and pull requests, if any
+        :rtype: int
+        """
+        if self._ncloc_with_branches is not None:
+            return self._ncloc_with_branches
+        if self.endpoint.edition() == "community":
+            self._ncloc_with_branches = super().loc()
+        else:
+            self._ncloc_with_branches = max([b.loc() for b in list(self.branches().values()) + list(self.pull_requests().values())])
+        return self._ncloc_with_branches
+
+    def get_measures(self, metrics_list):
+        """Retrieves a project list of measures
+
+        :param list metrics_list: List of metrics to return
+        :return: List of measures of a projects
+        :rtype: dict
+        """
+        m = measures.get(self, metrics_list)
+        if "ncloc" in m:
+            self.ncloc = 0 if not m["ncloc"].value else int(m["ncloc"].value)
+        return m
+
+    def branches(self, use_cache: bool = True) -> dict[str, branches.Branch]:
+        """
+        :return: Dict of branches of the project
+        :param use_cache: Whether to use local cache or query SonarQube, default True (use cache)
+        :type use_cache: bool
+        :rtype: dict{<branchName>: <Branch>}
+        """
+        if not self._branches or not use_cache:
+            try:
+                self._branches = branches.get_list(self)
+            except exceptions.UnsupportedOperation:
+                self._branches = {}
+        return self._branches
+
+    def main_branch(self):
+        """
+        :return: Main branch of the project
+        :rtype: Branch
+        """
+        for b in self.branches().values():
+            if b.is_main():
+                return b
+        if self.endpoint.edition() != "community":
+            util.logger.warning("Could not find main branch for %s", str(self))
+        return None
+
+    def pull_requests(self, use_cache: bool = True) -> dict[str, pull_requests.PullRequest]:
+        """
+        :return: List of pull requests of the project
+        :param use_cache: Whether to use local cache or query SonarQube, default True (use cache)
+        :type use_cache: bool
+        :rtype: dict{PR_ID: PullRequest}
+        """
+        if self._pull_requests is None or not use_cache:
+            try:
+                self._pull_requests = pull_requests.get_list(self)
+            except exceptions.UnsupportedOperation:
+                self._pull_requests = {}
+        return self._pull_requests
+
+    def delete(self):
+        """Deletes a project in SonarQube
+
+        :raises ObjectNotFound: If object to delete was not found in SonarQube
+        :raises request.HTTPError: In all other cases of HTTP Errors
+        :return: Nothing
+        """
+        loc = int(self.get_measure("ncloc", fallback="0"))
+        util.logger.info("Deleting %s, name '%s' with %d LoCs", str(self), self.name, loc)
+        ok = sqobject.delete_object(self, "projects/delete", {"project": self.key}, _OBJECTS[self.endpoint.url])
+        util.logger.info("Successfully deleted %s - %d LoCs", str(self), loc)
+        return ok
+
+    def has_binding(self):
+        """
+        :return: Whether the project has a DevOps platform binding
+        :rtype: bool
+        """
+        _ = self.binding()
+        return self._binding["has_binding"]
+
+    def binding(self):
+        """
+        :return: The project DevOps platform binding
+        :rtype: dict
+        """
+        if self._binding["has_binding"] and self._binding["binding"] is None:
+            try:
+                resp = self.get("alm_settings/get_binding", params={"project": self.key}, mute=(HTTPStatus.NOT_FOUND,))
+                self._binding["has_binding"] = True
+                self._binding["binding"] = json.loads(resp.text)
+            except HTTPError as e:
+                if e.response.status_code in (HTTPStatus.NOT_FOUND, HTTPStatus.BAD_REQUEST):
+                    # Hack: 8.9 returns 404, 9.x returns 400
+                    self._binding["has_binding"] = False
+                else:
+                    util.exit_fatal(
+                        f"alm_settings/get_binding returning status code {e.response.status_code}, exiting",
+                        options.ERR_SONAR_API,
+                    )
+        return self._binding["binding"]
+
+    def is_part_of_monorepo(self):
+        """
+        :return: From the DevOps binding, Whether the project is part of a monorepo
+        :rtype: bool
+        """
+        if self.binding() is None:
+            return False
+        return self.binding()["monorepo"]
+
+    def binding_key(self):
+        """Computes a unique project binding key
+
+        :meta private:
+        """
+        p_bind = self.binding()
+        if p_bind is None:
+            return None
+        key = p_bind["alm"] + _BIND_SEP + p_bind["repository"]
+        if p_bind["alm"] in ("azure", "bitbucket"):
+            key += _BIND_SEP + p_bind["slug"]
+        return key
+
+    def __audit_last_analysis(self, audit_settings):
+        """Audits whether the last analysis of the project is too old or not
+
+        :param audit_settings: Settings (thresholds) to raise problems
+        :type audit_settings: dict
+        :return: List of problems found, or empty list
+        :rtype: list[Problem]
+        """
+        util.logger.debug("Auditing %s last analysis date", str(self))
+        problems = []
+        age = util.age(self.last_analysis(include_branches=True), True)
+        if age is None:
+            if not audit_settings.get("audit.projects.neverAnalyzed", True):
+                util.logger.debug("Auditing of never analyzed projects is disabled, skipping")
+            else:
+                rule = rules.get_rule(rules.RuleId.PROJ_NOT_ANALYZED)
+                msg = rule.msg.format(str(self))
+                problems.append(pb.Problem(rule.type, rule.severity, msg, concerned_object=self))
+            return problems
+
+        max_age = audit_settings.get("audit.projects.maxLastAnalysisAge", 180)
+        if max_age == 0:
+            util.logger.debug("Auditing of projects with old analysis date is disabled, skipping")
+        elif age > max_age:
+            rule = rules.get_rule(rules.RuleId.PROJ_LAST_ANALYSIS)
+            severity = severities.Severity.HIGH if age > 365 else rule.severity
+            loc = self.get_measure("ncloc", fallback="0")
+            msg = rule.msg.format(str(self), loc, age)
+            problems.append(pb.Problem(rule.type, severity, msg, concerned_object=self))
+
+        util.logger.debug("%s last analysis is %d days old", str(self), age)
+        return problems
+
+    def __audit_branches(self, audit_settings):
+        """Audits project branches
+
+        :param audit_settings: Settings (thresholds) to raise problems
+        :type audit_settings: dict
+        :return: List of problems found, or empty list
+        :rtype: list[Problem]
+        """
+        if not audit_settings.get(_AUDIT_BRANCHES_PARAM, True):
+            util.logger.debug("Auditing of branchs is disabled, skipping...")
+            return []
+        util.logger.debug("Auditing %s branches", str(self))
+        problems = []
+        main_br_count = 0
+        for branch in self.branches().values():
+            problems += branch.audit(audit_settings)
+            if branch.name in ("main", "master"):
+                main_br_count += 1
+                if main_br_count > 1:
+                    rule = rules.get_rule(rules.RuleId.PROJ_MAIN_AND_MASTER)
+                    problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(str(self)), concerned_object=self))
+        return problems
+
+    def __audit_pull_requests(self, audit_settings):
+        """Audits project pul requests
+
+        :param audit_settings: Settings (thresholds) to raise problems
+        :type audit_settings: dict
+        :return: List of problems found, or empty list
+        :rtype: list[Problem]
+        """
+        max_age = audit_settings.get("audit.projects.pullRequests.maxLastAnalysisAge", 30)
+        if max_age == 0:
+            util.logger.debug("Auditing of pull request last analysis age is disabled, skipping...")
+            return []
+        problems = []
+        for pr in self.pull_requests().values():
+            problems += pr.audit(audit_settings)
+        return problems
+
+    def __audit_visibility(self, audit_settings):
+        """Audits project visibility and return problems if project is public
+
+        :param audit_settings: Options and Settings (thresholds) to raise problems
+        :type audit_settings: dict
+        :return: List of problems found, or empty list
+        :rtype: list[Problem]
+        """
+        if not audit_settings.get("audit.projects.visibility", True):
+            util.logger.debug("Project visibility audit is disabled by configuration, skipping...")
+            return []
+        util.logger.debug("Auditing %s visibility", str(self))
+        visi = self.visibility()
+        if visi != "private":
+            rule = rules.get_rule(rules.RuleId.PROJ_VISIBILITY)
+            return [pb.Problem(rule.type, rule.severity, rule.msg.format(str(self), visi), concerned_object=self)]
+        util.logger.debug("%s visibility is 'private'", str(self))
+        return []
+
+    def __audit_languages(self, audit_settings):
+        """Audits project utility languages and returns problems if too many LoCs of these
+
+        :param audit_settings: Settings (thresholds) to raise problems
+        :type audit_settings: dict
+        :return: List of problems found, or empty list
+        :rtype: list[Problem]
+        """
+        if not audit_settings.get("audit.projects.utilityLocs", False):
+            util.logger.debug("Utility LoCs audit disabled by configuration, skipping")
+            return []
+        util.logger.debug("Auditing %s utility LoC count", str(self))
+
+        total_locs = 0
+        languages = {}
+        resp = self.get_measure("ncloc_language_distribution")
+        if resp is None:
+            return []
+        for lang in self.get_measure("ncloc_language_distribution").split(";"):
+            (lang, ncloc) = lang.split("=")
+            languages[lang] = int(ncloc)
+            total_locs += int(ncloc)
+        utility_locs = sum(lcount for lang, lcount in languages.items() if lang in ("xml", "json"))
+        if total_locs > 100000 and (utility_locs / total_locs) > 0.5:
+            rule = rules.get_rule(rules.RuleId.PROJ_UTILITY_LOCS)
+            return [pb.Problem(rule.type, rule.severity, rule.msg.format(str(self), utility_locs), concerned_object=self)]
+        util.logger.debug("%s utility LoCs count (%d) seems reasonable", str(self), utility_locs)
+        return []
+
+    def __audit_zero_loc(self, audit_settings):
+        """Audits project utility projects with 0 LoCs
+
+        :param audit_settings: Settings (thresholds) to raise problems
+        :type audit_settings: dict
+        :return: List of problems found, or empty list
+        :rtype: list[Problem]
+        """
+        if (
+            (not audit_settings.get(_AUDIT_BRANCHES_PARAM, True) or self.endpoint.edition() == "community")
+            and self.last_analysis() is not None
+            and self.loc() == 0
+        ):
+            rule = rules.get_rule(rules.RuleId.PROJ_ZERO_LOC)
+            return [pb.Problem(rule.type, rule.severity, rule.msg.format(str(self)), concerned_object=self)]
+        return []
+
+    def __audit_binding_valid(self, audit_settings):
+        if self.endpoint.edition() == "community":
+            util.logger.info("Community edition, skipping binding validation...")
+            return []
+        elif not audit_settings.get("audit.projects.bindings", True):
+            util.logger.info(
+                "%s binding validation disabled, skipped",
+                str(self),
+            )
+        elif not self.has_binding():
+            util.logger.info(
+                "%s has no binding, skipping binding validation...",
+                str(self),
+            )
+            return []
+        try:
+            _ = self.get("alm_settings/validate_binding", params={"project": self.key})
+            util.logger.debug("%s binding is valid", str(self))
+            return []
+        except HTTPError as e:
+            # Hack: 8.9 returns 404, 9.x returns 400
+            if e.response.status_code in (HTTPStatus.BAD_REQUEST, HTTPStatus.NOT_FOUND):
+                rule = rules.get_rule(rules.RuleId.PROJ_INVALID_BINDING)
+                return [pb.Problem(rule.type, rule.severity, rule.msg.format(str(self)), concerned_object=self)]
+            else:
+                util.exit_fatal(f"alm_settings/validate_binding returning status code {e.response.status_code}, exiting", options.ERR_SONAR_API)
+
+    def audit(self, audit_settings):
+        """Audits a project and returns the list of problems found
+
+        :param dict audit_settings: Options of what to audit and thresholds to raise problems
+        :return: List of problems found, or empty list
+        :rtype: list[Problem]
+        """
+        util.logger.debug("Auditing %s", str(self))
+        return (
+            self.__audit_last_analysis(audit_settings)
+            + self.__audit_branches(audit_settings)
+            + self.__audit_pull_requests(audit_settings)
+            + self.__audit_visibility(audit_settings)
+            + self.__audit_languages(audit_settings)
+            + self.permissions().audit(audit_settings)
+            + self._audit_bg_task(audit_settings)
+            + self.__audit_binding_valid(audit_settings)
+            + self.__audit_zero_loc(audit_settings)
+        )
+
+    def export_zip(self, timeout=180):
+        """Exports project as zip file, synchronously
+
+        :param timeout: timeout in seconds to complete the export operation
+        :type timeout: int
+        :return: export status (success/failure/timeout), and zip file path
+        :rtype: dict
+        """
+        util.logger.info("Exporting %s (synchronously)", str(self))
+        if self.endpoint.version() < (9, 2, 0) and self.endpoint.edition() not in ("enterprise", "datacenter"):
+            raise exceptions.UnsupportedOperation(
+                "Project export is only available with Enterprise and Datacenter Edition, or with SonarQube 9.2 or higher for any Edition"
+            )
+        try:
+            resp = self.post("project_dump/export", params={"key": self.key})
+        except HTTPError as e:
+            return {"status": f"HTTP_ERROR {e.response.status_code}"}
+        data = json.loads(resp.text)
+        status = tasks.Task(data["taskId"], endpoint=self.endpoint, concerned_object=self, data=data).wait_for_completion(timeout=timeout)
+        if status != tasks.SUCCESS:
+            util.logger.error("%s export %s", str(self), status)
+            return {"status": status}
+        dump_file = json.loads(self.get("project_dump/status", params={"key": self.key}).text)["exportedDump"]
+        util.logger.debug("%s export %s, dump file %s", str(self), status, dump_file)
+        return {"status": status, "file": dump_file}
+
+    def export_async(self):
+        """Export project as zip file, synchronously
+
+        :return: export taskId or None if starting the export failed
+        :rtype: str or None
+        """
+        util.logger.info("Exporting %s (asynchronously)", str(self))
+        try:
+            return json.loads(self.post("project_dump/export", params={"key": self.key}).text)["taskId"]
+        except HTTPError:
+            return None
+
+    def import_zip(self):
+        """Imports a project zip file in SonarQube
+
+        :raises http.HTTPError:
+        :return: Whether the operation succeeded
+        :rtype: bool
+        """
+        util.logger.info("Importing %s (asynchronously)", str(self))
+        if self.endpoint.edition() not in ("enterprise", "datacenter"):
+            raise exceptions.UnsupportedOperation("Project import is only available with Enterprise and Datacenter Edition")
+        return self.post("project_dump/import", params={"key": self.key}).ok
+
+    def get_findings(self, branch=None, pr=None):
+        """Returns a project list of findings (issues and hotspots)
+
+        :param branch: branch name to consider, if any
+        :type branch: str, optional
+        :param pr: PR key to consider, if any
+        :type pr: str, optional
+        :return: JSON of all findings, with finding key as key
+        :rtype: dict{key: Finding}
+        """
+        if self.endpoint.version() < (9, 1, 0) or self.endpoint.edition() not in ("enterprise", "datacenter"):
+            util.logger.warning("export_findings only available in EE and DCE starting from SonarQube 9.1, returning no issues")
+            return {}
+        util.logger.info("Exporting findings for %s", str(self))
+        findings_list = {}
+        params = {"project": self.key}
+        if branch is not None:
+            params["branch"] = branch
+        elif pr is not None:
+            params["pullRequest"] = pr
+
+        data = json.loads(self.get("projects/export_findings", params=params).text)["export_findings"]
+        findings_conflicts = {"SECURITY_HOTSPOT": 0, "BUG": 0, "CODE_SMELL": 0, "VULNERABILITY": 0}
+        nbr_findings = {"SECURITY_HOTSPOT": 0, "BUG": 0, "CODE_SMELL": 0, "VULNERABILITY": 0}
+        util.logger.debug(util.json_dump(data))
+        for i in data:
+            key = i["key"]
+            if key in findings_list:
+                util.logger.warning("Finding %s (%s) already in past findings", i["key"], i["type"])
+                findings_conflicts[i["type"]] += 1
+            # FIXME - Hack for wrong projectKey returned in PR
+            # m = re.search(r"(\w+):PULL_REQUEST:(\w+)", i['projectKey'])
+            i["projectKey"] = self.key
+            i["branch"] = branch
+            i["pullRequest"] = pr
+            nbr_findings[i["type"]] += 1
+            if i["type"] == "SECURITY_HOTSPOT":
+                findings_list[key] = hotspots.get_object(key, endpoint=self.endpoint, data=i, from_export=True)
+            else:
+                findings_list[key] = issues.get_object(key, endpoint=self.endpoint, data=i, from_export=True)
+        for t in ("SECURITY_HOTSPOT", "BUG", "CODE_SMELL", "VULNERABILITY"):
+            if findings_conflicts[t] > 0:
+                util.logger.warning("%d %s findings missed because of JSON conflict", findings_conflicts[t], t)
+        util.logger.info("%d findings exported for %s branch %s PR %s", len(findings_list), str(self), branch, pr)
+        for t in ("SECURITY_HOTSPOT", "BUG", "CODE_SMELL", "VULNERABILITY"):
+            util.logger.info("%d %s exported", nbr_findings[t], t)
+
+        return findings_list
+
+    def get_hotspots(self):
+        """Returns a project main branch list of hotspots
+
+        :return: dict of Hotspots, with hotspot key as key
+        :rtype: dict{key: Hotspot}
+        """
+        return hotspots.search(
+            endpoint=self.endpoint,
+            params={
+                "projectKey": self.key,
+                "additionalFields": "comments",
+            },
+        )
+
+    def sync(self, another_project, sync_settings):
+        """Syncs project issues with another project
+
+        :param Project another_project: other project to sync issues into
+        :type another_project:
+        :param dict sync_settings: Parameters to configure the sync
+        :return: sync report as tuple, with counts of successful and unsuccessful issue syncs
+        :rtype: tuple(report, counters)
+        """
+        if self.endpoint.edition() == "community":
+            report, counters = [], {}
+            util.logger.info("Syncing %s issues", str(self))
+            (report, counters) = syncer.sync_lists(
+                self.get_issues(),
+                another_project.get_issues(),
+                self,
+                another_project,
+                sync_settings=sync_settings,
+            )
+            util.logger.info("Syncing %s issues", str(self))
+            (tmp_report, tmp_counts) = syncer.sync_lists(
+                self.get_hotspots(),
+                another_project.get_hotspots(),
+                self,
+                another_project,
+                sync_settings=sync_settings,
+            )
+            report += tmp_report
+            counters = util.dict_add(counters, tmp_counts)
+        else:
+            tgt_branches = another_project.branches().values()
+            report = []
+            counters = {}
+            for b_src in self.branches().values():
+                for b_tgt in tgt_branches:
+                    if b_src.name == b_tgt.name:
+                        (tmp_report, tmp_counts) = b_src.sync(b_tgt, sync_settings=sync_settings)
+                        report += tmp_report
+                        counters = util.dict_add(counters, tmp_counts)
+        return (report, counters)
+
+    def sync_branches(self, sync_settings):
+        """Syncs project issues across all its branches
+
+        :param dict sync_settings: Parameters to configure the sync
+        :return: sync report as tuple, with counts of successful and unsuccessful issue syncs
+        :rtype: tuple(report, counters)
+        """
+        my_branches = self.branches()
+        report = []
+        counters = {}
+        for b_src in my_branches.values():
+            for b_tgt in my_branches.values():
+                if b_src.name == b_tgt.name:
+                    continue
+                (tmp_report, tmp_counts) = b_src.sync(b_tgt, sync_settings=sync_settings)
+                report += tmp_report
+                counters = util.dict_add(counters, tmp_counts)
+        return (report, counters)
+
+    def quality_profiles(self):
+        """Returns the project quality profiles
+
+        :return: dict of quality profiles indexed by language
+        :rtype: dict{language: QualityProfile}
+        """
+        util.logger.debug("Getting %s quality profiles", str(self))
+        qp_list = qualityprofiles.get_list(self.endpoint)
+        return {qp.language: qp for qp in qp_list.values() if qp.used_by_project(self)}
+
+    def quality_gate(self):
+        """Returns the project quality gate
+
+        :return: name of quality gate and whether it's the default
+        :rtype: tuple(name, is_default)
+        """
+        data = json.loads(self.get(api="qualitygates/get_by_project", params={"project": self.key}).text)
+        return (data["qualityGate"]["name"], data["qualityGate"]["default"])
+
+    def webhooks(self):
+        """
+        :return: Project webhooks indexed by their key
+        :rtype: dict{key: WebHook}
+        """
+        util.logger.debug("Getting %s webhooks", str(self))
+        return webhooks.get_list(endpoint=self.endpoint, project_key=self.key)
+
+    def links(self):
+        """
+        :return: list of project links
+        :rtype: list[{type, name, url}]
+        """
+        data = json.loads(self.get(api="project_links/search", params={"projectKey": self.key}).text)
+        link_list = None
+        for link in data["links"]:
+            if link_list is None:
+                link_list = []
+            link_list.append({"type": link["type"], "name": link.get("name", link["type"]), "url": link["url"]})
+        return link_list
+
+    def __export_get_binding(self):
+        binding = self.binding()
+        if binding:
+            # Remove redundant fields
+            binding.pop("alm", None)
+            binding.pop("url", None)
+            if not binding["monorepo"]:
+                binding.pop("monorepo")
+        return binding
+
+    def __export_get_qp(self):
+        qp_json = {qp.language: f"{qp.name}" for qp in self.quality_profiles().values()}
+        if len(qp_json) == 0:
+            return None
+        return qp_json
+
+    def __get_branch_export(self):
+        branch_data = {}
+        my_branches = self.branches().values()
+        for branch in my_branches:
+            exp = branch.export(full_export=False)
+            if len(my_branches) == 1 and branch.is_main() and len(exp) <= 1:
+                # Don't export main branch with no data
+                continue
+            branch_data[branch.name] = exp
+        # If there is only 1 branch with no specific config except being main, don't return anything
+        if len(branch_data) == 0 or (len(branch_data) == 1 and len(exp) <= 1):
+            return None
+        return util.remove_nones(branch_data)
+
+    def export(self, settings_list=None, include_inherited=False, full=False):
+        """Exports the entire project configuration as JSON
+
+        :return: All project configuration settings
+        :rtype: dict
+        """
+        util.logger.info("Exporting %s", str(self))
+        json_data = self._json.copy()
+        json_data.update({"key": self.key, "name": self.name})
+        json_data["binding"] = self.__export_get_binding()
+        nc = self.new_code()
+        if nc != "":
+            json_data[settings.NEW_CODE_PERIOD] = nc
+        json_data["qualityProfiles"] = self.__export_get_qp()
+        json_data["links"] = self.links()
+        json_data["permissions"] = self.permissions().to_json(csv=True)
+        json_data["branches"] = self.__get_branch_export()
+        json_data["tags"] = util.list_to_csv(self.tags(), separator=", ")
+        json_data["visibility"] = self.visibility()
+        (json_data["qualityGate"], qg_is_default) = self.quality_gate()
+        if qg_is_default:
+            json_data.pop("qualityGate")
+
+        json_data["webhooks"] = webhooks.export(self.endpoint, self.key)
+        json_data = util.filter_export(json_data, _IMPORTABLE_PROPERTIES, full)
+        settings_dict = settings.get_bulk(endpoint=self, component=self, settings_list=settings_list, include_not_set=False)
+        # json_data.update({s.to_json() for s in settings_dict.values() if include_inherited or not s.inherited})
+        for s in settings_dict.values():
+            if not include_inherited and s.inherited:
+                continue
+            json_data.update(s.to_json())
+        return util.remove_nones(json_data)
+
+    def new_code(self):
+        """
+        :return: The project new code definition
+        :rtype: str
+        """
+        if self._new_code is None:
+            new_code = settings.Setting.read(settings.NEW_CODE_PERIOD, self.endpoint, component=self)
+            self._new_code = new_code.value if new_code else ""
+        return self._new_code
+
+    def permissions(self):
+        """
+        :return: The project permissions
+        :rtype: ProjectPermissions
+        """
+        if self._permissions is None:
+            self._permissions = pperms.ProjectPermissions(self)
+        return self._permissions
+
+    def set_permissions(self, desired_permissions):
+        """Sets project permissions
+
+        :param desired_permissions: dict describing permissions
+        :type desired_permissions: dict
+        :return: Nothing
+        """
+        self.permissions().set(desired_permissions)
+
+    def set_links(self, desired_links):
+        """Sets project links
+
+        :param desired_links: dict describing links
+        :type desired_links: dict
+        :return: Whether the operation was successful
+        """
+        params = {"projectKey": self.key}
+        ok = True
+        for link in desired_links.get("links", {}):
+            if link.get("type", "") != "custom":
+                continue
+            params.update(link)
+            ok = ok and self.post("project_links/create", params=params).ok
+
+    def set_tags(self, tags):
+        """Sets project tags
+
+        :param list tags: list of tags
+        :return: Whether the operation was successful
+        """
+        if tags is None:
+            return
+        my_tags = util.list_to_csv(tags) if isinstance(tags, list) else util.csv_normalize(tags)
+        r = self.post("project_tags/set", params={"project": self.key, "tags": my_tags})
+        self._tags = util.csv_to_list(my_tags)
+        return r.ok
+
+    def set_quality_gate(self, quality_gate):
+        """Sets project quality gate
+
+        :param quality_gate: quality gate name
+        :type quality_gate: str
+        :return: Whether the operation was successful
+        :rtype: bool
+        """
+        if quality_gate is None:
+            return False
+        try:
+            _ = qualitygates.QualityGate.get_object(self.endpoint, quality_gate)
+        except exceptions.ObjectNotFound:
+            util.logger.warning("Quality gate '%s' not found, can't set it for %s", quality_gate, str(self))
+            return False
+        util.logger.debug("Setting quality gate '%s' for %s", quality_gate, str(self))
+        r = self.post("qualitygates/select", params={"projectKey": self.key, "gateName": quality_gate})
+        return r.ok
+
+    def set_quality_profile(self, language, quality_profile):
+        """Sets project quality profile for a given language
+
+        :param language: Language mnemonic, following SonarQube convention
+        :type language: str
+        :param quality_profile: Name of the quality profile in the language
+        :type quality_profile: str
+        :return: Whether the operation was successful
+        :rtype: bool
+        """
+        if not qualityprofiles.exists(endpoint=self.endpoint, language=language, name=quality_profile):
+            util.logger.warning("Quality profile '%s' in language '%s' does not exist, can't set it for %s", quality_profile, language, str(self))
+            return False
+        util.logger.debug("Setting quality profile '%s' of language '%s' for %s", quality_profile, language, str(self))
+        r = self.post("qualityprofiles/add_project", params={"project": self.key, "qualityProfile": quality_profile, "language": language})
+        return r.ok
+
+    def rename_main_branch(self, main_branch_name):
+        """Renames the project main branch
+
+        :param main_branch_name: New main branch name
+        :type main_branch_name: str
+        :return: Whether the operation was successful
+        :rtype: bool
+        """
+        br = self.main_branch()
+        if br:
+            return br.rename(main_branch_name)
+        util.logger.warning("No main branch to rename found for %s", str(self))
+        return False
+
+    def set_webhooks(self, webhook_data):
+        """Sets project webhooks
+
+        :param dict webhook_data: JSON describing the webhooks
+        :return: Nothing
+        """
+        current_wh = self.webhooks()
+        current_wh_names = [wh.name for wh in current_wh.values()]
+        wh_map = {wh.name: k for k, wh in current_wh.items()}
+        # FIXME: Handle several webhooks with same name
+        for wh_name, wh in webhook_data.items():
+            if wh_name in current_wh_names:
+                current_wh[wh_map[wh_name]].update(name=wh_name, **wh)
+            else:
+                webhooks.update(name=wh_name, endpoint=self.endpoint, project=self.key, **wh)
+
+    def set_settings(self, data):
+        """Sets project settings (webhooks, settings, new code period)
+
+        :param dict data: JSON describing the settings
+        :return: Nothing
+        """
+        util.logger.debug("Setting %s settings with %s", str(self), util.json_dump(data))
+        for key, value in data.items():
+            if key in ("branches", settings.NEW_CODE_PERIOD):
+                continue
+            if key == "webhooks":
+                self.set_webhooks(value)
+            else:
+                settings.set_setting(endpoint=self.endpoint, key=key, value=value, component=self)
+
+        nc = data.get(settings.NEW_CODE_PERIOD, None)
+        if nc is not None:
+            (nc_type, nc_val) = settings.decode(settings.NEW_CODE_PERIOD, nc)
+            settings.set_new_code_period(self.endpoint, nc_type, nc_val, project_key=self.key)
+        # TODO: Update branches (main, new code definition, keepWhenInactive)
+        # util.logger.debug("Checking main branch")
+        # for branch, branch_data in data.get("branches", {}).items():
+        #    if branches.exists(branch_name=branch, project_key=self.key, endpoint=self.endpoint):
+        #        branches.get_object(branch, self, endpoint=self.endpoint).update(branch_data)()
+
+    def set_devops_binding(self, data):
+        """Sets project devops binding settings
+
+        :param dict data: JSON describing the devops binding
+        :return: Nothing
+        """
+        util.logger.debug("Setting devops binding of %s to %s", str(self), util.json_dump(data))
+        alm_key = data["key"]
+        if not devops.platform_exists(alm_key, self.endpoint):
+            util.logger.warning("DevOps platform '%s' does not exists, can't set it for %s", alm_key, str(self))
+            return False
+        alm_type = devops.devops_type(platform_key=alm_key, endpoint=self.endpoint)
+        mono = data.get("monorepo", False)
+        repo = data["repository"]
+        if alm_type == "github":
+            self.set_binding_github(alm_key, repository=repo, monorepo=mono, summary_comment=data.get("summaryComment", True))
+        elif alm_type == "gitlab":
+            self.set_binding_gitlab(alm_key, repository=repo, monorepo=mono)
+        elif alm_type == "azure":
+            self.set_binding_azure_devops(alm_key, repository=repo, monorepo=mono, slug=data["slug"])
+        elif alm_type == "bitbucket":
+            self.set_binding_bitbucket_server(alm_key, repository=repo, monorepo=mono, slug=data["slug"])
+        elif alm_type == "bitbucketcloud":
+            self.set_binding_bitbucket_cloud(alm_key, repository=repo, monorepo=mono)
+        else:
+            util.logger.error("Invalid devops platform type '%s' for %s, setting skipped", alm_key, str(self))
+            return False
+        return True
+
+    def __std_binding_params(self, alm_key, repo, monorepo):
+        return {"almSetting": alm_key, "project": self.key, "repository": repo, "monorepo": str(monorepo).lower()}
+
+    def set_binding_github(self, devops_platform_key, repository, monorepo=False, summary_comment=True):
+        """Sets project devops binding for github
+
+        :param str devops_platform_key: key of the platform in the global admin devops configuration
+        :param str repository: project repository name in github
+        :param monorepo: Whether the project is part of a monorepo, defaults to False
+        :type monorepo: bool, optional
+        :param summary_comment: Whether summary comments should be posted, defaults to True
+        :type summary_comment: bool, optional
+        :return: Nothing
+        """
+        params = self.__std_binding_params(devops_platform_key, repository, monorepo)
+        params["summaryCommentEnabled"] = str(summary_comment).lower()
+        self.post("alm_settings/set_github_binding", params=params)
+
+    def set_binding_gitlab(self, devops_platform_key, repository, monorepo=False):
+        """Sets project devops binding for gitlab
+
+        :param str devops_platform_key: key of the platform in the global admin devops configuration
+        :param str repository: project repository name in gitlab
+        :param monorepo: Whether the project is part of a monorepo, defaults to False
+        :type monorepo: bool, optional
+        :return: Nothing
+        """
+        params = self.__std_binding_params(devops_platform_key, repository, monorepo)
+        self.post("alm_settings/set_gitlab_binding", params=params)
+
+    def set_binding_bitbucket_server(self, devops_platform_key, repository, slug, monorepo=False):
+        """Sets project devops binding for bitbucket server
+
+        :param str devops_platform_key: key of the platform in the global admin devops configuration
+        :param str repository: project repository name in bitbucket server
+        :param str slug: project repository SLUG
+        :param monorepo: Whether the project is part of a monorepo, defaults to False
+        :type monorepo: bool, optional
+        :return: Nothing
+        """
+        params = self.__std_binding_params(devops_platform_key, repository, monorepo)
+        params["slug"] = slug
+        self.post("alm_settings/set_bitbucket_binding", params=params)
+
+    def set_binding_bitbucket_cloud(self, devops_platform_key, repository, monorepo=False):
+        """Sets project devops binding for bitbucket cloud
+
+        :param str devops_platform_key: key of the platform in the global admin devops configuration
+        :param str repository: project repository name in bitbucket cloud
+        :param str slug: project repository SLUG
+        :param monorepo: Whether the project is part of a monorepo, defaults to False
+        :type monorepo: bool, optional
+        :return: Nothing
+        """
+        params = self.__std_binding_params(devops_platform_key, repository, monorepo)
+        self.post("alm_settings/set_bitbucketcloud_binding", params=params)
+
+    def set_binding_azure_devops(self, devops_platform_key, slug, repository, monorepo=False):
+        """Sets project devops binding for azure devops
+
+        :param str devops_platform_key: key of the platform in the global admin devops configuration
+        :param str slug: project SLUG in Azure DevOps
+        :param str repository: project repository name in azure devops
+        :param monorepo: Whether the project is part of a monorepo, defaults to False
+        :type monorepo: bool, optional
+        :return: Nothing
+        """
+        params = self.__std_binding_params(devops_platform_key, repository, monorepo)
+        params["projectName"] = slug
+        params["repositoryName"] = params.pop("repository")
+        self.post("alm_settings/set_azure_binding", params=params)
+
+    def update(self, data):
+        """Updates a project with a whole configuration set
+
+        :param dict data: JSON of configuration settings
+        :return: Nothing
+        """
+        if "permissions" in data:
+            decoded_perms = {}
+            for ptype in perms.PERMISSION_TYPES:
+                if ptype not in data["permissions"]:
+                    continue
+                decoded_perms[ptype] = {u: perms.decode(v) for u, v in data["permissions"][ptype].items()}
+            self.set_permissions(decoded_perms)
+        self.set_links(data)
+        self.set_tags(data.get("tags", None))
+        self.set_quality_gate(data.get("qualityGate", None))
+        for lang, qp_name in data.get("qualityProfiles", {}).items():
+            self.set_quality_profile(language=lang, quality_profile=qp_name)
+        for bname, bdata in data.get("branches", {}).items():
+            if bdata.get("isMain", False):
+                self.rename_main_branch(bname)
+                break
+        if "binding" in data:
+            self.set_devops_binding(data["binding"])
+        else:
+            util.logger.debug("%s has no devops binding, skipped", str(self))
+        settings_to_apply = {
+            k: v for k, v in data.items() if k not in ("permissions", "tags", "links", "qualityGate", "qualityProfiles", "binding", "name")
+        }
+        # TODO: Set branch settings
+        self.set_settings(settings_to_apply)
+
+    def search_params(self):
+        """Return params used to search for that object
+
+        :meta private:
+        """
+        return {"project": self.key}
+
+
+def count(endpoint, params=None):
+    """Counts projects
+
+    :param params: list of parameters to filter projects to search
+    :type params: dict
+    :return: Count of projects
+    :rtype: int
+    """
+    new_params = {} if params is None else params.copy()
+    new_params.update({"ps": 1, "p": 1})
+    data = json.loads(endpoint.get(_SEARCH_API, params=params).text)
+    return data["paging"]["total"]
+
+
+def search(endpoint, params=None):
+    """Searches projects in SonarQube
+
+    :param endpoint: Reference to the SonarQube platform
+    :type endpoint: Platform
+    :param params: list of parameters to narrow down the search
+    :type params: dict
+    :return: list of projects
+    :rtype: dict{key: Project}
+    """
+    new_params = {} if params is None else params.copy()
+    new_params["qualifiers"] = "TRK"
+    return sqobject.search_objects(
+        api="projects/search",
+        params=new_params,
+        key_field="key",
+        returned_field="components",
+        endpoint=endpoint,
+        object_class=Project,
+    )
+
+
+def get_list(endpoint, key_list=None, use_cache=True):
+    """
+    :param endpoint: Reference to the SonarQube platform
+    :type endpoint: Platform
+    :param key_list: List of portfolios keys to get, if None or empty all portfolios are returned
+    :param use_cache: Whether to use local cache or query SonarQube, default True (use cache)
+    :type use_cache: bool
+    :return: the list of all quality profiles
+    :rtype: dict{key: QualityProfile}
+    """
+    with _CLASS_LOCK:
+        if key_list is None or len(key_list) == 0 or not use_cache:
+            util.logger.info("Listing projects")
+            return search(endpoint=endpoint)
+    return {key: Project.get_object(endpoint, key) for key in util.csv_to_list(key_list)}
+
+
+def __audit_thread(queue, results, audit_settings, bindings):
+    audit_bindings = audit_settings.get("audit.projects.bindings", True)
+    while not queue.empty():
+        util.logger.debug("Picking from the queue")
+        project = queue.get()
+        results += project.audit(audit_settings)
+        if project.endpoint.edition() == "community" or not audit_bindings or project.is_part_of_monorepo():
+            queue.task_done()
+            util.logger.debug("%s audit done", str(project))
+            continue
+        bindkey = project.binding_key()
+        if bindkey and bindkey in bindings:
+            rule = rules.get_rule(rules.RuleId.PROJ_DUPLICATE_BINDING)
+            results.append(pb.Problem(rule.type, rule.severity, rule.msg.format(str(project), str(bindings[bindkey])), concerned_object=project))
+        else:
+            bindings[bindkey] = project
+        queue.task_done()
+        util.logger.debug("%s audit done", str(project))
+    util.logger.debug("Queue empty, exiting thread")
+
+
+def audit(endpoint, audit_settings, key_list=None):
+    """Audits all or a list of projects
+
+    :param endpoint: reference to the SonarQube platform
+    :type endpoint: Platform
+    :param audit_settings: Configuration of audit
+    :type audit_settings: dict
+    :param key_list: List of project keys to audit, defaults to None (all projects)
+    :type key_list: str, optional
+    :return: list of problems found
+    :rtype: list[Problem]
+    """
+    util.logger.info("--- Auditing projects ---")
+    plist = get_list(endpoint, key_list)
+    problems = []
+    q = Queue(maxsize=0)
+    for p in plist.values():
+        q.put(p)
+    bindings = {}
+    for i in range(audit_settings.get("threads", 1)):
+        util.logger.debug("Starting project audit thread %d", i)
+        worker = Thread(target=__audit_thread, args=(q, problems, audit_settings, bindings))
+        worker.setDaemon(True)
+        worker.setName(f"ProjectAudit{i}")
+        worker.start()
+    q.join()
+    if not audit_settings.get("audit.projects.duplicates", True):
+        util.logger.info("Project duplicates auditing was disabled by configuration")
+        return problems
+    for key, p in plist.items():
+        util.logger.debug("Auditing for potential duplicate projects")
+        for key2 in plist:
+            if key2 != key and re.match(key2, key):
+                rule = rules.get_rule(rules.RuleId.PROJ_DUPLICATE)
+                problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(str(p), key2), concerned_object=p))
+    return problems
+
+
+def __export_thread(queue, results, full):
+    while not queue.empty():
+        project = queue.get()
+        results[project.key] = project.export(full=full)
+        results[project.key].pop("key")
+        queue.task_done()
+
+
+def export(endpoint, key_list=None, full=False, threads=8):
+    """Exports all or a list of projects configuration as dict
+
+    :param endpoint: reference to the SonarQube platform
+    :type endpoint: Platform
+    :param key_list: List of project keys to export, defaults to None (all projects)
+    :type key_list: str
+    :param full: Whether to export all settings including those useless for re-import, defaults to False
+    :type full: bool, optional
+    :param threads: Number of parallel threads for export, defaults to 8
+    :type threads: int, optional
+    :return: list of projects
+    :rtype: dict{key: Project}
+    """
+    for qp in qualityprofiles.get_list(endpoint).values():
+        qp.projects()
+
+    q = Queue(maxsize=0)
+    for p in get_list(endpoint=endpoint, key_list=key_list).values():
+        q.put(p)
+    project_settings = {}
+    for i in range(threads):
+        util.logger.debug("Starting project export thread %d", i)
+        worker = Thread(target=__export_thread, args=(q, project_settings, full))
+        worker.setDaemon(True)
+        worker.setName(f"ProjectExport{i}")
+        worker.start()
+    q.join()
+    return project_settings
+
+
+def exists(key, endpoint):
+    """
+    :param key: project key to check
+    :type key: str
+    :param endpoint: reference to the SonarQube platform
+    :type endpoint: Platform
+    :return: whether the project exists
+    :rtype: bool
+    """
+    try:
+        Project.get_object(endpoint, key)
+        return True
+    except exceptions.ObjectNotFound:
+        return False
+
+
+def loc_csv_header(**kwargs):
+    arr = ["# Project Key"]
+    if kwargs[options.WITH_NAME]:
+        arr.append("Project name")
+    arr.append("LoC")
+    if kwargs[options.WITH_LAST_ANALYSIS]:
+        arr.append("Last analysis")
+    if kwargs[options.WITH_URL]:
+        arr.append("URL")
+    return arr
+
+
+def import_config(endpoint, config_data, key_list=None):
+    """Imports a configuration in SonarQube
+
+    :param endpoint: reference to the SonarQube platform
+    :type endpoint: Platform
+    :param config_data: the configuration to import
+    :type config_data: dict
+    :param key_list: List of project keys to be considered for the import, defaults to None (all projects)
+    :type key_list: str
+    :return: Nothing
+    """
+    if "projects" not in config_data:
+        util.logger.info("No projects to import")
+        return
+    util.logger.info("Importing projects")
+    get_list(endpoint=endpoint)
+    nb_projects = len(config_data["projects"])
+    i = 0
+    new_key_list = util.csv_to_list(key_list)
+    for key, data in config_data["projects"].items():
+        if new_key_list and key not in new_key_list:
+            continue
+        util.logger.info("Importing project key '%s'", key)
+        try:
+            o = Project.get_object(endpoint, key)
+        except exceptions.ObjectNotFound:
+            o = Project.create(endpoint, key, data["name"])
+        o.update(data)
+        i += 1
+        if i % 20 == 0 or i == nb_projects:
+            util.logger.info("Imported %d/%d projects (%d%%)", i, nb_projects, (i * 100 // nb_projects))
+
+
+def __export_zip_thread(queue, results, statuses, export_timeout):
+    while not queue.empty():
+        project = queue.get()
+        try:
+            dump = project.export_zip(timeout=export_timeout)
+        except exceptions.UnsupportedOperation:
+            queue.task_done()
+            util.exit_fatal("Zip export unsupported on your SonarQube version", options.ERR_UNSUPPORTED_OPERATION)
+        status = dump["status"]
+        statuses[status] = 1 if status not in statuses else statuses[status] + 1
+        data = {"key": project.key, "status": status}
+        if status == "SUCCESS":
+            data["file"] = os.path.basename(dump["file"])
+            data["path"] = dump["file"]
+        results.append(data)
+        util.logger.info("%s", ", ".join([f"{k}:{v}" for k, v in statuses.items()]))
+        queue.task_done()
+
+
+def export_zip(endpoint, key_list=None, threads=8, export_timeout=30):
+    """Export as zip all or a list of projects
+
+    :param endpoint: reference to the SonarQube platform
+    :type endpoint: Platform
+    :param key_list: List of project keys to export, defaults to None (all projects)
+    :type key_list: str, optional
+    :param threads: Number of parallel threads for export, defaults to 8
+    :type threads: int, optional
+    :param export_timeout: Tiemout to export the project, defaults to 30
+    :type export_timeout: int, optional
+    :return: list of exported projects and platform version
+    :rtype: dict
+    """
+    statuses, exports = {}, []
+    projects_list = get_list(endpoint, key_list)
+    nbr_projects = len(projects_list)
+    util.logger.info("Exporting %d projects to export", nbr_projects)
+    q = Queue(maxsize=0)
+    for p in projects_list.values():
+        q.put(p)
+    for i in range(threads):
+        util.logger.debug("Starting project export thread %d", i)
+        worker = Thread(target=__export_zip_thread, args=(q, exports, statuses, export_timeout))
+        worker.setDaemon(True)
+        worker.setName(f"ZipExport{i}")
+        worker.start()
+    q.join()
+
+    return {
+        "sonarqube_environment": {
+            "version": endpoint.version(digits=2, as_string=True),
+            "plugins": endpoint.plugins(),
+        },
+        "project_exports": exports,
+    }
```

## sonar/projects/pull_requests.py

```diff
@@ -1,124 +1,124 @@
-#
-# sonar-tools
-# Copyright (C) 2022-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-"""
-
-    Abstraction of the SonarQube "pull request" concept
-
-"""
-
-import json
-import requests.utils
-from sonar import measures, components, sqobject, exceptions
-import sonar.utilities as util
-from sonar.audit import rules, problem
-
-_OBJECTS = {}
-
-_UNSUPPORTED_IN_CE = "Pull requests not available in Community Edition"
-
-
-class PullRequest(components.Component):
-    def __init__(self, project, key, endpoint=None, data=None):
-        if endpoint is not None:
-            super().__init__(key, endpoint)
-        else:
-            super().__init__(key, project.endpoint)
-        self.project = project
-        self.json = data
-        self._last_analysis = None
-        _OBJECTS[self._uuid()] = self
-        util.logger.debug("Created object %s", str(self))
-
-    def __str__(self):
-        return f"pull request key '{self.key}' of {str(self.project)}"
-
-    def url(self):
-        return f"{self.endpoint.url}/dashboard?id={self.project.key}&pullRequest={requests.utils.quote(self.key)}"
-
-    def _uuid(self):
-        return _uuid(self.project.key, self.key)
-
-    def last_analysis(self):
-        if self._last_analysis is None and "analysisDate" in self.json:
-            self._last_analysis = util.string_to_date(self.json["analysisDate"])
-        return self._last_analysis
-
-    def get_measures(self, metrics_list):
-        util.logger.debug("self.endpoint = %s", str(self.endpoint))
-        m = measures.get(self, metrics_list)
-        if "ncloc" in m:
-            self.ncloc = 0 if not m["ncloc"].value else int(m["ncloc"].value)
-        return m
-
-    def delete(self):
-        return sqobject.delete_object(self, "project_pull_requests/delete", self.search_params(), _OBJECTS)
-
-    def audit(self, audit_settings):
-        age = util.age(self.last_analysis())
-        if age is None:  # Main branch not analyzed yet
-            return []
-        max_age = audit_settings["audit.projects.pullRequests.maxLastAnalysisAge"]
-        problems = []
-        if age > max_age:
-            rule = rules.get_rule(rules.RuleId.PULL_REQUEST_LAST_ANALYSIS)
-            problems.append(problem.Problem(rule.type, rule.severity, rule.msg.format(str(self), age), concerned_object=self))
-        else:
-            util.logger.debug("%s age is %d days", str(self), age)
-        return problems
-
-    def search_params(self):
-        """Return params used to search for that object
-
-        :meta private:
-        """
-        return {"project": self.project.key, "pullRequest": self.key}
-
-
-def _uuid(project_key, pull_request_key):
-    return f"{project_key} {pull_request_key}"
-
-
-def get_object(pull_request_key, project, data=None):
-    if project.endpoint.edition() == "community":
-        util.logger.debug("Pull requests not available in Community Edition")
-        return None
-    p_id = _uuid(project.key, pull_request_key)
-    if p_id not in _OBJECTS:
-        _ = PullRequest(project, pull_request_key, endpoint=project.endpoint, data=data)
-    return _OBJECTS[p_id]
-
-
-def get_list(project):
-    """Retrieves the list of pull requests of a project
-
-    :param Project project: Project to get PRs from
-    :raises UnsupportedOperation: PRs not supported in Community Edition
-    :return: List of project PRs
-    :rtype: dict{PR_ID: PullRequest}
-    """
-    if project.endpoint.edition() == "community":
-        util.logger.debug(_UNSUPPORTED_IN_CE)
-        raise exceptions.UnsupportedOperation(_UNSUPPORTED_IN_CE)
-
-    data = json.loads(project.get("project_pull_requests/list", params={"project": project.key}).text)
-    pr_list = {}
-    for pr in data["pullRequests"]:
-        pr_list[pr["key"]] = get_object(pr["key"], project, pr)
-    return pr_list
+#
+# sonar-tools
+# Copyright (C) 2022-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+"""
+
+    Abstraction of the SonarQube "pull request" concept
+
+"""
+
+import json
+import requests.utils
+from sonar import measures, components, sqobject, exceptions
+import sonar.utilities as util
+from sonar.audit import rules, problem
+
+_OBJECTS = {}
+
+_UNSUPPORTED_IN_CE = "Pull requests not available in Community Edition"
+
+
+class PullRequest(components.Component):
+    def __init__(self, project, key, endpoint=None, data=None):
+        if endpoint is not None:
+            super().__init__(key, endpoint)
+        else:
+            super().__init__(key, project.endpoint)
+        self.project = project
+        self.json = data
+        self._last_analysis = None
+        _OBJECTS[self._uuid()] = self
+        util.logger.debug("Created object %s", str(self))
+
+    def __str__(self):
+        return f"pull request key '{self.key}' of {str(self.project)}"
+
+    def url(self):
+        return f"{self.endpoint.url}/dashboard?id={self.project.key}&pullRequest={requests.utils.quote(self.key)}"
+
+    def _uuid(self):
+        return _uuid(self.project.key, self.key)
+
+    def last_analysis(self):
+        if self._last_analysis is None and "analysisDate" in self.json:
+            self._last_analysis = util.string_to_date(self.json["analysisDate"])
+        return self._last_analysis
+
+    def get_measures(self, metrics_list):
+        util.logger.debug("self.endpoint = %s", str(self.endpoint))
+        m = measures.get(self, metrics_list)
+        if "ncloc" in m:
+            self.ncloc = 0 if not m["ncloc"].value else int(m["ncloc"].value)
+        return m
+
+    def delete(self):
+        return sqobject.delete_object(self, "project_pull_requests/delete", self.search_params(), _OBJECTS)
+
+    def audit(self, audit_settings):
+        age = util.age(self.last_analysis())
+        if age is None:  # Main branch not analyzed yet
+            return []
+        max_age = audit_settings.get("audit.projects.pullRequests.maxLastAnalysisAge", 30)
+        problems = []
+        if age > max_age:
+            rule = rules.get_rule(rules.RuleId.PULL_REQUEST_LAST_ANALYSIS)
+            problems.append(problem.Problem(rule.type, rule.severity, rule.msg.format(str(self), age), concerned_object=self))
+        else:
+            util.logger.debug("%s age is %d days", str(self), age)
+        return problems
+
+    def search_params(self):
+        """Return params used to search for that object
+
+        :meta private:
+        """
+        return {"project": self.project.key, "pullRequest": self.key}
+
+
+def _uuid(project_key, pull_request_key):
+    return f"{project_key} {pull_request_key}"
+
+
+def get_object(pull_request_key, project, data=None):
+    if project.endpoint.edition() == "community":
+        util.logger.debug("Pull requests not available in Community Edition")
+        return None
+    p_id = _uuid(project.key, pull_request_key)
+    if p_id not in _OBJECTS:
+        _ = PullRequest(project, pull_request_key, endpoint=project.endpoint, data=data)
+    return _OBJECTS[p_id]
+
+
+def get_list(project):
+    """Retrieves the list of pull requests of a project
+
+    :param Project project: Project to get PRs from
+    :raises UnsupportedOperation: PRs not supported in Community Edition
+    :return: List of project PRs
+    :rtype: dict{PR_ID: PullRequest}
+    """
+    if project.endpoint.edition() == "community":
+        util.logger.debug(_UNSUPPORTED_IN_CE)
+        raise exceptions.UnsupportedOperation(_UNSUPPORTED_IN_CE)
+
+    data = json.loads(project.get("project_pull_requests/list", params={"project": project.key}).text)
+    pr_list = {}
+    for pr in data["pullRequests"]:
+        pr_list[pr["key"]] = get_object(pr["key"], project, pr)
+    return pr_list
```

## tools/__init__.py

 * *Ordering differences only*

```diff
@@ -1,19 +1,19 @@
-#
-# sonar-tools
-# Copyright (C) 2022-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
+#
+# sonar-tools
+# Copyright (C) 2022-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
```

## tools/audit.py

 * *Ordering differences only*

```diff
@@ -1,170 +1,170 @@
-#!/usr/bin/python3
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-"""
-
-    Audits a SonarQube platform
-
-"""
-import sys
-import datetime
-import json
-
-from sonar import platform, users, groups, version, qualityprofiles, qualitygates, sif, options, portfolios, applications, exceptions
-from sonar.projects import projects
-import sonar.utilities as util
-from sonar.audit import problem, config
-
-_ALL_AUDITABLE = [
-    options.WHAT_SETTINGS,
-    options.WHAT_USERS,
-    options.WHAT_GROUPS,
-    options.WHAT_GATES,
-    options.WHAT_PROFILES,
-    options.WHAT_PROJECTS,
-    options.WHAT_APPS,
-    options.WHAT_PORTFOLIOS,
-]
-
-
-def __deduct_format__(fmt, file):
-    if fmt is not None:
-        return fmt
-    if file is not None:
-        ext = file.split(".").pop(-1).lower()
-        if ext in ("csv", "json"):
-            return ext
-    return "csv"
-
-
-def _audit_sif(sysinfo, audit_settings):
-    util.logger.info("Auditing SIF file '%s'", sysinfo)
-    try:
-        with open(sysinfo, "r", encoding="utf-8") as f:
-            sysinfo = json.loads(f.read())
-    except json.decoder.JSONDecodeError:
-        util.logger.critical("File %s does not seem to be a legit JSON file", sysinfo)
-        raise
-    except FileNotFoundError:
-        util.logger.critical("File %s does not exist", sysinfo)
-        raise
-    except PermissionError:
-        util.logger.critical("No permission to open file %s", sysinfo)
-        raise
-    return sif.Sif(sysinfo).audit(audit_settings)
-
-
-def _audit_sq(sq, settings, what_to_audit=None, key_list=None):
-    problems = []
-    if options.WHAT_PROJECTS in what_to_audit:
-        problems += projects.audit(endpoint=sq, audit_settings=settings, key_list=key_list)
-    if options.WHAT_PROFILES in what_to_audit:
-        problems += qualityprofiles.audit(endpoint=sq, audit_settings=settings)
-    if options.WHAT_GATES in what_to_audit:
-        problems += qualitygates.audit(endpoint=sq, audit_settings=settings)
-    if options.WHAT_SETTINGS in what_to_audit:
-        problems += sq.audit(audit_settings=settings)
-    if options.WHAT_USERS in what_to_audit:
-        problems += users.audit(endpoint=sq, audit_settings=settings)
-    if options.WHAT_GROUPS in what_to_audit:
-        problems += groups.audit(endpoint=sq, audit_settings=settings)
-    if options.WHAT_PORTFOLIOS in what_to_audit:
-        problems += portfolios.audit(endpoint=sq, audit_settings=settings, key_list=key_list)
-    if options.WHAT_APPS in what_to_audit:
-        problems += applications.audit(endpoint=sq, audit_settings=settings, key_list=key_list)
-    return problems
-
-
-def __parser_args(desc):
-    parser = util.set_common_args(desc)
-    parser = util.set_key_arg(parser)
-    parser = util.set_output_file_args(parser)
-    parser = options.set_url_arg(parser)
-    parser = options.add_thread_arg(parser, "project audit")
-    parser = util.set_what(parser, what_list=_ALL_AUDITABLE, operation="audit")
-    parser.add_argument("--sif", required=False, help="SIF file to audit when auditing SIF")
-    parser.add_argument(
-        "--config",
-        required=False,
-        dest="config",
-        action="store_true",
-        help="Creates the $HOME/.sonar-audit.properties configuration file, if not already present or outputs to stdout if it already exist",
-    )
-    args = parser.parse_args()
-    if args.sif is None and args.config is None and args.token is None:
-        util.exit_fatal(
-            "Token is missing (Argument -t/--token) when not analyzing local SIF",
-            options.ERR_TOKEN_MISSING,
-        )
-    return args
-
-
-def main():
-    args = __parser_args("Audits a SonarQube platform or a SIF (Support Info File or System Info File)")
-    kwargs = vars(args)
-    sq = platform.Platform(some_url=args.url, some_token=args.token, cert_file=args.clientCert)
-    util.check_environment(kwargs)
-    util.logger.info("sonar-tools version %s", version.PACKAGE_VERSION)
-    start_time = datetime.datetime.today()
-
-    settings = config.load("sonar-audit")
-    settings["threads"] = kwargs["threads"]
-    if kwargs.get("config", False):
-        config.configure()
-        sys.exit(0)
-
-    if kwargs.get("sif", None) is not None:
-        err = options.ERR_SIF_AUDIT_ERROR
-        try:
-            problems = _audit_sif(kwargs["sif"], settings)
-        except json.decoder.JSONDecodeError:
-            util.exit_fatal(f"File {kwargs['sif']} does not seem to be a legit JSON file, aborting...", err)
-        except FileNotFoundError:
-            util.exit_fatal(f"File {kwargs['sif']} does not exist, aborting...", err)
-        except PermissionError:
-            util.exit_fatal(f"No permissiont to open file {kwargs['sif']}, aborting...", err)
-        except sif.NotSystemInfo:
-            util.exit_fatal(f"File {kwargs['sif']} does not seem to be a system info or support info file, aborting...", err)
-    else:
-        util.check_token(args.token)
-        key_list = util.csv_to_list(args.projectKeys)
-        if len(key_list) > 0 and "projects" in util.csv_to_list(args.what):
-            for key in key_list:
-                if not projects.exists(key, sq):
-                    util.exit_fatal(f"Project key '{key}' does not exist", options.ERR_NO_SUCH_KEY)
-        try:
-            problems = _audit_sq(sq, settings, what_to_audit=util.check_what(args.what, _ALL_AUDITABLE, "audited"), key_list=key_list)
-        except exceptions.ObjectNotFound as e:
-            util.exit_fatal(e.message, options.ERR_NO_SUCH_KEY)
-
-    kwargs["format"] = __deduct_format__(args.format, args.file)
-    ofile = kwargs.pop("file", None)
-    problem.dump_report(problems, ofile, **kwargs)
-
-    util.logger.info("Total audit execution time: %s", str(datetime.datetime.today() - start_time))
-    if problems:
-        util.logger.warning("%d issues found during audit", len(problems))
-    else:
-        util.logger.info("%d issues found during audit", len(problems))
-    sys.exit(0)
-
-
-if __name__ == "__main__":
-    main()
+#!/usr/bin/python3
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+"""
+
+    Audits a SonarQube platform
+
+"""
+import sys
+import datetime
+import json
+
+from sonar import platform, users, groups, version, qualityprofiles, qualitygates, sif, options, portfolios, applications, exceptions
+from sonar.projects import projects
+import sonar.utilities as util
+from sonar.audit import problem, config
+
+_ALL_AUDITABLE = [
+    options.WHAT_SETTINGS,
+    options.WHAT_USERS,
+    options.WHAT_GROUPS,
+    options.WHAT_GATES,
+    options.WHAT_PROFILES,
+    options.WHAT_PROJECTS,
+    options.WHAT_APPS,
+    options.WHAT_PORTFOLIOS,
+]
+
+
+def __deduct_format__(fmt, file):
+    if fmt is not None:
+        return fmt
+    if file is not None:
+        ext = file.split(".").pop(-1).lower()
+        if ext in ("csv", "json"):
+            return ext
+    return "csv"
+
+
+def _audit_sif(sysinfo, audit_settings):
+    util.logger.info("Auditing SIF file '%s'", sysinfo)
+    try:
+        with open(sysinfo, "r", encoding="utf-8") as f:
+            sysinfo = json.loads(f.read())
+    except json.decoder.JSONDecodeError:
+        util.logger.critical("File %s does not seem to be a legit JSON file", sysinfo)
+        raise
+    except FileNotFoundError:
+        util.logger.critical("File %s does not exist", sysinfo)
+        raise
+    except PermissionError:
+        util.logger.critical("No permission to open file %s", sysinfo)
+        raise
+    return sif.Sif(sysinfo).audit(audit_settings)
+
+
+def _audit_sq(sq, settings, what_to_audit=None, key_list=None):
+    problems = []
+    if options.WHAT_PROJECTS in what_to_audit:
+        problems += projects.audit(endpoint=sq, audit_settings=settings, key_list=key_list)
+    if options.WHAT_PROFILES in what_to_audit:
+        problems += qualityprofiles.audit(endpoint=sq, audit_settings=settings)
+    if options.WHAT_GATES in what_to_audit:
+        problems += qualitygates.audit(endpoint=sq, audit_settings=settings)
+    if options.WHAT_SETTINGS in what_to_audit:
+        problems += sq.audit(audit_settings=settings)
+    if options.WHAT_USERS in what_to_audit:
+        problems += users.audit(endpoint=sq, audit_settings=settings)
+    if options.WHAT_GROUPS in what_to_audit:
+        problems += groups.audit(endpoint=sq, audit_settings=settings)
+    if options.WHAT_PORTFOLIOS in what_to_audit:
+        problems += portfolios.audit(endpoint=sq, audit_settings=settings, key_list=key_list)
+    if options.WHAT_APPS in what_to_audit:
+        problems += applications.audit(endpoint=sq, audit_settings=settings, key_list=key_list)
+    return problems
+
+
+def __parser_args(desc):
+    parser = util.set_common_args(desc)
+    parser = util.set_key_arg(parser)
+    parser = util.set_output_file_args(parser)
+    parser = options.set_url_arg(parser)
+    parser = options.add_thread_arg(parser, "project audit")
+    parser = util.set_what(parser, what_list=_ALL_AUDITABLE, operation="audit")
+    parser.add_argument("--sif", required=False, help="SIF file to audit when auditing SIF")
+    parser.add_argument(
+        "--config",
+        required=False,
+        dest="config",
+        action="store_true",
+        help="Creates the $HOME/.sonar-audit.properties configuration file, if not already present or outputs to stdout if it already exist",
+    )
+    args = parser.parse_args()
+    if args.sif is None and args.config is None and args.token is None:
+        util.exit_fatal(
+            "Token is missing (Argument -t/--token) when not analyzing local SIF",
+            options.ERR_TOKEN_MISSING,
+        )
+    return args
+
+
+def main():
+    args = __parser_args("Audits a SonarQube platform or a SIF (Support Info File or System Info File)")
+    kwargs = vars(args)
+    sq = platform.Platform(some_url=args.url, some_token=args.token, cert_file=args.clientCert)
+    util.check_environment(kwargs)
+    util.logger.info("sonar-tools version %s", version.PACKAGE_VERSION)
+    start_time = datetime.datetime.today()
+
+    settings = config.load("sonar-audit")
+    settings["threads"] = kwargs["threads"]
+    if kwargs.get("config", False):
+        config.configure()
+        sys.exit(0)
+
+    if kwargs.get("sif", None) is not None:
+        err = options.ERR_SIF_AUDIT_ERROR
+        try:
+            problems = _audit_sif(kwargs["sif"], settings)
+        except json.decoder.JSONDecodeError:
+            util.exit_fatal(f"File {kwargs['sif']} does not seem to be a legit JSON file, aborting...", err)
+        except FileNotFoundError:
+            util.exit_fatal(f"File {kwargs['sif']} does not exist, aborting...", err)
+        except PermissionError:
+            util.exit_fatal(f"No permissiont to open file {kwargs['sif']}, aborting...", err)
+        except sif.NotSystemInfo:
+            util.exit_fatal(f"File {kwargs['sif']} does not seem to be a system info or support info file, aborting...", err)
+    else:
+        util.check_token(args.token)
+        key_list = util.csv_to_list(args.projectKeys)
+        if len(key_list) > 0 and "projects" in util.csv_to_list(args.what):
+            for key in key_list:
+                if not projects.exists(key, sq):
+                    util.exit_fatal(f"Project key '{key}' does not exist", options.ERR_NO_SUCH_KEY)
+        try:
+            problems = _audit_sq(sq, settings, what_to_audit=util.check_what(args.what, _ALL_AUDITABLE, "audited"), key_list=key_list)
+        except exceptions.ObjectNotFound as e:
+            util.exit_fatal(e.message, options.ERR_NO_SUCH_KEY)
+
+    kwargs["format"] = __deduct_format__(args.format, args.file)
+    ofile = kwargs.pop("file", None)
+    problem.dump_report(problems, ofile, **kwargs)
+
+    util.logger.info("Total audit execution time: %s", str(datetime.datetime.today() - start_time))
+    if problems:
+        util.logger.warning("%d issues found during audit", len(problems))
+    else:
+        util.logger.info("%d issues found during audit", len(problems))
+    sys.exit(0)
+
+
+if __name__ == "__main__":
+    main()
```

## tools/config.py

 * *Ordering differences only*

```diff
@@ -1,197 +1,197 @@
-#!/usr/local/bin/python3
-#
-# sonar-tools
-# Copyright (C) 2022-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-"""
-    Exports SonarQube platform configuration as JSON
-"""
-import sys
-import datetime
-from sonar import platform, version, rules, qualityprofiles, qualitygates, portfolios, applications, users, groups, options, utilities, exceptions
-from sonar.projects import projects
-
-_EVERYTHING = [
-    options.WHAT_SETTINGS,
-    options.WHAT_USERS,
-    options.WHAT_GROUPS,
-    options.WHAT_GATES,
-    options.WHAT_RULES,
-    options.WHAT_PROFILES,
-    options.WHAT_PROJECTS,
-    options.WHAT_APPS,
-    options.WHAT_PORTFOLIOS,
-]
-
-__JSON_KEY_PLATFORM = "platform"
-
-__JSON_KEY_SETTINGS = "globalSettings"
-__JSON_KEY_USERS = "users"
-__JSON_KEY_GROUPS = "groups"
-__JSON_KEY_GATES = "qualityGates"
-__JSON_KEY_RULES = "rules"
-__JSON_KEY_PROFILES = "qualityProfiles"
-__JSON_KEY_PROJECTS = "projects"
-__JSON_KEY_APPS = "applications"
-__JSON_KEY_PORTFOLIOS = "portfolios"
-
-__MAP = {
-    options.WHAT_SETTINGS: __JSON_KEY_SETTINGS,
-    options.WHAT_USERS: __JSON_KEY_USERS,
-    options.WHAT_GROUPS: __JSON_KEY_GROUPS,
-    options.WHAT_GATES: __JSON_KEY_GATES,
-    options.WHAT_RULES: __JSON_KEY_RULES,
-    options.WHAT_PROFILES: __JSON_KEY_PROFILES,
-    options.WHAT_PROJECTS: __JSON_KEY_PROJECTS,
-    options.WHAT_APPS: __JSON_KEY_APPS,
-    options.WHAT_PORTFOLIOS: __JSON_KEY_PORTFOLIOS,
-}
-
-
-def __map(k):
-    return __MAP.get(k, k)
-
-
-def __parse_args(desc):
-    parser = utilities.set_common_args(desc)
-    parser = utilities.set_key_arg(parser)
-    parser = utilities.set_output_file_args(parser, json_fmt=True, csv_fmt=False)
-    parser = options.add_thread_arg(parser, "project export")
-    parser = utilities.set_what(parser, what_list=_EVERYTHING, operation="export or import")
-    group = parser.add_mutually_exclusive_group()
-    group.add_argument(
-        "-e",
-        "--export",
-        required=False,
-        default=False,
-        action="store_true",
-        help="to export configuration (exclusive of --import)",
-    )
-    group.add_argument(
-        "-i",
-        "--import",
-        required=False,
-        default=False,
-        action="store_true",
-        help="to import configuration (exclusive of --export)",
-    )
-    parser.add_argument(
-        "--fullExport",
-        required=False,
-        default=False,
-        action="store_true",
-        help="Also exports informative data that would be ignored as part of an import. Informative field are prefixed with _."
-        "This option is ignored in case of import",
-    )
-    args = utilities.parse_and_check_token(parser)
-    utilities.check_environment(vars(args))
-    utilities.check_token(args.token)
-    utilities.logger.info("sonar-tools version %s", version.PACKAGE_VERSION)
-    return args
-
-
-def __export_config(endpoint, what, args):
-    key_list = utilities.csv_to_list(args.projectKeys)
-    if len(key_list) > 0 and "projects" in utilities.csv_to_list(args.what):
-        for key in key_list:
-            if not projects.exists(key, endpoint):
-                utilities.exit_fatal(f"Project key '{key}' does not exist", options.ERR_NO_SUCH_KEY)
-    utilities.logger.info("Exporting configuration from %s", args.url)
-    sq_settings = {}
-    sq_settings[__JSON_KEY_PLATFORM] = endpoint.basics()
-    if options.WHAT_SETTINGS in what:
-        sq_settings[__JSON_KEY_SETTINGS] = endpoint.export(full=args.fullExport)
-    if options.WHAT_RULES in what:
-        sq_settings[__JSON_KEY_RULES] = rules.export(endpoint, full=args.fullExport)
-    if options.WHAT_PROFILES in what:
-        if options.WHAT_RULES not in what:
-            sq_settings[__JSON_KEY_RULES] = rules.export(endpoint, full=args.fullExport)
-        sq_settings[__JSON_KEY_PROFILES] = qualityprofiles.export(endpoint, full=args.fullExport)
-    if options.WHAT_GATES in what:
-        sq_settings[__JSON_KEY_GATES] = qualitygates.export(endpoint, full=args.fullExport)
-    if options.WHAT_PROJECTS in what:
-        sq_settings[__JSON_KEY_PROJECTS] = projects.export(endpoint, key_list=args.projectKeys, full=args.fullExport, threads=args.threads)
-    if options.WHAT_APPS in what:
-        try:
-            sq_settings[__JSON_KEY_APPS] = applications.export(endpoint, key_list=args.projectKeys, full=args.fullExport)
-        except exceptions.UnsupportedOperation as e:
-            utilities.logger.info("%s", e.message)
-    if options.WHAT_PORTFOLIOS in what:
-        try:
-            sq_settings[__JSON_KEY_PORTFOLIOS] = portfolios.export(endpoint, key_list=args.projectKeys, full=args.fullExport)
-        except exceptions.UnsupportedOperation as e:
-            utilities.logger.info("%s", e.message)
-    if options.WHAT_USERS in what:
-        sq_settings[__JSON_KEY_USERS] = users.export(endpoint, full=args.fullExport)
-    if options.WHAT_GROUPS in what:
-        sq_settings[__JSON_KEY_GROUPS] = groups.export(endpoint)
-
-    utilities.remove_nones(sq_settings)
-    with utilities.open_file(args.file) as fd:
-        print(utilities.json_dump(sq_settings), file=fd)
-    utilities.logger.info("Exporting configuration from %s completed", args.url)
-
-
-def __import_config(endpoint, what, args):
-    utilities.logger.info("Importing configuration to %s", args.url)
-    data = utilities.load_json_file(args.file)
-    if options.WHAT_GROUPS in what:
-        groups.import_config(endpoint, data)
-    if options.WHAT_USERS in what:
-        users.import_config(endpoint, data)
-    if options.WHAT_GATES in what:
-        qualitygates.import_config(endpoint, data)
-    if options.WHAT_RULES in what:
-        rules.import_config(endpoint, data)
-    if options.WHAT_PROFILES in what:
-        if options.WHAT_RULES not in what:
-            rules.import_config(endpoint, data)
-        qualityprofiles.import_config(endpoint, data)
-    if options.WHAT_SETTINGS in what:
-        endpoint.import_config(data)
-    if options.WHAT_PROJECTS in what:
-        projects.import_config(endpoint, data, key_list=args.projectKeys)
-    if options.WHAT_APPS in what:
-        applications.import_config(endpoint, data, key_list=args.projectKeys)
-    if options.WHAT_PORTFOLIOS in what:
-        portfolios.import_config(endpoint, data, key_list=args.projectKeys)
-    utilities.logger.info("Importing configuration to %s completed", args.url)
-
-
-def main():
-    args = __parse_args("Extract SonarQube platform configuration")
-    kwargs = vars(args)
-    if not kwargs["export"] and not kwargs["import"]:
-        utilities.exit_fatal("One of --export or --import option must be chosen", exit_code=options.ERR_ARGS_ERROR)
-
-    start_time = datetime.datetime.today()
-    endpoint = platform.Platform(some_url=args.url, some_token=args.token, cert_file=args.clientCert)
-    what = utilities.check_what(args.what, _EVERYTHING, "exported or imported")
-    if kwargs["export"]:
-        try:
-            __export_config(endpoint, what, args)
-        except exceptions.ObjectNotFound as e:
-            utilities.exit_fatal(e.message, options.ERR_NO_SUCH_KEY)
-    if kwargs["import"]:
-        __import_config(endpoint, what, args)
-    utilities.logger.info("Total execution time: %s", str(datetime.datetime.today() - start_time))
-    sys.exit(0)
-
-
-if __name__ == "__main__":
-    main()
+#!/usr/local/bin/python3
+#
+# sonar-tools
+# Copyright (C) 2022-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+"""
+    Exports SonarQube platform configuration as JSON
+"""
+import sys
+import datetime
+from sonar import platform, version, rules, qualityprofiles, qualitygates, portfolios, applications, users, groups, options, utilities, exceptions
+from sonar.projects import projects
+
+_EVERYTHING = [
+    options.WHAT_SETTINGS,
+    options.WHAT_USERS,
+    options.WHAT_GROUPS,
+    options.WHAT_GATES,
+    options.WHAT_RULES,
+    options.WHAT_PROFILES,
+    options.WHAT_PROJECTS,
+    options.WHAT_APPS,
+    options.WHAT_PORTFOLIOS,
+]
+
+__JSON_KEY_PLATFORM = "platform"
+
+__JSON_KEY_SETTINGS = "globalSettings"
+__JSON_KEY_USERS = "users"
+__JSON_KEY_GROUPS = "groups"
+__JSON_KEY_GATES = "qualityGates"
+__JSON_KEY_RULES = "rules"
+__JSON_KEY_PROFILES = "qualityProfiles"
+__JSON_KEY_PROJECTS = "projects"
+__JSON_KEY_APPS = "applications"
+__JSON_KEY_PORTFOLIOS = "portfolios"
+
+__MAP = {
+    options.WHAT_SETTINGS: __JSON_KEY_SETTINGS,
+    options.WHAT_USERS: __JSON_KEY_USERS,
+    options.WHAT_GROUPS: __JSON_KEY_GROUPS,
+    options.WHAT_GATES: __JSON_KEY_GATES,
+    options.WHAT_RULES: __JSON_KEY_RULES,
+    options.WHAT_PROFILES: __JSON_KEY_PROFILES,
+    options.WHAT_PROJECTS: __JSON_KEY_PROJECTS,
+    options.WHAT_APPS: __JSON_KEY_APPS,
+    options.WHAT_PORTFOLIOS: __JSON_KEY_PORTFOLIOS,
+}
+
+
+def __map(k):
+    return __MAP.get(k, k)
+
+
+def __parse_args(desc):
+    parser = utilities.set_common_args(desc)
+    parser = utilities.set_key_arg(parser)
+    parser = utilities.set_output_file_args(parser, json_fmt=True, csv_fmt=False)
+    parser = options.add_thread_arg(parser, "project export")
+    parser = utilities.set_what(parser, what_list=_EVERYTHING, operation="export or import")
+    group = parser.add_mutually_exclusive_group()
+    group.add_argument(
+        "-e",
+        "--export",
+        required=False,
+        default=False,
+        action="store_true",
+        help="to export configuration (exclusive of --import)",
+    )
+    group.add_argument(
+        "-i",
+        "--import",
+        required=False,
+        default=False,
+        action="store_true",
+        help="to import configuration (exclusive of --export)",
+    )
+    parser.add_argument(
+        "--fullExport",
+        required=False,
+        default=False,
+        action="store_true",
+        help="Also exports informative data that would be ignored as part of an import. Informative field are prefixed with _."
+        "This option is ignored in case of import",
+    )
+    args = utilities.parse_and_check_token(parser)
+    utilities.check_environment(vars(args))
+    utilities.check_token(args.token)
+    utilities.logger.info("sonar-tools version %s", version.PACKAGE_VERSION)
+    return args
+
+
+def __export_config(endpoint, what, args):
+    key_list = utilities.csv_to_list(args.projectKeys)
+    if len(key_list) > 0 and "projects" in utilities.csv_to_list(args.what):
+        for key in key_list:
+            if not projects.exists(key, endpoint):
+                utilities.exit_fatal(f"Project key '{key}' does not exist", options.ERR_NO_SUCH_KEY)
+    utilities.logger.info("Exporting configuration from %s", args.url)
+    sq_settings = {}
+    sq_settings[__JSON_KEY_PLATFORM] = endpoint.basics()
+    if options.WHAT_SETTINGS in what:
+        sq_settings[__JSON_KEY_SETTINGS] = endpoint.export(full=args.fullExport)
+    if options.WHAT_RULES in what:
+        sq_settings[__JSON_KEY_RULES] = rules.export(endpoint, full=args.fullExport)
+    if options.WHAT_PROFILES in what:
+        if options.WHAT_RULES not in what:
+            sq_settings[__JSON_KEY_RULES] = rules.export(endpoint, full=args.fullExport)
+        sq_settings[__JSON_KEY_PROFILES] = qualityprofiles.export(endpoint, full=args.fullExport)
+    if options.WHAT_GATES in what:
+        sq_settings[__JSON_KEY_GATES] = qualitygates.export(endpoint, full=args.fullExport)
+    if options.WHAT_PROJECTS in what:
+        sq_settings[__JSON_KEY_PROJECTS] = projects.export(endpoint, key_list=args.projectKeys, full=args.fullExport, threads=args.threads)
+    if options.WHAT_APPS in what:
+        try:
+            sq_settings[__JSON_KEY_APPS] = applications.export(endpoint, key_list=args.projectKeys, full=args.fullExport)
+        except exceptions.UnsupportedOperation as e:
+            utilities.logger.info("%s", e.message)
+    if options.WHAT_PORTFOLIOS in what:
+        try:
+            sq_settings[__JSON_KEY_PORTFOLIOS] = portfolios.export(endpoint, key_list=args.projectKeys, full=args.fullExport)
+        except exceptions.UnsupportedOperation as e:
+            utilities.logger.info("%s", e.message)
+    if options.WHAT_USERS in what:
+        sq_settings[__JSON_KEY_USERS] = users.export(endpoint, full=args.fullExport)
+    if options.WHAT_GROUPS in what:
+        sq_settings[__JSON_KEY_GROUPS] = groups.export(endpoint)
+
+    utilities.remove_nones(sq_settings)
+    with utilities.open_file(args.file) as fd:
+        print(utilities.json_dump(sq_settings), file=fd)
+    utilities.logger.info("Exporting configuration from %s completed", args.url)
+
+
+def __import_config(endpoint, what, args):
+    utilities.logger.info("Importing configuration to %s", args.url)
+    data = utilities.load_json_file(args.file)
+    if options.WHAT_GROUPS in what:
+        groups.import_config(endpoint, data)
+    if options.WHAT_USERS in what:
+        users.import_config(endpoint, data)
+    if options.WHAT_GATES in what:
+        qualitygates.import_config(endpoint, data)
+    if options.WHAT_RULES in what:
+        rules.import_config(endpoint, data)
+    if options.WHAT_PROFILES in what:
+        if options.WHAT_RULES not in what:
+            rules.import_config(endpoint, data)
+        qualityprofiles.import_config(endpoint, data)
+    if options.WHAT_SETTINGS in what:
+        endpoint.import_config(data)
+    if options.WHAT_PROJECTS in what:
+        projects.import_config(endpoint, data, key_list=args.projectKeys)
+    if options.WHAT_APPS in what:
+        applications.import_config(endpoint, data, key_list=args.projectKeys)
+    if options.WHAT_PORTFOLIOS in what:
+        portfolios.import_config(endpoint, data, key_list=args.projectKeys)
+    utilities.logger.info("Importing configuration to %s completed", args.url)
+
+
+def main():
+    args = __parse_args("Extract SonarQube platform configuration")
+    kwargs = vars(args)
+    if not kwargs["export"] and not kwargs["import"]:
+        utilities.exit_fatal("One of --export or --import option must be chosen", exit_code=options.ERR_ARGS_ERROR)
+
+    start_time = datetime.datetime.today()
+    endpoint = platform.Platform(some_url=args.url, some_token=args.token, cert_file=args.clientCert)
+    what = utilities.check_what(args.what, _EVERYTHING, "exported or imported")
+    if kwargs["export"]:
+        try:
+            __export_config(endpoint, what, args)
+        except exceptions.ObjectNotFound as e:
+            utilities.exit_fatal(e.message, options.ERR_NO_SUCH_KEY)
+    if kwargs["import"]:
+        __import_config(endpoint, what, args)
+    utilities.logger.info("Total execution time: %s", str(datetime.datetime.today() - start_time))
+    sys.exit(0)
+
+
+if __name__ == "__main__":
+    main()
```

## tools/cust_measures.py

 * *Ordering differences only*

```diff
@@ -1,65 +1,65 @@
-#!/usr/local/bin/python3
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-"""
-    This script manipulates custom measures. You may:
-
-    Update a custom measure value:
-        Usage: cust_measures.py -t <SQ_TOKEN> -u <SQ_URL> -k <projectKey> -m <metricKey> --updateValue <value>
-"""
-
-from sonar import custom_measures, platform, utilities, options
-
-
-def parse_args(desc):
-    parser = utilities.set_common_args(desc)
-    parser = utilities.set_key_arg(parser)
-    parser.add_argument("-m", "--metricKey", required=True, help="What custom metric to work on")
-    parser.add_argument("--value", required=False, help="Updates the value of the metric")
-    parser.add_argument("--description", required=False, help="Updates the description of the metric")
-    return utilities.parse_and_check_token(parser)
-
-
-def main():
-    args = parse_args("Manipulate custom metrics")
-    sqenv = platform.Platform(some_url=args.url, some_token=args.token)
-    if sqenv.version() >= (9, 0, 0):
-        utilities.exit_fatal("Custom measures are no longer supported after 8.9.x", options.UnsupportedOperation)
-    else:
-        utilities.logger.warning("Custom measures are are deprecated in 8.9 and lower and are dropped starting from SonarQube 9.0")
-    # Remove unset params from the dict
-    params = vars(args)
-    for key in params.copy():
-        if params[key] is None:
-            del params[key]
-    # Add SQ environment
-    params.update({"env": sqenv})
-
-    if params.get("value", None) is not None:
-        custom_measures.update(
-            project_key=params["componentKeys"],
-            metric_key=params["metricKey"],
-            value=params["value"],
-            description=params.get("description", None),
-        )
-
-
-if __name__ == "__main__":
-    main()
+#!/usr/local/bin/python3
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+"""
+    This script manipulates custom measures. You may:
+
+    Update a custom measure value:
+        Usage: cust_measures.py -t <SQ_TOKEN> -u <SQ_URL> -k <projectKey> -m <metricKey> --updateValue <value>
+"""
+
+from sonar import custom_measures, platform, utilities, options
+
+
+def parse_args(desc):
+    parser = utilities.set_common_args(desc)
+    parser = utilities.set_key_arg(parser)
+    parser.add_argument("-m", "--metricKey", required=True, help="What custom metric to work on")
+    parser.add_argument("--value", required=False, help="Updates the value of the metric")
+    parser.add_argument("--description", required=False, help="Updates the description of the metric")
+    return utilities.parse_and_check_token(parser)
+
+
+def main():
+    args = parse_args("Manipulate custom metrics")
+    sqenv = platform.Platform(some_url=args.url, some_token=args.token)
+    if sqenv.version() >= (9, 0, 0):
+        utilities.exit_fatal("Custom measures are no longer supported after 8.9.x", options.UnsupportedOperation)
+    else:
+        utilities.logger.warning("Custom measures are are deprecated in 8.9 and lower and are dropped starting from SonarQube 9.0")
+    # Remove unset params from the dict
+    params = vars(args)
+    for key in params.copy():
+        if params[key] is None:
+            del params[key]
+    # Add SQ environment
+    params.update({"env": sqenv})
+
+    if params.get("value", None) is not None:
+        custom_measures.update(
+            project_key=params["componentKeys"],
+            metric_key=params["metricKey"],
+            value=params["value"],
+            description=params.get("description", None),
+        )
+
+
+if __name__ == "__main__":
+    main()
```

## tools/findings_export.py

```diff
@@ -1,357 +1,357 @@
-#!/usr/local/bin/python3
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-"""
-    This script exports findings as CSV or JSON
-
-    Usage: sonar-findings-export.py -t <SQ_TOKEN> -u <SQ_URL> [<filters>]
-
-    Filters can be:
-    [-k <projectKey>]
-    [-s <statuses>] (FIXED, CLOSED, REOPENED, REVIEWED)
-    [-r <resolutions>] (UNRESOLVED, FALSE-POSITIVE, WONTFIX)
-    [-a <createdAfter>] findings created on or after a given date (YYYY-MM-DD)
-    [-b <createdBefore>] findings created before or on a given date (YYYY-MM-DD)
-    [--severities <severities>] Comma separated desired severities: BLOCKER, CRITICAL, MAJOR, MINOR, INFO
-    [--types <types>] Comma separated findings types (VULNERABILITY,BUG,CODE_SMELL,SECURITY_HOTSPOT)
-    [--tags]
-"""
-
-import sys
-import os
-import time
-import datetime
-from queue import Queue
-from threading import Thread
-
-from sonar import platform, version, options, exceptions
-from sonar.projects import projects
-import sonar.utilities as util
-from sonar.findings import findings, issues, hotspots
-
-WRITE_END = object()
-TOTAL_FINDINGS = 0
-
-
-def parse_args(desc):
-    parser = util.set_common_args(desc)
-    parser = util.set_key_arg(parser)
-    parser = util.set_output_file_args(parser)
-    parser = options.add_thread_arg(parser, "findings search")
-    parser.add_argument(
-        "-b",
-        "--branches",
-        required=False,
-        default=None,
-        help="Comma separated list of branches to export. Use * to export findings from all branches. "
-        "If not specified, only findings of the main branch will be exported",
-    )
-    parser.add_argument(
-        "-p",
-        "--pullRequests",
-        required=False,
-        default=None,
-        help="Comma separated list of pull request. Use * to export findings from all PRs. "
-        "If not specified, only findings of the main branch will be exported",
-    )
-    parser.add_argument(
-        "--statuses",
-        required=False,
-        help="comma separated status among " + util.list_to_csv(issues.STATUSES + hotspots.STATUSES),
-    )
-    parser.add_argument(
-        "--createdAfter",
-        required=False,
-        help="findings created on or after a given date (YYYY-MM-DD)",
-    )
-    parser.add_argument(
-        "--createdBefore",
-        required=False,
-        help="findings created on or before a given date (YYYY-MM-DD)",
-    )
-    parser.add_argument(
-        "--resolutions",
-        required=False,
-        help="Comma separated resolution of the findings among " + util.list_to_csv(issues.RESOLUTIONS + hotspots.RESOLUTIONS),
-    )
-    parser.add_argument(
-        "--severities",
-        required=False,
-        help="Comma separated severities among" + util.list_to_csv(issues.SEVERITIES + hotspots.SEVERITIES),
-    )
-    parser.add_argument(
-        "--types",
-        required=False,
-        help="Comma separated types among " + util.list_to_csv(issues.TYPES + hotspots.TYPES),
-    )
-    parser.add_argument("--tags", help="Comma separated findings tags", required=False)
-    parser.add_argument(
-        "--useFindings",
-        required=False,
-        default=False,
-        action="store_true",
-        help="Use export_findings() whenever possible",
-    )
-    parser.add_argument(
-        "--" + options.WITH_URL,
-        required=False,
-        default=False,
-        action="store_true",
-        help="Generate finding URL in the report, false by default",
-    )
-    args = util.parse_and_check_token(parser)
-    util.check_token(args.token)
-    return args
-
-
-def __write_header(file, format):
-    util.logger.info("Dumping report to %s", f"file '{file}'" if file else "stdout")
-    with util.open_file(file) as f:
-        print("[" if format == "json" else findings.to_csv_header(), file=f)
-
-
-def __write_footer(file, format):
-    if format != "json":
-        return
-    with util.open_file(file, mode="a") as f:
-        print("]\n", file=f)
-
-
-def __dump_findings(findings_list, file, file_format, is_last=False, **kwargs):
-    i = len(findings_list)
-    util.logger.info("Writing %d more findings to %s", i, f"file '{file}'" if file else "stdout")
-    with util.open_file(file, mode="a") as f:
-        url = ""
-        sep = kwargs.get(options.CSV_SEPARATOR, ",")
-        comma = ","
-        for _, finding in findings_list.items():
-            i -= 1
-            if file_format == "json":
-                finding_json = finding.to_json()
-                if not kwargs[options.WITH_URL]:
-                    finding_json.pop("url", None)
-                if is_last and i == 0:
-                    comma = ""
-                print(f"{util.json_dump(finding_json, indent=1)}{comma}\n", file=f, end="")
-            else:
-                if kwargs[options.WITH_URL]:
-                    url = f'{sep}"{finding.url()}"'
-                print(f"{finding.to_csv(sep)}{url}", file=f)
-
-
-def __write_findings(queue, file_to_write, file_format, with_url, separator):
-    while True:
-        while queue.empty():
-            time.sleep(0.5)
-        (data, is_last) = queue.get()
-        if data == WRITE_END:
-            queue.task_done()
-            break
-
-        global TOTAL_FINDINGS
-        TOTAL_FINDINGS += len(data)
-        __dump_findings(data, file_to_write, file_format, is_last, withURL=with_url, csvSeparator=separator)
-        queue.task_done()
-
-
-def __dump_compact(finding_list, file, **kwargs):
-    new_dict = {}
-    for finding in finding_list.values():
-        f_json = finding.to_json()
-        if not kwargs[options.WITH_URL]:
-            f_json.pop("url", None)
-        pkey = f_json.pop("projectKey")
-        ftype = f_json.pop("type")
-        if pkey in new_dict:
-            if ftype in new_dict[pkey]:
-                new_dict[pkey][ftype].append(f_json)
-            else:
-                new_dict[pkey].update({ftype: [f_json]})
-        else:
-            new_dict[pkey] = {ftype: [f_json]}
-    with util.open_file(file) as f:
-        print(util.json_dump(new_dict, indent=1), file=f)
-
-
-def __get_list(project, list_str, list_type):
-    if list_str == "*":
-        list_array = [b.name for b in project.branches()] if list_type == "branch" else [p.key for p in project.pull_requests()]
-    elif list_str is not None:
-        list_array = util.csv_to_list(list_str)
-    else:
-        list_array = []
-    return list_array
-
-
-def __verify_inputs(params):
-    diff = util.difference(util.csv_to_list(params.get("resolutions", None)), issues.RESOLUTIONS + hotspots.RESOLUTIONS)
-    if diff:
-        util.exit_fatal(f"Resolutions {str(diff)} are not legit resolutions", options.ERR_WRONG_SEARCH_CRITERIA)
-
-    diff = util.difference(util.csv_to_list(params.get("statuses", None)), issues.STATUSES + hotspots.STATUSES)
-    if diff:
-        util.exit_fatal(f"Statuses {str(diff)} are not legit statuses", options.ERR_WRONG_SEARCH_CRITERIA)
-
-    diff = util.difference(util.csv_to_list(params.get("severities", None)), issues.SEVERITIES + hotspots.SEVERITIES)
-    if diff:
-        util.exit_fatal(f"Severities {str(diff)} are not legit severities", options.ERR_WRONG_SEARCH_CRITERIA)
-
-    diff = util.difference(util.csv_to_list(params.get("types", None)), issues.TYPES + hotspots.TYPES)
-    if diff:
-        util.exit_fatal(f"Types {str(diff)} are not legit types", options.ERR_WRONG_SEARCH_CRITERIA)
-
-    return True
-
-
-def __get_project_findings(queue, write_queue):
-    while not queue.empty():
-        (key, endpoint, params) = queue.get()
-        search_findings = params["useFindings"]
-        status_list = util.csv_to_list(params.get("statuses", None))
-        i_statuses = util.intersection(status_list, issues.STATUSES)
-        h_statuses = util.intersection(status_list, hotspots.STATUSES)
-        resol_list = util.csv_to_list(params.get("resolutions", None))
-        i_resols = util.intersection(resol_list, issues.RESOLUTIONS)
-        h_resols = util.intersection(resol_list, hotspots.RESOLUTIONS)
-        type_list = util.csv_to_list(params.get("types", None))
-        i_types = util.intersection(type_list, issues.TYPES)
-        h_types = util.intersection(type_list, hotspots.TYPES)
-        sev_list = util.csv_to_list(params.get("severities", None))
-        i_sevs = util.intersection(sev_list, issues.SEVERITIES)
-        h_sevs = util.intersection(sev_list, hotspots.SEVERITIES)
-
-        if status_list or resol_list or type_list or sev_list:
-            search_findings = False
-
-        util.logger.debug("WriteQueue %s task %s put", str(write_queue), key)
-        if search_findings:
-            findings_list = findings.export_findings(endpoint, key, branch=params.get("branch", None), pull_request=params.get("pullRequest", None))
-
-            write_queue.put([findings_list, queue.empty()])
-        else:
-            new_params = issues.get_search_criteria(params)
-            new_params.update({"branch": params.get("branch", None), "pullRequest": params.get("pullRequest", None)})
-            findings_list = {}
-            if (i_statuses or not status_list) and (i_resols or not resol_list) and (i_types or not type_list) and (i_sevs or not sev_list):
-                findings_list = issues.search_by_project(key, params=new_params, endpoint=endpoint)
-            else:
-                util.logger.debug("Status = %s, Types = %s, Resol = %s, Sev = %s", str(i_statuses), str(i_types), str(i_resols), str(i_sevs))
-                util.logger.info("Selected types, severities, resolutions or statuses disables issue search")
-
-            if (h_statuses or not status_list) and (h_resols or not resol_list) and (h_types or not type_list) and (h_sevs or not sev_list):
-                new_params = hotspots.get_search_criteria(params)
-                new_params.update({"branch": params.get("branch", None), "pullRequest": params.get("pullRequest", None)})
-                findings_list.update(hotspots.search_by_project(key, endpoint=endpoint, params=new_params))
-            else:
-                util.logger.debug("Status = %s, Types = %s, Resol = %s, Sev = %s", str(h_statuses), str(h_types), str(h_resols), str(h_sevs))
-                util.logger.info("Selected types, severities, resolutions or statuses disables issue search")
-            write_queue.put([findings_list, queue.empty()])
-        util.logger.debug("Queue %s task %s done", str(queue), key)
-        queue.task_done()
-
-
-def store_findings(project_list, params, endpoint, file, format, threads=4, with_url=False, csv_separator=","):
-    my_queue = Queue(maxsize=0)
-    write_queue = Queue(maxsize=0)
-    for key, project in project_list.items():
-        branches = __get_list(project, params.pop("branches", None), "branch")
-        prs = __get_list(project, params.pop("pullRequests", None), "pullrequest")
-        for b in branches:
-            params["branch"] = b
-            util.logger.debug("Queue %s task %s put", str(my_queue), key)
-            my_queue.put((key, endpoint, params.copy()))
-        params.pop("branch", None)
-        for p in prs:
-            params["pullRequest"] = p
-            util.logger.debug("Queue %s task %s put", str(my_queue), key)
-            my_queue.put((key, endpoint, params.copy()))
-        params.pop("pullRequest", None)
-        if not (branches or prs):
-            util.logger.debug("Queue %s task %s put", str(my_queue), key)
-            my_queue.put((key, endpoint, params.copy()))
-
-    for i in range(threads):
-        util.logger.debug("Starting finding search thread 'findingSearch%d'", i)
-        worker = Thread(target=__get_project_findings, args=[my_queue, write_queue])
-        worker.setDaemon(True)
-        worker.setName(f"findingSearch{i}")
-        worker.start()
-
-    util.logger.info("Starting finding writer thread 'findingWriter'")
-    write_worker = Thread(target=__write_findings, args=[write_queue, file, format, with_url, csv_separator])
-    write_worker.setDaemon(True)
-    write_worker.setName("findingWriter")
-    write_worker.start()
-
-    my_queue.join()
-    # Tell the writer thread that writing is complete
-    util.logger.debug("WriteQueue %s task %s put", str(write_queue), str(WRITE_END))
-    write_queue.put((WRITE_END, True))
-    write_queue.join()
-
-
-def main():
-    kwargs = vars(parse_args("Sonar findings extractor"))
-    sqenv = platform.Platform(some_url=kwargs["url"], some_token=kwargs["token"], cert_file=kwargs["clientCert"])
-    del kwargs["token"]
-    util.check_environment(kwargs)
-    util.logger.info("sonar-tools version %s", version.PACKAGE_VERSION)
-    start_time = datetime.datetime.today()
-    params = util.remove_nones(kwargs.copy())
-    __verify_inputs(params)
-
-    for p in ("statuses", "createdAfter", "createdBefore", "resolutions", "severities", "types", "tags"):
-        if params.get(p, None) is not None:
-            if params["useFindings"]:
-                util.logger.warning("Selected search criteria %s will disable --useFindings", params[p])
-            params["useFindings"] = False
-            break
-    try:
-        project_list = projects.get_list(endpoint=sqenv, key_list=util.csv_to_list(kwargs.get("projectKeys", None)))
-    except exceptions.ObjectNotFound as e:
-        util.exit_fatal(e.message, options.ERR_NO_SUCH_KEY)
-    fmt = kwargs.pop("format", None)
-    fname = kwargs.pop("file", None)
-    if fname is not None:
-        ext = fname.split(".")[-1].lower()
-        if os.path.exists(fname):
-            os.remove(fname)
-        if ext in ("csv", "json"):
-            fmt = ext
-
-    util.logger.info("Exporting findings for %d projects with params %s", len(project_list), str(params))
-    __write_header(fname, fmt)
-    store_findings(
-        project_list,
-        params=params,
-        endpoint=sqenv,
-        file=fname,
-        format=fmt,
-        threads=kwargs[options.NBR_THREADS],
-        with_url=kwargs[options.WITH_URL],
-        csv_separator=kwargs[options.CSV_SEPARATOR],
-    )
-    __write_footer(fname, fmt)
-    util.logger.info("Returned findings: %d - Total execution time: %s", TOTAL_FINDINGS, str(datetime.datetime.today() - start_time))
-    sys.exit(0)
-
-
-if __name__ == "__main__":
-    main()
+#!/usr/local/bin/python3
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+"""
+    This script exports findings as CSV or JSON
+
+    Usage: sonar-findings-export.py -t <SQ_TOKEN> -u <SQ_URL> [<filters>]
+
+    Filters can be:
+    [-k <projectKey>]
+    [-s <statuses>] (FIXED, CLOSED, REOPENED, REVIEWED)
+    [-r <resolutions>] (UNRESOLVED, FALSE-POSITIVE, WONTFIX)
+    [-a <createdAfter>] findings created on or after a given date (YYYY-MM-DD)
+    [-b <createdBefore>] findings created before or on a given date (YYYY-MM-DD)
+    [--severities <severities>] Comma separated desired severities: BLOCKER, CRITICAL, MAJOR, MINOR, INFO
+    [--types <types>] Comma separated findings types (VULNERABILITY,BUG,CODE_SMELL,SECURITY_HOTSPOT)
+    [--tags]
+"""
+
+import sys
+import os
+import time
+import datetime
+from queue import Queue
+from threading import Thread
+
+from sonar import platform, version, options, exceptions
+from sonar.projects import projects
+import sonar.utilities as util
+from sonar.findings import findings, issues, hotspots
+
+WRITE_END = object()
+TOTAL_FINDINGS = 0
+
+
+def parse_args(desc):
+    parser = util.set_common_args(desc)
+    parser = util.set_key_arg(parser)
+    parser = util.set_output_file_args(parser)
+    parser = options.add_thread_arg(parser, "findings search")
+    parser.add_argument(
+        "-b",
+        "--branches",
+        required=False,
+        default=None,
+        help="Comma separated list of branches to export. Use * to export findings from all branches. "
+        "If not specified, only findings of the main branch will be exported",
+    )
+    parser.add_argument(
+        "-p",
+        "--pullRequests",
+        required=False,
+        default=None,
+        help="Comma separated list of pull request. Use * to export findings from all PRs. "
+        "If not specified, only findings of the main branch will be exported",
+    )
+    parser.add_argument(
+        "--statuses",
+        required=False,
+        help="comma separated status among " + util.list_to_csv(issues.STATUSES + hotspots.STATUSES),
+    )
+    parser.add_argument(
+        "--createdAfter",
+        required=False,
+        help="findings created on or after a given date (YYYY-MM-DD)",
+    )
+    parser.add_argument(
+        "--createdBefore",
+        required=False,
+        help="findings created on or before a given date (YYYY-MM-DD)",
+    )
+    parser.add_argument(
+        "--resolutions",
+        required=False,
+        help="Comma separated resolution of the findings among " + util.list_to_csv(issues.RESOLUTIONS + hotspots.RESOLUTIONS),
+    )
+    parser.add_argument(
+        "--severities",
+        required=False,
+        help="Comma separated severities among" + util.list_to_csv(issues.SEVERITIES + hotspots.SEVERITIES),
+    )
+    parser.add_argument(
+        "--types",
+        required=False,
+        help="Comma separated types among " + util.list_to_csv(issues.TYPES + hotspots.TYPES),
+    )
+    parser.add_argument("--tags", help="Comma separated findings tags", required=False)
+    parser.add_argument(
+        "--useFindings",
+        required=False,
+        default=False,
+        action="store_true",
+        help="Use export_findings() whenever possible",
+    )
+    parser.add_argument(
+        "--" + options.WITH_URL,
+        required=False,
+        default=False,
+        action="store_true",
+        help="Generate finding URL in the report, false by default",
+    )
+    args = util.parse_and_check_token(parser)
+    util.check_token(args.token)
+    return args
+
+
+def __write_header(file, format):
+    util.logger.info("Dumping report to %s", f"file '{file}'" if file else "stdout")
+    with util.open_file(file) as f:
+        print("[" if format == "json" else findings.to_csv_header(), file=f)
+
+
+def __write_footer(file, format):
+    if format != "json":
+        return
+    with util.open_file(file, mode="a") as f:
+        print("]\n", file=f)
+
+
+def __dump_findings(findings_list, file, file_format, is_last=False, **kwargs):
+    i = len(findings_list)
+    util.logger.info("Writing %d more findings to %s", i, f"file '{file}'" if file else "stdout")
+    with util.open_file(file, mode="a") as f:
+        url = ""
+        sep = kwargs.get(options.CSV_SEPARATOR, ",")
+        comma = ","
+        for _, finding in findings_list.items():
+            i -= 1
+            if file_format == "json":
+                finding_json = finding.to_json()
+                if not kwargs[options.WITH_URL]:
+                    finding_json.pop("url", None)
+                if is_last and i == 0:
+                    comma = ""
+                print(f"{util.json_dump(finding_json, indent=1)}{comma}\n", file=f, end="")
+            else:
+                if kwargs[options.WITH_URL]:
+                    url = f'{sep}"{finding.url()}"'
+                print(f"{finding.to_csv(sep)}{url}", file=f)
+
+
+def __write_findings(queue, file_to_write, file_format, with_url, separator):
+    while True:
+        while queue.empty():
+            time.sleep(0.5)
+        (data, is_last) = queue.get()
+        if data == WRITE_END:
+            queue.task_done()
+            break
+
+        global TOTAL_FINDINGS
+        TOTAL_FINDINGS += len(data)
+        __dump_findings(data, file_to_write, file_format, is_last, withURL=with_url, csvSeparator=separator)
+        queue.task_done()
+
+
+def __dump_compact(finding_list, file, **kwargs):
+    new_dict = {}
+    for finding in finding_list.values():
+        f_json = finding.to_json()
+        if not kwargs[options.WITH_URL]:
+            f_json.pop("url", None)
+        pkey = f_json.pop("projectKey")
+        ftype = f_json.pop("type")
+        if pkey in new_dict:
+            if ftype in new_dict[pkey]:
+                new_dict[pkey][ftype].append(f_json)
+            else:
+                new_dict[pkey].update({ftype: [f_json]})
+        else:
+            new_dict[pkey] = {ftype: [f_json]}
+    with util.open_file(file) as f:
+        print(util.json_dump(new_dict, indent=1), file=f)
+
+
+def __get_list(project, list_str, list_type):
+    if list_str == "*":
+        list_array = project.branches().keys() if list_type == "branch" else project.pull_requests().keys()
+    elif list_str is not None:
+        list_array = util.csv_to_list(list_str)
+    else:
+        list_array = []
+    return list_array
+
+
+def __verify_inputs(params):
+    diff = util.difference(util.csv_to_list(params.get("resolutions", None)), issues.RESOLUTIONS + hotspots.RESOLUTIONS)
+    if diff:
+        util.exit_fatal(f"Resolutions {str(diff)} are not legit resolutions", options.ERR_WRONG_SEARCH_CRITERIA)
+
+    diff = util.difference(util.csv_to_list(params.get("statuses", None)), issues.STATUSES + hotspots.STATUSES)
+    if diff:
+        util.exit_fatal(f"Statuses {str(diff)} are not legit statuses", options.ERR_WRONG_SEARCH_CRITERIA)
+
+    diff = util.difference(util.csv_to_list(params.get("severities", None)), issues.SEVERITIES + hotspots.SEVERITIES)
+    if diff:
+        util.exit_fatal(f"Severities {str(diff)} are not legit severities", options.ERR_WRONG_SEARCH_CRITERIA)
+
+    diff = util.difference(util.csv_to_list(params.get("types", None)), issues.TYPES + hotspots.TYPES)
+    if diff:
+        util.exit_fatal(f"Types {str(diff)} are not legit types", options.ERR_WRONG_SEARCH_CRITERIA)
+
+    return True
+
+
+def __get_project_findings(queue, write_queue):
+    while not queue.empty():
+        (key, endpoint, params) = queue.get()
+        search_findings = params["useFindings"]
+        status_list = util.csv_to_list(params.get("statuses", None))
+        i_statuses = util.intersection(status_list, issues.STATUSES)
+        h_statuses = util.intersection(status_list, hotspots.STATUSES)
+        resol_list = util.csv_to_list(params.get("resolutions", None))
+        i_resols = util.intersection(resol_list, issues.RESOLUTIONS)
+        h_resols = util.intersection(resol_list, hotspots.RESOLUTIONS)
+        type_list = util.csv_to_list(params.get("types", None))
+        i_types = util.intersection(type_list, issues.TYPES)
+        h_types = util.intersection(type_list, hotspots.TYPES)
+        sev_list = util.csv_to_list(params.get("severities", None))
+        i_sevs = util.intersection(sev_list, issues.SEVERITIES)
+        h_sevs = util.intersection(sev_list, hotspots.SEVERITIES)
+
+        if status_list or resol_list or type_list or sev_list:
+            search_findings = False
+
+        util.logger.debug("WriteQueue %s task %s put", str(write_queue), key)
+        if search_findings:
+            findings_list = findings.export_findings(endpoint, key, branch=params.get("branch", None), pull_request=params.get("pullRequest", None))
+
+            write_queue.put([findings_list, queue.empty()])
+        else:
+            new_params = issues.get_search_criteria(params)
+            new_params.update({"branch": params.get("branch", None), "pullRequest": params.get("pullRequest", None)})
+            findings_list = {}
+            if (i_statuses or not status_list) and (i_resols or not resol_list) and (i_types or not type_list) and (i_sevs or not sev_list):
+                findings_list = issues.search_by_project(key, params=new_params, endpoint=endpoint)
+            else:
+                util.logger.debug("Status = %s, Types = %s, Resol = %s, Sev = %s", str(i_statuses), str(i_types), str(i_resols), str(i_sevs))
+                util.logger.info("Selected types, severities, resolutions or statuses disables issue search")
+
+            if (h_statuses or not status_list) and (h_resols or not resol_list) and (h_types or not type_list) and (h_sevs or not sev_list):
+                new_params = hotspots.get_search_criteria(params)
+                new_params.update({"branch": params.get("branch", None), "pullRequest": params.get("pullRequest", None)})
+                findings_list.update(hotspots.search_by_project(key, endpoint=endpoint, params=new_params))
+            else:
+                util.logger.debug("Status = %s, Types = %s, Resol = %s, Sev = %s", str(h_statuses), str(h_types), str(h_resols), str(h_sevs))
+                util.logger.info("Selected types, severities, resolutions or statuses disables issue search")
+            write_queue.put([findings_list, queue.empty()])
+        util.logger.debug("Queue %s task %s done", str(queue), key)
+        queue.task_done()
+
+
+def store_findings(project_list, params, endpoint, file, format, threads=4, with_url=False, csv_separator=","):
+    my_queue = Queue(maxsize=0)
+    write_queue = Queue(maxsize=0)
+    for key, project in project_list.items():
+        branches = __get_list(project, params.pop("branches", None), "branch")
+        prs = __get_list(project, params.pop("pullRequests", None), "pullrequest")
+        for b in branches:
+            params["branch"] = b
+            util.logger.debug("Queue %s task %s put", str(my_queue), key)
+            my_queue.put((key, endpoint, params.copy()))
+        params.pop("branch", None)
+        for p in prs:
+            params["pullRequest"] = p
+            util.logger.debug("Queue %s task %s put", str(my_queue), key)
+            my_queue.put((key, endpoint, params.copy()))
+        params.pop("pullRequest", None)
+        if not (branches or prs):
+            util.logger.debug("Queue %s task %s put", str(my_queue), key)
+            my_queue.put((key, endpoint, params.copy()))
+
+    for i in range(threads):
+        util.logger.debug("Starting finding search thread 'findingSearch%d'", i)
+        worker = Thread(target=__get_project_findings, args=[my_queue, write_queue])
+        worker.setDaemon(True)
+        worker.setName(f"findingSearch{i}")
+        worker.start()
+
+    util.logger.info("Starting finding writer thread 'findingWriter'")
+    write_worker = Thread(target=__write_findings, args=[write_queue, file, format, with_url, csv_separator])
+    write_worker.setDaemon(True)
+    write_worker.setName("findingWriter")
+    write_worker.start()
+
+    my_queue.join()
+    # Tell the writer thread that writing is complete
+    util.logger.debug("WriteQueue %s task %s put", str(write_queue), str(WRITE_END))
+    write_queue.put((WRITE_END, True))
+    write_queue.join()
+
+
+def main():
+    kwargs = vars(parse_args("Sonar findings extractor"))
+    sqenv = platform.Platform(some_url=kwargs["url"], some_token=kwargs["token"], cert_file=kwargs["clientCert"])
+    del kwargs["token"]
+    util.check_environment(kwargs)
+    util.logger.info("sonar-tools version %s", version.PACKAGE_VERSION)
+    start_time = datetime.datetime.today()
+    params = util.remove_nones(kwargs.copy())
+    __verify_inputs(params)
+
+    for p in ("statuses", "createdAfter", "createdBefore", "resolutions", "severities", "types", "tags"):
+        if params.get(p, None) is not None:
+            if params["useFindings"]:
+                util.logger.warning("Selected search criteria %s will disable --useFindings", params[p])
+            params["useFindings"] = False
+            break
+    try:
+        project_list = projects.get_list(endpoint=sqenv, key_list=util.csv_to_list(kwargs.get("projectKeys", None)))
+    except exceptions.ObjectNotFound as e:
+        util.exit_fatal(e.message, options.ERR_NO_SUCH_KEY)
+    fmt = kwargs.pop("format", None)
+    fname = kwargs.pop("file", None)
+    if fname is not None:
+        ext = fname.split(".")[-1].lower()
+        if os.path.exists(fname):
+            os.remove(fname)
+        if ext in ("csv", "json"):
+            fmt = ext
+
+    util.logger.info("Exporting findings for %d projects with params %s", len(project_list), str(params))
+    __write_header(fname, fmt)
+    store_findings(
+        project_list,
+        params=params,
+        endpoint=sqenv,
+        file=fname,
+        format=fmt,
+        threads=kwargs[options.NBR_THREADS],
+        with_url=kwargs[options.WITH_URL],
+        csv_separator=kwargs[options.CSV_SEPARATOR],
+    )
+    __write_footer(fname, fmt)
+    util.logger.info("Returned findings: %d - Total execution time: %s", TOTAL_FINDINGS, str(datetime.datetime.today() - start_time))
+    sys.exit(0)
+
+
+if __name__ == "__main__":
+    main()
```

## tools/findings_sync.py

 * *Ordering differences only*

```diff
@@ -1,190 +1,190 @@
-#!/usr/local/bin/python3
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-"""
-    This script propagates the manual issue changes (FP, WF, Change
-    of severity, of issue type, comments) from:
-    - One project to another (normally on different platforms but not necessarily).
-      The 2 platform don't need to be identical in version, edition or plugins
-    - One branch of a project to another branch of the same project (normally LLBs)
-
-    Only issues with a 100% match are synchronized. When there's a doubt, nothing is done
-"""
-
-from sonar import platform, version, syncer, options, exceptions
-from sonar.projects import projects
-from sonar.projects.branches import Branch
-import sonar.utilities as util
-
-_WITH_COMMENTS = {"additionalFields": "comments"}
-
-
-def __parse_args(desc):
-    parser = util.set_common_args(desc)
-    parser = util.set_key_arg(parser)
-    parser = util.set_output_file_args(parser)
-    parser = util.set_target_sonar_args(parser)
-    parser.add_argument(
-        "-r",
-        "--recover",
-        required=False,
-        help="""What information to replicate. Default is FP and WF, but issue assignment,
-                        tags, severity and type change can be recovered too""",
-    )
-    parser.add_argument("-b", "--sourceBranch", required=False, help="Name of the source branch")
-    parser.add_argument("-B", "--targetBranch", required=False, help="Name of the target branch")
-    parser.add_argument(
-        "-K",
-        "--targetProjectKey",
-        required=False,
-        help="""key of the target project when synchronizing 2 projects
-                        or 2 branches on a same platform""",
-    )
-    parser.add_argument(
-        "--login",
-        required=True,
-        help="One (or several comma separated) service account(s) used for issue-sync",
-    )
-    parser.add_argument(
-        "--nocomment",
-        required=False,
-        default=False,
-        action="store_true",
-        help="If specified, will not comment related to the sync in the target issue",
-    )
-    # parser.add_argument('--noassign', required=False, default=False, action='store_true',
-    #                    help="If specified, will not apply issue assignment in the target issue")
-    parser.add_argument(
-        "--nolink",
-        required=False,
-        default=False,
-        action="store_true",
-        help="If specified, will not add a link to source issue in the target issue comments",
-    )
-
-    args = util.parse_and_check_token(parser)
-    util.check_token(args.token)
-    return args
-
-
-def __dump_report(report, file):
-    txt = util.json_dump(report)
-    if file is None:
-        util.logger.info("Dumping report to stdout")
-        print(txt)
-    else:
-        util.logger.info("Dumping report to file '%s'", file)
-        with open(file, "w", encoding="utf-8") as fh:
-            print(txt, file=fh)
-
-
-def main():
-    args = __parse_args(
-        "Synchronizes issues changelog of different branches of same or different projects, "
-        "see: https://pypi.org/project/sonar-tools/#sonar-issues-sync"
-    )
-
-    util.logger.info("sonar-tools version %s", version.PACKAGE_VERSION)
-    source_env = platform.Platform(some_url=args.url, some_token=args.token, cert_file=args.clientCert)
-    params = vars(args)
-    util.check_environment(params)
-    source_key = params["projectKeys"]
-    target_key = params.get("targetProjectKey", None)
-    source_branch = params.get("sourceBranch", None)
-    target_branch = params.get("targetBranch", None)
-    target_url = params.get("urlTarget", None)
-
-    settings = {
-        syncer.SYNC_ADD_COMMENTS: not params["nocomment"],
-        syncer.SYNC_ADD_LINK: not params["nolink"],
-        syncer.SYNC_ASSIGN: True,
-        syncer.SYNC_IGNORE_COMPONENTS: False,
-        syncer.SYNC_SERVICE_ACCOUNTS: util.csv_to_list(args.login),
-    }
-    report = []
-    try:
-        if not projects.exists(source_key, endpoint=source_env):
-            raise exceptions.ObjectNotFound(source_key, f"Project key '{source_key}' does not exist")
-        if target_url is None and target_key is None and source_branch is None and target_branch is None:
-            util.logger.info("Syncing findings between all branches of a same project")
-            (report, counters) = projects.Project.get_object(key=source_key, endpoint=source_env).sync_branches(sync_settings=settings)
-        elif target_url is None and target_key is None and source_branch is not None and target_branch is not None:
-            util.logger.info("Syncing findings between 2 branches of same project")
-            if source_branch != target_branch:
-                src_branch = Branch.get_object(projects.Project.get_object(source_key, source_env), source_branch)
-                tgt_branch = Branch.get_object(projects.Project.get_object(source_key, source_env), target_branch)
-                (report, counters) = src_branch.sync(tgt_branch, sync_settings=settings)
-            else:
-                util.logger.critical("Can't sync same source and target branch or a same project, aborting...")
-
-        elif target_url is None and target_key is not None:
-            util.logger.info("Syncing findings between 2 different projects of same platform")
-            if not projects.exists(target_key, endpoint=source_env):
-                raise exceptions.ObjectNotFound(target_key, f"Project key '{target_key}' does not exist")
-            settings[syncer.SYNC_IGNORE_COMPONENTS] = target_key != source_key
-            src_branch = Branch.get_object(projects.Project.get_object(key=source_key, endpoint=source_env), source_branch)
-            tgt_branch = Branch.get_object(projects.Project.get_object(key=target_key, endpoint=source_env), target_branch)
-            (report, counters) = src_branch.sync(tgt_branch, sync_settings=settings)
-
-        elif target_url is not None and target_key is not None:
-            util.check_token(args.tokenTarget)
-            target_env = platform.Platform(some_url=args.urlTarget, some_token=args.tokenTarget, cert_file=args.clientCert)
-            if not projects.exists(target_key, endpoint=target_env):
-                raise exceptions.ObjectNotFound(target_key, f"Project key '{target_key}' does not exist")
-            settings[syncer.SYNC_IGNORE_COMPONENTS] = target_key != source_key
-            if source_branch is not None or target_branch is not None:
-                util.logger.info("Syncing findings between main branch of 2 projects of different platforms")
-                src_branch = Branch.get_object(projects.Project.get_object(key=source_key, endpoint=source_env), source_branch)
-                tgt_branch = Branch.get_object(projects.Project.get_object(key=target_key, endpoint=target_env), target_branch)
-                (report, counters) = src_branch.sync(tgt_branch, sync_settings=settings)
-            else:
-                util.logger.info("Syncing findings between all branches of 2 projects of different platforms")
-                src_project = projects.Project.get_object(key=source_key, endpoint=source_env)
-                tgt_project = projects.Project.get_object(key=target_key, endpoint=target_env)
-                (report, counters) = src_project.sync(tgt_project, sync_settings=settings)
-
-        __dump_report(report, args.file)
-        util.logger.info("%d issues needed to be synchronized", counters.get("nb_to_sync", 0))
-        util.logger.info("%d issues were synchronized successfully", counters.get("nb_applies", 0))
-        util.logger.info(
-            "%d issues could not be synchronized because no match was found in target",
-            counters.get("nb_no_match", 0),
-        )
-        util.logger.info(
-            "%d issues could not be synchronized because there were multiple matches",
-            counters.get("nb_multiple_matches", 0),
-        )
-        util.logger.info(
-            "%d issues could not be synchronized because the match was approximate",
-            counters.get("nb_approx_match", 0),
-        )
-        util.logger.info(
-            "%d issues could not be synchronized because target issue already had a changelog",
-            counters.get("nb_tgt_has_changelog", 0),
-        )
-
-    except exceptions.ObjectNotFound as e:
-        util.exit_fatal(e.message, options.ERR_NO_SUCH_KEY)
-    except exceptions.UnsupportedOperation as e:
-        util.exit_fatal(e.message, options.ERR_UNSUPPORTED_OPERATION)
-
-
-if __name__ == "__main__":
-    main()
+#!/usr/local/bin/python3
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+"""
+    This script propagates the manual issue changes (FP, WF, Change
+    of severity, of issue type, comments) from:
+    - One project to another (normally on different platforms but not necessarily).
+      The 2 platform don't need to be identical in version, edition or plugins
+    - One branch of a project to another branch of the same project (normally LLBs)
+
+    Only issues with a 100% match are synchronized. When there's a doubt, nothing is done
+"""
+
+from sonar import platform, version, syncer, options, exceptions
+from sonar.projects import projects
+from sonar.projects.branches import Branch
+import sonar.utilities as util
+
+_WITH_COMMENTS = {"additionalFields": "comments"}
+
+
+def __parse_args(desc):
+    parser = util.set_common_args(desc)
+    parser = util.set_key_arg(parser)
+    parser = util.set_output_file_args(parser)
+    parser = util.set_target_sonar_args(parser)
+    parser.add_argument(
+        "-r",
+        "--recover",
+        required=False,
+        help="""What information to replicate. Default is FP and WF, but issue assignment,
+                        tags, severity and type change can be recovered too""",
+    )
+    parser.add_argument("-b", "--sourceBranch", required=False, help="Name of the source branch")
+    parser.add_argument("-B", "--targetBranch", required=False, help="Name of the target branch")
+    parser.add_argument(
+        "-K",
+        "--targetProjectKey",
+        required=False,
+        help="""key of the target project when synchronizing 2 projects
+                        or 2 branches on a same platform""",
+    )
+    parser.add_argument(
+        "--login",
+        required=True,
+        help="One (or several comma separated) service account(s) used for issue-sync",
+    )
+    parser.add_argument(
+        "--nocomment",
+        required=False,
+        default=False,
+        action="store_true",
+        help="If specified, will not comment related to the sync in the target issue",
+    )
+    # parser.add_argument('--noassign', required=False, default=False, action='store_true',
+    #                    help="If specified, will not apply issue assignment in the target issue")
+    parser.add_argument(
+        "--nolink",
+        required=False,
+        default=False,
+        action="store_true",
+        help="If specified, will not add a link to source issue in the target issue comments",
+    )
+
+    args = util.parse_and_check_token(parser)
+    util.check_token(args.token)
+    return args
+
+
+def __dump_report(report, file):
+    txt = util.json_dump(report)
+    if file is None:
+        util.logger.info("Dumping report to stdout")
+        print(txt)
+    else:
+        util.logger.info("Dumping report to file '%s'", file)
+        with open(file, "w", encoding="utf-8") as fh:
+            print(txt, file=fh)
+
+
+def main():
+    args = __parse_args(
+        "Synchronizes issues changelog of different branches of same or different projects, "
+        "see: https://pypi.org/project/sonar-tools/#sonar-issues-sync"
+    )
+
+    util.logger.info("sonar-tools version %s", version.PACKAGE_VERSION)
+    source_env = platform.Platform(some_url=args.url, some_token=args.token, cert_file=args.clientCert)
+    params = vars(args)
+    util.check_environment(params)
+    source_key = params["projectKeys"]
+    target_key = params.get("targetProjectKey", None)
+    source_branch = params.get("sourceBranch", None)
+    target_branch = params.get("targetBranch", None)
+    target_url = params.get("urlTarget", None)
+
+    settings = {
+        syncer.SYNC_ADD_COMMENTS: not params["nocomment"],
+        syncer.SYNC_ADD_LINK: not params["nolink"],
+        syncer.SYNC_ASSIGN: True,
+        syncer.SYNC_IGNORE_COMPONENTS: False,
+        syncer.SYNC_SERVICE_ACCOUNTS: util.csv_to_list(args.login),
+    }
+    report = []
+    try:
+        if not projects.exists(source_key, endpoint=source_env):
+            raise exceptions.ObjectNotFound(source_key, f"Project key '{source_key}' does not exist")
+        if target_url is None and target_key is None and source_branch is None and target_branch is None:
+            util.logger.info("Syncing findings between all branches of a same project")
+            (report, counters) = projects.Project.get_object(key=source_key, endpoint=source_env).sync_branches(sync_settings=settings)
+        elif target_url is None and target_key is None and source_branch is not None and target_branch is not None:
+            util.logger.info("Syncing findings between 2 branches of same project")
+            if source_branch != target_branch:
+                src_branch = Branch.get_object(projects.Project.get_object(source_key, source_env), source_branch)
+                tgt_branch = Branch.get_object(projects.Project.get_object(source_key, source_env), target_branch)
+                (report, counters) = src_branch.sync(tgt_branch, sync_settings=settings)
+            else:
+                util.logger.critical("Can't sync same source and target branch or a same project, aborting...")
+
+        elif target_url is None and target_key is not None:
+            util.logger.info("Syncing findings between 2 different projects of same platform")
+            if not projects.exists(target_key, endpoint=source_env):
+                raise exceptions.ObjectNotFound(target_key, f"Project key '{target_key}' does not exist")
+            settings[syncer.SYNC_IGNORE_COMPONENTS] = target_key != source_key
+            src_branch = Branch.get_object(projects.Project.get_object(key=source_key, endpoint=source_env), source_branch)
+            tgt_branch = Branch.get_object(projects.Project.get_object(key=target_key, endpoint=source_env), target_branch)
+            (report, counters) = src_branch.sync(tgt_branch, sync_settings=settings)
+
+        elif target_url is not None and target_key is not None:
+            util.check_token(args.tokenTarget)
+            target_env = platform.Platform(some_url=args.urlTarget, some_token=args.tokenTarget, cert_file=args.clientCert)
+            if not projects.exists(target_key, endpoint=target_env):
+                raise exceptions.ObjectNotFound(target_key, f"Project key '{target_key}' does not exist")
+            settings[syncer.SYNC_IGNORE_COMPONENTS] = target_key != source_key
+            if source_branch is not None or target_branch is not None:
+                util.logger.info("Syncing findings between main branch of 2 projects of different platforms")
+                src_branch = Branch.get_object(projects.Project.get_object(key=source_key, endpoint=source_env), source_branch)
+                tgt_branch = Branch.get_object(projects.Project.get_object(key=target_key, endpoint=target_env), target_branch)
+                (report, counters) = src_branch.sync(tgt_branch, sync_settings=settings)
+            else:
+                util.logger.info("Syncing findings between all branches of 2 projects of different platforms")
+                src_project = projects.Project.get_object(key=source_key, endpoint=source_env)
+                tgt_project = projects.Project.get_object(key=target_key, endpoint=target_env)
+                (report, counters) = src_project.sync(tgt_project, sync_settings=settings)
+
+        __dump_report(report, args.file)
+        util.logger.info("%d issues needed to be synchronized", counters.get("nb_to_sync", 0))
+        util.logger.info("%d issues were synchronized successfully", counters.get("nb_applies", 0))
+        util.logger.info(
+            "%d issues could not be synchronized because no match was found in target",
+            counters.get("nb_no_match", 0),
+        )
+        util.logger.info(
+            "%d issues could not be synchronized because there were multiple matches",
+            counters.get("nb_multiple_matches", 0),
+        )
+        util.logger.info(
+            "%d issues could not be synchronized because the match was approximate",
+            counters.get("nb_approx_match", 0),
+        )
+        util.logger.info(
+            "%d issues could not be synchronized because target issue already had a changelog",
+            counters.get("nb_tgt_has_changelog", 0),
+        )
+
+    except exceptions.ObjectNotFound as e:
+        util.exit_fatal(e.message, options.ERR_NO_SUCH_KEY)
+    except exceptions.UnsupportedOperation as e:
+        util.exit_fatal(e.message, options.ERR_UNSUPPORTED_OPERATION)
+
+
+if __name__ == "__main__":
+    main()
```

## tools/housekeeper.py

 * *Ordering differences only*

```diff
@@ -1,234 +1,234 @@
-#!/usr/local/bin/python3
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-"""
-
-    Removes obsolete data from SonarQube platform
-    Currently:
-    - projects, branches, PR not analyzed since a given number of days
-    - Tokens not renewed since a given number of days
-
-"""
-import sys
-from sonar import platform, tokens, users, groups, version, options
-from sonar.projects import projects, branches, pull_requests
-import sonar.utilities as util
-import sonar.exceptions as ex
-from sonar.audit import config, problem
-
-
-def get_project_problems(max_days_proj, max_days_branch, max_days_pr, nb_threads, endpoint):
-    problems = []
-    if max_days_proj < 90:
-        util.logger.error("As a safety measure, can't delete projects more recent than 90 days")
-        return problems
-
-    settings = {
-        "audit.projects.maxLastAnalysisAge": max_days_proj,
-        "audit.projects.branches.maxLastAnalysisAge": max_days_branch,
-        "audit.projects.pullRequests.maxLastAnalysisAge": max_days_pr,
-        "audit.projects.neverAnalyzed": False,
-        "audit.projects.duplicates": False,
-        "audit.projects.visibility": False,
-        "audit.projects.permissions": False,
-    }
-    settings = config.load(config_name="sonar-audit", settings=settings)
-    settings["threads"] = nb_threads
-    problems = projects.audit(endpoint=endpoint, audit_settings=settings)
-    nb_proj = 0
-    total_loc = 0
-    for p in problems:
-        if p.concerned_object is not None and isinstance(p.concerned_object, projects.Project):
-            nb_proj += 1
-            total_loc += int(p.concerned_object.get_measure("ncloc", fallback="0"))
-
-    if nb_proj == 0:
-        util.logger.info("%d projects older than %d days found during audit", nb_proj, max_days_proj)
-    else:
-        util.logger.warning(
-            "%d projects older than %d days for a total of %d LoC found during audit",
-            nb_proj,
-            max_days_proj,
-            total_loc,
-        )
-    return problems
-
-
-def get_user_problems(max_days, endpoint):
-    settings = {
-        "audit.tokens.maxAge": max_days,
-        "audit.tokens.maxUnusedAge": 30,
-        "audit.groups.empty": True,
-    }
-    settings = config.load(config_name="sonar-audit", settings=settings)
-    user_problems = users.audit(endpoint=endpoint, audit_settings=settings)
-    nb_problems = len(user_problems)
-    if nb_problems == 0:
-        util.logger.info(
-            "%d user tokens older than %d days found during audit",
-            nb_problems,
-            max_days,
-        )
-    else:
-        util.logger.warning(
-            "%d user tokens older than %d days found during audit",
-            nb_problems,
-            max_days,
-        )
-    group_problems = groups.audit(endpoint=endpoint, audit_settings=settings)
-    user_problems += group_problems
-    nb_problems = len(group_problems)
-    if nb_problems == 0:
-        util.logger.info("%d empty groups found during audit", nb_problems)
-    else:
-        util.logger.warning("%d empty groups found during audit", nb_problems)
-    return user_problems
-
-
-def _parse_arguments():
-    _DEFAULT_PROJECT_OBSOLESCENCE = 365
-    _DEFAULT_BRANCH_OBSOLESCENCE = 90
-    _DEFAULT_PR_OBSOLESCENCE = 30
-    _DEFAULT_TOKEN_OBSOLESCENCE = 365
-    parser = util.set_common_args("Deletes projects not analyzed since a given numbr of days")
-    parser = options.add_thread_arg(parser, "auditing before housekeeping")
-    parser.add_argument(
-        "--mode",
-        required=False,
-        choices=["dry-run", "delete"],
-        default="dry-run",
-        help="""
-                        If 'dry-run', script only lists objects (projects, branches, PRs or tokens) to delete,
-                        If 'delete' it deletes projects or tokens
-                        """,
-    )
-    parser.add_argument(
-        "-P",
-        "--projects",
-        required=False,
-        type=int,
-        default=_DEFAULT_PROJECT_OBSOLESCENCE,
-        help=f"Deletes projects not analyzed since a given number of days, by default {_DEFAULT_PROJECT_OBSOLESCENCE} days",
-    )
-    parser.add_argument(
-        "-B",
-        "--branches",
-        required=False,
-        type=int,
-        default=_DEFAULT_BRANCH_OBSOLESCENCE,
-        help=f"Deletes branches not to be kept and not analyzed since a given number of days, by default {_DEFAULT_BRANCH_OBSOLESCENCE} days",
-    )
-    parser.add_argument(
-        "-R",
-        "--pullrequests",
-        required=False,
-        type=int,
-        default=_DEFAULT_BRANCH_OBSOLESCENCE,
-        help=f"Deletes pull requests not analyzed since a given number of days, by default {_DEFAULT_PR_OBSOLESCENCE} days",
-    )
-    parser.add_argument(
-        "-T",
-        "--tokens",
-        required=False,
-        type=int,
-        default=_DEFAULT_TOKEN_OBSOLESCENCE,
-        help=f"Deletes user tokens older than a certain number of days, by default {_DEFAULT_TOKEN_OBSOLESCENCE} days",
-    )
-    args = util.parse_and_check_token(parser)
-    util.check_token(args.token)
-    return args
-
-
-def _delete_objects(problems, mode):
-    revoked_token_count = 0
-    deleted_projects = {}
-    deleted_branch_count = 0
-    deleted_pr_count = 0
-    deleted_loc = 0
-    for p in problems:
-        obj = p.concerned_object
-        if obj is None:
-            continue  # BUG
-        try:
-            if isinstance(obj, projects.Project):
-                loc = int(obj.get_measure("ncloc", fallback="0"))
-                if mode == "delete":
-                    util.logger.info("Deleting %s, %d LoC", str(obj), loc)
-                else:
-                    util.logger.info("%s, %d LoC should be deleted", str(obj), loc)
-                if mode != "delete" or obj.delete():
-                    deleted_projects[obj.key] = obj
-                    deleted_loc += loc
-            if isinstance(obj, branches.Branch):
-                if obj.concerned_object.key in deleted_projects:
-                    util.logger.info("%s deleted, so no need to delete %s", str(obj.concerned_object), str(obj))
-                elif mode != "delete" or obj.delete():
-                    deleted_branch_count += 1
-            if isinstance(obj, pull_requests.PullRequest):
-                if obj.project.key in deleted_projects:
-                    util.logger.info("%s deleted, so no need to delete %s", str(obj.project), str(obj))
-                elif mode != "delete" or obj.delete():
-                    deleted_pr_count += 1
-            if isinstance(obj, tokens.UserToken) and (mode != "delete" or obj.revoke()):
-                revoked_token_count += 1
-        except ex.ObjectNotFound:
-            util.logger.warning("%s does not exist, deletion skipped...", str(obj))
-
-    return (
-        len(deleted_projects),
-        deleted_loc,
-        deleted_branch_count,
-        deleted_pr_count,
-        revoked_token_count,
-    )
-
-
-def main():
-    args = _parse_arguments()
-
-    sq = platform.Platform(some_url=args.url, some_token=args.token, cert_file=args.clientCert)
-    kwargs = vars(args)
-    mode = args.mode
-    util.check_environment(kwargs)
-    util.logger.debug("Args = %s", str(kwargs))
-    util.logger.info("sonar-tools version %s", version.PACKAGE_VERSION)
-    problems = []
-    if args.projects > 0 or args.branches > 0 or args.pullrequests > 0:
-        problems = get_project_problems(args.projects, args.branches, args.pullrequests, args.threads, sq)
-
-    if args.tokens:
-        problems += get_user_problems(args.tokens, sq)
-
-    problem.dump_report(problems, file=None, file_format="csv")
-
-    op = "to delete"
-    if mode == "delete":
-        op = "deleted"
-    (deleted_proj, deleted_loc, deleted_branches, deleted_prs, revoked_tokens) = _delete_objects(problems, mode)
-
-    util.logger.info("%d projects older than %d days (%d LoCs) %s", deleted_proj, args.projects, deleted_loc, op)
-    util.logger.info("%d branches older than %d days %s", deleted_branches, args.branches, op)
-    util.logger.info("%d pull requests older than %d days %s", deleted_prs, args.pullrequests, op)
-    util.logger.info("%d tokens older than %d days revoked", revoked_tokens, args.tokens)
-    sys.exit(0)
-
-
-if __name__ == "__main__":
-    main()
+#!/usr/local/bin/python3
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+"""
+
+    Removes obsolete data from SonarQube platform
+    Currently:
+    - projects, branches, PR not analyzed since a given number of days
+    - Tokens not renewed since a given number of days
+
+"""
+import sys
+from sonar import platform, tokens, users, groups, version, options
+from sonar.projects import projects, branches, pull_requests
+import sonar.utilities as util
+import sonar.exceptions as ex
+from sonar.audit import config, problem
+
+
+def get_project_problems(max_days_proj, max_days_branch, max_days_pr, nb_threads, endpoint):
+    problems = []
+    if max_days_proj < 90:
+        util.logger.error("As a safety measure, can't delete projects more recent than 90 days")
+        return problems
+
+    settings = {
+        "audit.projects.maxLastAnalysisAge": max_days_proj,
+        "audit.projects.branches.maxLastAnalysisAge": max_days_branch,
+        "audit.projects.pullRequests.maxLastAnalysisAge": max_days_pr,
+        "audit.projects.neverAnalyzed": False,
+        "audit.projects.duplicates": False,
+        "audit.projects.visibility": False,
+        "audit.projects.permissions": False,
+    }
+    settings = config.load(config_name="sonar-audit", settings=settings)
+    settings["threads"] = nb_threads
+    problems = projects.audit(endpoint=endpoint, audit_settings=settings)
+    nb_proj = 0
+    total_loc = 0
+    for p in problems:
+        if p.concerned_object is not None and isinstance(p.concerned_object, projects.Project):
+            nb_proj += 1
+            total_loc += int(p.concerned_object.get_measure("ncloc", fallback="0"))
+
+    if nb_proj == 0:
+        util.logger.info("%d projects older than %d days found during audit", nb_proj, max_days_proj)
+    else:
+        util.logger.warning(
+            "%d projects older than %d days for a total of %d LoC found during audit",
+            nb_proj,
+            max_days_proj,
+            total_loc,
+        )
+    return problems
+
+
+def get_user_problems(max_days, endpoint):
+    settings = {
+        "audit.tokens.maxAge": max_days,
+        "audit.tokens.maxUnusedAge": 30,
+        "audit.groups.empty": True,
+    }
+    settings = config.load(config_name="sonar-audit", settings=settings)
+    user_problems = users.audit(endpoint=endpoint, audit_settings=settings)
+    nb_problems = len(user_problems)
+    if nb_problems == 0:
+        util.logger.info(
+            "%d user tokens older than %d days found during audit",
+            nb_problems,
+            max_days,
+        )
+    else:
+        util.logger.warning(
+            "%d user tokens older than %d days found during audit",
+            nb_problems,
+            max_days,
+        )
+    group_problems = groups.audit(endpoint=endpoint, audit_settings=settings)
+    user_problems += group_problems
+    nb_problems = len(group_problems)
+    if nb_problems == 0:
+        util.logger.info("%d empty groups found during audit", nb_problems)
+    else:
+        util.logger.warning("%d empty groups found during audit", nb_problems)
+    return user_problems
+
+
+def _parse_arguments():
+    _DEFAULT_PROJECT_OBSOLESCENCE = 365
+    _DEFAULT_BRANCH_OBSOLESCENCE = 90
+    _DEFAULT_PR_OBSOLESCENCE = 30
+    _DEFAULT_TOKEN_OBSOLESCENCE = 365
+    parser = util.set_common_args("Deletes projects not analyzed since a given numbr of days")
+    parser = options.add_thread_arg(parser, "auditing before housekeeping")
+    parser.add_argument(
+        "--mode",
+        required=False,
+        choices=["dry-run", "delete"],
+        default="dry-run",
+        help="""
+                        If 'dry-run', script only lists objects (projects, branches, PRs or tokens) to delete,
+                        If 'delete' it deletes projects or tokens
+                        """,
+    )
+    parser.add_argument(
+        "-P",
+        "--projects",
+        required=False,
+        type=int,
+        default=_DEFAULT_PROJECT_OBSOLESCENCE,
+        help=f"Deletes projects not analyzed since a given number of days, by default {_DEFAULT_PROJECT_OBSOLESCENCE} days",
+    )
+    parser.add_argument(
+        "-B",
+        "--branches",
+        required=False,
+        type=int,
+        default=_DEFAULT_BRANCH_OBSOLESCENCE,
+        help=f"Deletes branches not to be kept and not analyzed since a given number of days, by default {_DEFAULT_BRANCH_OBSOLESCENCE} days",
+    )
+    parser.add_argument(
+        "-R",
+        "--pullrequests",
+        required=False,
+        type=int,
+        default=_DEFAULT_BRANCH_OBSOLESCENCE,
+        help=f"Deletes pull requests not analyzed since a given number of days, by default {_DEFAULT_PR_OBSOLESCENCE} days",
+    )
+    parser.add_argument(
+        "-T",
+        "--tokens",
+        required=False,
+        type=int,
+        default=_DEFAULT_TOKEN_OBSOLESCENCE,
+        help=f"Deletes user tokens older than a certain number of days, by default {_DEFAULT_TOKEN_OBSOLESCENCE} days",
+    )
+    args = util.parse_and_check_token(parser)
+    util.check_token(args.token)
+    return args
+
+
+def _delete_objects(problems, mode):
+    revoked_token_count = 0
+    deleted_projects = {}
+    deleted_branch_count = 0
+    deleted_pr_count = 0
+    deleted_loc = 0
+    for p in problems:
+        obj = p.concerned_object
+        if obj is None:
+            continue  # BUG
+        try:
+            if isinstance(obj, projects.Project):
+                loc = int(obj.get_measure("ncloc", fallback="0"))
+                if mode == "delete":
+                    util.logger.info("Deleting %s, %d LoC", str(obj), loc)
+                else:
+                    util.logger.info("%s, %d LoC should be deleted", str(obj), loc)
+                if mode != "delete" or obj.delete():
+                    deleted_projects[obj.key] = obj
+                    deleted_loc += loc
+            if isinstance(obj, branches.Branch):
+                if obj.concerned_object.key in deleted_projects:
+                    util.logger.info("%s deleted, so no need to delete %s", str(obj.concerned_object), str(obj))
+                elif mode != "delete" or obj.delete():
+                    deleted_branch_count += 1
+            if isinstance(obj, pull_requests.PullRequest):
+                if obj.project.key in deleted_projects:
+                    util.logger.info("%s deleted, so no need to delete %s", str(obj.project), str(obj))
+                elif mode != "delete" or obj.delete():
+                    deleted_pr_count += 1
+            if isinstance(obj, tokens.UserToken) and (mode != "delete" or obj.revoke()):
+                revoked_token_count += 1
+        except ex.ObjectNotFound:
+            util.logger.warning("%s does not exist, deletion skipped...", str(obj))
+
+    return (
+        len(deleted_projects),
+        deleted_loc,
+        deleted_branch_count,
+        deleted_pr_count,
+        revoked_token_count,
+    )
+
+
+def main():
+    args = _parse_arguments()
+
+    sq = platform.Platform(some_url=args.url, some_token=args.token, cert_file=args.clientCert)
+    kwargs = vars(args)
+    mode = args.mode
+    util.check_environment(kwargs)
+    util.logger.debug("Args = %s", str(kwargs))
+    util.logger.info("sonar-tools version %s", version.PACKAGE_VERSION)
+    problems = []
+    if args.projects > 0 or args.branches > 0 or args.pullrequests > 0:
+        problems = get_project_problems(args.projects, args.branches, args.pullrequests, args.threads, sq)
+
+    if args.tokens:
+        problems += get_user_problems(args.tokens, sq)
+
+    problem.dump_report(problems, file=None, file_format="csv")
+
+    op = "to delete"
+    if mode == "delete":
+        op = "deleted"
+    (deleted_proj, deleted_loc, deleted_branches, deleted_prs, revoked_tokens) = _delete_objects(problems, mode)
+
+    util.logger.info("%d projects older than %d days (%d LoCs) %s", deleted_proj, args.projects, deleted_loc, op)
+    util.logger.info("%d branches older than %d days %s", deleted_branches, args.branches, op)
+    util.logger.info("%d pull requests older than %d days %s", deleted_prs, args.pullrequests, op)
+    util.logger.info("%d tokens older than %d days revoked", revoked_tokens, args.tokens)
+    sys.exit(0)
+
+
+if __name__ == "__main__":
+    main()
```

## tools/loc.py

 * *Ordering differences only*

```diff
@@ -1,178 +1,178 @@
-#!/usr/local/bin/python3
-#
-# sonar-tools
-# Copyright (C) 2022-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-"""
-    Exports LoC per projects
-"""
-import sys
-import csv
-
-from sonar import platform, portfolios, version, options
-from sonar.projects import projects
-import sonar.utilities as util
-
-
-def __deduct_format(fmt, file):
-    if fmt is not None:
-        return fmt
-    if file is not None:
-        ext = file.split(".").pop(-1).lower()
-        util.logger.debug("File extension = %s", ext)
-        if ext == "json":
-            return ext
-    return "csv"
-
-
-def __dump_csv(object_list, fd, **kwargs):
-    writer = csv.writer(fd, delimiter=kwargs[options.CSV_SEPARATOR])
-
-    nb_loc, nb_objects = 0, 0
-    arr = ["# Key", "ncloc"]
-    if kwargs.get(options.WITH_NAME, False):
-        arr.append("name")
-    if kwargs.get(options.WITH_LAST_ANALYSIS, False):
-        arr.append("lastAnalysis")
-    if kwargs.get(options.WITH_URL, False):
-        arr.append("URL")
-    writer.writerow(arr)
-
-    util.logger.info("%d objects with LoCs to export...", len(object_list))
-    obj_type = None
-    for o in object_list.values():
-        if obj_type is None:
-            obj_type = type(o).__name__.lower()
-        loc = o.loc()
-        arr = [o.key, o.loc()]
-        if kwargs.get(options.WITH_NAME, False):
-            arr.append(o.name)
-        if kwargs.get(options.WITH_LAST_ANALYSIS, False):
-            arr.append(o.last_analysis())
-        if kwargs.get(options.WITH_URL, False):
-            arr.append(o.url())
-        writer.writerow(arr)
-        nb_objects += 1
-        nb_loc += loc
-
-        if nb_objects % 50 == 0:
-            util.logger.info("%d %ss and %d LoCs, still counting...", nb_objects, obj_type, nb_loc)
-
-    util.logger.info("%d %ss and %d LoCs in total", len(object_list), obj_type, nb_loc)
-
-
-def __dump_json(object_list, fd, **kwargs):
-    nb_loc, nb_objects = 0, 0
-    data = []
-    util.logger.info("%d objects with LoCs to export...", len(object_list))
-    obj_type = None
-    for o in object_list.values():
-        if obj_type is None:
-            obj_type = type(o).__name__.lower()
-        d = {"key": o.key, "ncloc": o.loc()}
-        if kwargs.get(options.WITH_NAME, False):
-            d["name"] = o.name
-        if kwargs.get(options.WITH_LAST_ANALYSIS, False):
-            d["lastAnalysis"] = util.date_to_string(o.last_analysis())
-        if kwargs.get(options.WITH_URL, False):
-            d["url"] = o.url()
-        data.append(d)
-        nb_objects += 1
-        if nb_objects % 50 == 0:
-            util.logger.info("%d %ss and %d LoCs, still counting...", nb_objects, str(obj_type), nb_loc)
-
-    print(util.json_dump(data), file=fd)
-    util.logger.info("%d %ss and %d LoCs in total", len(object_list), str(obj_type), nb_loc)
-
-
-def __dump_loc(object_list, file, **kwargs):
-    with util.open_file(file) as fd:
-        if kwargs[options.FORMAT] == "json":
-            __dump_json(object_list, fd, **kwargs)
-        else:
-            __dump_csv(object_list, fd, **kwargs)
-
-
-def __parse_args(desc):
-    parser = util.set_common_args(desc)
-    parser = util.set_key_arg(parser)
-    parser = util.set_output_file_args(parser)
-    parser.add_argument(
-        "-n",
-        "--withName",
-        required=False,
-        default=False,
-        action="store_true",
-        help="Also list the project name on top of the project key",
-    )
-    parser.add_argument(
-        "-a",
-        "--" + options.WITH_LAST_ANALYSIS,
-        required=False,
-        default=False,
-        action="store_true",
-        help="Also list the last analysis date on top of nbr of LoC",
-    )
-    parser.add_argument(
-        "--" + options.WITH_URL,
-        required=False,
-        default=False,
-        action="store_true",
-        help="Also list the URL of the objects",
-    )
-    parser.add_argument(
-        "--portfolios",
-        required=False,
-        default=False,
-        action="store_true",
-        help="Export portfolios LoCs instead of projects",
-    )
-    parser.add_argument(
-        "--topLevelOnly",
-        required=False,
-        default=False,
-        action="store_true",
-        help="Extracts only toplevel portfolios LoCs, not sub-portfolios",
-    )
-    args = util.parse_and_check_token(parser)
-    util.check_environment(vars(args))
-    util.check_token(args.token)
-    util.logger.info("sonar-tools version %s", version.PACKAGE_VERSION)
-    return args
-
-
-def main():
-    args = __parse_args("Extract projects or portfolios lines of code, as computed for the licence")
-    endpoint = platform.Platform(some_url=args.url, some_token=args.token, cert_file=args.clientCert)
-    kwargs = vars(args)
-    ofile = kwargs.pop("file", None)
-    args.format = __deduct_format(args.format, ofile)
-
-    if args.portfolios:
-        params = {}
-        if args.topLevelOnly:
-            params["qualifiers"] = "VW"
-        objects_list = portfolios.search(endpoint, params=params)
-    else:
-        objects_list = projects.search(endpoint)
-    __dump_loc(objects_list, ofile, **vars(args))
-    sys.exit(0)
-
-
-if __name__ == "__main__":
-    main()
+#!/usr/local/bin/python3
+#
+# sonar-tools
+# Copyright (C) 2022-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+"""
+    Exports LoC per projects
+"""
+import sys
+import csv
+
+from sonar import platform, portfolios, version, options
+from sonar.projects import projects
+import sonar.utilities as util
+
+
+def __deduct_format(fmt, file):
+    if fmt is not None:
+        return fmt
+    if file is not None:
+        ext = file.split(".").pop(-1).lower()
+        util.logger.debug("File extension = %s", ext)
+        if ext == "json":
+            return ext
+    return "csv"
+
+
+def __dump_csv(object_list, fd, **kwargs):
+    writer = csv.writer(fd, delimiter=kwargs[options.CSV_SEPARATOR])
+
+    nb_loc, nb_objects = 0, 0
+    arr = ["# Key", "ncloc"]
+    if kwargs.get(options.WITH_NAME, False):
+        arr.append("name")
+    if kwargs.get(options.WITH_LAST_ANALYSIS, False):
+        arr.append("lastAnalysis")
+    if kwargs.get(options.WITH_URL, False):
+        arr.append("URL")
+    writer.writerow(arr)
+
+    util.logger.info("%d objects with LoCs to export...", len(object_list))
+    obj_type = None
+    for o in object_list.values():
+        if obj_type is None:
+            obj_type = type(o).__name__.lower()
+        loc = o.loc()
+        arr = [o.key, o.loc()]
+        if kwargs.get(options.WITH_NAME, False):
+            arr.append(o.name)
+        if kwargs.get(options.WITH_LAST_ANALYSIS, False):
+            arr.append(o.last_analysis())
+        if kwargs.get(options.WITH_URL, False):
+            arr.append(o.url())
+        writer.writerow(arr)
+        nb_objects += 1
+        nb_loc += loc
+
+        if nb_objects % 50 == 0:
+            util.logger.info("%d %ss and %d LoCs, still counting...", nb_objects, obj_type, nb_loc)
+
+    util.logger.info("%d %ss and %d LoCs in total", len(object_list), obj_type, nb_loc)
+
+
+def __dump_json(object_list, fd, **kwargs):
+    nb_loc, nb_objects = 0, 0
+    data = []
+    util.logger.info("%d objects with LoCs to export...", len(object_list))
+    obj_type = None
+    for o in object_list.values():
+        if obj_type is None:
+            obj_type = type(o).__name__.lower()
+        d = {"key": o.key, "ncloc": o.loc()}
+        if kwargs.get(options.WITH_NAME, False):
+            d["name"] = o.name
+        if kwargs.get(options.WITH_LAST_ANALYSIS, False):
+            d["lastAnalysis"] = util.date_to_string(o.last_analysis())
+        if kwargs.get(options.WITH_URL, False):
+            d["url"] = o.url()
+        data.append(d)
+        nb_objects += 1
+        if nb_objects % 50 == 0:
+            util.logger.info("%d %ss and %d LoCs, still counting...", nb_objects, str(obj_type), nb_loc)
+
+    print(util.json_dump(data), file=fd)
+    util.logger.info("%d %ss and %d LoCs in total", len(object_list), str(obj_type), nb_loc)
+
+
+def __dump_loc(object_list, file, **kwargs):
+    with util.open_file(file) as fd:
+        if kwargs[options.FORMAT] == "json":
+            __dump_json(object_list, fd, **kwargs)
+        else:
+            __dump_csv(object_list, fd, **kwargs)
+
+
+def __parse_args(desc):
+    parser = util.set_common_args(desc)
+    parser = util.set_key_arg(parser)
+    parser = util.set_output_file_args(parser)
+    parser.add_argument(
+        "-n",
+        "--withName",
+        required=False,
+        default=False,
+        action="store_true",
+        help="Also list the project name on top of the project key",
+    )
+    parser.add_argument(
+        "-a",
+        "--" + options.WITH_LAST_ANALYSIS,
+        required=False,
+        default=False,
+        action="store_true",
+        help="Also list the last analysis date on top of nbr of LoC",
+    )
+    parser.add_argument(
+        "--" + options.WITH_URL,
+        required=False,
+        default=False,
+        action="store_true",
+        help="Also list the URL of the objects",
+    )
+    parser.add_argument(
+        "--portfolios",
+        required=False,
+        default=False,
+        action="store_true",
+        help="Export portfolios LoCs instead of projects",
+    )
+    parser.add_argument(
+        "--topLevelOnly",
+        required=False,
+        default=False,
+        action="store_true",
+        help="Extracts only toplevel portfolios LoCs, not sub-portfolios",
+    )
+    args = util.parse_and_check_token(parser)
+    util.check_environment(vars(args))
+    util.check_token(args.token)
+    util.logger.info("sonar-tools version %s", version.PACKAGE_VERSION)
+    return args
+
+
+def main():
+    args = __parse_args("Extract projects or portfolios lines of code, as computed for the licence")
+    endpoint = platform.Platform(some_url=args.url, some_token=args.token, cert_file=args.clientCert)
+    kwargs = vars(args)
+    ofile = kwargs.pop("file", None)
+    args.format = __deduct_format(args.format, ofile)
+
+    if args.portfolios:
+        params = {}
+        if args.topLevelOnly:
+            params["qualifiers"] = "VW"
+        objects_list = portfolios.search(endpoint, params=params)
+    else:
+        objects_list = projects.search(endpoint)
+    __dump_loc(objects_list, ofile, **vars(args))
+    sys.exit(0)
+
+
+if __name__ == "__main__":
+    main()
```

## tools/measures_export.py

```diff
@@ -1,263 +1,263 @@
-#!/usr/local/bin/python3
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-"""
-    Exports some measures of all projects
-    - Either all measures (-m _all)
-    - Or the main measures (-m _main)
-    - Or a custom selection of measures (-m <measure1,measure2,measure3...>)
-"""
-import sys
-from sonar import measures, metrics, platform, version, options, exceptions
-from sonar.projects import projects
-import sonar.utilities as util
-
-RATINGS = "letters"
-PERCENTS = "float"
-DATEFMT = "datetime"
-CONVERT_OPTIONS = {"ratings": "letters", "percents": "float", "dates": "datetime"}
-
-
-def __last_analysis(project_or_branch):
-    last_analysis = project_or_branch.last_analysis()
-    with_time = True
-    if CONVERT_OPTIONS["dates"] == "dateonly":
-        with_time = False
-    if last_analysis is None:
-        last_analysis = "Never"
-    else:
-        last_analysis = util.date_to_string(last_analysis, with_time)
-    return last_analysis
-
-
-def __get_csv_header(wanted_metrics, edition, **kwargs):
-    sep = kwargs["csvSeparator"]
-    if edition == "community" or not kwargs[options.WITH_BRANCHES]:
-        header = f"# Project Key:1{sep}Project Name:2{sep}Last Analysis:3"
-        i = 4
-    else:
-        header = f"# Project Key:1{sep}Project Name:2{sep}Branch:3{sep}Last Analysis:4"
-        i = 5
-    for m in util.csv_to_list(wanted_metrics):
-        header += f"{sep}{m}:{i}"
-        i += 1
-    if kwargs[options.WITH_URL]:
-        header += f"{sep}URL:{i}"
-    return header
-
-
-def __get_object_measures(obj, wanted_metrics):
-    util.logger.info("Getting measures for %s", str(obj))
-    measures_d = {k: v.value if v else "" for k, v in obj.get_measures(wanted_metrics).items()}
-    measures_d["lastAnalysis"] = __last_analysis(obj)
-    measures_d["url"] = obj.url()
-    proj = obj
-    if not isinstance(obj, projects.Project):
-        proj = obj.concerned_object
-        measures_d["branch"] = obj.name
-    measures_d["projectKey"] = proj.key
-    measures_d["projectName"] = proj.name
-    return measures_d
-
-
-def __get_json_measures(obj, wanted_metrics, **kwargs):
-    d = __get_object_measures(obj, wanted_metrics)
-    if not kwargs[options.WITH_URL]:
-        d.pop("url", None)
-    if not kwargs[options.WITH_BRANCHES]:
-        d.pop("branch", None)
-    return d
-
-
-def __get_csv_measures(obj, wanted_metrics, **kwargs):
-    measures_d = __get_object_measures(obj, wanted_metrics)
-    sep = kwargs[options.CSV_SEPARATOR]
-    overall_metrics = "projectKey" + sep + "projectName"
-    if kwargs[options.WITH_BRANCHES]:
-        overall_metrics += sep + "branch"
-    overall_metrics += sep + "lastAnalysis" + sep + wanted_metrics
-    if kwargs[options.WITH_BRANCHES]:
-        overall_metrics += sep + "url"
-    line = ""
-    for metric in util.csv_to_list(overall_metrics):
-        val = ""
-        if metric in measures_d and measures_d[metric] is not None:
-            if isinstance(measures_d[metric], str) and sep in measures_d[metric]:
-                val = util.quote(measures_d[metric], sep)
-            else:
-                val = str(measures.format(metric, measures_d[metric], **CONVERT_OPTIONS))
-        line += val + sep
-    return line[: -len(sep)]
-
-
-def __get_wanted_metrics(args, endpoint):
-    main_metrics = util.list_to_csv(metrics.MAIN_METRICS)
-    wanted_metrics = args.metricKeys
-    if wanted_metrics == "_all":
-        all_metrics = metrics.search(endpoint).keys()
-        # Hack: With SonarQube 7.9 and below new_development_cost measure can't be retrieved
-        if endpoint.version() < (8, 0, 0):
-            all_metrics.pop("new_development_cost")
-        util.logger.info("Exporting %s metrics", len(all_metrics))
-        wanted_metrics = main_metrics + "," + util.list_to_csv(set(all_metrics) - set(metrics.MAIN_METRICS))
-    elif wanted_metrics == "_main" or wanted_metrics is None:
-        wanted_metrics = main_metrics
-    return wanted_metrics
-
-
-def __get_fmt_and_file(args):
-    kwargs = vars(args)
-    fmt = kwargs["format"]
-    fname = kwargs.get("file", None)
-    if fname is not None:
-        ext = fname.split(".")[-1].lower()
-        if ext in ("csv", "json"):
-            fmt = ext
-    return (fmt, fname)
-
-
-def __parse_args(desc):
-    parser = util.set_common_args(desc)
-    parser = util.set_key_arg(parser)
-    parser = util.set_output_file_args(parser)
-    parser.add_argument(
-        "-m",
-        "--metricKeys",
-        required=False,
-        help="Comma separated list of metrics or _all or _main",
-    )
-    parser.add_argument(
-        "-b",
-        "--" + options.WITH_BRANCHES,
-        required=False,
-        action="store_true",
-        help="Also extract branches metrics",
-    )
-    parser.add_argument(
-        "--withTags",
-        required=False,
-        action="store_true",
-        help="Also extract project tags",
-    )
-    parser.set_defaults(withBranches=False, withTags=False)
-    parser.add_argument(
-        "-r",
-        "--ratingsAsNumbers",
-        action="store_true",
-        default=False,
-        required=False,
-        help="Reports ratings as 12345 numbers instead of ABCDE letters",
-    )
-    parser.add_argument(
-        "-p",
-        "--percentsAsString",
-        action="store_true",
-        default=False,
-        required=False,
-        help="Reports percentages as string xy.z%% instead of float values 0.xyz",
-    )
-    parser.add_argument(
-        "-d",
-        "--datesWithoutTime",
-        action="store_true",
-        default=False,
-        required=False,
-        help="Reports timestamps only with date, not time",
-    )
-    parser.add_argument(
-        "--" + options.WITH_URL,
-        action="store_true",
-        default=False,
-        required=False,
-        help="Add projects/branches URLs in report",
-    )
-
-    args = util.parse_and_check_token(parser)
-    util.check_environment(vars(args))
-    util.check_token(args.token)
-    util.logger.info("sonar-tools version %s", version.PACKAGE_VERSION)
-    if args.ratingsAsNumbers:
-        CONVERT_OPTIONS["ratings"] = "numbers"
-    if args.percentsAsString:
-        CONVERT_OPTIONS["percents"] = "percents"
-    if args.datesWithoutTime:
-        CONVERT_OPTIONS["dates"] = "dateonly"
-
-    return args
-
-
-def main():
-    args = __parse_args("Extract measures of projects")
-    endpoint = platform.Platform(some_url=args.url, some_token=args.token, cert_file=args.clientCert)
-
-    with_branches = args.withBranches
-    if endpoint.edition() == "community":
-        with_branches = False
-
-    wanted_metrics = __get_wanted_metrics(args, endpoint)
-    (fmt, file) = __get_fmt_and_file(args)
-
-    try:
-        project_list = projects.get_list(endpoint=endpoint, key_list=args.projectKeys)
-    except exceptions.ObjectNotFound as e:
-        util.exit_fatal(e.message, options.ERR_NO_SUCH_KEY)
-    is_first = True
-    obj_list = []
-    if with_branches:
-        for project in project_list.values():
-            obj_list += project.branches()
-    else:
-        obj_list = project_list.values()
-    nb_branches = len(obj_list)
-
-    with util.open_file(file) as fd:
-        if fmt == "json":
-            print("[", end="", file=fd)
-        else:
-            print(
-                __get_csv_header(wanted_metrics, endpoint.edition(), **vars(args)),
-                file=fd,
-            )
-
-        for obj in obj_list:
-            if fmt == "json":
-                if not is_first:
-                    print(",", end="", file=fd)
-                values = __get_json_measures(obj, wanted_metrics, **vars(args))
-                json_str = util.json_dump(values)
-                print(json_str, file=fd)
-                is_first = False
-            else:
-                print(__get_csv_measures(obj, wanted_metrics, **vars(args)), file=fd)
-
-        if fmt == "json":
-            print("\n]\n", file=fd)
-
-    util.logger.info("Computing LoCs")
-    nb_loc = 0
-    for project in project_list.values():
-        nb_loc += project.loc()
-
-    util.logger.info("%d PROJECTS %d branches %d LoCs", len(project_list), nb_branches, nb_loc)
-    sys.exit(0)
-
-
-if __name__ == "__main__":
-    main()
+#!/usr/local/bin/python3
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+"""
+    Exports some measures of all projects
+    - Either all measures (-m _all)
+    - Or the main measures (-m _main)
+    - Or a custom selection of measures (-m <measure1,measure2,measure3...>)
+"""
+import sys
+from sonar import measures, metrics, platform, version, options, exceptions
+from sonar.projects import projects
+import sonar.utilities as util
+
+RATINGS = "letters"
+PERCENTS = "float"
+DATEFMT = "datetime"
+CONVERT_OPTIONS = {"ratings": "letters", "percents": "float", "dates": "datetime"}
+
+
+def __last_analysis(project_or_branch):
+    last_analysis = project_or_branch.last_analysis()
+    with_time = True
+    if CONVERT_OPTIONS["dates"] == "dateonly":
+        with_time = False
+    if last_analysis is None:
+        last_analysis = "Never"
+    else:
+        last_analysis = util.date_to_string(last_analysis, with_time)
+    return last_analysis
+
+
+def __get_csv_header(wanted_metrics, edition, **kwargs):
+    sep = kwargs["csvSeparator"]
+    if edition == "community" or not kwargs[options.WITH_BRANCHES]:
+        header = f"# Project Key:1{sep}Project Name:2{sep}Last Analysis:3"
+        i = 4
+    else:
+        header = f"# Project Key:1{sep}Project Name:2{sep}Branch:3{sep}Last Analysis:4"
+        i = 5
+    for m in util.csv_to_list(wanted_metrics):
+        header += f"{sep}{m}:{i}"
+        i += 1
+    if kwargs[options.WITH_URL]:
+        header += f"{sep}URL:{i}"
+    return header
+
+
+def __get_object_measures(obj, wanted_metrics):
+    util.logger.info("Getting measures for %s", str(obj))
+    measures_d = {k: v.value if v else "" for k, v in obj.get_measures(wanted_metrics).items()}
+    measures_d["lastAnalysis"] = __last_analysis(obj)
+    measures_d["url"] = obj.url()
+    proj = obj
+    if not isinstance(obj, projects.Project):
+        proj = obj.concerned_object
+        measures_d["branch"] = obj.name
+    measures_d["projectKey"] = proj.key
+    measures_d["projectName"] = proj.name
+    return measures_d
+
+
+def __get_json_measures(obj, wanted_metrics, **kwargs):
+    d = __get_object_measures(obj, wanted_metrics)
+    if not kwargs[options.WITH_URL]:
+        d.pop("url", None)
+    if not kwargs[options.WITH_BRANCHES]:
+        d.pop("branch", None)
+    return d
+
+
+def __get_csv_measures(obj, wanted_metrics, **kwargs):
+    measures_d = __get_object_measures(obj, wanted_metrics)
+    sep = kwargs[options.CSV_SEPARATOR]
+    overall_metrics = "projectKey" + sep + "projectName"
+    if kwargs[options.WITH_BRANCHES]:
+        overall_metrics += sep + "branch"
+    overall_metrics += sep + "lastAnalysis" + sep + wanted_metrics
+    if kwargs[options.WITH_BRANCHES]:
+        overall_metrics += sep + "url"
+    line = ""
+    for metric in util.csv_to_list(overall_metrics):
+        val = ""
+        if metric in measures_d and measures_d[metric] is not None:
+            if isinstance(measures_d[metric], str) and sep in measures_d[metric]:
+                val = util.quote(measures_d[metric], sep)
+            else:
+                val = str(measures.format(metric, measures_d[metric], **CONVERT_OPTIONS))
+        line += val + sep
+    return line[: -len(sep)]
+
+
+def __get_wanted_metrics(args, endpoint):
+    main_metrics = util.list_to_csv(metrics.MAIN_METRICS)
+    wanted_metrics = args.metricKeys
+    if wanted_metrics == "_all":
+        all_metrics = metrics.search(endpoint).keys()
+        # Hack: With SonarQube 7.9 and below new_development_cost measure can't be retrieved
+        if endpoint.version() < (8, 0, 0):
+            all_metrics.pop("new_development_cost")
+        util.logger.info("Exporting %s metrics", len(all_metrics))
+        wanted_metrics = main_metrics + "," + util.list_to_csv(set(all_metrics) - set(metrics.MAIN_METRICS))
+    elif wanted_metrics == "_main" or wanted_metrics is None:
+        wanted_metrics = main_metrics
+    return wanted_metrics
+
+
+def __get_fmt_and_file(args):
+    kwargs = vars(args)
+    fmt = kwargs["format"]
+    fname = kwargs.get("file", None)
+    if fname is not None:
+        ext = fname.split(".")[-1].lower()
+        if ext in ("csv", "json"):
+            fmt = ext
+    return (fmt, fname)
+
+
+def __parse_args(desc):
+    parser = util.set_common_args(desc)
+    parser = util.set_key_arg(parser)
+    parser = util.set_output_file_args(parser)
+    parser.add_argument(
+        "-m",
+        "--metricKeys",
+        required=False,
+        help="Comma separated list of metrics or _all or _main",
+    )
+    parser.add_argument(
+        "-b",
+        "--" + options.WITH_BRANCHES,
+        required=False,
+        action="store_true",
+        help="Also extract branches metrics",
+    )
+    parser.add_argument(
+        "--withTags",
+        required=False,
+        action="store_true",
+        help="Also extract project tags",
+    )
+    parser.set_defaults(withBranches=False, withTags=False)
+    parser.add_argument(
+        "-r",
+        "--ratingsAsNumbers",
+        action="store_true",
+        default=False,
+        required=False,
+        help="Reports ratings as 12345 numbers instead of ABCDE letters",
+    )
+    parser.add_argument(
+        "-p",
+        "--percentsAsString",
+        action="store_true",
+        default=False,
+        required=False,
+        help="Reports percentages as string xy.z%% instead of float values 0.xyz",
+    )
+    parser.add_argument(
+        "-d",
+        "--datesWithoutTime",
+        action="store_true",
+        default=False,
+        required=False,
+        help="Reports timestamps only with date, not time",
+    )
+    parser.add_argument(
+        "--" + options.WITH_URL,
+        action="store_true",
+        default=False,
+        required=False,
+        help="Add projects/branches URLs in report",
+    )
+
+    args = util.parse_and_check_token(parser)
+    util.check_environment(vars(args))
+    util.check_token(args.token)
+    util.logger.info("sonar-tools version %s", version.PACKAGE_VERSION)
+    if args.ratingsAsNumbers:
+        CONVERT_OPTIONS["ratings"] = "numbers"
+    if args.percentsAsString:
+        CONVERT_OPTIONS["percents"] = "percents"
+    if args.datesWithoutTime:
+        CONVERT_OPTIONS["dates"] = "dateonly"
+
+    return args
+
+
+def main():
+    args = __parse_args("Extract measures of projects")
+    endpoint = platform.Platform(some_url=args.url, some_token=args.token, cert_file=args.clientCert)
+
+    with_branches = args.withBranches
+    if endpoint.edition() == "community":
+        with_branches = False
+
+    wanted_metrics = __get_wanted_metrics(args, endpoint)
+    (fmt, file) = __get_fmt_and_file(args)
+
+    try:
+        project_list = projects.get_list(endpoint=endpoint, key_list=args.projectKeys)
+    except exceptions.ObjectNotFound as e:
+        util.exit_fatal(e.message, options.ERR_NO_SUCH_KEY)
+    is_first = True
+    obj_list = []
+    if with_branches:
+        for project in project_list.values():
+            obj_list += project.branches().values()
+    else:
+        obj_list = project_list.values()
+    nb_branches = len(obj_list)
+
+    with util.open_file(file) as fd:
+        if fmt == "json":
+            print("[", end="", file=fd)
+        else:
+            print(
+                __get_csv_header(wanted_metrics, endpoint.edition(), **vars(args)),
+                file=fd,
+            )
+
+        for obj in obj_list:
+            if fmt == "json":
+                if not is_first:
+                    print(",", end="", file=fd)
+                values = __get_json_measures(obj, wanted_metrics, **vars(args))
+                json_str = util.json_dump(values)
+                print(json_str, file=fd)
+                is_first = False
+            else:
+                print(__get_csv_measures(obj, wanted_metrics, **vars(args)), file=fd)
+
+        if fmt == "json":
+            print("\n]\n", file=fd)
+
+    util.logger.info("Computing LoCs")
+    nb_loc = 0
+    for project in project_list.values():
+        nb_loc += project.loc()
+
+    util.logger.info("%d PROJECTS %d branches %d LoCs", len(project_list), nb_branches, nb_loc)
+    sys.exit(0)
+
+
+if __name__ == "__main__":
+    main()
```

## tools/projects_export.py

 * *Ordering differences only*

```diff
@@ -1,67 +1,67 @@
-#!/usr/local/bin/python3
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-"""
-
-    Exports all projects of a SonarQube platform
-
-"""
-import sys
-import datetime
-from sonar import options, platform, utilities, version
-from sonar.projects import projects
-
-
-def main():
-    parser = utilities.set_common_args("Exports all projects of a SonarQube platform")
-    parser = utilities.set_key_arg(parser)
-    parser = utilities.set_output_file_args(parser, json_fmt=True, csv_fmt=False)
-    parser = options.add_thread_arg(parser, "projects zip export")
-    parser.add_argument(
-        "--exportTimeout",
-        required=False,
-        type=int,
-        default=180,
-        help="Maximum wait time for export",
-    )
-    args = utilities.parse_and_check_token(parser)
-    utilities.check_environment(vars(args))
-    utilities.check_token(args.token)
-    utilities.logger.info("sonar-tools version %s", version.PACKAGE_VERSION)
-    start_time = datetime.datetime.today()
-    sq = platform.Platform(some_url=args.url, some_token=args.token, cert_file=args.clientCert)
-
-    if sq.edition() in ("community", "developer") and sq.version(digits=2) < (9, 2):
-        utilities.exit_fatal(
-            "Can't export projects on Community and Developer Edition before 9.2, aborting...",
-            options.ERR_UNSUPPORTED_OPERATION,
-        )
-
-    with utilities.open_file(args.file) as fd:
-        print(
-            utilities.json_dump(projects.export_zip(endpoint=sq, key_list=args.projectKeys, export_timeout=args.exportTimeout, threads=args.threads)),
-            file=fd,
-        )
-    utilities.logger.info("Total execution time: %s", str(datetime.datetime.today() - start_time))
-    sys.exit(0)
-
-
-if __name__ == "__main__":
-    main()
+#!/usr/local/bin/python3
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+"""
+
+    Exports all projects of a SonarQube platform
+
+"""
+import sys
+import datetime
+from sonar import options, platform, utilities, version
+from sonar.projects import projects
+
+
+def main():
+    parser = utilities.set_common_args("Exports all projects of a SonarQube platform")
+    parser = utilities.set_key_arg(parser)
+    parser = utilities.set_output_file_args(parser, json_fmt=True, csv_fmt=False)
+    parser = options.add_thread_arg(parser, "projects zip export")
+    parser.add_argument(
+        "--exportTimeout",
+        required=False,
+        type=int,
+        default=180,
+        help="Maximum wait time for export",
+    )
+    args = utilities.parse_and_check_token(parser)
+    utilities.check_environment(vars(args))
+    utilities.check_token(args.token)
+    utilities.logger.info("sonar-tools version %s", version.PACKAGE_VERSION)
+    start_time = datetime.datetime.today()
+    sq = platform.Platform(some_url=args.url, some_token=args.token, cert_file=args.clientCert)
+
+    if sq.edition() in ("community", "developer") and sq.version(digits=2) < (9, 2):
+        utilities.exit_fatal(
+            "Can't export projects on Community and Developer Edition before 9.2, aborting...",
+            options.ERR_UNSUPPORTED_OPERATION,
+        )
+
+    with utilities.open_file(args.file) as fd:
+        print(
+            utilities.json_dump(projects.export_zip(endpoint=sq, key_list=args.projectKeys, export_timeout=args.exportTimeout, threads=args.threads)),
+            file=fd,
+        )
+    utilities.logger.info("Total execution time: %s", str(datetime.datetime.today() - start_time))
+    sys.exit(0)
+
+
+if __name__ == "__main__":
+    main()
```

## tools/projects_import.py

 * *Ordering differences only*

```diff
@@ -1,105 +1,105 @@
-#!/usr/local/bin/python3
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-"""
-
-    Imports a list of projects to a SonarQube platform
-
-"""
-import sys
-import json
-from sonar import options, platform, version, exceptions
-from sonar.projects import projects
-import sonar.utilities as util
-
-
-def _check_sq_environments(import_sq, export_sq):
-    imp_version = import_sq.version(digits=2, as_string=True)
-    if imp_version != export_sq["version"]:
-        util.exit_fatal(
-            "Export was not performed with same SonarQube version, aborting...",
-            options.ERR_UNSUPPORTED_OPERATION,
-        )
-    for export_plugin in export_sq["plugins"]:
-        e_name = export_plugin["name"]
-        e_vers = export_plugin["version"]
-        found = False
-        for import_plugin in import_sq.plugins():
-            if import_plugin["name"] == e_name and import_plugin["version"] == e_vers:
-                found = True
-                break
-        if not found:
-            util.exit_fatal(
-                f"Plugin '{e_name}' version '{e_vers}' was not found or not in same version on import platform, aborting...",
-                options.ERR_UNSUPPORTED_OPERATION,
-            )
-
-
-def main():
-    parser = util.set_common_args("Imports a list of projects in a SonarQube platform")
-    parser.add_argument("-f", "--projectsFile", required=True, help="File with the list of projects")
-    args = util.parse_and_check_token(parser)
-    util.check_environment(vars(args))
-    util.check_token(args.token)
-    util.logger.info("sonar-tools version %s", version.PACKAGE_VERSION)
-    sq = platform.Platform(some_url=args.url, some_token=args.token, cert_file=args.clientCert)
-
-    with open(args.projectsFile, "r", encoding="utf-8") as file:
-        data = json.load(file)
-    project_list = data["project_exports"]
-    _check_sq_environments(sq, data["sonarqube_environment"])
-
-    nb_projects = len(project_list)
-    util.logger.info("%d projects to import", nb_projects)
-    i = 0
-    statuses = {}
-    for project in project_list:
-        try:
-            o_proj = projects.Project.create(key=project["key"], endpoint=sq, name=project["key"])
-            status = o_proj.import_zip()
-            s = f"IMPORT {status}"
-            if s in statuses:
-                statuses[s] += 1
-            else:
-                statuses[s] = 1
-        except exceptions.ObjectAlreadyExists:
-            s = "CREATE projectAlreadyExist"
-            if s in statuses:
-                statuses[s] += 1
-            else:
-                statuses[s] = 1
-        i += 1
-        util.logger.info(
-            "%d/%d exports (%d%%) - Latest: %s - %s",
-            i,
-            nb_projects,
-            int(i * 100 / nb_projects),
-            project["key"],
-            status,
-        )
-        summary = ""
-        for k, v in statuses.items():
-            summary += f"{k}:{v}, "
-        util.logger.info("%s", summary[:-2])
-    sys.exit(0)
-
-
-if __name__ == "__main__":
-    main()
+#!/usr/local/bin/python3
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+"""
+
+    Imports a list of projects to a SonarQube platform
+
+"""
+import sys
+import json
+from sonar import options, platform, version, exceptions
+from sonar.projects import projects
+import sonar.utilities as util
+
+
+def _check_sq_environments(import_sq, export_sq):
+    imp_version = import_sq.version(digits=2, as_string=True)
+    if imp_version != export_sq["version"]:
+        util.exit_fatal(
+            "Export was not performed with same SonarQube version, aborting...",
+            options.ERR_UNSUPPORTED_OPERATION,
+        )
+    for export_plugin in export_sq["plugins"]:
+        e_name = export_plugin["name"]
+        e_vers = export_plugin["version"]
+        found = False
+        for import_plugin in import_sq.plugins():
+            if import_plugin["name"] == e_name and import_plugin["version"] == e_vers:
+                found = True
+                break
+        if not found:
+            util.exit_fatal(
+                f"Plugin '{e_name}' version '{e_vers}' was not found or not in same version on import platform, aborting...",
+                options.ERR_UNSUPPORTED_OPERATION,
+            )
+
+
+def main():
+    parser = util.set_common_args("Imports a list of projects in a SonarQube platform")
+    parser.add_argument("-f", "--projectsFile", required=True, help="File with the list of projects")
+    args = util.parse_and_check_token(parser)
+    util.check_environment(vars(args))
+    util.check_token(args.token)
+    util.logger.info("sonar-tools version %s", version.PACKAGE_VERSION)
+    sq = platform.Platform(some_url=args.url, some_token=args.token, cert_file=args.clientCert)
+
+    with open(args.projectsFile, "r", encoding="utf-8") as file:
+        data = json.load(file)
+    project_list = data["project_exports"]
+    _check_sq_environments(sq, data["sonarqube_environment"])
+
+    nb_projects = len(project_list)
+    util.logger.info("%d projects to import", nb_projects)
+    i = 0
+    statuses = {}
+    for project in project_list:
+        try:
+            o_proj = projects.Project.create(key=project["key"], endpoint=sq, name=project["key"])
+            status = o_proj.import_zip()
+            s = f"IMPORT {status}"
+            if s in statuses:
+                statuses[s] += 1
+            else:
+                statuses[s] = 1
+        except exceptions.ObjectAlreadyExists:
+            s = "CREATE projectAlreadyExist"
+            if s in statuses:
+                statuses[s] += 1
+            else:
+                statuses[s] = 1
+        i += 1
+        util.logger.info(
+            "%d/%d exports (%d%%) - Latest: %s - %s",
+            i,
+            nb_projects,
+            int(i * 100 / nb_projects),
+            project["key"],
+            status,
+        )
+        summary = ""
+        for k, v in statuses.items():
+            summary += f"{k}:{v}, "
+        util.logger.info("%s", summary[:-2])
+    sys.exit(0)
+
+
+if __name__ == "__main__":
+    main()
```

## tools/support.py

 * *Ordering differences only*

```diff
@@ -1,179 +1,179 @@
-#!/usr/local/bin/python3
-#
-# sonar-tools
-# Copyright (C) 2022-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-"""
-
-    Audits a SUPPORT ticket SIF
-
-"""
-from http import HTTPStatus
-import sys
-import os
-import json
-import argparse
-import requests
-from sonar import version, sif, options
-from sonar.audit import severities
-import sonar.utilities as util
-from sonar.audit import problem, config
-
-PRIVATE_COMMENT = [{"key": "sd.public.comment", "value": {"internal": "true"}}]
-
-
-def __get_args(desc):
-    parser = argparse.ArgumentParser(description=desc)
-    parser.add_argument(
-        "-p",
-        "--password",
-        required=False,
-        default=os.getenv("JIRA_PASSWORD", None),
-        help="Password to authenticate to JIRA, default is environment variable $JIRA_PASSWORD",
-    )
-    parser.add_argument(
-        "-l",
-        "--login",
-        required=False,
-        default=os.getenv("JIRA_LOGIN"),
-        help="Password to authenticate to JIRA, default is environment variable $JIRA_LOGIN",
-    )
-    parser.add_argument(
-        "-u", "--url", required=False, default="https://services.sonarsource.com", help="ServiceDesk URL, default is https://services.sonarsource.com"
-    )
-    parser.add_argument(
-        "-v",
-        "--" + util.OPT_VERBOSE,
-        required=False,
-        choices=["ERROR", "WARN", "INFO", "DEBUG"],
-        default="ERROR",
-        help="Logging verbosity level, default is ERROR",
-    )
-    parser.add_argument(
-        "-c",
-        "--comment",
-        required=False,
-        dest="comment",
-        action="store_true",
-        default=False,
-        help="Post a comment in the ticket after audit",
-    )
-    parser.add_argument("-t", "--ticket", required=True, help="Support ticket to audit, in format SUPPORT-XXXXX or XXXXX")
-    args = parser.parse_args()
-    if not args.login or not args.password:
-        util.exit_fatal("Login and Password are required to authenticate to ServiceDesk", options.ERR_TOKEN_MISSING)
-    return args
-
-
-def __get_issue_id(**kwargs):
-    """Converts a ticket number into issue id needed to post on the issue"""
-    tix = kwargs["ticket"]
-    url = f'{kwargs["url"]}/rest/servicedeskapi/request/{tix}'
-    r = requests.get(url, auth=kwargs["creds"])
-    if not r.ok:
-        if r.status_code == HTTPStatus.NOT_FOUND:
-            return None
-        else:
-            util.exit_fatal(f"Ticket {tix}: URL '{url}' status code {r.status_code}", options.ERR_SONAR_API)
-    return json.loads(r.text)["issueId"]
-
-
-def __add_comment(comment, **kwargs):
-    url = f'{kwargs["url"]}/rest/api/2/issue/{__get_issue_id(**kwargs)}/comment'
-    requests.post(url, auth=kwargs["creds"], json={"body": comment, "properties": PRIVATE_COMMENT})
-
-
-def __get_sysinfo_from_ticket(**kwargs):
-    tix = kwargs["ticket"]
-    url = f"{kwargs['url']}/rest/servicedeskapi/request/{tix}"
-    util.logger.debug("Check %s - URL %s", kwargs["ticket"], url)
-    r = requests.get(url, auth=kwargs["creds"])
-    if not r.ok:
-        if r.status_code == HTTPStatus.NOT_FOUND:
-            print(f"Ticket {tix} not found")
-            sys.exit(3)
-        else:
-            util.exit_fatal(f"Ticket {tix}: URL '{url}' status code {r.status_code}", options.ERR_SONAR_API)
-
-    data = json.loads(r.text)
-    util.logger.debug("Ticket %s found: searching SIF", tix)
-    sif_list = {}
-    for d in data["requestFieldValues"]:
-        if d.get("fieldId", "") != "attachment":
-            continue
-        for v in d["value"]:
-            file_type = v["filename"].split(".")[-1].lower()
-            if file_type not in ("json", "txt"):
-                continue
-            attachment_url = v["content"]
-            attachment_file = attachment_url.split("/")[-1]
-            util.logger.info("Ticket %s: Verifying attachment '%s' found", tix, attachment_file)
-            r = requests.get(attachment_url, auth=kwargs["creds"])
-            if not r.ok:
-                util.exit_fatal(f"ERROR: Ticket {tix} get attachment status code {r.status_code}", options.ERR_SONAR_API)
-            try:
-                sif_list[attachment_file] = json.loads(r.text)
-            except json.decoder.JSONDecodeError:
-                util.logger.info("Ticket %s: Attachment '%s' is not a JSON file, skipping", tix, attachment_file)
-                continue
-    return sif_list
-
-
-def main():
-    kwargs = vars(__get_args("Audits a Sonar ServiceDesk ticket (Searches for SIF attachment and audits SIF)"))
-    util.check_environment(kwargs)
-    util.logger.info("sonar-tools version %s", version.PACKAGE_VERSION)
-    kwargs["creds"] = (kwargs.pop("login"), kwargs.pop("password"))
-    if not kwargs["ticket"].startswith("SUPPORT-"):
-        kwargs["ticket"] = f'SUPPORT-{kwargs["ticket"]}'
-    sif_list = __get_sysinfo_from_ticket(**kwargs)
-    if len(sif_list) == 0:
-        print(f"No SIF found in ticket {kwargs['ticket']}")
-        sys.exit(2)
-    problems = []
-    settings = config.load("sonar-audit")
-    found_problems = False
-    comment = ""
-    for file, sysinfo in sif_list.items():
-        try:
-            problems = sif.Sif(sysinfo).audit(settings)
-            comment += f"h3. SIF *[^{file}]* audit:\n"
-            print(f"SIF file '{file}' audit:")
-            if problems:
-                found_problems = True
-                util.logger.warning("%d issues found during audit", len(problems))
-                problem.dump_report(problems, None, format="csv")
-                for p in problems:
-                    sev = "(x)" if p.severity in (severities.Severity.HIGH, severities.Severity.CRITICAL) else "(!)"
-                    comment += f"{sev} {p.message}\n"
-            else:
-                util.logger.info("%d issues found during audit", len(problems))
-                print("No issues found is SIFs")
-                comment += "(y) No issues found\n"
-
-        except sif.NotSystemInfo:
-            util.logger.info("File '%s' does not seem to be a legit JSON file, skipped", file)
-
-    if kwargs.pop("comment"):
-        __add_comment(comment, **kwargs)
-
-    sys.exit(1 if found_problems else 0)
-
-
-if __name__ == "__main__":
-    main()
+#!/usr/local/bin/python3
+#
+# sonar-tools
+# Copyright (C) 2022-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+"""
+
+    Audits a SUPPORT ticket SIF
+
+"""
+from http import HTTPStatus
+import sys
+import os
+import json
+import argparse
+import requests
+from sonar import version, sif, options
+from sonar.audit import severities
+import sonar.utilities as util
+from sonar.audit import problem, config
+
+PRIVATE_COMMENT = [{"key": "sd.public.comment", "value": {"internal": "true"}}]
+
+
+def __get_args(desc):
+    parser = argparse.ArgumentParser(description=desc)
+    parser.add_argument(
+        "-p",
+        "--password",
+        required=False,
+        default=os.getenv("JIRA_PASSWORD", None),
+        help="Password to authenticate to JIRA, default is environment variable $JIRA_PASSWORD",
+    )
+    parser.add_argument(
+        "-l",
+        "--login",
+        required=False,
+        default=os.getenv("JIRA_LOGIN"),
+        help="Password to authenticate to JIRA, default is environment variable $JIRA_LOGIN",
+    )
+    parser.add_argument(
+        "-u", "--url", required=False, default="https://services.sonarsource.com", help="ServiceDesk URL, default is https://services.sonarsource.com"
+    )
+    parser.add_argument(
+        "-v",
+        "--" + util.OPT_VERBOSE,
+        required=False,
+        choices=["ERROR", "WARN", "INFO", "DEBUG"],
+        default="ERROR",
+        help="Logging verbosity level, default is ERROR",
+    )
+    parser.add_argument(
+        "-c",
+        "--comment",
+        required=False,
+        dest="comment",
+        action="store_true",
+        default=False,
+        help="Post a comment in the ticket after audit",
+    )
+    parser.add_argument("-t", "--ticket", required=True, help="Support ticket to audit, in format SUPPORT-XXXXX or XXXXX")
+    args = parser.parse_args()
+    if not args.login or not args.password:
+        util.exit_fatal("Login and Password are required to authenticate to ServiceDesk", options.ERR_TOKEN_MISSING)
+    return args
+
+
+def __get_issue_id(**kwargs):
+    """Converts a ticket number into issue id needed to post on the issue"""
+    tix = kwargs["ticket"]
+    url = f'{kwargs["url"]}/rest/servicedeskapi/request/{tix}'
+    r = requests.get(url, auth=kwargs["creds"])
+    if not r.ok:
+        if r.status_code == HTTPStatus.NOT_FOUND:
+            return None
+        else:
+            util.exit_fatal(f"Ticket {tix}: URL '{url}' status code {r.status_code}", options.ERR_SONAR_API)
+    return json.loads(r.text)["issueId"]
+
+
+def __add_comment(comment, **kwargs):
+    url = f'{kwargs["url"]}/rest/api/2/issue/{__get_issue_id(**kwargs)}/comment'
+    requests.post(url, auth=kwargs["creds"], json={"body": comment, "properties": PRIVATE_COMMENT})
+
+
+def __get_sysinfo_from_ticket(**kwargs):
+    tix = kwargs["ticket"]
+    url = f"{kwargs['url']}/rest/servicedeskapi/request/{tix}"
+    util.logger.debug("Check %s - URL %s", kwargs["ticket"], url)
+    r = requests.get(url, auth=kwargs["creds"])
+    if not r.ok:
+        if r.status_code == HTTPStatus.NOT_FOUND:
+            print(f"Ticket {tix} not found")
+            sys.exit(3)
+        else:
+            util.exit_fatal(f"Ticket {tix}: URL '{url}' status code {r.status_code}", options.ERR_SONAR_API)
+
+    data = json.loads(r.text)
+    util.logger.debug("Ticket %s found: searching SIF", tix)
+    sif_list = {}
+    for d in data["requestFieldValues"]:
+        if d.get("fieldId", "") != "attachment":
+            continue
+        for v in d["value"]:
+            file_type = v["filename"].split(".")[-1].lower()
+            if file_type not in ("json", "txt"):
+                continue
+            attachment_url = v["content"]
+            attachment_file = attachment_url.split("/")[-1]
+            util.logger.info("Ticket %s: Verifying attachment '%s' found", tix, attachment_file)
+            r = requests.get(attachment_url, auth=kwargs["creds"])
+            if not r.ok:
+                util.exit_fatal(f"ERROR: Ticket {tix} get attachment status code {r.status_code}", options.ERR_SONAR_API)
+            try:
+                sif_list[attachment_file] = json.loads(r.text)
+            except json.decoder.JSONDecodeError:
+                util.logger.info("Ticket %s: Attachment '%s' is not a JSON file, skipping", tix, attachment_file)
+                continue
+    return sif_list
+
+
+def main():
+    kwargs = vars(__get_args("Audits a Sonar ServiceDesk ticket (Searches for SIF attachment and audits SIF)"))
+    util.check_environment(kwargs)
+    util.logger.info("sonar-tools version %s", version.PACKAGE_VERSION)
+    kwargs["creds"] = (kwargs.pop("login"), kwargs.pop("password"))
+    if not kwargs["ticket"].startswith("SUPPORT-"):
+        kwargs["ticket"] = f'SUPPORT-{kwargs["ticket"]}'
+    sif_list = __get_sysinfo_from_ticket(**kwargs)
+    if len(sif_list) == 0:
+        print(f"No SIF found in ticket {kwargs['ticket']}")
+        sys.exit(2)
+    problems = []
+    settings = config.load("sonar-audit")
+    found_problems = False
+    comment = ""
+    for file, sysinfo in sif_list.items():
+        try:
+            problems = sif.Sif(sysinfo).audit(settings)
+            comment += f"h3. SIF *[^{file}]* audit:\n"
+            print(f"SIF file '{file}' audit:")
+            if problems:
+                found_problems = True
+                util.logger.warning("%d issues found during audit", len(problems))
+                problem.dump_report(problems, None, format="csv")
+                for p in problems:
+                    sev = "(x)" if p.severity in (severities.Severity.HIGH, severities.Severity.CRITICAL) else "(!)"
+                    comment += f"{sev} {p.message}\n"
+            else:
+                util.logger.info("%d issues found during audit", len(problems))
+                print("No issues found is SIFs")
+                comment += "(y) No issues found\n"
+
+        except sif.NotSystemInfo:
+            util.logger.info("File '%s' does not seem to be a legit JSON file, skipped", file)
+
+    if kwargs.pop("comment"):
+        __add_comment(comment, **kwargs)
+
+    sys.exit(1 if found_problems else 0)
+
+
+if __name__ == "__main__":
+    main()
```

## Comparing `sonar_tools-2.8.2.data/scripts/sonar-tools` & `sonar_tools-2.9.data/scripts/sonar-tools`

 * *Files 8% similar despite different names*

```diff
@@ -1,40 +1,40 @@
-#!/usr/bin/env python3
-#
-# sonar-tools
-# Copyright (C) 2019-2024 Olivier Korach
-# mailto:olivier.korach AT gmail DOT com
-#
-# This program is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this program; if not, write to the Free Software Foundation,
-# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-#
-from sonar import version
-
-print(f'''
-sonar-tools version {version.PACKAGE_VERSION}
-Collections of utilities for SonarQube:
-- sonar-audit: Audits a SonarQube platform for bad practices, performance, configuration problems
-- sonar-housekeeper: Deletes projects that have not been analyzed since a given number of days
-- sonar-loc: Produces a list of projects with their LoC count as computed by the SonarQube
-  commercial licenses (ie taking the largest branch or PR)
-- sonar-measures-export: Exports measures/metrics of one, several or all projects of the platform in CSV or JSON
-- sonar-findings-export: Exports findings (potentially filtered) from the platform in CSV or JSON
-  (also available as sonar-issues-export for backward compatibility, but deprecated)
-- sonar-findings-sync: Synchronizes issues between 2 branches of a same project, a whole project
-  branches of 2 different projects (potentially on different platforms).
-  (also available as sonar-issues-sync for backward compatibility, but deprecated)
-- sonar-projects-export: Exports all projects from a platform (All editions)
-- sonar-projects-import: Imports a list of projects into a platform (EE and higher)
-- sonar-config: Exports and imports an entire (or subsets of a) SonarQube platform configuration as code (JSON)
-See tools built-in -h help and https://github.com/okorach/sonar-tools for more documentation
+#!python
+#
+# sonar-tools
+# Copyright (C) 2019-2024 Olivier Korach
+# mailto:olivier.korach AT gmail DOT com
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU Lesser General Public
+# License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation,
+# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#
+from sonar import version
+
+print(f'''
+sonar-tools version {version.PACKAGE_VERSION}
+Collections of utilities for SonarQube:
+- sonar-audit: Audits a SonarQube platform for bad practices, performance, configuration problems
+- sonar-housekeeper: Deletes projects that have not been analyzed since a given number of days
+- sonar-loc: Produces a list of projects with their LoC count as computed by the SonarQube
+  commercial licenses (ie taking the largest branch or PR)
+- sonar-measures-export: Exports measures/metrics of one, several or all projects of the platform in CSV or JSON
+- sonar-findings-export: Exports findings (potentially filtered) from the platform in CSV or JSON
+  (also available as sonar-issues-export for backward compatibility, but deprecated)
+- sonar-findings-sync: Synchronizes issues between 2 branches of a same project, a whole project
+  branches of 2 different projects (potentially on different platforms).
+  (also available as sonar-issues-sync for backward compatibility, but deprecated)
+- sonar-projects-export: Exports all projects from a platform (All editions)
+- sonar-projects-import: Imports a list of projects into a platform (EE and higher)
+- sonar-config: Exports and imports an entire (or subsets of a) SonarQube platform configuration as code (JSON)
+See tools built-in -h help and https://github.com/okorach/sonar-tools for more documentation
 ''')
```

## Comparing `sonar_tools-2.8.2.dist-info/LICENSE` & `sonar_tools-2.9.dist-info/LICENSE`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,165 +1,165 @@
-                   GNU LESSER GENERAL PUBLIC LICENSE
-                       Version 3, 29 June 2007
-
- Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
- Everyone is permitted to copy and distribute verbatim copies
- of this license document, but changing it is not allowed.
-
-
-  This version of the GNU Lesser General Public License incorporates
-the terms and conditions of version 3 of the GNU General Public
-License, supplemented by the additional permissions listed below.
-
-  0. Additional Definitions.
-
-  As used herein, "this License" refers to version 3 of the GNU Lesser
-General Public License, and the "GNU GPL" refers to version 3 of the GNU
-General Public License.
-
-  "The Library" refers to a covered work governed by this License,
-other than an Application or a Combined Work as defined below.
-
-  An "Application" is any work that makes use of an interface provided
-by the Library, but which is not otherwise based on the Library.
-Defining a subclass of a class defined by the Library is deemed a mode
-of using an interface provided by the Library.
-
-  A "Combined Work" is a work produced by combining or linking an
-Application with the Library.  The particular version of the Library
-with which the Combined Work was made is also called the "Linked
-Version".
-
-  The "Minimal Corresponding Source" for a Combined Work means the
-Corresponding Source for the Combined Work, excluding any source code
-for portions of the Combined Work that, considered in isolation, are
-based on the Application, and not on the Linked Version.
-
-  The "Corresponding Application Code" for a Combined Work means the
-object code and/or source code for the Application, including any data
-and utility programs needed for reproducing the Combined Work from the
-Application, but excluding the System Libraries of the Combined Work.
-
-  1. Exception to Section 3 of the GNU GPL.
-
-  You may convey a covered work under sections 3 and 4 of this License
-without being bound by section 3 of the GNU GPL.
-
-  2. Conveying Modified Versions.
-
-  If you modify a copy of the Library, and, in your modifications, a
-facility refers to a function or data to be supplied by an Application
-that uses the facility (other than as an argument passed when the
-facility is invoked), then you may convey a copy of the modified
-version:
-
-   a) under this License, provided that you make a good faith effort to
-   ensure that, in the event an Application does not supply the
-   function or data, the facility still operates, and performs
-   whatever part of its purpose remains meaningful, or
-
-   b) under the GNU GPL, with none of the additional permissions of
-   this License applicable to that copy.
-
-  3. Object Code Incorporating Material from Library Header Files.
-
-  The object code form of an Application may incorporate material from
-a header file that is part of the Library.  You may convey such object
-code under terms of your choice, provided that, if the incorporated
-material is not limited to numerical parameters, data structure
-layouts and accessors, or small macros, inline functions and templates
-(ten or fewer lines in length), you do both of the following:
-
-   a) Give prominent notice with each copy of the object code that the
-   Library is used in it and that the Library and its use are
-   covered by this License.
-
-   b) Accompany the object code with a copy of the GNU GPL and this license
-   document.
-
-  4. Combined Works.
-
-  You may convey a Combined Work under terms of your choice that,
-taken together, effectively do not restrict modification of the
-portions of the Library contained in the Combined Work and reverse
-engineering for debugging such modifications, if you also do each of
-the following:
-
-   a) Give prominent notice with each copy of the Combined Work that
-   the Library is used in it and that the Library and its use are
-   covered by this License.
-
-   b) Accompany the Combined Work with a copy of the GNU GPL and this license
-   document.
-
-   c) For a Combined Work that displays copyright notices during
-   execution, include the copyright notice for the Library among
-   these notices, as well as a reference directing the user to the
-   copies of the GNU GPL and this license document.
-
-   d) Do one of the following:
-
-       0) Convey the Minimal Corresponding Source under the terms of this
-       License, and the Corresponding Application Code in a form
-       suitable for, and under terms that permit, the user to
-       recombine or relink the Application with a modified version of
-       the Linked Version to produce a modified Combined Work, in the
-       manner specified by section 6 of the GNU GPL for conveying
-       Corresponding Source.
-
-       1) Use a suitable shared library mechanism for linking with the
-       Library.  A suitable mechanism is one that (a) uses at run time
-       a copy of the Library already present on the user's computer
-       system, and (b) will operate properly with a modified version
-       of the Library that is interface-compatible with the Linked
-       Version.
-
-   e) Provide Installation Information, but only if you would otherwise
-   be required to provide such information under section 6 of the
-   GNU GPL, and only to the extent that such information is
-   necessary to install and execute a modified version of the
-   Combined Work produced by recombining or relinking the
-   Application with a modified version of the Linked Version. (If
-   you use option 4d0, the Installation Information must accompany
-   the Minimal Corresponding Source and Corresponding Application
-   Code. If you use option 4d1, you must provide the Installation
-   Information in the manner specified by section 6 of the GNU GPL
-   for conveying Corresponding Source.)
-
-  5. Combined Libraries.
-
-  You may place library facilities that are a work based on the
-Library side by side in a single library together with other library
-facilities that are not Applications and are not covered by this
-License, and convey such a combined library under terms of your
-choice, if you do both of the following:
-
-   a) Accompany the combined library with a copy of the same work based
-   on the Library, uncombined with any other library facilities,
-   conveyed under the terms of this License.
-
-   b) Give prominent notice with the combined library that part of it
-   is a work based on the Library, and explaining where to find the
-   accompanying uncombined form of the same work.
-
-  6. Revised Versions of the GNU Lesser General Public License.
-
-  The Free Software Foundation may publish revised and/or new versions
-of the GNU Lesser General Public License from time to time. Such new
-versions will be similar in spirit to the present version, but may
-differ in detail to address new problems or concerns.
-
-  Each version is given a distinguishing version number. If the
-Library as you received it specifies that a certain numbered version
-of the GNU Lesser General Public License "or any later version"
-applies to it, you have the option of following the terms and
-conditions either of that published version or of any later version
-published by the Free Software Foundation. If the Library as you
-received it does not specify a version number of the GNU Lesser
-General Public License, you may choose any version of the GNU Lesser
-General Public License ever published by the Free Software Foundation.
-
-  If the Library as you received it specifies that a proxy can decide
-whether future versions of the GNU Lesser General Public License shall
-apply, that proxy's public statement of acceptance of any version is
-permanent authorization for you to choose that version for the
-Library.
+                   GNU LESSER GENERAL PUBLIC LICENSE
+                       Version 3, 29 June 2007
+
+ Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
+ Everyone is permitted to copy and distribute verbatim copies
+ of this license document, but changing it is not allowed.
+
+
+  This version of the GNU Lesser General Public License incorporates
+the terms and conditions of version 3 of the GNU General Public
+License, supplemented by the additional permissions listed below.
+
+  0. Additional Definitions.
+
+  As used herein, "this License" refers to version 3 of the GNU Lesser
+General Public License, and the "GNU GPL" refers to version 3 of the GNU
+General Public License.
+
+  "The Library" refers to a covered work governed by this License,
+other than an Application or a Combined Work as defined below.
+
+  An "Application" is any work that makes use of an interface provided
+by the Library, but which is not otherwise based on the Library.
+Defining a subclass of a class defined by the Library is deemed a mode
+of using an interface provided by the Library.
+
+  A "Combined Work" is a work produced by combining or linking an
+Application with the Library.  The particular version of the Library
+with which the Combined Work was made is also called the "Linked
+Version".
+
+  The "Minimal Corresponding Source" for a Combined Work means the
+Corresponding Source for the Combined Work, excluding any source code
+for portions of the Combined Work that, considered in isolation, are
+based on the Application, and not on the Linked Version.
+
+  The "Corresponding Application Code" for a Combined Work means the
+object code and/or source code for the Application, including any data
+and utility programs needed for reproducing the Combined Work from the
+Application, but excluding the System Libraries of the Combined Work.
+
+  1. Exception to Section 3 of the GNU GPL.
+
+  You may convey a covered work under sections 3 and 4 of this License
+without being bound by section 3 of the GNU GPL.
+
+  2. Conveying Modified Versions.
+
+  If you modify a copy of the Library, and, in your modifications, a
+facility refers to a function or data to be supplied by an Application
+that uses the facility (other than as an argument passed when the
+facility is invoked), then you may convey a copy of the modified
+version:
+
+   a) under this License, provided that you make a good faith effort to
+   ensure that, in the event an Application does not supply the
+   function or data, the facility still operates, and performs
+   whatever part of its purpose remains meaningful, or
+
+   b) under the GNU GPL, with none of the additional permissions of
+   this License applicable to that copy.
+
+  3. Object Code Incorporating Material from Library Header Files.
+
+  The object code form of an Application may incorporate material from
+a header file that is part of the Library.  You may convey such object
+code under terms of your choice, provided that, if the incorporated
+material is not limited to numerical parameters, data structure
+layouts and accessors, or small macros, inline functions and templates
+(ten or fewer lines in length), you do both of the following:
+
+   a) Give prominent notice with each copy of the object code that the
+   Library is used in it and that the Library and its use are
+   covered by this License.
+
+   b) Accompany the object code with a copy of the GNU GPL and this license
+   document.
+
+  4. Combined Works.
+
+  You may convey a Combined Work under terms of your choice that,
+taken together, effectively do not restrict modification of the
+portions of the Library contained in the Combined Work and reverse
+engineering for debugging such modifications, if you also do each of
+the following:
+
+   a) Give prominent notice with each copy of the Combined Work that
+   the Library is used in it and that the Library and its use are
+   covered by this License.
+
+   b) Accompany the Combined Work with a copy of the GNU GPL and this license
+   document.
+
+   c) For a Combined Work that displays copyright notices during
+   execution, include the copyright notice for the Library among
+   these notices, as well as a reference directing the user to the
+   copies of the GNU GPL and this license document.
+
+   d) Do one of the following:
+
+       0) Convey the Minimal Corresponding Source under the terms of this
+       License, and the Corresponding Application Code in a form
+       suitable for, and under terms that permit, the user to
+       recombine or relink the Application with a modified version of
+       the Linked Version to produce a modified Combined Work, in the
+       manner specified by section 6 of the GNU GPL for conveying
+       Corresponding Source.
+
+       1) Use a suitable shared library mechanism for linking with the
+       Library.  A suitable mechanism is one that (a) uses at run time
+       a copy of the Library already present on the user's computer
+       system, and (b) will operate properly with a modified version
+       of the Library that is interface-compatible with the Linked
+       Version.
+
+   e) Provide Installation Information, but only if you would otherwise
+   be required to provide such information under section 6 of the
+   GNU GPL, and only to the extent that such information is
+   necessary to install and execute a modified version of the
+   Combined Work produced by recombining or relinking the
+   Application with a modified version of the Linked Version. (If
+   you use option 4d0, the Installation Information must accompany
+   the Minimal Corresponding Source and Corresponding Application
+   Code. If you use option 4d1, you must provide the Installation
+   Information in the manner specified by section 6 of the GNU GPL
+   for conveying Corresponding Source.)
+
+  5. Combined Libraries.
+
+  You may place library facilities that are a work based on the
+Library side by side in a single library together with other library
+facilities that are not Applications and are not covered by this
+License, and convey such a combined library under terms of your
+choice, if you do both of the following:
+
+   a) Accompany the combined library with a copy of the same work based
+   on the Library, uncombined with any other library facilities,
+   conveyed under the terms of this License.
+
+   b) Give prominent notice with the combined library that part of it
+   is a work based on the Library, and explaining where to find the
+   accompanying uncombined form of the same work.
+
+  6. Revised Versions of the GNU Lesser General Public License.
+
+  The Free Software Foundation may publish revised and/or new versions
+of the GNU Lesser General Public License from time to time. Such new
+versions will be similar in spirit to the present version, but may
+differ in detail to address new problems or concerns.
+
+  Each version is given a distinguishing version number. If the
+Library as you received it specifies that a certain numbered version
+of the GNU Lesser General Public License "or any later version"
+applies to it, you have the option of following the terms and
+conditions either of that published version or of any later version
+published by the Free Software Foundation. If the Library as you
+received it does not specify a version number of the GNU Lesser
+General Public License, you may choose any version of the GNU Lesser
+General Public License ever published by the Free Software Foundation.
+
+  If the Library as you received it specifies that a proxy can decide
+whether future versions of the GNU Lesser General Public License shall
+apply, that proxy's public statement of acceptance of any version is
+permanent authorization for you to choose that version for the
+Library.
```

## Comparing `sonar_tools-2.8.2.dist-info/METADATA` & `sonar_tools-2.9.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: sonar-tools
-Version: 2.8.2
+Version: 2.9
 Summary: A collection of utility scripts for SonarQube
 Home-page: https://github.com/okorach/sonar-tools
 Author: Olivier Korach
 Author-email: olivier.korach@gmail.com
 License: UNKNOWN
 Project-URL: Bug Tracker, https://github.com/okorach/sonar-tools/issues
 Project-URL: Documentation, https://github.com/okorach/sonar-tools/README.md
@@ -12,20 +12,20 @@
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: GNU Lesser General Public License v3 or later (LGPLv3+)
 Classifier: Operating System :: OS Independent
 Requires-Python: >=3.6
 Description-Content-Type: text/markdown
 License-File: LICENSE
+Requires-Dist: pytz
 Requires-Dist: argparse
 Requires-Dist: datetime
-Requires-Dist: jprops
 Requires-Dist: python-dateutil
-Requires-Dist: pytz
 Requires-Dist: requests
+Requires-Dist: jprops
 
 # sonar-tools
 Command line tools to help in SonarQube administration tasks.
 
 ![Downloads](https://img.shields.io/pypi/dm/sonar-tools?color=informational)
 ![Python-Versions](https://img.shields.io/pypi/pyversions/sonar-tools)
 ![License](https://img.shields.io/pypi/l/sonar-tools?color=informational)
@@ -330,14 +330,15 @@
 - Code 5: Non existing project key provided
 - Code 6: Incorrect finding search criteria provided
 - Code 7: Unsupported operation requested (because of SonarQube edition or configuration)
 - Code 8: Audit rule loading failed (at startup)
 - Code 9: SIF audit error (file not found, can't open file, not a legit JSON file, ...)
 - Code 10: Incorrect command line arguments
 - Code 11: Global analysis or project analysis token provided (user token needed for sonar-tools)
+- Code 12: HTTP request time-out using the SonarQube API
 
 # License
 
 Copyright (C) 2019-2024 Olivier Korach
 mailto:olivier.korach AT gmail DOT com
 
 This program is free software; you can redistribute it and/or
```

## Comparing `sonar_tools-2.8.2.dist-info/entry_points.txt` & `sonar_tools-2.9.dist-info/entry_points.txt`

 * *Files identical despite different names*

## Comparing `sonar_tools-2.8.2.dist-info/RECORD` & `sonar_tools-2.9.dist-info/RECORD`

 * *Files 13% similar despite different names*

```diff
@@ -1,79 +1,79 @@
-sonar/__init__.py,sha256=6AmQe2Xaj_FcFrqrsVFtvar9O0I_myJ6NRXRJV2BMJo,847
-sonar/aggregations.py,sha256=3YxAyhDTJm7cG-Uu2kGZjSHXC_g9YI3d_YRzd47UU-A,3391
-sonar/applications.py,sha256=orryo2q0oRSOYreALp4fzDAc5wPBiikyDmbtHXIJ0HY,20715
-sonar/components.py,sha256=xA99k_C2QpCP0X-wAOjW2qMEypyQatsYPNPy1pf6-5c,7190
-sonar/custom_measures.py,sha256=s7tcuCovjV_chgPq_KpwxmffKz4Ny7qSK_raspanjOQ,3015
-sonar/devops.py,sha256=sO7XbAQ3qn0li97YjFTSzzsFS1cGsxNhCooTlfmtr6A,11148
-sonar/exceptions.py,sha256=3OOUrPIYlyEu27d1ntNFHYWMflyWoF_hpyUdNIy9n9g,1544
-sonar/groups.py,sha256=bd1pBxpGA4NacGU67kh1N26QQ2ZtliEEwPM6jsk_NdE,12636
-sonar/languages.py,sha256=p0WMm8sNyiu2y6FJHvpj1Roem68nN0bJSiDxaEuC7mc,3856
-sonar/measures.py,sha256=OiVu-iyfn3oIPzMaCZ0LpOtYFIVcPI4sjiTtns8AMXA,9711
-sonar/metrics.py,sha256=T_QTiq3on6Sc-lgHQaVcInOxUjQwKJx1SvzmXiAktp4,6493
-sonar/options.py,sha256=7bAkkMpxrjLha0LnDMshd-Oe3IAGRC3hbgVIxqglXBU,2229
-sonar/platform.py,sha256=cSj4lkJzDoFYc-rJG8f0gBF9ZBlem_HVeCmJCkRdRNA,34203
-sonar/portfolios.py,sha256=sxMuFWzVNNQskZvkOAOoiACCQJq_c6BOstL5Y_or3XM,30565
-sonar/qualitygates.py,sha256=Y8rdzV0pK7qaxlK0TKn4koXpJkmVokSCotXmXCOhIQo,18122
-sonar/qualityprofiles.py,sha256=P_vbYTusDTnXGOJbYDJO8KZB5oFkn9rYUxZuEHGPn3Y,29776
-sonar/rules.py,sha256=pSgKnk51ZiRkMp54L-EQX7lwDnUyOEube4wGoexUo2s,11184
-sonar/settings.py,sha256=nQbTYmbPzatxaImXFwdUpxckTSj0XjbIEB9rgqdXomM,16616
-sonar/sif.py,sha256=z5L9pWkP2SqI5trgZ0SAJzaWZ7xlkJqD8rmKT19p6AU,19366
-sonar/sqobject.py,sha256=2RTYj5RPySXBa1v3TJcFyjZexInLj3kRhzYzJXpol4Y,6123
-sonar/syncer.py,sha256=lyP4E9tjgIKdtuEgWBM8RdDKBj_PW3_aJD0fU1dJyKs,8685
-sonar/tasks.py,sha256=Lj1uPozsE89LTuedHzq57wBB2NOM-UqFg7EWj-3nWe8,21302
-sonar/tokens.py,sha256=7QChqFK9uuxD2AK8Ve1BtxWcN9-EjZgtH9lYrxAx41M,3580
-sonar/users.py,sha256=W86IacCyzOz8e2dRFhZinabgQtUZXawXdiXMVBpyLTQ,17735
-sonar/utilities.py,sha256=4PBmoa2T6jjawKTjuH9uL8N02tXe1_jjh_vpVikrpr0,15966
-sonar/version.py,sha256=dgY2zwapmK7Jb3hnFcvTSQOqgTitE1HXZX6sQ1EwGm0,874
-sonar/webhooks.py,sha256=KIxRpssGRXoHVmPeZlkN7Y8j1DuScMLsu5t2JGsWMNs,5470
-sonar/audit/__init__.py,sha256=dzTDV5r9sHIhIJY4P3dNAp_0-oO3-vjs5JgU7kW_GyM,1061
-sonar/audit/config.py,sha256=YXYEqqRDIbFaf3iFyH953Sn2ch7U8iGyY9cBxQstOjs,2935
-sonar/audit/problem.py,sha256=8E4gjFF9Y07o8lEcGzANby3_lsL4v2S-vsOJCmNWBCo,2594
-sonar/audit/rules.json,sha256=MiE3pY4Kvj8hzvhMcVgSGtOuSJ2dbQukLRtwEZb_w3E,20474
-sonar/audit/rules.py,sha256=VgbEIsNhX3q2ptLLADSQsTB-NJbLtclfyrpUedZXxQk,6250
-sonar/audit/severities.py,sha256=tXG1yjUMT7c9mah75zucVB3xz3iuwig_XzofDHlZYdo,1167
-sonar/audit/sonar-audit.properties,sha256=u81_cZDNzMP1Gscq5vi3FnP0X1hFsAEJ8yzktBXl2_c,13464
-sonar/audit/types.py,sha256=52-cZcqhWZ3NLFxAYGjQnB4wfCIOkr5lHyy_DmVB-3w,1218
-sonar/dce/__init__.py,sha256=YR3dYlPZW4Sn7DfdjCEHetOe7yJSF8BST4vbd9FZ6Ow,847
-sonar/dce/app_nodes.py,sha256=-GYfmETSEdyMVFpx3Pll6cn98VHiM3DJ8a3QZWNeBoI,9962
-sonar/dce/nodes.py,sha256=VOJCVtSq7TzXjRsfsUBSgAV8FkmEFlh-kLMwPf9KvIA,1123
-sonar/dce/search_nodes.py,sha256=kkMA6j4SgbBU8Kg6LUYVemV8zI1PQ4hIobMQqXJ9s5g,4198
-sonar/findings/__init__.py,sha256=YR3dYlPZW4Sn7DfdjCEHetOe7yJSF8BST4vbd9FZ6Ow,847
-sonar/findings/changelog.py,sha256=fiCmHY_iUhPW-JEtD-X6YQZ28lXBKbuqpfOOIz3LxD0,7103
-sonar/findings/findings.py,sha256=DFOk3kikFJbKTz-l1hSMD1weYv75DekF0h-hNLM9_5o,12898
-sonar/findings/hotspots.py,sha256=2Hg_gj1-toKPgDPrva-hdpujnlNPTQRhwjFO86Nb-IQ,16122
-sonar/findings/issues.py,sha256=GrvCzCeehrurujSh79wxdSnR5JxYCKlVtayX94pa0Wo,32150
-sonar/permissions/__init__.py,sha256=YR3dYlPZW4Sn7DfdjCEHetOe7yJSF8BST4vbd9FZ6Ow,847
-sonar/permissions/aggregation_permissions.py,sha256=KUVoDxXkqlbxlwUAaibI_ZS6J-hgV0RoTUuz4__pA88,1872
-sonar/permissions/application_permissions.py,sha256=VS0viThwvLK2VJNaLcIr3Itk_hQ6PAn5vsHfyUmPfkQ,1048
-sonar/permissions/global_permissions.py,sha256=juki4_2UkaArLrVc5xqlOA5MTXWB2DAlRY1QcW3lbVg,3415
-sonar/permissions/permission_templates.py,sha256=midH-fzLN5d-P0vBaZz0mS2ryWrA_d_KhFjsJgRgwro,9823
-sonar/permissions/permissions.py,sha256=M8PLZgIUy0PA7jfx_CUy73XRA-Kh_AghydkyGhwamXo,11688
-sonar/permissions/portfolio_permissions.py,sha256=LP61ozFICnlLgAMjZSxHhbcAlhy3Wm1gVWl3gO_QUZs,1045
-sonar/permissions/project_permissions.py,sha256=hQH1Jp1Z0jmYOR1MeFEEaNQBS8xtVM9YOcGiM-F2XXM,8358
-sonar/permissions/quality_permissions.py,sha256=G7CFZcaiyxQElB39cpHJfavTX2vSC8SrrUKYz-NYCPM,4511
-sonar/permissions/qualitygate_permissions.py,sha256=5cSleRhirAklYoID07XmRriKMPFZscWn7pw4osVpz1Y,2246
-sonar/permissions/qualityprofile_permissions.py,sha256=mbnPwIc0EcRa3xHOPMwBVRFHfUinVmNflyB3vYJi4S8,2407
-sonar/permissions/template_permissions.py,sha256=jncFPWThIfvukuHh-FTSAF1l4YkXSMExYbsmZBTJ0w8,3014
-sonar/projects/__init__.py,sha256=YR3dYlPZW4Sn7DfdjCEHetOe7yJSF8BST4vbd9FZ6Ow,847
-sonar/projects/branches.py,sha256=O_O2HnJVGQq2bNsD4jclJOM6jPaDa_rbrHOjEhZxbN4,17320
-sonar/projects/projects.py,sha256=KrK_huhDtEWa7DNbL4Io0E78qtgB-fhaxeNGYgQ9awQ,57922
-sonar/projects/pull_requests.py,sha256=oipoMfnu3Zh4zczfOmBgAh7oYA9REZXmSUs6RWxcHRA,4600
-sonar_tools-2.8.2.data/scripts/sonar-tools,sha256=bRMnNjGjzbWFi6UZ8bc7n7Oe3ERSQQMl0UtvPSyoft8,2276
-tools/__init__.py,sha256=YR3dYlPZW4Sn7DfdjCEHetOe7yJSF8BST4vbd9FZ6Ow,847
-tools/audit.py,sha256=-nLYFwRlF9E03LQrgNuBECY6WFNTrQz8KvTIfxr960g,6870
-tools/config.py,sha256=X5IsTDG6fhkaSt-55s6y_mvGGCD4s6egBaHamX6C254,8129
-tools/cust_measures.py,sha256=tr6rDyTXOKYijExR2Z7xvB5ysoQzlZjiUjTbxnfrDSI,2604
-tools/findings_export.py,sha256=XxxzMkJ_cKXAcSXisMR-UT_-S55A2FTReMDqqw-shLQ,15114
-tools/findings_sync.py,sha256=OO0OZlFmNIx_bu8B2i4uJMcx9M2b34h1TdMG-8WVXI4,9385
-tools/housekeeper.py,sha256=iXHJ4EReKkVpWJ-6BNTgNmvUkzgxxw1I4mRVXv7v4uU,9142
-tools/loc.py,sha256=hNZ1Ru8dBBLhD2EmvImqGXZ64XpoUuTOknbCawaFCA0,5996
-tools/measures_export.py,sha256=eLBpu1ywj_g6bf5uUaFJJE8kgxcebWZbyVYAoM_pc5s,9057
-tools/projects_export.py,sha256=CZAgs2a0qZpLKObsy2T4e5Kw0F3pgRUBxGzV0uUk7Zk,2575
-tools/projects_import.py,sha256=K83JrdR3MI3ibUDFogO4qcqp8yloQeQTzTnZnnTx7Ss,3855
-tools/support.py,sha256=bDKKJPWQip3Z6bWoSH0mbH0lnbOA2N2MyL3GS_6n8tw,7008
-sonar_tools-2.8.2.dist-info/LICENSE,sha256=fTqV5eBpeAZO0_jit8j4Ref9ikBSlHJ8xwj5TLg7gFk,7817
-sonar_tools-2.8.2.dist-info/METADATA,sha256=3BaN2YwBKJrxIlYIUDJ_JSsfvW1Z848MTHW4jPXkNks,20121
-sonar_tools-2.8.2.dist-info/WHEEL,sha256=oiQVh_5PnQM0E3gPdiz09WCNmwiHDMaGer_elqB3coM,92
-sonar_tools-2.8.2.dist-info/entry_points.txt,sha256=tqqjNP53MeqgNKUEH1-U07ANYBT67ef-sfWBkgvlON0,582
-sonar_tools-2.8.2.dist-info/top_level.txt,sha256=4UurrhJYnkA9Gtw-URkzRpfPjRIOIqSCQFJGrwE8Nvo,12
-sonar_tools-2.8.2.dist-info/RECORD,,
+sonar/__init__.py,sha256=yItDqma8FgLXKkN-Ju9N-kXCA9VXEp3ZmXdGm6Peaus,828
+sonar/aggregations.py,sha256=IlPJO7mS3Wrrr5b4klhX8IE4omzuKDF82CKSRzZR0OY,4052
+sonar/applications.py,sha256=aOcTg02_dsEoFxYznwTJDsgwVFmqTbj60psN7Sw-UCc,20257
+sonar/components.py,sha256=biMZBRaIcCeeaUUUhp1xVAgkakreAAeyXUGXc9AMCv8,7001
+sonar/custom_measures.py,sha256=CCl_Q1WHWNAxsvF8hOh8NvIk5HeNrMga-9xcwmwlKmk,2921
+sonar/devops.py,sha256=ErGZkmnqXroSVhP81NfOMj14460ztBXPEqaMDIJG6no,10862
+sonar/exceptions.py,sha256=Z8mUZ8FONc14j_wg5Y9MFf3AkNiKKqpE9jCqsMnKMpw,1493
+sonar/groups.py,sha256=NkL-RCbJ4yx97tQQ6ISH27NWY871pvQMRsLjAntstYs,12296
+sonar/languages.py,sha256=T2eYC5l3UX1FtMBcDaOYZuB4m-V9T-snlt1I7bh1FX4,3750
+sonar/measures.py,sha256=5WI3yrfLvlStcZtG6oYXE8IY0Ez8ddvThJQ_9gPZcUw,9455
+sonar/metrics.py,sha256=cYg96089Tysywvvdmpd-mG562agRCM5K0cE0eSzCyl8,6281
+sonar/options.py,sha256=xpdtXP58aq8tW3dQuyNGqHxc8Y2yACTtAAhXgAx4CXs,2207
+sonar/platform.py,sha256=vIv9bKSm27Ofh9oD8SbE9GjsQBEH4Lqts5NJKIsE3Fs,33955
+sonar/portfolios.py,sha256=quJ3IKOWOBQ8XuoBOFS5RQ5tFnL7ElwSkmmoUHW6k4w,29924
+sonar/qualitygates.py,sha256=pi0CdqGfckb3UAhcymzdJaDazy0bVFlEEfty5_8XaAc,17660
+sonar/qualityprofiles.py,sha256=RXXYfT2rv7x35XsEp-EFrtlC1PLQZFLy6LUV9EBbFMk,29108
+sonar/rules.py,sha256=pTZ7wNdBUV23ovE9sn2sB-KRQeHupxQXKsntS1wcdVc,11288
+sonar/settings.py,sha256=F-piA13EITyeLbw3M14DKF_9mVWwwgOFESP21F-YgBE,16190
+sonar/sif.py,sha256=5G-hIg5sy1SHXMRZSQN1jCXXjlzBYZkKthoB1EHhcEs,19125
+sonar/sqobject.py,sha256=10VHp3gidC3Han5weHr0jUTajBYfZZpMTEErN2CfClY,6010
+sonar/syncer.py,sha256=OHfuCuQZBBvTeRj28qnhetDdMvAerPL0QV2inAMiyCA,8443
+sonar/tasks.py,sha256=BnX5Z9LRA4D9bFeX9dqxWngwNBRuXRzE7_8AP5mqoeI,21269
+sonar/tokens.py,sha256=xNZJozcFnD2m-57TTFTxafoHMsOsuHUBYXBEOTuswo4,3486
+sonar/users.py,sha256=Yl1yxurTPs_u_5fR7rIvP-p-04lmx5dAOBnrf5qyRPE,17339
+sonar/utilities.py,sha256=yZZZxMscwgWgyrC2Hy6va0yTashKRjCPDOmH7bhS7aQ,16352
+sonar/version.py,sha256=Ur9wleSFHlm0kTa1wvOz4iLRc3obZoHzYPnLuZWKFB0,896
+sonar/webhooks.py,sha256=Sh7I72fYjCtf8-M1bQbh-74Umesq9T9eljH5k3nDpCU,5315
+sonar/audit/__init__.py,sha256=lI74dctpwxuPbsRe8d0SRwHGqHuCzfif4v0P5pikrHc,1034
+sonar/audit/config.py,sha256=Sg8IEomDNR4u1JeT4IojO2nzwVvIJi81wTwVNlIZtrc,2851
+sonar/audit/problem.py,sha256=tAeOgi9fYBeVuLbwZB8OrZDuXo-GV43qdd8dKHXPVlM,2526
+sonar/audit/rules.json,sha256=0SS6e7Ut_2Y1eHzKdqn2rxE3jnGCGZleFuEPN00VCeg,20583
+sonar/audit/rules.py,sha256=lU_u29NE1h8QFA2DSZJrRHMcXPVTdPR2c7LEyjlm0iI,6153
+sonar/audit/severities.py,sha256=zR3UzGTloWH0c-0k4FCHnnvm6bc9Ys7oSM4GFj__EFg,1130
+sonar/audit/sonar-audit.properties,sha256=mviFERQ2tdjWd0dQbO5l2XJW_QV-G4hZ6Cj-0eTEdOw,13229
+sonar/audit/types.py,sha256=St7UUg_nGBc6Hhc_YvlTuWZtOipXWxSGsXNzfwc54mI,1179
+sonar/dce/__init__.py,sha256=ipA9RjscR6Qgt8-mWm-MqYVdQx0YVas0S7msFus8Djo,828
+sonar/dce/app_nodes.py,sha256=z4o3Zf_yMRZBHn4-IiP4EiXbpyABrl0e_4ApFOsIE70,12510
+sonar/dce/nodes.py,sha256=-o6vqrYhawTXrGSDYQAQ0LbyQWconYZkrKcmycP76QY,1086
+sonar/dce/search_nodes.py,sha256=5pWWh1ppJPZ5QxNYmcAM-EKafIN_1eBYyoPiyCuPFrI,4586
+sonar/findings/__init__.py,sha256=ipA9RjscR6Qgt8-mWm-MqYVdQx0YVas0S7msFus8Djo,828
+sonar/findings/changelog.py,sha256=kHuOlm63vWmsj4tmAZ6l15OxxM-3UDyBkDYAY17lB-8,6900
+sonar/findings/findings.py,sha256=6XnVa5FNxRZgN8v4NBRKASpz2JG5Xh9-x9fN7ATONo8,12542
+sonar/findings/hotspots.py,sha256=3PxbA2DFWDMjrRv27sB9U-3KuNFgQTN7edu12pI6Kkk,15687
+sonar/findings/issues.py,sha256=82INKOtDsnYH4avSxsUzAp7O-IiJON6h2ahXkLYEQ88,31311
+sonar/permissions/__init__.py,sha256=ipA9RjscR6Qgt8-mWm-MqYVdQx0YVas0S7msFus8Djo,828
+sonar/permissions/aggregation_permissions.py,sha256=6V_-xzNSsyB1QuIelqkszzkxbWa1YbptZhcr5vQOMZM,1820
+sonar/permissions/application_permissions.py,sha256=jHozT6vVgqhxPwjFdhllQIYzgG89rOtAQYYyaewi6D0,1021
+sonar/permissions/global_permissions.py,sha256=yxrhmVWRzhggt4T9g2eGs2FB1z5NT6ahbQr6oZ9zU3g,3341
+sonar/permissions/permission_templates.py,sha256=mUbCGkBVc7nzsJRJL-d9lVN3709XoDHlQy9fOOoLcc8,9576
+sonar/permissions/permissions.py,sha256=LweMzA4zO9XNbhIEGJrcs8NUtpMiVPUbia5m641dUr8,11397
+sonar/permissions/portfolio_permissions.py,sha256=jpp4iiIjBgBNe-BcQd16BF6uKS04sEUKsgXE_RSszEo,1018
+sonar/permissions/project_permissions.py,sha256=eesjkmuNJeCED1RGz4ku7wgwSwOMA-hnyjsV5oBzidw,8252
+sonar/permissions/quality_permissions.py,sha256=nSViz1ei5MtzMaPMYA6CvrYjcZ-5OdcSHOywlhJSkc0,4406
+sonar/permissions/qualitygate_permissions.py,sha256=4o84cK7RSU8_Pgpg3aOSKnWUgqjKqyhgMQAGxpPeHDo,2196
+sonar/permissions/qualityprofile_permissions.py,sha256=yGl420BTH3V--D0j6nLsid8d4GDBVtcxrlJPS53wlfw,2349
+sonar/permissions/template_permissions.py,sha256=kYH4NcX8YxC6fCOm1N4IEdDFYkSbQBSQHiME2mKMkqY,2955
+sonar/projects/__init__.py,sha256=ipA9RjscR6Qgt8-mWm-MqYVdQx0YVas0S7msFus8Djo,828
+sonar/projects/branches.py,sha256=uNc1rgdqAKnBKxcgivRZP0f4nkzFep3JZCnz_XtL2zo,17542
+sonar/projects/projects.py,sha256=M7S8a8GLgSOWFWT6J5oKGxasYqkAZYKUlpi7iIA8WrY,56961
+sonar/projects/pull_requests.py,sha256=ODlKiRihhow-qIR1ZhnYZvS8UcrNgwPfq-qOZFrEUzI,4484
+sonar_tools-2.9.data/scripts/sonar-tools,sha256=hWGI3S-RldL4jHXYrL9yrv5cgpCscQq5a2lz6sCgFxQ,2223
+tools/__init__.py,sha256=ipA9RjscR6Qgt8-mWm-MqYVdQx0YVas0S7msFus8Djo,828
+tools/audit.py,sha256=wo0MrgQxRGkLo6RRH3S3wnFXhKfTxhCuoC7H2NAaGPQ,6700
+tools/config.py,sha256=zJBw55tW-Y4xK-xoh04ijC2UvfN5IpBUSGKUXJZpAe4,7932
+tools/cust_measures.py,sha256=EvQypeRL3dOckkAHOoVf6aaJgVNXZ4nnm5113wENBSI,2539
+tools/findings_export.py,sha256=kRqLGTAU9hK0uORVS_OAEQGwi1Yf3HzO6EKZidKEFlk,14736
+tools/findings_sync.py,sha256=VKAJ0RtcK5u_J5gabqbwJeJYJT73IepiOp0EOsKesoY,9195
+tools/housekeeper.py,sha256=Kd_rqonBjBOofUxXVYuJLYa-lPlBYDO7F3-n3EnewJU,8908
+tools/loc.py,sha256=TQYgYE6KkJs1G-LpGRje7PJSE3QRDEkygM5a1YvtAGo,5818
+tools/measures_export.py,sha256=iSiLrhrMXeR_vp-tOsSRdEMyhDvEiLdawK9QkYDk9Ds,8803
+tools/projects_export.py,sha256=XmCOlmrxF8NiGe7DOTNjWY7X3wWaZPtg7adGmIrP3SY,2508
+tools/projects_import.py,sha256=30SxetQogq3XloiKADTyQKdyx-Gx22-mfP6VLhpOlnE,3750
+tools/support.py,sha256=Z6yrdb8vq4RG8Az5OkJPC9R0TM9xCygSXVoRrAboNIQ,6829
+sonar_tools-2.9.dist-info/LICENSE,sha256=46mU2C5kSwOnkqkw9XQAJlhBL2JAf1_uCD8lVcXyMRg,7652
+sonar_tools-2.9.dist-info/METADATA,sha256=TVLrp265o1p8MhhpgAuMwPDKq8nbu_PzAmtqxBkYD5Y,20176
+sonar_tools-2.9.dist-info/WHEEL,sha256=oiQVh_5PnQM0E3gPdiz09WCNmwiHDMaGer_elqB3coM,92
+sonar_tools-2.9.dist-info/entry_points.txt,sha256=tqqjNP53MeqgNKUEH1-U07ANYBT67ef-sfWBkgvlON0,582
+sonar_tools-2.9.dist-info/top_level.txt,sha256=4UurrhJYnkA9Gtw-URkzRpfPjRIOIqSCQFJGrwE8Nvo,12
+sonar_tools-2.9.dist-info/RECORD,,
```

