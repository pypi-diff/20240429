# Comparing `tmp/wrap_torch2jax-0.4.10-py3-none-any.whl.zip` & `tmp/wrap_torch2jax-0.4.7-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,22 +1,22 @@
-Zip file size: 23530 bytes, number of entries: 20
--rw-rw-r--  2.0 unx      442 b- defN 24-Apr-29 04:44 wrap_torch2jax/__init__.py
+Zip file size: 22838 bytes, number of entries: 20
+-rw-rw-r--  2.0 unx      286 b- defN 23-Jul-17 18:33 wrap_torch2jax/__init__.py
 -rw-rw-r--  2.0 unx    10059 b- defN 23-Jul-19 00:37 wrap_torch2jax/api.py
 -rw-rw-r--  2.0 unx       57 b- defN 23-Jul-10 16:33 wrap_torch2jax/compat.py
--rw-rw-r--  2.0 unx     3366 b- defN 24-Apr-29 04:44 wrap_torch2jax/compile.py
--rw-rw-r--  2.0 unx     2480 b- defN 24-Apr-29 05:00 wrap_torch2jax/dlpack_passing.py
--rw-rw-r--  2.0 unx     9732 b- defN 24-Apr-29 04:44 wrap_torch2jax/gradients.py
+-rw-rw-r--  2.0 unx     2852 b- defN 23-Oct-06 13:55 wrap_torch2jax/compile.py
+-rw-rw-r--  2.0 unx     2230 b- defN 23-Jul-03 22:49 wrap_torch2jax/dlpack_passing.py
+-rw-rw-r--  2.0 unx     9766 b- defN 23-Jul-20 22:35 wrap_torch2jax/gradients.py
 -rw-rw-r--  2.0 unx     3152 b- defN 23-Oct-06 13:59 wrap_torch2jax/lowering_rule.py
 -rw-rw-r--  2.0 unx     4319 b- defN 23-Jul-17 18:31 wrap_torch2jax/utils.py
 -rw-rw-r--  2.0 unx     1597 b- defN 23-Jul-22 22:24 wrap_torch2jax/cpp/cpu_impl.cpp
--rw-rw-r--  2.0 unx      230 b- defN 24-Apr-29 04:44 wrap_torch2jax/cpp/cpu_impl.h
--rw-rw-r--  2.0 unx     1466 b- defN 24-Apr-29 04:44 wrap_torch2jax/cpp/gpu_impl.cu
--rw-rw-r--  2.0 unx      342 b- defN 24-Apr-29 04:44 wrap_torch2jax/cpp/gpu_impl.h
+-rw-rw-r--  2.0 unx      230 b- defN 23-Jul-07 23:27 wrap_torch2jax/cpp/cpu_impl.h
+-rw-rw-r--  2.0 unx      988 b- defN 23-Jul-22 21:47 wrap_torch2jax/cpp/gpu_impl.cu
+-rw-rw-r--  2.0 unx      316 b- defN 23-Jul-07 23:27 wrap_torch2jax/cpp/gpu_impl.h
 -rw-rw-r--  2.0 unx      826 b- defN 23-Jul-07 23:27 wrap_torch2jax/cpp/main.cpp
 -rw-rw-r--  2.0 unx      896 b- defN 23-Jul-07 23:27 wrap_torch2jax/cpp/main.cu
--rw-rw-r--  2.0 unx     5219 b- defN 24-Apr-29 04:44 wrap_torch2jax/cpp/main.h
--rw-rw-r--  2.0 unx     8097 b- defN 24-Apr-29 04:44 wrap_torch2jax/cpp/utils.cpp
--rw-rw-r--  2.0 unx    12040 b- defN 24-Apr-29 05:08 wrap_torch2jax-0.4.10.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 24-Apr-29 05:08 wrap_torch2jax-0.4.10.dist-info/WHEEL
--rw-rw-r--  2.0 unx       15 b- defN 24-Apr-29 05:08 wrap_torch2jax-0.4.10.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     1671 b- defN 24-Apr-29 05:08 wrap_torch2jax-0.4.10.dist-info/RECORD
-20 files, 66098 bytes uncompressed, 20800 bytes compressed:  68.5%
+-rw-rw-r--  2.0 unx     5132 b- defN 23-Jul-22 22:31 wrap_torch2jax/cpp/main.h
+-rw-rw-r--  2.0 unx     7857 b- defN 23-Jul-22 22:40 wrap_torch2jax/cpp/utils.cpp
+-rw-rw-r--  2.0 unx    11553 b- defN 23-Nov-13 00:56 wrap_torch2jax-0.4.7.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Nov-13 00:56 wrap_torch2jax-0.4.7.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       15 b- defN 23-Nov-13 00:56 wrap_torch2jax-0.4.7.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     1666 b- defN 23-Nov-13 00:56 wrap_torch2jax-0.4.7.dist-info/RECORD
+20 files, 63889 bytes uncompressed, 20116 bytes compressed:  68.5%
```

## zipnote {}

```diff
@@ -42,20 +42,20 @@
 
 Filename: wrap_torch2jax/cpp/main.h
 Comment: 
 
 Filename: wrap_torch2jax/cpp/utils.cpp
 Comment: 
 
-Filename: wrap_torch2jax-0.4.10.dist-info/METADATA
+Filename: wrap_torch2jax-0.4.7.dist-info/METADATA
 Comment: 
 
-Filename: wrap_torch2jax-0.4.10.dist-info/WHEEL
+Filename: wrap_torch2jax-0.4.7.dist-info/WHEEL
 Comment: 
 
-Filename: wrap_torch2jax-0.4.10.dist-info/top_level.txt
+Filename: wrap_torch2jax-0.4.7.dist-info/top_level.txt
 Comment: 
 
-Filename: wrap_torch2jax-0.4.10.dist-info/RECORD
+Filename: wrap_torch2jax-0.4.7.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## wrap_torch2jax/__init__.py

```diff
@@ -1,14 +1,8 @@
 from .api import torch2jax, dtype_t2j
-from .compile import compile_and_import_module  # noqa: F401
-from .dlpack_passing import j2t, t2j, tree_j2t, tree_t2j  # noqa: F401
-from .gradients import torch2jax_with_vjp  # noqa: F401
+from .compile import compile_and_import_module # noqa: F401
+from .dlpack_passing import j2t, t2j, tree_j2t, tree_t2j # noqa: F401
+from .gradients import torch2jax_with_vjp # noqa: F401
 
-from torch import Size  # noqa: F401
-
-try:
-    from importlib.metadata import version
-except ModuleNotFoundError:
-    from importlib_metadata import version
-__version__ = version(__name__)
+from torch import Size # noqa: F401
 
 wrap_torch_fn = torch2jax
```

## wrap_torch2jax/compile.py

```diff
@@ -6,75 +6,63 @@
 from types import ModuleType
 from importlib import import_module
 
 import torch
 from torch.utils import cpp_extension
 from jax.lib import xla_client
 
-try:
-    from importlib.metadata import version
-except ModuleNotFoundError:
-    from importlib_metadata import version
-__version__ = version(__name__.split(".", 1)[0])
-
 CPP_MODULE_CACHED = None
 
 
 def _generate_extension_version() -> str:
     py_impl = sys.implementation.name
     py_version = "".join(map(str, sys.version_info[:2]))
     py_abi_tag = sys.abiflags
     py_name_version = f"{py_impl}-{py_version}{py_abi_tag}"
     system_info = f"{platform.system().lower()}-{platform.machine()}"
-    return f"{py_name_version}-{system_info}-torch2jax-{__version__}"
+    return f"{py_name_version}-{system_info}"
 
 
 def compile_extension(force_recompile: bool = False) -> ModuleType:
     global CPP_MODULE_CACHED
     if not force_recompile and CPP_MODULE_CACHED is not None:
         return CPP_MODULE_CACHED
 
     mod_version = _generate_extension_version()
     build_dir = Path(f"~/.cache/torch2jax/{mod_version}").expanduser().absolute()
     if force_recompile and build_dir.exists():
         if build_dir.is_dir():
-            print(f"Removing the existing build directory at {build_dir}")
             rmtree(build_dir)
-            if build_dir.exists():
-                os.removedirs(build_dir)
         else:
             os.remove(build_dir)
     build_dir.mkdir(exist_ok=True, parents=True)
 
     if str(build_dir) not in sys.path:
         sys.path.insert(0, str(build_dir))
     try:
-        assert not force_recompile
         mod = import_module("torch2jax_cpp")
         # import torch2jax_cpp as mod  # noqa: F811
-    except (ImportError, AssertionError):
+    except ImportError:
         print("Cache empty, we will compile the C++ extension component now...")
         source_prefix = Path(__file__).parent.absolute() / "cpp"
         source_list = ["main.cpp", "cpu_impl.cpp", "utils.cpp"]
         extra_cflags = ["-O3"]
         extra_cuda_cflags = None
 
         if torch.cuda.is_available():
-            source_list.remove("main.cpp")
             source_list.extend(["main.cu", "gpu_impl.cu"])
             extra_cflags.append("-DTORCH2JAX_WITH_CUDA")
             extra_cuda_cflags = ["-DTORCH2JAX_WITH_CUDA", "-O3"]
         mod = cpp_extension.load(
             "torch2jax_cpp",
             sources=[source_prefix / fname for fname in source_list],
             build_directory=build_dir,
-            verbose=True,
+            verbose=False,
             extra_cflags=extra_cflags,
             extra_cuda_cflags=extra_cuda_cflags,
-            extra_ldflags=["-lcuda" if torch.cuda.is_available() else ""],
         )
     for _name, _value in mod.cpu_registrations().items():
         xla_client.register_custom_call_target(_name, _value, platform="cpu")
     if hasattr(mod, "gpu_registrations"):
         for _name, _value in mod.gpu_registrations().items():
             xla_client.register_custom_call_target(_name, _value, platform="gpu")
     CPP_MODULE_CACHED = mod
```

## wrap_torch2jax/dlpack_passing.py

```diff
@@ -35,20 +35,15 @@
                     device.type if isinstance(device, torch.device) else device
                 )[0]
             return jax.device_put(jax.numpy.array(x.detach().cpu().numpy()), device=jax_device)
 
 
 def j2t(x: Array, via: str = "dlpack") -> Tensor:
     try:
-        devices = x.devices()
-        if len(devices) > 1:
-            msg = "You are attempting to convert a JAX array with multiple devices to a PyTorch tensor."
-            msg += " This is not supported"
-            raise RuntimeError(msg)
-        device = list(devices)[0]
+        device = x.device()
     except ConcretizationTypeError:
         msg = "You are attempting to convert a non-concrete JAX array to a PyTorch tensor."
         msg += " This is not supported, since that JAX array does not contain any numbers."
         raise RuntimeError(msg)
     return transfer(x, via=via, device=device)
```

## wrap_torch2jax/gradients.py

```diff
@@ -84,15 +84,16 @@
         >>> dW.shape, db.shape
         ((20, 5), (5,))
     """
 
     use_torch_vmap = use_torch_vjp if use_torch_vmap is None else use_torch_vmap
 
     if output_shapes is None:
-        outputs = torch_fn(*example_args)
+        with torch.no_grad():
+            outputs = torch_fn(*example_args)
         output_shapes = tree_map(
             lambda x: ShapeDtypeStruct(dtype=dtype_t2j(x.dtype), shape=x.shape), outputs
         )
     fn = torch2jax(
         torch_fn, *example_args, output_shapes=output_shapes, use_torch_vmap=use_torch_vmap
     )
```

## wrap_torch2jax/cpp/cpu_impl.h

```diff
@@ -1,15 +1,14 @@
 #ifndef _CPU_IMPL_H
 #define _CPU_IMPL_H
 
 #include "main.h"
 
-
 using namespace std;
 namespace py = pybind11;
 
 //template <typename T>
 void cpu_apply_torch_call(void *out_tuple, const void **in);
 
 py::dict CPURegistrations();
 
-#endif
+#endif
```

## wrap_torch2jax/cpp/gpu_impl.cu

```diff
@@ -20,21 +20,8 @@
 
   DescriptorDataAccessor da(reinterpret_cast<const int64_t*>(opaque), nullptr);
   DynamicTorchCallDescriptor d;
   deserialize_descriptor(d, da);
 
   // apply_torch_call<T>(buffers, d);
   apply_torch_call(buffers, d);
-}
-
-TorchCallDevice actual_cuda_device(const TorchCallDevice& device_desc, void* buffer) {
-#ifdef TORCH2JAX_WITH_CUDA
-    CUdevice device_ordinal;
-    CUresult err = cuPointerGetAttribute((void*)&device_ordinal, CU_POINTER_ATTRIBUTE_DEVICE_ORDINAL, (CUdeviceptr)buffer);
-    if (err != CUDA_SUCCESS) return device_desc;
-    TorchCallDevice new_device_desc = device_desc;
-    new_device_desc.index = device_ordinal;
-    return new_device_desc;
-#else
-  return device_desc;
-#endif
-}
+}
```

## wrap_torch2jax/cpp/gpu_impl.h

```diff
@@ -1,12 +1,11 @@
 #ifndef _GPU_IMPL_H
 #define _GPU_IMPL_H
 
 #include <cuda.h>
-#include <cuda_runtime.h>
 
 #include "main.h"
 
 using namespace std;
 namespace py = pybind11;
 
 // template <typename T>
```

## wrap_torch2jax/cpp/main.h

```diff
@@ -138,16 +138,14 @@
                                   const int64_t dtype);
 torch::TensorOptions tensor_device(torch::TensorOptions opts,
                                    const TorchCallDevice device);
 
 torch::TensorOptions tensor_options(int64_t dtype,
                                     const TorchCallDevice device);
 
-TorchCallDevice actual_cuda_device(const TorchCallDevice& device_desc, void* buffer);
-
 ////////////////////////////////////////////////////////////////////////////////
 
 /// @brief The main torch call routine, wraps JAX arrays as Torch tensors and
 /// calls the torch fn
 /// @tparam T
 /// @param buffers Array of pointers to input and then output buffers
 /// @param d The Torch call descriptor, contains input & output shapes and
```

## wrap_torch2jax/cpp/utils.cpp

```diff
@@ -127,15 +127,14 @@
     for (int64_t j = 0; j < d.shapes_out[i].ndim; j++)
       d.shapes_out[i].shape.push_back(data.get(k++));
     d.shapes_out[i].dtype = data.get(k++);
   }
   return k;
 }
 
-
 /// @brief The main torch call routine, wraps JAX arrays as Torch tensors and
 /// calls the torch fn
 /// @tparam T
 /// @param buffers Array of pointers to input and then output buffers
 /// @param d The Torch call descriptor, contains input & output shapes and
 /// device and call id
 // template <typename T>
@@ -161,18 +160,15 @@
 #endif
 
   // 1. wrap the input buffers as Torch tensors
   for (int64_t i = 0; i < d.nargin; i++) {
     auto size = torch::IntArrayRef((int64_t *)d.shapes_in[i].shape.data(),
                                    (size_t)d.shapes_in[i].ndim);
     // T *buf = reinterpret_cast<T *>(buffers[i]);
-
-    TorchCallDevice device_desc = actual_cuda_device(d.device, buffers[i]);
-    //auto options = tensor_options(d.shapes_in[i].dtype, d.device);
-    auto options = tensor_options(d.shapes_in[i].dtype, device_desc);
+    auto options = tensor_options(d.shapes_in[i].dtype, d.device);
     // torch::Tensor tharray = torch::from_blob(buf, size, options);
     torch::Tensor tharray = torch::from_blob(buffers[i], size, options);
     my_list.append(THPVariable_Wrap(tharray));
   }
 
   // 2. bind the input tensors to the Python module in an identifiable place
   auto mod = py::module_::import("torch");
@@ -186,16 +182,15 @@
 
   // 4. unwrap the output tensors and copy them to the output buffers
   assert(results.size() == d.nargout);
   for (int64_t i = 0; i < d.nargout; i++) {
     auto size = torch::IntArrayRef((int64_t *)d.shapes_out[i].shape.data(),
                                    (size_t)d.shapes_out[i].ndim);
     // T *buf = reinterpret_cast<T *>(buffers[nargin + i]);
-    TorchCallDevice device_desc = actual_cuda_device(d.device, buffers[d.nargin + i]);
-    auto options = tensor_options(d.shapes_out[i].dtype, device_desc);
+    auto options = tensor_options(d.shapes_out[i].dtype, d.device);
     // torch::Tensor tharray = torch::from_blob(buf, size, options);
     torch::Tensor tharray =
         torch::from_blob(buffers[d.nargin + i], size, options);
     PyObject *out = results[i].ptr();
     THPVariable_Check(out);
     tharray.copy_(THPVariable_Unpack(out));
   }
```

## Comparing `wrap_torch2jax-0.4.10.dist-info/METADATA` & `wrap_torch2jax-0.4.7.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
-Name: wrap_torch2jax
-Version: 0.4.10
+Name: wrap-torch2jax
+Version: 0.4.7
 Summary: Wrap your PyTorch for JAX! This package allows no-copy PyTorch calling from JAX under both eager execution and JIT.
 Author-email: Robert Dyro <robert.dyro@gmail.com>
 Project-URL: Homepage, https://github.com/rdyro/torch2jax
 Project-URL: Bug Tracker, https://github.com/rdyro/torch2jax
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: MIT License
 Classifier: Operating System :: OS Independent
@@ -56,23 +56,14 @@
 
 # Install
 
 ```bash
 $ pip install git+https://github.com/rdyro/torch2jax.git
 ```
 
-`torch2jax` is now available on PyPI under the alias `wrap_torch2jax`:
-
-```bash
-$ pip install wrap-torch2jax
-$ # then
-$ python3
-$ >>> from wrap_torch2jax import torch2jax, torch2jax_with_vjp
-```
-
 # Usage
 
 With a single output
 
 ```python
 import torch
 import jax
@@ -225,21 +216,14 @@
   `jax.ShapeDtypeStruct`
 - the current implementation does not support batching, that's on the roadmap
 - the current implementation does not define the VJP rule, in current design, this has to be done in 
   Python
 
 # Changelog
 
-- version 0.4.10
-  - support for multiple GPUs, currently, all arguments must and the output
-    must be on the same GPU (but you can call the wrapped function with
-    different GPUs in separate calls)
-  - fixed the coming depreciation in JAX deprecating `.device()` for
-    `.devices()`
-
 - no version change
   - added helper script `install_package_aliased.py` to automatically install
     the package with a different name (to avoid a name conflict)
 
 - version 0.4.7
   - support for newest JAX (0.4.17) with backwards compatibility maintained
   - compilation now delegated to python version subfolders for multi-python systems
@@ -307,15 +291,15 @@
 - [x] (user-friendly) support arbitrary argument input and output structure (use pytrees on the 
       Python side)
 - [x] (feature) support batching (e.g., support for `jax.vmap`)
 - [x] (feature) support integer input/output types
 - [x] (feature) support mixed-precision arguments in inputs/outputs
 - [x] (feature) support defining VJP for the wrapped function (import the experimental functionality 
       from [jit-JAXFriendlyInterface](https://github.com/rdyro/jfi-JAXFriendlyInterface))
-- [x] (tests) test how well device mapping works on multiple GPUs
+- [ ] (tests) test how well device mapping works on multiple GPUs
 - [ ] (tests) setup automatic tests for multiple versions of Python, PyTorch and JAX
 - [ ] (feature) look into supporting in-place functions (support for output without copy)
 - [ ] (feature) support TPU
 
 # Related Work
 
 Our Python package wraps PyTorch code as-is (so custom code and mutating code
```

## Comparing `wrap_torch2jax-0.4.10.dist-info/RECORD` & `wrap_torch2jax-0.4.7.dist-info/RECORD`

 * *Files 15% similar despite different names*

```diff
@@ -1,20 +1,20 @@
-wrap_torch2jax/__init__.py,sha256=6a5iR27O5c7xwkGMwCcAsmYLkxex4l7HDQuzprot9hI,442
+wrap_torch2jax/__init__.py,sha256=N0yvISRvwBgzuR8EKRUuvfPTxzYrt24KjMxOKuYJRPg,286
 wrap_torch2jax/api.py,sha256=-4r453jtnmmJkmo-m_uKwSRf_nLhO-cO4XkJGQ1Z6tg,10059
 wrap_torch2jax/compat.py,sha256=hi2gwkMxhO6XbXQyXaXq6gHVI00nf6gwHchbijMi2YE,57
-wrap_torch2jax/compile.py,sha256=JY0yamCd4Eo43ExnMhp6_2wo3rYNFvjxiZnwqE47kog,3366
-wrap_torch2jax/dlpack_passing.py,sha256=Iz4YISccKAyAx-LBfudKz2bfpnzlAYtyBbxKgxRS_Wg,2480
-wrap_torch2jax/gradients.py,sha256=KyknU6vnHAqfz_JMTg6abGZgETFymNWkEvDZtGhRDhs,9732
+wrap_torch2jax/compile.py,sha256=4RJMqpH2_gyxo0lVC1MxW1nvMXjJ7pCvTCf-IM_Kbno,2852
+wrap_torch2jax/dlpack_passing.py,sha256=-p2o2XgDbm-yOCH0lOcwFTbK1Ok-M3jEW_fvkU2M838,2230
+wrap_torch2jax/gradients.py,sha256=ti9r8CnOECIEQbunoSSZw6f8XEWe3KoPJ9X3zEy6wzI,9766
 wrap_torch2jax/lowering_rule.py,sha256=AKZxpjOLZyjACU8HcTeOPj6H2moZXVI3X7b66_iVEcI,3152
 wrap_torch2jax/utils.py,sha256=x1hdGIptvvBBZk_QqZhnIM0vBUrPT4ikTLJUjxP0LKg,4319
 wrap_torch2jax/cpp/cpu_impl.cpp,sha256=_TT5pZhHtnixPgAMorXgUnN3qz1ku4nJfOJl2fdBKr0,1597
-wrap_torch2jax/cpp/cpu_impl.h,sha256=8Q3Adn3Y9Y2sf6pLEvTyR22_DNBnDsOMdf40R78H4OE,230
-wrap_torch2jax/cpp/gpu_impl.cu,sha256=6Jh3M1HbIcNVVgjzcWmg9bA0r_G4Gf0_Tf0-nnK-lxc,1466
-wrap_torch2jax/cpp/gpu_impl.h,sha256=rfylghbUh-H9CPqcThPp7vBPK5YbDGj8xyTNRRdc3eg,342
+wrap_torch2jax/cpp/cpu_impl.h,sha256=T4MqmMk3QxcHCHB_rs6aKfoPApQnYbo6EN7TECdgk68,230
+wrap_torch2jax/cpp/gpu_impl.cu,sha256=_Batg0_SkV9ngrEeelme9odLKObSxKFTbqQJUetYhKc,988
+wrap_torch2jax/cpp/gpu_impl.h,sha256=TPk3ne835stVD0yf1bFRINiNryNfduOTIg9Jom-IqAI,316
 wrap_torch2jax/cpp/main.cpp,sha256=lcxthPm_I0zbN3kbGGgIDFTUbXK3M6F6sC0_RJ5Bzok,826
 wrap_torch2jax/cpp/main.cu,sha256=6Ar8-cVs5OwQjwyDfxghWke7XvK-71spTBc7Ih-GB04,896
-wrap_torch2jax/cpp/main.h,sha256=Ze3TeHweE7BBwyh4mCTLgupdCFeavcsCcbtYpjAMmZQ,5219
-wrap_torch2jax/cpp/utils.cpp,sha256=_3GlDmbW8neQCLRjmEyqeuo8ggK6Qoqdc7f2-BAty1s,8097
-wrap_torch2jax-0.4.10.dist-info/METADATA,sha256=9vdDgnM0HNHd5HLv2CPhHhmdiemiajuNAnjVVIoP1u4,12040
-wrap_torch2jax-0.4.10.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-wrap_torch2jax-0.4.10.dist-info/top_level.txt,sha256=4Le98i_tR89N39qK8p4W0Vud-ZZZetVsC6oubw8AwLQ,15
-wrap_torch2jax-0.4.10.dist-info/RECORD,,
+wrap_torch2jax/cpp/main.h,sha256=xZplBVm5JGvSf_XbxfQWr9FsmkrW96xAmph1JtDs2ys,5132
+wrap_torch2jax/cpp/utils.cpp,sha256=vmSrB6Sy6KtD0HHR0BkSDf4zR10mwo_QJyKH-COLVdo,7857
+wrap_torch2jax-0.4.7.dist-info/METADATA,sha256=FYsZGu9iULoiZHm4qhBoefNvX8O68C6a1tFFCwlL5vo,11553
+wrap_torch2jax-0.4.7.dist-info/WHEEL,sha256=Xo9-1PvkuimrydujYJAjF7pCkriuXBpUPEjma1nZyJ0,92
+wrap_torch2jax-0.4.7.dist-info/top_level.txt,sha256=4Le98i_tR89N39qK8p4W0Vud-ZZZetVsC6oubw8AwLQ,15
+wrap_torch2jax-0.4.7.dist-info/RECORD,,
```

